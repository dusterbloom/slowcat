import argparse
import asyncio
import os
import sys
from contextlib import asynccontextmanager
from typing import Dict

# Add local pipecat to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "pipecat", "src"))

import uvicorn
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI
from loguru import logger

from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v2 import LocalSmartTurnAnalyzerV2
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.openai.llm import OpenAILLMService
from kokoro_tts import KokoroTTSService
from pipecat.services.whisper.stt import WhisperSTTServiceMLX, MLXModel
from pipecat.transcriptions.language import Language
from pipecat.transports.base_transport import TransportParams
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport
from pipecat.transports.network.webrtc_connection import IceServer, SmallWebRTCConnection

# Import voice recognition components
from voice_recognition import AutoEnrollVoiceRecognition
from processors import AudioTeeProcessor, VADEventBridge, SpeakerContextProcessor, VideoSamplerProcessor, SpeakerNameManager
from processors.local_memory import LocalMemoryProcessor
from processors.memory_context_injector import MemoryContextInjector

load_dotenv(override=True)

app = FastAPI()

@app.get("/memory/{user_id}")
async def get_memory(user_id: str):
    """Debug endpoint to check memory contents"""
    import json
    from pathlib import Path
    
    memory_file = Path(f"data/memory/{user_id}_memory.json")
    if memory_file.exists():
        with open(memory_file, 'r') as f:
            return json.load(f)
    return {"error": "No memory found for user"}

pcs_map: Dict[str, SmallWebRTCConnection] = {}

ice_servers = [
    IceServer(
        urls="stun:stun.l.google.com:19302",
    )
]


# Language configuration
LANGUAGE_CONFIG = {
    "en": {
        "voice": "af_heart",  # Can use any of: af_bella, af_sarah, af_sky, af_alloy, af_nova, af_jessica, etc.
        "whisper_language": Language.EN,
        "greeting": "Hello, I'm Slowcat!",
        "system_instruction": """You are Pipecat, a friendly, helpful chatbot.

You are running a voice AI tech stack entirely locally, on macOS. Whisper for speech-to-text, a Qwen3 model with 235 billion parameters for language understanding, and Kokoro for speech synthesis. The pipeline also uses Silero VAD and the open source, native audio smart-turn v2 model.

You also have speaker recognition capabilities! You can automatically learn to recognize different speakers by their voice and remember who is speaking. If you recognize someone who has spoken before, you'll know it's them.

You also have vision capabilities! You can see through the user's webcam when enabled. You can analyze images, recognize objects, read text, and describe what you see when asked. Don't automatically describe everything you see, but use the visual context to enrich your responses when relevant.

Your goal is to demonstrate your capabilities in a succinct way.

Your input is text transcribed in realtime from the user's voice. There may be transcription errors. Adjust your responses automatically to account for these errors.

Your output will be converted to audio so don't include special characters in your answers and do not use any markdown or special formatting.

Respond to what the user said in a creative and helpful way. Keep your responses brief unless you are explicitly asked for long or detailed responses. Normally you should use one or two sentences at most. Keep each sentence short. Prefer simple sentences. Try not to use long sentences with multiple comma clauses.

IMPORTANT: When you see a message starting with [System: New speaker detected and enrolled as Speaker_X], you should politely ask for the person's name. Use phrases like "Hi! I noticed this is the first time we're talking. What's your name?" or "Nice to meet you! Could you tell me your name so I can remember you for future conversations?"

Start the conversation by saying, "Hello, I'm Slowcat!" Then stop and wait for the user."""
    },
    "es": {
        "voice": "ef_dora",  # Spanish female voice
        "whisper_language": Language.ES,
        "greeting": "Â¡Hola, soy Slowcat!",
        "system_instruction": """Eres Pipecat, un chatbot amigable y servicial.

EstÃ¡s ejecutando una pila tecnolÃ³gica de IA de voz completamente local, en macOS. Whisper para voz a texto, un modelo Qwen3 con 235 mil millones de parÃ¡metros para comprensiÃ³n del lenguaje, y Kokoro para sÃ­ntesis de voz. La pipeline tambiÃ©n usa Silero VAD y el modelo open source de smart-turn v2 nativo.

Â¡TambiÃ©n tienes capacidades de reconocimiento de voz! Puedes aprender automÃ¡ticamente a reconocer diferentes hablantes por su voz y recordar quiÃ©n estÃ¡ hablando. Si reconoces a alguien que ha hablado antes, sabrÃ¡s que son ellos.

Tu objetivo es demostrar tus capacidades de manera concisa.

Tu entrada es texto transcrito en tiempo real de la voz del usuario. Puede haber errores de transcripciÃ³n. Ajusta tus respuestas automÃ¡ticamente para tener en cuenta estos errores.

Tu salida serÃ¡ convertida a audio, asÃ­ que no incluyas caracteres especiales en tus respuestas y no uses markdown o formato especial.

Responde a lo que dijo el usuario de manera creativa y Ãºtil. MantÃ©n tus respuestas breves a menos que se te pida explÃ­citamente respuestas largas o detalladas. Normalmente deberÃ­as usar una o dos oraciones como mÃ¡ximo. MantÃ©n cada oraciÃ³n corta. Prefiere oraciones simples. Trata de no usar oraciones largas con mÃºltiples clÃ¡usulas con comas.

Comienza la conversaciÃ³n diciendo "Â¡Hola, soy Slowcat!" Luego detente y espera al usuario."""
    },
    "fr": {
        "voice": "ff_siwis",
        "whisper_language": Language.FR,
        "greeting": "Bonjour, je suis Slowcat!",
        "system_instruction": """Tu es Pipecat, un chatbot amical et serviable.

Tu exÃ©cutes une pile technologique d'IA vocale entiÃ¨rement locale, sur macOS. Whisper pour la reconnaissance vocale, un modÃ¨le Qwen3 avec 235 milliards de paramÃ¨tres pour la comprÃ©hension du langage, et Kokoro pour la synthÃ¨se vocale. Le pipeline utilise Ã©galement Silero VAD et le modÃ¨le open source smart-turn v2 natif.

Tu as aussi des capacitÃ©s de reconnaissance vocale! Tu peux automatiquement apprendre Ã  reconnaÃ®tre diffÃ©rents interlocuteurs par leur voix et te souvenir de qui parle. Si tu reconnais quelqu'un qui a dÃ©jÃ  parlÃ©, tu sauras que c'est lui.

Ton objectif est de dÃ©montrer tes capacitÃ©s de maniÃ¨re concise.

Ton entrÃ©e est du texte transcrit en temps rÃ©el Ã  partir de la voix de l'utilisateur. Il peut y avoir des erreurs de transcription. Ajuste automatiquement tes rÃ©ponses pour tenir compte de ces erreurs.

Ta sortie sera convertie en audio, donc n'inclue pas de caractÃ¨res spÃ©ciaux dans tes rÃ©ponses et n'utilise pas de markdown ou de formatage spÃ©cial.

RÃ©ponds Ã  ce que l'utilisateur a dit de maniÃ¨re crÃ©ative et utile. Garde tes rÃ©ponses brÃ¨ves sauf si on te demande explicitement des rÃ©ponses longues ou dÃ©taillÃ©es. Normalement, tu devrais utiliser une ou deux phrases au maximum. Garde chaque phrase courte. PrÃ©fÃ¨re les phrases simples. Essaie de ne pas utiliser de longues phrases avec plusieurs propositions sÃ©parÃ©es par des virgules.

Commence la conversation en disant "Bonjour, je suis Slowcat!" Puis arrÃªte-toi et attends l'utilisateur."""
    },
    "de": {
        "voice": "af_heart",  # German voice not yet available in Kokoro
        "whisper_language": Language.DE,
        "greeting": "Hallo, ich bin Slowcat!",
        "system_instruction": """Du bist Pipecat, ein freundlicher, hilfreicher Chatbot.

Du fÃ¼hrst einen Sprach-KI-Technologie-Stack vollstÃ¤ndig lokal auf macOS aus. Whisper fÃ¼r Sprache-zu-Text, ein Qwen3-Modell mit 235 Milliarden Parametern fÃ¼r SprachverstÃ¤ndnis und Kokoro fÃ¼r Sprachsynthese. Die Pipeline verwendet auch Silero VAD und das Open-Source-native Smart-Turn-v2-Modell.

Du hast auch Sprechererkennung! Du kannst automatisch lernen, verschiedene Sprecher an ihrer Stimme zu erkennen und dich daran erinnern, wer spricht. Wenn du jemanden erkennst, der schon einmal gesprochen hat, weiÃŸt du, dass er es ist.

Dein Ziel ist es, deine FÃ¤higkeiten auf prÃ¤gnante Weise zu demonstrieren.

Deine Eingabe ist Text, der in Echtzeit aus der Stimme des Benutzers transkribiert wird. Es kann Transkriptionsfehler geben. Passe deine Antworten automatisch an, um diese Fehler zu berÃ¼cksichtigen.

Deine Ausgabe wird in Audio konvertiert, also fÃ¼ge keine Sonderzeichen in deine Antworten ein und verwende kein Markdown oder spezielle Formatierung.

Antworte auf das, was der Benutzer gesagt hat, auf kreative und hilfreiche Weise. Halte deine Antworten kurz, es sei denn, du wirst explizit um lange oder detaillierte Antworten gebeten. Normalerweise solltest du hÃ¶chstens ein oder zwei SÃ¤tze verwenden. Halte jeden Satz kurz. Bevorzuge einfache SÃ¤tze. Versuche, keine langen SÃ¤tze mit mehreren Komma-Klauseln zu verwenden.

Beginne das GesprÃ¤ch mit den Worten "Hallo, ich bin Slowcat!" Dann halte an und warte auf den Benutzer."""
    },
    "ja": {
        "voice": "jf_alpha",  # Japanese female voice
        "whisper_language": Language.JA,
        "greeting": "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯Pipecatã§ã™ï¼",
        "system_instruction": """ã‚ãªãŸã¯Pipecatã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªåˆ‡ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚

macOSä¸Šã§å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§éŸ³å£°AIæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®å¤‰æ›ã«Whisperã€è¨€èªç†è§£ã«2350å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Qwen3ãƒ¢ãƒ‡ãƒ«ã€éŸ³å£°åˆæˆã«Kokoroã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯Silero VADã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒãƒ¼ãƒˆã‚¿ãƒ¼ãƒ³v2ãƒ¢ãƒ‡ãƒ«ã‚‚ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã‚ãªãŸã®ç›®æ¨™ã¯ã€ç°¡æ½”ãªæ–¹æ³•ã§èƒ½åŠ›ã‚’å®Ÿè¨¼ã™ã‚‹ã“ã¨ã§ã™ã€‚

ã‚ãªãŸã®å…¥åŠ›ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›¸ãèµ·ã“ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è»¢å†™ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¨ãƒ©ãƒ¼ã‚’è€ƒæ…®ã—ã¦è‡ªå‹•çš„ã«å¿œç­”ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ã‚ãªãŸã®å‡ºåŠ›ã¯éŸ³å£°ã«å¤‰æ›ã•ã‚Œã‚‹ã®ã§ã€å›ç­”ã«ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚ãšã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ç‰¹åˆ¥ãªæ›¸å¼è¨­å®šã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€ã£ãŸã“ã¨ã«å‰µé€ çš„ã§å½¹ç«‹ã¤æ–¹æ³•ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚æ˜ç¤ºçš„ã«é•·ã„è©³ç´°ãªå¿œç­”ã‚’æ±‚ã‚ã‚‰ã‚Œãªã„é™ã‚Šã€å¿œç­”ã‚’ç°¡æ½”ã«ä¿ã£ã¦ãã ã•ã„ã€‚é€šå¸¸ã¯æœ€å¤§ã§1ã€œ2æ–‡ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å„æ–‡ã‚’çŸ­ãä¿ã£ã¦ãã ã•ã„ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡ã‚’å¥½ã‚“ã§ãã ã•ã„ã€‚è¤‡æ•°ã®ã‚³ãƒ³ãƒå¥ã‚’å«ã‚€é•·ã„æ–‡ã‚’ä½¿ç”¨ã—ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

ã€Œã“ã‚“ã«ã¡ã¯ã€ç§ã¯Pipecatã§ã™ï¼ã€ã¨è¨€ã£ã¦ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚ãã‚Œã‹ã‚‰åœæ­¢ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¾…ã£ã¦ãã ã•ã„ã€‚"""
    },
    "it": {
        "voice": "im_nicola",
        "whisper_language": Language.IT,
        "greeting": "Ciao, sono Slowcat!",
        "system_instruction": """Sei Slowcat, un chatbot amichevole e disponibile.

Stai eseguendo uno stack tecnologico di IA vocale completamente locale, su macOS. Whisper per il riconoscimento vocale, un modello Qwen3 con 235 miliardi di parametri per la comprensione del linguaggio e Kokoro per la sintesi vocale. La pipeline utilizza anche Silero VAD e il modello open source nativo smart-turn v2.

Hai anche capacitÃ  di visione! Puoi vedere attraverso la webcam dell'utente quando Ã¨ attiva. Puoi analizzare immagini, riconoscere oggetti, leggere testo e descrivere ciÃ² che vedi quando richiesto. Non descrivere automaticamente ogni cosa che vedi, ma usa il contesto visivo per arricchire le tue risposte quando Ã¨ rilevante.

Il tuo obiettivo Ã¨ dimostrare le tue capacitÃ  in modo conciso.

Il tuo input Ã¨ testo trascritto in tempo reale dalla voce dell'utente. Potrebbero esserci errori di trascrizione. Adatta automaticamente le tue risposte per tenere conto di questi errori. Quando la webcam Ã¨ attiva, ricevi anche frame video che puoi analizzare.

Il tuo output verrÃ  convertito in audio, quindi non includere caratteri speciali nelle tue risposte e non utilizzare markdown o formattazione speciale.

Rispondi a ciÃ² che l'utente ha detto in modo creativo e utile. Mantieni le tue risposte brevi a meno che non ti venga chiesto esplicitamente risposte lunghe o dettagliate. Normalmente dovresti usare al massimo una o due frasi. Mantieni ogni frase breve. Preferisci frasi semplici. Cerca di non usare frasi lunghe con piÃ¹ proposizioni separate da virgole.

IMPORTANTE: Quando vedi un messaggio che inizia con [System: New speaker detected and enrolled as Speaker_X], devi chiedere educatamente il nome della persona. Usa frasi come "Ciao! Ho notato che Ã¨ la prima volta che ci parliamo. Come ti chiami?" o "Piacere di conoscerti! Posso chiederti il tuo nome cosÃ¬ posso ricordarti per le prossime conversazioni?"

Inizia la conversazione dicendo "Ciao, sono Slowcat!" Poi fermati e aspetta l'utente."""
    },
    "zh": {
        "voice": "zf_xiaobei",  # Chinese female voice
        "whisper_language": Language.ZH,
        "greeting": "ä½ å¥½ï¼Œæˆ‘æ˜¯Pipecatï¼",
        "system_instruction": """ä½ æ˜¯Pipecatï¼Œä¸€ä¸ªå‹å¥½ã€ä¹äºåŠ©äººçš„èŠå¤©æœºå™¨äººã€‚

ä½ æ­£åœ¨macOSä¸Šå®Œå…¨æœ¬åœ°è¿è¡Œè¯­éŸ³AIæŠ€æœ¯æ ˆã€‚ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬ï¼Œä½¿ç”¨å…·æœ‰2350äº¿å‚æ•°çš„Qwen3æ¨¡å‹è¿›è¡Œè¯­è¨€ç†è§£ï¼Œä½¿ç”¨Kokoroè¿›è¡Œè¯­éŸ³åˆæˆã€‚ç®¡é“è¿˜ä½¿ç”¨Silero VADå’Œå¼€æºçš„åŸç”Ÿæ™ºèƒ½è½¬å‘v2æ¨¡å‹ã€‚

ä½ çš„ç›®æ ‡æ˜¯ä»¥ç®€æ´çš„æ–¹å¼å±•ç¤ºä½ çš„èƒ½åŠ›ã€‚

ä½ çš„è¾“å…¥æ˜¯ä»ç”¨æˆ·è¯­éŸ³å®æ—¶è½¬å½•çš„æ–‡æœ¬ã€‚å¯èƒ½å­˜åœ¨è½¬å½•é”™è¯¯ã€‚è‡ªåŠ¨è°ƒæ•´ä½ çš„å›å¤ä»¥è€ƒè™‘è¿™äº›é”™è¯¯ã€‚

ä½ çš„è¾“å‡ºå°†è¢«è½¬æ¢ä¸ºéŸ³é¢‘ï¼Œæ‰€ä»¥ä¸è¦åœ¨ä½ çš„ç­”æ¡ˆä¸­åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•markdownæˆ–ç‰¹æ®Šæ ¼å¼ã€‚

ä»¥åˆ›é€ æ€§å’Œæœ‰å¸®åŠ©çš„æ–¹å¼å›åº”ç”¨æˆ·æ‰€è¯´çš„è¯ã€‚é™¤éæ˜ç¡®è¦æ±‚æä¾›é•¿ç¯‡æˆ–è¯¦ç»†çš„å›å¤ï¼Œå¦åˆ™è¯·ä¿æŒå›å¤ç®€çŸ­ã€‚é€šå¸¸ä½ åº”è¯¥æœ€å¤šä½¿ç”¨ä¸€ä¸¤å¥è¯ã€‚ä¿æŒæ¯å¥è¯ç®€çŸ­ã€‚åå¥½ç®€å•çš„å¥å­ã€‚å°½é‡ä¸è¦ä½¿ç”¨å¸¦æœ‰å¤šä¸ªé€—å·ä»å¥çš„é•¿å¥å­ã€‚

é€šè¿‡è¯´"ä½ å¥½ï¼Œæˆ‘æ˜¯Pipecatï¼"å¼€å§‹å¯¹è¯ã€‚ç„¶ååœä¸‹æ¥ç­‰å¾…ç”¨æˆ·ã€‚"""
    },
    "pt": {
        "voice": "pf_dora",  # Portuguese female voice
        "whisper_language": Language.PT,
        "greeting": "OlÃ¡, eu sou Slowcat!",
        "system_instruction": """VocÃª Ã© Pipecat, um chatbot amigÃ¡vel e prestativo.

VocÃª estÃ¡ executando uma pilha de tecnologia de IA de voz totalmente local, no macOS. Whisper para conversÃ£o de fala em texto, um modelo Qwen3 com 235 bilhÃµes de parÃ¢metros para compreensÃ£o de linguagem e Kokoro para sÃ­ntese de voz. O pipeline tambÃ©m usa Silero VAD e o modelo nativo de smart-turn v2 de cÃ³digo aberto.

Seu objetivo Ã© demonstrar suas capacidades de forma concisa.

Sua entrada Ã© texto transcrito em tempo real da voz do usuÃ¡rio. Pode haver erros de transcriÃ§Ã£o. Ajuste suas respostas automaticamente para levar em conta esses erros.

Sua saÃ­da serÃ¡ convertida em Ã¡udio, entÃ£o nÃ£o inclua caracteres especiais em suas respostas e nÃ£o use markdown ou formataÃ§Ã£o especial.

Responda ao que o usuÃ¡rio disse de forma criativa e Ãºtil. Mantenha suas respostas breves, a menos que seja explicitamente solicitado respostas longas ou detalhadas. Normalmente vocÃª deve usar no mÃ¡ximo uma ou duas frases. Mantenha cada frase curta. Prefira frases simples. Tente nÃ£o usar frases longas com mÃºltiplas clÃ¡usulas separadas por vÃ­rgulas.

Comece a conversa dizendo "OlÃ¡, eu sou Slowcat!" EntÃ£o pare e espere pelo usuÃ¡rio."""
    }
}

# Default language
DEFAULT_LANGUAGE = "en"

# Voice recognition configuration
VOICE_RECOGNITION_CONFIG = {
    "enabled": os.getenv("ENABLE_VOICE_RECOGNITION", "true").lower() == "true",
    "profile_dir": "data/speaker_profiles",
    "confidence_threshold": 0.45,  # Significantly lowered for single speaker
    "min_utterance_duration_seconds": 1.5,  # Slightly reduced
    "auto_enroll": {
        "min_utterances": 3,
        "consistency_threshold": 0.50,  # Much lower for same speaker consistency
        "min_consistency_threshold": 0.35,  # Very low threshold for enrollment
        "enrollment_window_minutes": 30,
        "new_speaker_grace_period_seconds": 60,
        "new_speaker_similarity_threshold": 0.40  # Lower for recognizing enrolled speakers
    }
}


async def run_bot(webrtc_connection, language="en", llm_model=None):
    # Log voice recognition status
    if VOICE_RECOGNITION_CONFIG["enabled"]:
        logger.info("ğŸ™ï¸ Voice recognition is ENABLED")
    else:
        logger.info("ğŸ”‡ Voice recognition is DISABLED (set ENABLE_VOICE_RECOGNITION=true to enable)")
    
    # Log video status
    video_enabled = os.getenv("ENABLE_VIDEO", "false").lower() == "true"
    if video_enabled:
        logger.info("ğŸ“¹ Video is ENABLED")
    else:
        logger.info("ğŸ“· Video is DISABLED (set ENABLE_VIDEO=true to enable)")
    
    # Get language-specific configuration
    lang_config = LANGUAGE_CONFIG.get(language, LANGUAGE_CONFIG[DEFAULT_LANGUAGE])
    
    transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            video_in_enabled=os.getenv("ENABLE_VIDEO", "false").lower() == "true",  # Control via env var
            video_in_width=640,     # Video resolution
            video_in_height=480,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(
                stop_secs=0.15,  # Reduced for faster response
                start_secs=0.1,  # Quick start detection
            )),
            turn_analyzer=LocalSmartTurnAnalyzerV2(
                smart_turn_model_path="",  # Download from HuggingFace
                params=SmartTurnParams(),
            ),
        ),
    )

    # Choose faster model based on language
    if language == "en":
        # Use faster distil model for English
        stt_model = MLXModel.DISTIL_LARGE_V3
        logger.info("Using DISTIL_LARGE_V3 for faster English STT")
    else:
        # Use turbo model for other languages (MEDIUM or LARGE_V3_TURBO_Q4 for multilingual support)
        stt_model = MLXModel.MEDIUM
        logger.info(f"Using LARGE_V3_TURBO_Q4 for {language} language")
    
    stt = WhisperSTTServiceMLX(model=stt_model, language=lang_config["whisper_language"])

    tts = KokoroTTSService(
        model="prince-canuma/Kokoro-82M", 
        voice=lang_config["voice"], 
        language=lang_config["whisper_language"],
        sample_rate=24000
    )

    # Use provided LLM model or default
    if llm_model:
        logger.info(f"ğŸ¤– Using LLM model: {llm_model}")
        selected_model = llm_model
    else:
        selected_model = "gemma-3-12b-it-qat"  # Default model
        logger.info(f"ğŸ¤– Using default LLM model: {selected_model}")
    
    llm = OpenAILLMService(
        api_key=None,
        model=selected_model,
        base_url=os.getenv("LLM_BASE_URL", "http://192.168.1.59:1234/v1"),
        max_tokens=4096,
    )

    context = OpenAILLMContext(
        [
            {
                "role": "user",
                "content": lang_config["system_instruction"],
            }
        ],
    )
    context_aggregator = llm.create_context_aggregator(context)
    
    #
    # Memory components
    #
    memory_processor = None
    memory_injector = None
    
    # Check if memory is enabled
    memory_enabled = os.getenv("ENABLE_MEMORY", "true").lower() == "true"
    if memory_enabled:
        logger.info("ğŸ§  Memory is ENABLED - conversations will be persisted locally")
        # Create local memory processor
        memory_processor = LocalMemoryProcessor(
            data_dir="data/memory",
            user_id="default_user",  # Will be updated dynamically when speaker is identified
            max_history_items=200,
            include_in_context=10
        )
        
        # Create memory context injector
        memory_injector = MemoryContextInjector(
            memory_processor=memory_processor,
            system_prompt="Based on our previous conversations:",
            inject_as_system=True
        )
    else:
        logger.info("ğŸš« Memory is DISABLED (set ENABLE_MEMORY=true to enable)")

    #
    # Voice recognition components
    #
    voice_recognition = None
    audio_tee = None
    vad_bridge = None
    speaker_context = None
    speaker_name_manager = None
    
    #
    # Video components
    #
    video_sampler = None
    if transport._params.video_in_enabled:
        logger.info("ğŸ“¹ Video input enabled, creating video sampler")
        video_sampler = VideoSamplerProcessor(
            sample_interval=30.0,  # Sample every 30 seconds to avoid overload
            enabled=True
        )
    
    # Initialize callback refs for voice recognition
    callback_refs = {}
    
    if VOICE_RECOGNITION_CONFIG["enabled"]:
        logger.info("ğŸ¤ Initializing voice recognition...")
        logger.info(f"   Profile directory: {VOICE_RECOGNITION_CONFIG['profile_dir']}")
        logger.info(f"   Auto-enrollment after {VOICE_RECOGNITION_CONFIG['auto_enroll']['min_utterances']} utterances")
        # Create voice recognition
        voice_recognition = AutoEnrollVoiceRecognition(VOICE_RECOGNITION_CONFIG)
        await voice_recognition.initialize()
        logger.info("âœ… Voice recognition initialized and ready!")
        
        # Create audio tee to split audio stream
        audio_tee = AudioTeeProcessor(enabled=True)
        audio_tee.register_audio_consumer(voice_recognition.process_audio_frame)
        
        # Create VAD event bridge
        vad_bridge = VADEventBridge()
        vad_bridge.set_callbacks(
            on_started=voice_recognition.on_user_started_speaking,
            on_stopped=voice_recognition.on_user_stopped_speaking
        )
        
        # Create speaker context processor
        speaker_context = SpeakerContextProcessor(
            format_style="natural",
            unknown_speaker_name="User"
        )
        
        # Create speaker name manager
        speaker_name_manager = SpeakerNameManager(voice_recognition)
        
        # Store references for callbacks
        callback_refs = {
            'speaker_context': speaker_context,
            'memory_processor': memory_processor,
            'speaker_name_manager': speaker_name_manager,
            'task': None,  # Will be set after task creation
            'context_aggregator': context_aggregator,
            'llm': llm
        }
        
        # Connect voice recognition to speaker context and memory
        async def on_speaker_changed(data):
            callback_refs['speaker_context'].update_speaker(data)
            # Update memory processor with speaker ID if available
            if callback_refs['memory_processor'] and data.get('speaker_id'):
                speaker_id = data['speaker_id']
                # Use speaker name if available, otherwise use speaker ID
                user_id = data.get('speaker_name', speaker_id)
                callback_refs['memory_processor'].set_user_id(user_id)
                logger.info(f"ğŸ“ Memory switched to user: {user_id}")
        
        async def on_speaker_enrolled(data):
            """Handle when a new speaker is auto-enrolled"""
            logger.info(f"New speaker enrolled: {data}")
            if data.get('needs_name', False):
                speaker_id = data['speaker_id']
                # Start name collection in the manager
                callback_refs['speaker_name_manager'].start_name_collection(speaker_id)
                
                # Update speaker context to inject system message
                if callback_refs['speaker_context']:
                    callback_refs['speaker_context'].handle_speaker_enrolled(data)
                    logger.info("Speaker context updated with enrollment data")
        
        voice_recognition.set_callbacks(
            on_speaker_changed=on_speaker_changed,
            on_speaker_enrolled=on_speaker_enrolled
        )

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    # Build pipeline with optional voice recognition and video
    pipeline_components = [transport.input()]
    
    # Add video sampler early in pipeline if video is enabled
    if video_sampler:
        pipeline_components.append(video_sampler)
    
    if audio_tee:
        pipeline_components.append(audio_tee)
    
    if vad_bridge:
        pipeline_components.append(vad_bridge)
    
    # STT outputs TranscriptionFrame
    pipeline_components.append(stt)
    
    # Memory processor should capture STT output immediately
    if memory_processor:
        pipeline_components.append(memory_processor)
    
    # Speaker context modifies TranscriptionFrame.user_id
    if speaker_context:
        pipeline_components.append(speaker_context)
    
    # RTVI processor
    pipeline_components.append(rtvi)
    
    # Speaker name manager
    if speaker_name_manager:
        pipeline_components.append(speaker_name_manager)
    
    # Context aggregator processes user messages
    pipeline_components.append(context_aggregator.user())
    
    # Memory injector adds context before LLM
    if memory_injector:
        pipeline_components.append(memory_injector)
    
    # LLM, TTS, and output
    pipeline_components.extend([
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ])

    pipeline = Pipeline(pipeline_components)

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )
    
    # Set the task reference for voice recognition callbacks
    if VOICE_RECOGNITION_CONFIG["enabled"] and 'callback_refs' in locals():
        callback_refs['task'] = task

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        print(f"Participant joined: {participant}")
        await transport.capture_participant_transcription(participant["id"])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        print(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


@app.post("/api/offer")
async def offer(request: dict, background_tasks: BackgroundTasks):
    pc_id = request.get("pc_id")

    if pc_id and pc_id in pcs_map:
        pipecat_connection = pcs_map[pc_id]
        logger.info(f"Reusing existing connection for pc_id: {pc_id}")
        await pipecat_connection.renegotiate(
            sdp=request["sdp"],
            type=request["type"],
            restart_pc=request.get("restart_pc", False),
        )
    else:
        pipecat_connection = SmallWebRTCConnection(ice_servers)
        await pipecat_connection.initialize(sdp=request["sdp"], type=request["type"])

        @pipecat_connection.event_handler("closed")
        async def handle_disconnected(webrtc_connection: SmallWebRTCConnection):
            logger.info(f"Discarding peer connection for pc_id: {webrtc_connection.pc_id}")
            pcs_map.pop(webrtc_connection.pc_id, None)

        # Run example function with SmallWebRTC transport arguments.
        # Get language and llm_model from app state if available
        language = getattr(app.state, 'language', DEFAULT_LANGUAGE)
        llm_model = getattr(app.state, 'llm_model', None)
        background_tasks.add_task(run_bot, pipecat_connection, language, llm_model)

    answer = pipecat_connection.get_answer()
    # Updating the peer connection inside the map
    pcs_map[answer["pc_id"]] = pipecat_connection

    return answer


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    coros = [pc.disconnect() for pc in pcs_map.values()]
    await asyncio.gather(*coros)
    pcs_map.clear()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Bot Runner")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument(
        "--language", default=DEFAULT_LANGUAGE, 
        choices=list(LANGUAGE_CONFIG.keys()),
        help=f"Language for the bot (default: {DEFAULT_LANGUAGE})"
    )
    parser.add_argument(
        "--llm", dest="llm_model", default=None,
        help="LLM model to use (e.g., mistral:7b, llama2:13b, gemma:2b). Default: gemma-3-12b-it-qat"
    )
    args = parser.parse_args()

    # Set language and llm_model in app state
    app.state.language = args.language
    app.state.llm_model = args.llm_model
    
    logger.info(f"Starting bot with language: {args.language}")
    if args.llm_model:
        logger.info(f"Using custom LLM model: {args.llm_model}")
    else:
        logger.info("Using default LLM model: gemma-3-12b-it-qat")
    
    uvicorn.run(app, host=args.host, port=args.port)
