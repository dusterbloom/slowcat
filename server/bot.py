import argparse
import asyncio
import os
import sys
from contextlib import asynccontextmanager
from typing import Dict

# Add local pipecat to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "pipecat", "src"))

import uvicorn
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI
from loguru import logger

from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v2 import LocalSmartTurnAnalyzerV2
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.openai.llm import OpenAILLMService
from kokoro_tts import KokoroTTSService
from pipecat.services.whisper.stt import WhisperSTTServiceMLX, MLXModel
from pipecat.transcriptions.language import Language
from pipecat.transports.base_transport import TransportParams
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport
from pipecat.transports.network.webrtc_connection import IceServer, SmallWebRTCConnection

# Import voice recognition components
from voice_recognition import AutoEnrollVoiceRecognition
from processors import AudioTeeProcessor, VADEventBridge, SpeakerContextProcessor

load_dotenv(override=True)

app = FastAPI()

pcs_map: Dict[str, SmallWebRTCConnection] = {}

ice_servers = [
    IceServer(
        urls="stun:stun.l.google.com:19302",
    )
]


# Language configuration
LANGUAGE_CONFIG = {
    "en": {
        "voice": "af_heart",  # Can use any of: af_bella, af_sarah, af_sky, af_alloy, af_nova, af_jessica, etc.
        "whisper_language": Language.EN,
        "greeting": "Hello, I'm Slowcat!",
        "system_instruction": """You are Pipecat, a friendly, helpful chatbot.

You are running a voice AI tech stack entirely locally, on macOS. Whisper for speech-to-text, a Qwen3 model with 235 billion parameters for language understanding, and Kokoro for speech synthesis. The pipeline also uses Silero VAD and the open source, native audio smart-turn v2 model.

You also have speaker recognition capabilities! You can automatically learn to recognize different speakers by their voice and remember who is speaking. If you recognize someone who has spoken before, you'll know it's them.

Your goal is to demonstrate your capabilities in a succinct way.

Your input is text transcribed in realtime from the user's voice. There may be transcription errors. Adjust your responses automatically to account for these errors.

Your output will be converted to audio so don't include special characters in your answers and do not use any markdown or special formatting.

Respond to what the user said in a creative and helpful way. Keep your responses brief unless you are explicitly asked for long or detailed responses. Normally you should use one or two sentences at most. Keep each sentence short. Prefer simple sentences. Try not to use long sentences with multiple comma clauses.

Start the conversation by saying, "Hello, I'm Slowcat!" Then stop and wait for the user."""
    },
    "es": {
        "voice": "ef_dora",  # Spanish female voice
        "whisper_language": Language.ES,
        "greeting": "Â¡Hola, soy Slowcat!",
        "system_instruction": """Eres Pipecat, un chatbot amigable y servicial.

EstÃ¡s ejecutando una pila tecnolÃ³gica de IA de voz completamente local, en macOS. Whisper para voz a texto, un modelo Qwen3 con 235 mil millones de parÃ¡metros para comprensiÃ³n del lenguaje, y Kokoro para sÃ­ntesis de voz. La pipeline tambiÃ©n usa Silero VAD y el modelo open source de smart-turn v2 nativo.

Â¡TambiÃ©n tienes capacidades de reconocimiento de voz! Puedes aprender automÃ¡ticamente a reconocer diferentes hablantes por su voz y recordar quiÃ©n estÃ¡ hablando. Si reconoces a alguien que ha hablado antes, sabrÃ¡s que son ellos.

Tu objetivo es demostrar tus capacidades de manera concisa.

Tu entrada es texto transcrito en tiempo real de la voz del usuario. Puede haber errores de transcripciÃ³n. Ajusta tus respuestas automÃ¡ticamente para tener en cuenta estos errores.

Tu salida serÃ¡ convertida a audio, asÃ­ que no incluyas caracteres especiales en tus respuestas y no uses markdown o formato especial.

Responde a lo que dijo el usuario de manera creativa y Ãºtil. MantÃ©n tus respuestas breves a menos que se te pida explÃ­citamente respuestas largas o detalladas. Normalmente deberÃ­as usar una o dos oraciones como mÃ¡ximo. MantÃ©n cada oraciÃ³n corta. Prefiere oraciones simples. Trata de no usar oraciones largas con mÃºltiples clÃ¡usulas con comas.

Comienza la conversaciÃ³n diciendo "Â¡Hola, soy Slowcat!" Luego detente y espera al usuario."""
    },
    "fr": {
        "voice": "ff_siwis",
        "whisper_language": Language.FR,
        "greeting": "Bonjour, je suis Slowcat!",
        "system_instruction": """Tu es Pipecat, un chatbot amical et serviable.

Tu exÃ©cutes une pile technologique d'IA vocale entiÃ¨rement locale, sur macOS. Whisper pour la reconnaissance vocale, un modÃ¨le Qwen3 avec 235 milliards de paramÃ¨tres pour la comprÃ©hension du langage, et Kokoro pour la synthÃ¨se vocale. Le pipeline utilise Ã©galement Silero VAD et le modÃ¨le open source smart-turn v2 natif.

Tu as aussi des capacitÃ©s de reconnaissance vocale! Tu peux automatiquement apprendre Ã  reconnaÃ®tre diffÃ©rents interlocuteurs par leur voix et te souvenir de qui parle. Si tu reconnais quelqu'un qui a dÃ©jÃ  parlÃ©, tu sauras que c'est lui.

Ton objectif est de dÃ©montrer tes capacitÃ©s de maniÃ¨re concise.

Ton entrÃ©e est du texte transcrit en temps rÃ©el Ã  partir de la voix de l'utilisateur. Il peut y avoir des erreurs de transcription. Ajuste automatiquement tes rÃ©ponses pour tenir compte de ces erreurs.

Ta sortie sera convertie en audio, donc n'inclue pas de caractÃ¨res spÃ©ciaux dans tes rÃ©ponses et n'utilise pas de markdown ou de formatage spÃ©cial.

RÃ©ponds Ã  ce que l'utilisateur a dit de maniÃ¨re crÃ©ative et utile. Garde tes rÃ©ponses brÃ¨ves sauf si on te demande explicitement des rÃ©ponses longues ou dÃ©taillÃ©es. Normalement, tu devrais utiliser une ou deux phrases au maximum. Garde chaque phrase courte. PrÃ©fÃ¨re les phrases simples. Essaie de ne pas utiliser de longues phrases avec plusieurs propositions sÃ©parÃ©es par des virgules.

Commence la conversation en disant "Bonjour, je suis Slowcat!" Puis arrÃªte-toi et attends l'utilisateur."""
    },
    "de": {
        "voice": "af_heart",  # German voice not yet available in Kokoro
        "whisper_language": Language.DE,
        "greeting": "Hallo, ich bin Slowcat!",
        "system_instruction": """Du bist Pipecat, ein freundlicher, hilfreicher Chatbot.

Du fÃ¼hrst einen Sprach-KI-Technologie-Stack vollstÃ¤ndig lokal auf macOS aus. Whisper fÃ¼r Sprache-zu-Text, ein Qwen3-Modell mit 235 Milliarden Parametern fÃ¼r SprachverstÃ¤ndnis und Kokoro fÃ¼r Sprachsynthese. Die Pipeline verwendet auch Silero VAD und das Open-Source-native Smart-Turn-v2-Modell.

Du hast auch Sprechererkennung! Du kannst automatisch lernen, verschiedene Sprecher an ihrer Stimme zu erkennen und dich daran erinnern, wer spricht. Wenn du jemanden erkennst, der schon einmal gesprochen hat, weiÃŸt du, dass er es ist.

Dein Ziel ist es, deine FÃ¤higkeiten auf prÃ¤gnante Weise zu demonstrieren.

Deine Eingabe ist Text, der in Echtzeit aus der Stimme des Benutzers transkribiert wird. Es kann Transkriptionsfehler geben. Passe deine Antworten automatisch an, um diese Fehler zu berÃ¼cksichtigen.

Deine Ausgabe wird in Audio konvertiert, also fÃ¼ge keine Sonderzeichen in deine Antworten ein und verwende kein Markdown oder spezielle Formatierung.

Antworte auf das, was der Benutzer gesagt hat, auf kreative und hilfreiche Weise. Halte deine Antworten kurz, es sei denn, du wirst explizit um lange oder detaillierte Antworten gebeten. Normalerweise solltest du hÃ¶chstens ein oder zwei SÃ¤tze verwenden. Halte jeden Satz kurz. Bevorzuge einfache SÃ¤tze. Versuche, keine langen SÃ¤tze mit mehreren Komma-Klauseln zu verwenden.

Beginne das GesprÃ¤ch mit den Worten "Hallo, ich bin Slowcat!" Dann halte an und warte auf den Benutzer."""
    },
    "ja": {
        "voice": "jf_alpha",  # Japanese female voice
        "whisper_language": Language.JA,
        "greeting": "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯Pipecatã§ã™ï¼",
        "system_instruction": """ã‚ãªãŸã¯Pipecatã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªåˆ‡ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚

macOSä¸Šã§å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§éŸ³å£°AIæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®å¤‰æ›ã«Whisperã€è¨€èªç†è§£ã«2350å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Qwen3ãƒ¢ãƒ‡ãƒ«ã€éŸ³å£°åˆæˆã«Kokoroã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯Silero VADã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒãƒ¼ãƒˆã‚¿ãƒ¼ãƒ³v2ãƒ¢ãƒ‡ãƒ«ã‚‚ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã‚ãªãŸã®ç›®æ¨™ã¯ã€ç°¡æ½”ãªæ–¹æ³•ã§èƒ½åŠ›ã‚’å®Ÿè¨¼ã™ã‚‹ã“ã¨ã§ã™ã€‚

ã‚ãªãŸã®å…¥åŠ›ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›¸ãèµ·ã“ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è»¢å†™ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¨ãƒ©ãƒ¼ã‚’è€ƒæ…®ã—ã¦è‡ªå‹•çš„ã«å¿œç­”ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ã‚ãªãŸã®å‡ºåŠ›ã¯éŸ³å£°ã«å¤‰æ›ã•ã‚Œã‚‹ã®ã§ã€å›ç­”ã«ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚ãšã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„ç‰¹åˆ¥ãªæ›¸å¼è¨­å®šã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€ã£ãŸã“ã¨ã«å‰µé€ çš„ã§å½¹ç«‹ã¤æ–¹æ³•ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚æ˜ç¤ºçš„ã«é•·ã„è©³ç´°ãªå¿œç­”ã‚’æ±‚ã‚ã‚‰ã‚Œãªã„é™ã‚Šã€å¿œç­”ã‚’ç°¡æ½”ã«ä¿ã£ã¦ãã ã•ã„ã€‚é€šå¸¸ã¯æœ€å¤§ã§1ã€œ2æ–‡ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å„æ–‡ã‚’çŸ­ãä¿ã£ã¦ãã ã•ã„ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡ã‚’å¥½ã‚“ã§ãã ã•ã„ã€‚è¤‡æ•°ã®ã‚³ãƒ³ãƒå¥ã‚’å«ã‚€é•·ã„æ–‡ã‚’ä½¿ç”¨ã—ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

ã€Œã“ã‚“ã«ã¡ã¯ã€ç§ã¯Pipecatã§ã™ï¼ã€ã¨è¨€ã£ã¦ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚ãã‚Œã‹ã‚‰åœæ­¢ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¾…ã£ã¦ãã ã•ã„ã€‚"""
    },
    "it": {
        "voice": "im_nicola",
        "whisper_language": Language.IT,
        "greeting": "Ciao, sono Slowcat!",
        "system_instruction": """Sei Slowcat, un chatbot amichevole e disponibile.

Stai eseguendo uno stack tecnologico di IA vocale completamente locale, su macOS. Whisper per il riconoscimento vocale, un modello Qwen3 con 235 miliardi di parametri per la comprensione del linguaggio e Kokoro per la sintesi vocale. La pipeline utilizza anche Silero VAD e il modello open source nativo smart-turn v2.

Il tuo obiettivo Ã¨ dimostrare le tue capacitÃ  in modo conciso.

Il tuo input Ã¨ testo trascritto in tempo reale dalla voce dell'utente. Potrebbero esserci errori di trascrizione. Adatta automaticamente le tue risposte per tenere conto di questi errori.

Il tuo output verrÃ  convertito in audio, quindi non includere caratteri speciali nelle tue risposte e non utilizzare markdown o formattazione speciale.

Rispondi a ciÃ² che l'utente ha detto in modo creativo e utile. Mantieni le tue risposte brevi a meno che non ti venga chiesto esplicitamente risposte lunghe o dettagliate. Normalmente dovresti usare al massimo una o due frasi. Mantieni ogni frase breve. Preferisci frasi semplici. Cerca di non usare frasi lunghe con piÃ¹ proposizioni separate da virgole.

Inizia la conversazione dicendo "Ciao, sono Slowcat!" Poi fermati e aspetta l'utente."""
    },
    "zh": {
        "voice": "zf_xiaobei",  # Chinese female voice
        "whisper_language": Language.ZH,
        "greeting": "ä½ å¥½ï¼Œæˆ‘æ˜¯Pipecatï¼",
        "system_instruction": """ä½ æ˜¯Pipecatï¼Œä¸€ä¸ªå‹å¥½ã€ä¹äºåŠ©äººçš„èŠå¤©æœºå™¨äººã€‚

ä½ æ­£åœ¨macOSä¸Šå®Œå…¨æœ¬åœ°è¿è¡Œè¯­éŸ³AIæŠ€æœ¯æ ˆã€‚ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬ï¼Œä½¿ç”¨å…·æœ‰2350äº¿å‚æ•°çš„Qwen3æ¨¡å‹è¿›è¡Œè¯­è¨€ç†è§£ï¼Œä½¿ç”¨Kokoroè¿›è¡Œè¯­éŸ³åˆæˆã€‚ç®¡é“è¿˜ä½¿ç”¨Silero VADå’Œå¼€æºçš„åŸç”Ÿæ™ºèƒ½è½¬å‘v2æ¨¡å‹ã€‚

ä½ çš„ç›®æ ‡æ˜¯ä»¥ç®€æ´çš„æ–¹å¼å±•ç¤ºä½ çš„èƒ½åŠ›ã€‚

ä½ çš„è¾“å…¥æ˜¯ä»ç”¨æˆ·è¯­éŸ³å®æ—¶è½¬å½•çš„æ–‡æœ¬ã€‚å¯èƒ½å­˜åœ¨è½¬å½•é”™è¯¯ã€‚è‡ªåŠ¨è°ƒæ•´ä½ çš„å›å¤ä»¥è€ƒè™‘è¿™äº›é”™è¯¯ã€‚

ä½ çš„è¾“å‡ºå°†è¢«è½¬æ¢ä¸ºéŸ³é¢‘ï¼Œæ‰€ä»¥ä¸è¦åœ¨ä½ çš„ç­”æ¡ˆä¸­åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•markdownæˆ–ç‰¹æ®Šæ ¼å¼ã€‚

ä»¥åˆ›é€ æ€§å’Œæœ‰å¸®åŠ©çš„æ–¹å¼å›åº”ç”¨æˆ·æ‰€è¯´çš„è¯ã€‚é™¤éæ˜ç¡®è¦æ±‚æä¾›é•¿ç¯‡æˆ–è¯¦ç»†çš„å›å¤ï¼Œå¦åˆ™è¯·ä¿æŒå›å¤ç®€çŸ­ã€‚é€šå¸¸ä½ åº”è¯¥æœ€å¤šä½¿ç”¨ä¸€ä¸¤å¥è¯ã€‚ä¿æŒæ¯å¥è¯ç®€çŸ­ã€‚åå¥½ç®€å•çš„å¥å­ã€‚å°½é‡ä¸è¦ä½¿ç”¨å¸¦æœ‰å¤šä¸ªé€—å·ä»å¥çš„é•¿å¥å­ã€‚

é€šè¿‡è¯´"ä½ å¥½ï¼Œæˆ‘æ˜¯Pipecatï¼"å¼€å§‹å¯¹è¯ã€‚ç„¶ååœä¸‹æ¥ç­‰å¾…ç”¨æˆ·ã€‚"""
    },
    "pt": {
        "voice": "pf_dora",  # Portuguese female voice
        "whisper_language": Language.PT,
        "greeting": "OlÃ¡, eu sou Slowcat!",
        "system_instruction": """VocÃª Ã© Pipecat, um chatbot amigÃ¡vel e prestativo.

VocÃª estÃ¡ executando uma pilha de tecnologia de IA de voz totalmente local, no macOS. Whisper para conversÃ£o de fala em texto, um modelo Qwen3 com 235 bilhÃµes de parÃ¢metros para compreensÃ£o de linguagem e Kokoro para sÃ­ntese de voz. O pipeline tambÃ©m usa Silero VAD e o modelo nativo de smart-turn v2 de cÃ³digo aberto.

Seu objetivo Ã© demonstrar suas capacidades de forma concisa.

Sua entrada Ã© texto transcrito em tempo real da voz do usuÃ¡rio. Pode haver erros de transcriÃ§Ã£o. Ajuste suas respostas automaticamente para levar em conta esses erros.

Sua saÃ­da serÃ¡ convertida em Ã¡udio, entÃ£o nÃ£o inclua caracteres especiais em suas respostas e nÃ£o use markdown ou formataÃ§Ã£o especial.

Responda ao que o usuÃ¡rio disse de forma criativa e Ãºtil. Mantenha suas respostas breves, a menos que seja explicitamente solicitado respostas longas ou detalhadas. Normalmente vocÃª deve usar no mÃ¡ximo uma ou duas frases. Mantenha cada frase curta. Prefira frases simples. Tente nÃ£o usar frases longas com mÃºltiplas clÃ¡usulas separadas por vÃ­rgulas.

Comece a conversa dizendo "OlÃ¡, eu sou Slowcat!" EntÃ£o pare e espere pelo usuÃ¡rio."""
    }
}

# Default language
DEFAULT_LANGUAGE = "en"

# Voice recognition configuration
VOICE_RECOGNITION_CONFIG = {
    "enabled": os.getenv("ENABLE_VOICE_RECOGNITION", "true").lower() == "true",
    "profile_dir": "data/speaker_profiles",
    "confidence_threshold": 0.75,
    "min_utterance_duration_seconds": 1.0,
    "auto_enroll": {
        "min_utterances": 3,
        "consistency_threshold": 0.85,
        "min_consistency_threshold": 0.70,
        "enrollment_window_minutes": 30,
        "new_speaker_grace_period_seconds": 60,
        "new_speaker_similarity_threshold": 0.65
    }
}


async def run_bot(webrtc_connection, language="en"):
    # Log voice recognition status
    if VOICE_RECOGNITION_CONFIG["enabled"]:
        logger.info("ğŸ™ï¸ Voice recognition is ENABLED")
    else:
        logger.info("ğŸ”‡ Voice recognition is DISABLED (set ENABLE_VOICE_RECOGNITION=true to enable)")
    
    # Get language-specific configuration
    lang_config = LANGUAGE_CONFIG.get(language, LANGUAGE_CONFIG[DEFAULT_LANGUAGE])
    
    transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV2(
                smart_turn_model_path="",  # Download from HuggingFace
                params=SmartTurnParams(),
            ),
        ),
    )

    stt = WhisperSTTServiceMLX(model=MLXModel.LARGE_V3_TURBO_Q4, language=lang_config["whisper_language"])

    tts = KokoroTTSService(
        model="prince-canuma/Kokoro-82M", 
        voice=lang_config["voice"], 
        language=lang_config["whisper_language"],
        sample_rate=24000
    )

    llm = OpenAILLMService(
        api_key=None,
        model="gemma-3-12b-it-qat",  # Medium-sized model. Uses ~8.5GB of RAM.
        # model="mlx-community/Qwen3-235B-A22B-Instruct-2507-3bit-DWQ", # Large model. Uses ~110GB of RAM!
        base_url="http://192.168.1.59:1234/v1",
        max_tokens=4096,
    )

    context = OpenAILLMContext(
        [
            {
                "role": "user",
                "content": lang_config["system_instruction"],
            }
        ],
    )
    context_aggregator = llm.create_context_aggregator(context)

    #
    # Voice recognition components
    #
    voice_recognition = None
    audio_tee = None
    vad_bridge = None
    speaker_context = None
    
    if VOICE_RECOGNITION_CONFIG["enabled"]:
        logger.info("ğŸ¤ Initializing voice recognition...")
        logger.info(f"   Profile directory: {VOICE_RECOGNITION_CONFIG['profile_dir']}")
        logger.info(f"   Auto-enrollment after {VOICE_RECOGNITION_CONFIG['auto_enroll']['min_utterances']} utterances")
        # Create voice recognition
        voice_recognition = AutoEnrollVoiceRecognition(VOICE_RECOGNITION_CONFIG)
        await voice_recognition.initialize()
        logger.info("âœ… Voice recognition initialized and ready!")
        
        # Create audio tee to split audio stream
        audio_tee = AudioTeeProcessor(enabled=True)
        audio_tee.register_audio_consumer(voice_recognition.process_audio_frame)
        
        # Create VAD event bridge
        vad_bridge = VADEventBridge()
        vad_bridge.set_callbacks(
            on_started=voice_recognition.on_user_started_speaking,
            on_stopped=voice_recognition.on_user_stopped_speaking
        )
        
        # Create speaker context processor
        speaker_context = SpeakerContextProcessor(
            format_style="natural",
            unknown_speaker_name="User"
        )
        
        # Connect voice recognition to speaker context
        async def on_speaker_changed(data):
            speaker_context.update_speaker(data)
        
        voice_recognition.set_callbacks(on_speaker_changed=on_speaker_changed)

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    # Build pipeline with optional voice recognition
    pipeline_components = [transport.input()]
    
    if audio_tee:
        pipeline_components.append(audio_tee)
    
    if vad_bridge:
        pipeline_components.append(vad_bridge)
    
    pipeline_components.extend([
        stt,
        rtvi,
    ])
    
    if speaker_context:
        pipeline_components.append(speaker_context)
    
    pipeline_components.extend([
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ])

    pipeline = Pipeline(pipeline_components)

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        print(f"Participant joined: {participant}")
        await transport.capture_participant_transcription(participant["id"])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        print(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


@app.post("/api/offer")
async def offer(request: dict, background_tasks: BackgroundTasks):
    pc_id = request.get("pc_id")

    if pc_id and pc_id in pcs_map:
        pipecat_connection = pcs_map[pc_id]
        logger.info(f"Reusing existing connection for pc_id: {pc_id}")
        await pipecat_connection.renegotiate(
            sdp=request["sdp"],
            type=request["type"],
            restart_pc=request.get("restart_pc", False),
        )
    else:
        pipecat_connection = SmallWebRTCConnection(ice_servers)
        await pipecat_connection.initialize(sdp=request["sdp"], type=request["type"])

        @pipecat_connection.event_handler("closed")
        async def handle_disconnected(webrtc_connection: SmallWebRTCConnection):
            logger.info(f"Discarding peer connection for pc_id: {webrtc_connection.pc_id}")
            pcs_map.pop(webrtc_connection.pc_id, None)

        # Run example function with SmallWebRTC transport arguments.
        # Get language from app state if available
        language = getattr(app.state, 'language', DEFAULT_LANGUAGE)
        background_tasks.add_task(run_bot, pipecat_connection, language)

    answer = pipecat_connection.get_answer()
    # Updating the peer connection inside the map
    pcs_map[answer["pc_id"]] = pipecat_connection

    return answer


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    coros = [pc.disconnect() for pc in pcs_map.values()]
    await asyncio.gather(*coros)
    pcs_map.clear()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Bot Runner")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument(
        "--language", default=DEFAULT_LANGUAGE, 
        choices=list(LANGUAGE_CONFIG.keys()),
        help=f"Language for the bot (default: {DEFAULT_LANGUAGE})"
    )
    args = parser.parse_args()

    # Set language in app state
    app.state.language = args.language
    logger.info(f"Starting bot with language: {args.language}")
    uvicorn.run(app, host=args.host, port=args.port)
