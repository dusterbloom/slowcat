#!/usr/bin/env python3
"""
Test Ollama memory configuration for the --ollama-memo flag.
"""

import sys
from pathlib import Path

# Add server directory to path
sys.path.append(str(Path(__file__).parent))

from config import config

def test_ollama_config():
    """Test if Ollama configuration is properly set for memory operations."""
    print("ğŸ” Testing Ollama memory configuration...")
    
    print(f"ğŸ“Š MemoBase enabled: {config.memobase.enabled}")
    print(f"ğŸ”§ Memory LLM base URL: {config.memobase.memory_llm.base_url}")
    print(f"ğŸ¤– Memory LLM model: {config.memobase.memory_llm.model}")
    print(f"ğŸ¢ Memory LLM provider: {config.memobase.memory_llm.provider_name}")
    print(f"ğŸ”‘ Memory LLM API key: {config.memobase.memory_llm.api_key}")
    print()
    
    print(f"ğŸ” Embedding base URL: {config.memobase.embedding.base_url}")
    print(f"ğŸ§² Embedding model: {config.memobase.embedding.model}")
    print(f"ğŸ¢ Embedding provider: {config.memobase.embedding.provider_name}")
    print()
    
    print(f"ğŸŒ Main LLM base URL: {config.network.llm_base_url}")
    print(f"ğŸ¤– Main LLM model: {config.models.default_llm_model}")
    print()
    
    # Check if we're using separate providers
    using_separate_memory_llm = (
        config.memobase.memory_llm.base_url != config.network.llm_base_url or
        config.memobase.memory_llm.model != config.models.default_llm_model
    )
    
    print(f"ğŸ”„ Using separate Memory LLM: {using_separate_memory_llm}")
    
    if using_separate_memory_llm:
        print("âœ… Ollama configuration detected for memory operations")
    else:
        print("â„¹ï¸ Using same LLM for main and memory operations")

if __name__ == "__main__":
    test_ollama_config()