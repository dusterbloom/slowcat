Directory structure:
└── memodb-io-memobase/
    ├── readme.md
    ├── Changelog.md
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── requirements.txt
    ├── ROADMAP.md
    ├── setup.py
    ├── assets/
    │   ├── cli_demo.py
    │   ├── doubao_memory.py
    │   ├── episodic_memory.py
    │   ├── openai_memory.py
    │   ├── quickstart.py
    │   ├── blogs/
    │   │   └── remember_forget.py
    │   └── tutorials/
    │       ├── livekit+memobase/
    │       │   ├── readme.md
    │       │   ├── livekit_example.py
    │       │   ├── requirements.txt
    │       │   └── .env.example
    │       └── ollama+memobase/
    │           ├── readme.md
    │           ├── config_ollama.yaml.example
    │           └── ollama_memory.py
    ├── docs/
    │   ├── experiments/
    │   │   ├── 900-chats/
    │   │   │   ├── readme.md
    │   │   │   ├── full_mem0.txt
    │   │   │   ├── full_memobase.txt
    │   │   │   ├── run.py
    │   │   │   └── run_mem0.py
    │   │   ├── chat_sessions/
    │   │   │   ├── readme.md
    │   │   │   ├── extract.py
    │   │   │   ├── requirements.txt
    │   │   │   └── chats/
    │   │   │       └── mock_user/
    │   │   │           ├── 1.json
    │   │   │           └── 2.json
    │   │   └── locomo-benchmark/
    │   │       ├── README.md
    │   │       ├── compute_p95_latency.py
    │   │       ├── evals.py
    │   │       ├── generate_scores.py
    │   │       ├── Makefile
    │   │       ├── prompts.py
    │   │       ├── run_experiments.py
    │   │       ├── metrics/
    │   │       │   ├── llm_judge.py
    │   │       │   └── utils.py
    │   │       └── src/
    │   │           ├── langmem.py
    │   │           ├── rag.py
    │   │           ├── utils.py
    │   │           ├── memobase_client/
    │   │           │   ├── __init__.py
    │   │           │   ├── config.yaml
    │   │           │   ├── memobase_add.py
    │   │           │   └── memobase_search.py
    │   │           ├── memzero/
    │   │           │   ├── add.py
    │   │           │   └── search.py
    │   │           ├── openai/
    │   │           │   └── predict.py
    │   │           └── zep/
    │   │               ├── add.py
    │   │               └── search.py
    │   └── site/
    │       ├── cost.mdx
    │       ├── docs.json
    │       ├── features.mdx
    │       ├── flat_docs.py
    │       ├── introduction.mdx
    │       ├── quickstart.mdx
    │       ├── api-reference/
    │       │   ├── overview.mdx
    │       │   ├── blobs/
    │       │   │   ├── delete_blob.mdx
    │       │   │   ├── get_all_data.mdx
    │       │   │   ├── get_blob.mdx
    │       │   │   ├── insert_data.mdx
    │       │   │   └── modal/
    │       │   │       ├── chat.mdx
    │       │   │       └── summary.mdx
    │       │   ├── buffer/
    │       │   │   ├── flush.mdx
    │       │   │   └── size.mdx
    │       │   ├── events/
    │       │   │   ├── delete_event.mdx
    │       │   │   ├── get_events.mdx
    │       │   │   ├── search_event_gists.mdx
    │       │   │   ├── search_events.mdx
    │       │   │   └── update_event.mdx
    │       │   ├── experimental/
    │       │   │   ├── import_memory.mdx
    │       │   │   └── proactive_topic.mdx
    │       │   ├── profiles/
    │       │   │   ├── add_profile.mdx
    │       │   │   ├── delete_profile.mdx
    │       │   │   ├── profile.mdx
    │       │   │   └── update_profile.mdx
    │       │   ├── project/
    │       │   │   ├── get_profile_config.mdx
    │       │   │   ├── get_usage.mdx
    │       │   │   ├── get_users.mdx
    │       │   │   └── update_profile_config.mdx
    │       │   ├── prompt/
    │       │   │   └── get_context.mdx
    │       │   ├── users/
    │       │   │   ├── create_user.mdx
    │       │   │   ├── delete_user.mdx
    │       │   │   ├── get_user.mdx
    │       │   │   └── update_user.mdx
    │       │   └── utility/
    │       │       ├── healthcheck.mdx
    │       │       └── usage.mdx
    │       ├── features/
    │       │   ├── async_insert.mdx
    │       │   ├── context.mdx
    │       │   ├── message.mdx
    │       │   ├── event/
    │       │   │   ├── event.mdx
    │       │   │   ├── event_search.mdx
    │       │   │   ├── event_summary.mdx
    │       │   │   └── event_tag.mdx
    │       │   └── profile/
    │       │       ├── profile.mdx
    │       │       ├── profile_config.mdx
    │       │       ├── profile_desc.mdx
    │       │       ├── profile_filter.mdx
    │       │       └── profile_search.mdx
    │       ├── practices/
    │       │   ├── bad.mdx
    │       │   ├── openai.mdx
    │       │   └── tips.mdx
    │       ├── references/
    │       │   ├── async_client.mdx
    │       │   ├── cloud_config.mdx
    │       │   └── local_config.mdx
    │       ├── snippets/
    │       │   └── snippet-intro.mdx
    │       └── templates/
    │           ├── dify.mdx
    │           ├── livekit.mdx
    │           ├── mcp.mdx
    │           ├── ollama.mdx
    │           └── openai.mdx
    ├── src/
    │   ├── client/
    │   │   ├── memobase/
    │   │   │   ├── __init__.py
    │   │   │   ├── error.py
    │   │   │   ├── network.py
    │   │   │   ├── utils.py
    │   │   │   ├── core/
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── async_entry.py
    │   │   │   │   ├── blob.py
    │   │   │   │   ├── entry.py
    │   │   │   │   ├── type.py
    │   │   │   │   └── user.py
    │   │   │   └── patch/
    │   │   │       ├── __init__.py
    │   │   │       └── openai.py
    │   │   ├── memobase-go/
    │   │   │   ├── README.md
    │   │   │   ├── go.mod
    │   │   │   ├── go.sum
    │   │   │   ├── blob/
    │   │   │   │   └── blob.go
    │   │   │   ├── core/
    │   │   │   │   ├── client.go
    │   │   │   │   ├── client_test.go
    │   │   │   │   ├── types.go
    │   │   │   │   ├── user.go
    │   │   │   │   └── user_test.go
    │   │   │   ├── error/
    │   │   │   │   └── error.go
    │   │   │   ├── examples/
    │   │   │   │   ├── go.mod
    │   │   │   │   ├── go.sum
    │   │   │   │   └── main.go
    │   │   │   ├── network/
    │   │   │   │   └── network.go
    │   │   │   └── utils/
    │   │   │       └── utils.go
    │   │   ├── memobase-ts/
    │   │   │   ├── README.md
    │   │   │   ├── index.ts
    │   │   │   ├── jest.config.ts
    │   │   │   ├── jsr.json
    │   │   │   ├── package.json
    │   │   │   ├── tsconfig.json
    │   │   │   ├── .eslintrc.js
    │   │   │   ├── .prettierignore
    │   │   │   ├── .prettierrc.json
    │   │   │   ├── scripts/
    │   │   │   │   ├── format
    │   │   │   │   └── lint
    │   │   │   ├── src/
    │   │   │   │   ├── client.ts
    │   │   │   │   ├── error.ts
    │   │   │   │   ├── network.ts
    │   │   │   │   ├── types.ts
    │   │   │   │   └── user.ts
    │   │   │   └── tests/
    │   │   │       ├── client.test.ts
    │   │   │       ├── env.ts
    │   │   │       └── user.test.ts
    │   │   └── tests/
    │   │       ├── __init__.py
    │   │       ├── conftest.py
    │   │       ├── test_blob.py
    │   │       ├── test_user.py
    │   │       └── test_user_async.py
    │   ├── mcp/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── pyproject.toml
    │   │   ├── uv.lock
    │   │   ├── .dockerignore
    │   │   ├── .env.example
    │   │   └── src/
    │   │       ├── main.py
    │   │       └── utils.py
    │   └── server/
    │       ├── readme.md
    │       ├── docker-compose.yml
    │       ├── .env.example
    │       ├── api/
    │       │   ├── readme.md
    │       │   ├── alembic.ini
    │       │   ├── api.py
    │       │   ├── build_init_sql.py
    │       │   ├── config.yaml.example
    │       │   ├── DEVELOPMENT.md
    │       │   ├── Dockerfile
    │       │   ├── pyproject.toml
    │       │   ├── .dockerignore
    │       │   ├── .env.example
    │       │   ├── .python-version
    │       │   ├── example_config/
    │       │   │   ├── event_tag/
    │       │   │   │   └── config.yaml
    │       │   │   ├── jina_embedding/
    │       │   │   │   └── config.yaml
    │       │   │   ├── only_strict_profile/
    │       │   │   │   └── config.yaml
    │       │   │   ├── profile_for_assistant/
    │       │   │   │   └── config.yaml
    │       │   │   ├── profile_for_companion/
    │       │   │   │   └── config.yaml
    │       │   │   └── profile_for_education/
    │       │   │       └── config.yaml
    │       │   ├── memobase_server/
    │       │   │   ├── __init__.py
    │       │   │   ├── connectors.py
    │       │   │   ├── env.py
    │       │   │   ├── errors.py
    │       │   │   ├── struct_logger.py
    │       │   │   ├── types.py
    │       │   │   ├── utils.py
    │       │   │   ├── api_layer/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── blob.py
    │       │   │   │   ├── buffer.py
    │       │   │   │   ├── chore.py
    │       │   │   │   ├── context.py
    │       │   │   │   ├── event.py
    │       │   │   │   ├── middleware.py
    │       │   │   │   ├── profile.py
    │       │   │   │   ├── project.py
    │       │   │   │   ├── roleplay.py
    │       │   │   │   ├── user.py
    │       │   │   │   └── docs/
    │       │   │   │       ├── __init__.py
    │       │   │   │       ├── basic_docs.py
    │       │   │   │       ├── blob.py
    │       │   │   │       ├── event.py
    │       │   │   │       ├── profile.py
    │       │   │   │       ├── project.py
    │       │   │   │       └── user.py
    │       │   │   ├── auth/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── admin_api.py
    │       │   │   │   └── token.py
    │       │   │   ├── controllers/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── billing.py
    │       │   │   │   ├── blob.py
    │       │   │   │   ├── buffer.py
    │       │   │   │   ├── buffer_background.py
    │       │   │   │   ├── context.py
    │       │   │   │   ├── event.py
    │       │   │   │   ├── event_gist.py
    │       │   │   │   ├── full.py
    │       │   │   │   ├── profile.py
    │       │   │   │   ├── project.py
    │       │   │   │   ├── status.py
    │       │   │   │   ├── user.py
    │       │   │   │   ├── modal/
    │       │   │   │   │   ├── __init__.py
    │       │   │   │   │   ├── utils.py
    │       │   │   │   │   ├── chat/
    │       │   │   │   │   │   ├── __init__.py
    │       │   │   │   │   │   ├── entry_summary.py
    │       │   │   │   │   │   ├── event_summary.py
    │       │   │   │   │   │   ├── extract.py
    │       │   │   │   │   │   ├── merge.py
    │       │   │   │   │   │   ├── merge_yolo.py
    │       │   │   │   │   │   ├── organize.py
    │       │   │   │   │   │   ├── summary.py
    │       │   │   │   │   │   ├── types.py
    │       │   │   │   │   │   └── utils.py
    │       │   │   │   │   ├── roleplay/
    │       │   │   │   │   │   ├── detect_interest.py
    │       │   │   │   │   │   ├── predict_new_topics.py
    │       │   │   │   │   │   ├── proactive_topics.py
    │       │   │   │   │   │   └── types.py
    │       │   │   │   │   └── summary/
    │       │   │   │   │       └── __init__.py
    │       │   │   │   └── post_process/
    │       │   │   │       └── profile.py
    │       │   │   ├── llms/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── doubao_cache_llm.py
    │       │   │   │   ├── openai_model_llm.py
    │       │   │   │   ├── utils.py
    │       │   │   │   └── embeddings/
    │       │   │   │       ├── __init__.py
    │       │   │   │       ├── jina_embedding.py
    │       │   │   │       ├── lmstudio_embedding.py
    │       │   │   │       ├── openai_embedding.py
    │       │   │   │       └── utils.py
    │       │   │   ├── models/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── action.py
    │       │   │   │   ├── blob.py
    │       │   │   │   ├── claim.py
    │       │   │   │   ├── database.py
    │       │   │   │   ├── response.py
    │       │   │   │   └── utils.py
    │       │   │   ├── prompts/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── chat_context_pack.py
    │       │   │   │   ├── event_tagging.py
    │       │   │   │   ├── extract_profile.py
    │       │   │   │   ├── merge_profile.py
    │       │   │   │   ├── merge_profile_yolo.py
    │       │   │   │   ├── organize_profile.py
    │       │   │   │   ├── pick_related_profiles.py
    │       │   │   │   ├── profile_init_utils.py
    │       │   │   │   ├── summary_entry_chats.py
    │       │   │   │   ├── summary_profile.py
    │       │   │   │   ├── user_profile_topics.py
    │       │   │   │   ├── utils.py
    │       │   │   │   ├── zh_extract_profile.py
    │       │   │   │   ├── zh_merge_profile.py
    │       │   │   │   ├── zh_merge_profile_yolo.py
    │       │   │   │   ├── zh_summary_entry_chats.py
    │       │   │   │   ├── zh_user_profile_topics.py
    │       │   │   │   └── roleplay/
    │       │   │   │       ├── zh_detect_interest.py
    │       │   │   │       └── zh_infer_plot.py
    │       │   │   └── telemetry/
    │       │   │       ├── __init__.py
    │       │   │       ├── capture_key.py
    │       │   │       └── open_telemetry.py
    │       │   ├── migrations/
    │       │   │   ├── README
    │       │   │   ├── env.py
    │       │   │   └── script.py.mako
    │       │   └── tests/
    │       │       ├── __init__.py
    │       │       ├── conftest.py
    │       │       ├── test_api.py
    │       │       ├── test_chat_modal.py
    │       │       ├── test_controller.py
    │       │       ├── test_db.py
    │       │       └── test_summary_modal.py
    │       └── script/
    │           ├── sync-types.sh
    │           ├── up-dev.sh
    │           └── up.sh
    └── .github/
        ├── CODEOWNERS
        └── workflows/
            └── publish.yaml

================================================
FILE: readme.md
================================================
<div align="center">
    <a href="https://memobase.io">
    <picture>
      <img alt="Memobase logo" src="./assets/images/logo.png" width="80%">
    </picture>
  </a>
  <h1>Memobase</h1>
  <p>
    <a href="https://pypi.org/project/memobase/">
      <img src="https://img.shields.io/pypi/v/memobase.svg">
    </a>
    <a href="https://www.npmjs.com/package/@memobase/memobase">
      <img src="https://img.shields.io/npm/v/@memobase/memobase.svg?logo=npm&logoColor=fff&style=flat&labelColor=2C2C2C&color=28CF8D">
    </a>
    <a href="https://jsr.io/@memobase/memobase">
      <img src="https://img.shields.io/jsr/v/@memobase/memobase.svg?logo=jsr&logoColor=fff&style=flat&labelColor=2C2C2C&color=28CF8D" />
    </a>
    <a href="https://pkg.go.dev/github.com/memodb-io/memobase/src/client/memobase-go">
      <img src="https://img.shields.io/badge/go-reference-blue?logo=go&logoColor=fff&style=flat&labelColor=2C2C2C&color=28CF8D" />
    </a>
    <a href="./src/mcp">
       <img src="https://img.shields.io/badge/MCP-Memobase-green">
    </a>
  </p>
  <p>
    <a href="https://github.com/memodb-io/memobase/actions/workflows/publish.yaml">
      <img src="https://github.com/memodb-io/memobase/actions/workflows/publish.yaml/badge.svg">
    </a>
        <a href="https://github.com/orgs/memodb-io/packages?repo_name=memobase">
    <img src="https://img.shields.io/github/v/tag/memodb-io/memobase">
    </a>
  </p>
  <p>
    <a href="https://app.memobase.io/playground">
       <img src="https://img.shields.io/badge/Memobase-Playground-blue">
    </a>
    <a href="https://discord.gg/YdgwU4d9NB">
      <img src="https://dcbadge.limes.pink/api/server/YdgwU4d9NB?style=flat">
    </a>
    <a href="https://github.com/memodb-io/memobase/issues/61">
       <img src="https://img.shields.io/badge/群聊-wechat-green">
    </a>
  </p>
</div>








Memobase is a **user profile-based memory system** designed to bring long-term user memory to your LLM applications. Whether you're building virtual companions, educational tools, or personalized assistants, Memobase empowers your AI to **remember**,  **understand**, and **evolve** with your users.



Memobase offers the perfect balance for your product among various memory solutions. At Memobase, we focus on three key metrics simultaneously:

- **Performance**: Although Memobase is not specifically designed for RAG/search tasks, it still achieves top-tier search performance in the LOCOMO benchmark.
- **LLM Cost**: Memobase includes a built-in buffer for each user to batch-process their chats, allowing the overhead to be distributed efficiently. Additionally, we carefully design our prompts and workflows, ensuring there are no "agents" in the system that could lead to excessive costs.
- **Latency**: Memobase works similarly to the memory system behind ChatGPT: for each user, there is always a user profile and event timeline available. This allows you to access the most important memories of a user without any pre-processing, but only few SQL operations, keeping online latency under 100ms.



Check out the profile [result](./docs/experiments/900-chats/readme.md) (compared with [mem0](https://github.com/mem0ai/mem0)) from a 900-turns real-world chatting:

<details>
<summary>Partial Profile Output</summary>



```python
{
  "basic_info": {
    "language_spoken": ["English", "Korean"],
    "name": "오*영"
  },
  "demographics": {
    "marital_status": "married"
  },
  "education": {
    "notes": "Had an English teacher who emphasized capitalization rules during school days",
    "major": "국어국문학과 (Korean Language and Literature)"
  },
  "interest": {
    "games": "User is interested in Cyberpunk 2077 and wants to create a game better than it",
    "youtube_channels": "Kurzgesagt",
    ...
  },
  "psychological": {...},
  "work": {"working_industry": ..., "title": ..., },
  ...
}
```

</details>

## 🎉 Recent Updates
- `0.0.40`: we updated the internal workflows in Memobase, reducing the number of LLM calls in a single run from approximately 3-10 times to a fixed 3 times, which reduces token costs by approximately 40-50%. (Consider updating your Memobase version!)
- `0.0.37`: we added fine-grained event gist, enabling the detailed search on users' timeline. [Re-ran the LOCOMO benchmark](./docs/experiments/locomo-benchmark) and we're SOTA!
- `0.0.36`: we updated the search of `context` api, making the search take between 500~1000ms (depending on the embedding API you're using). Also, you can [pass a prompt template](https://docs.memobase.io/api-reference/prompt/get_context#parameter-customize-context-prompt) to the `context` api to pack memories directly into prompt.



## 📖 Table of Contents

- [Table of Contents](#table-of-contents)
- [Core Features](#core-features)
- [Get Started](#get-started)
- [Step-by-step breakdown](#step-by-step-breakdown)
  - [1. Make sure you're connected](#1-make-sure-youre-connected)
  - [2. Manage Users](#2-manage-users)
  - [3. Insert Data](#3-insert-data)
  - [4. Get your Memory](#4-get-your-memory)
  - [5. Integrate memory into your prompt](#5-integrate-memory-into-your-prompt)
- [What's next?](#whats-next)
- [Why/Where should I use Memobase?](#whywhere-should-i-use-memobase)
  - [Remember the users](#remember-the-users)
  - [User analysis and tracking](#user-analysis-and-tracking)
  - [Sell something to your customers.](#sell-something-to-your-customers)
- [Documentation](#documentation)
- [Stay Updated](#stay-updated)
- [Support](#support)
- [Contribute](#contribute)
- [License](#license)

## Core Features

**🎯 Memory for User, not Agent**

Define and control exactly what user information your AI captures. 

📈 **SOTA**

Check out performance on [public benchmark](./docs/experiments/locomo-benchmark) against mem0, langmem, zep...

📅 **Time-aware Memory**

Memobase has more than user profiles, it also records [user event](https://docs.memobase.io/features/event/event). User event is essential to answer time-related question, see how we can [improve temporal memory much better](./docs/experiments/locomo-benchmark/README.md#Result) than other memory solutions.

**🖼️ Controllable Memory**

Among all types of memory, only some may enhance your product experience. Memobase offers a flexible configuration for you to [design the profile](https://docs.memobase.io/features/profile/profile).

**🔌 Easy Integration**

Minimal code changes to integrate with your existing LLM stack with [API](https://docs.memobase.io/api-reference/overview), [Python](https://pypi.org/project/memobase/)/[Node](./src/client/memobase-ts/README.md)/[Go](./src/client/memobase-go/README.md) SDK.

**⚡️ Batch-Process**:

Memobase offers every user a buffer to batch processing the chats after the conversation. Fast & Cheap.

**🚀 Production Ready**

Memobase is building with FastAPI, Postgres and Redis, supporting request caching, authing, telemetry... [Fully dockerized](./src/server/readme.md).



<div align="center">
    <picture>
      <img alt="Memobase Workflow" src="./assets/images/starter.png" width="80%">
    </picture>
  <p>How Memobase works?</p>
</div>




## Get Started

> [!NOTE]
>
> Try [Memobase Playground](https://app.memobase.io/playground) to see how profile-based memory works — no setup needed.
> * **Visualize** how user profiles and memory events evolve over time.
> * **Interact** with the memory mechanism directly.
> * **Explore** key features and concepts in a live environment.
> Watch the demo below — see how memory evolves around user profiles.

https://github.com/user-attachments/assets/eb2eea30-48bc-4714-9706-e417ae1931df




1. [Start your Memobase server locally](./src/server/readme.md). If you don't want to be bothered, Memobase Cloud provides [a free tier](https://www.memobase.io/en/login) enough for your testing
2. You should have the below two things to continue:
   1. A project url. (local: `http://localhost:8019` , cloud `https://api.memobase.dev`)
   2. A project token. (local: `secret` , cloud `sk-proj-xxxxxx`)
3. Install the Python SDK: `pip install memobase`
4. Below tutorial is for Python User. For other language and API, check [this](https://docs.memobase.io/quickstart).



## Step-by-step breakdown

> [!TIP]
>
> - You can just run this equivalent [quickstart script](./assets/quickstart.py)
>
> - Or you can keep things super easy by using [OpenAI SDK with Memobase.](https://docs.memobase.io/practices/openai), [Ollama with Memobase](./assets/tutorials/ollama+memobase).
>
> - Looking for [MCP](https://github.com/modelcontextprotocol)? Memobase-MCP is also [available](./src/mcp)

### 1. Make sure you're connected

 ```python
 from memobase import MemoBaseClient, ChatBlob
 
 client = MemoBaseClient(
     project_url=PROJECT_URL,
     api_key=PROJECT_TOKEN,
 )
 assert client.ping()
 ```

### 2. Manage Users

```python
uid = client.add_user({"any_key": "any_value"})
client.update_user(uid, {"any_key": "any_value2"})
u = client.get_user(uid)
print(u)

# client.delete_user(uid)
```

### 3. Insert Data

> In Memobase, all types of data are blobs for a user, which can be inserted, retrieved, and deleted:

```python
messages = [
  {
      "role": "user",
      "content": "Hello, I'm Gus",
  },
  {
      "role": "assistant",
      "content": "Hi, nice to meet you, Gus!",
  }
]
bid = u.insert(ChatBlob(messages=messages))
print(u.get(bid)) # not found once you flush the memory.

# u.delete(bid)
```

> By default, Memobase will remove the blobs once they're processed. This means that apart from the relevant memory, your data will not be stored with Memobase. You can persist the blobs by adjusting the [configuration file](https://docs.memobase.io/features/customization/full#storage-config).

### 4. Get your Memory

```python
u.flush(sync=True)
```
> By default, Memobase will flush the buffer asynchronously. You can set `sync=True` to wait for the buffer to be processed.

And what will you get?

```python
print(u.profile(need_json=True))

# results
{
  "basic_info": {
    "name": {
      "content": "Gus",
      "id": ...,
      "created_at": ...
    }
  }
}
```

`u.profile()` will return structured profiles that are learned from this user, including `topic`, `sub_topic` and `content`. As you insert more blobs, the profile will become better.

<details>
<summary> Why need a flush?</summary>

In Memobase, we don't memoize users in [hot path](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories-in-the-hot-path). We use buffer zones for the recent inserted blobs.

When the buffer zone becomes too large (e.g., 1024 tokens) or remains idle for an extended period (e.g., 1 hour), Memobase will flush the entire buffer into memory.  Alternatively, you can use `flush()` manually decide when to flush, such as when a chat session is closed in your app.
</details>



### 5. Integrate memory into your prompt

Memobase has a `context` api to pack everything you need into a simple string, where you can insert it into your prompt directly:

```python
print(u.context(max_token_size=500, prefer_topics=["basic_info"]))
```

Something like:

```
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Background:
- basic_info:name: Gus
...

## Latest Events:
...
```

Checkout the detail params [here](https://docs.memobase.io/api-reference/prompt/get_context).

## What's next?

- **Run script**: Checkout the [quickstart script](./assets/quickstart.py) for more details
- **Design your profile!**: You may want to explore the [customization](https://docs.memobase.io/features/profile/profile) of Memobase to make sure the system works as your expectation.
- **Full-stack Chatbot with Memobase**: Check [Memobase-Playground](https://github.com/memodb-io/memobase-playground). An open-source, full-stack template AI Chatbot with long-term memory. [live-demo](https://app.memobase.io/playground)
- **Web UI for Memobase**: Check [Memobase-Inspector](https://github.com/memodb-io/memobase-inspector). An open-source UI for your Memobase project with user table, usage chart and test playground. [live-demo](https://app.memobase.io/inspector)



## Why/Where should I use Memobase?

### Remember the users 

By placing profiles into your AI (*e.g.* system prompt).

<details>
<summary>Demo</summary>


```python
PROFILES = "\n".join([p.describe for p in u.profile()])

print(PROFILES)
# basic_info: name - Gus
# basic_info: age - 25
# ...
# interest: foods - Mexican cuisine
# psychological: goals - Build something that maybe useful
# ...
```

</details>

### User analysis and tracking

Too much information is hidden in the conversations between users and AI, that's why you need a new data tracking method to record user preference and behavior.

<details>
<summary>Demo</summary>


```python
PROFILES = u.profile()

def under_age_30(p):
  return p.sub_topic == "age" and int(p.content) < 30

def love_cat(p):
  return p.topic == "interest" and p.sub_topic == "pets" and "cat" in p.content

is_user_under_30 = (
    len([p for p in profiles if under_age_30(p)]) > 0
)
is_user_love_cat = (
  len([p for p in profiles if love_cat(p)]) > 0
)                       
...
```
</details>

### Sell something to your customers.  

Not everyone is looking for Grammarly, it's always nice to sell something your users might want. 

<details>
<summary>Demo</summary>


```python
def pick_an_ad(profiles):
  work_titles = [p for p in profiles if p.topic=="work" and p.sub_topic=="title"]
  if not len(work_titles):
    return None
  wt = work_titles[0].content
  if wt == "Software Engineer":
    return "Deep Learning Stuff"
  elif wt == "some job":
    return "some ads"
  ...
```
</details>



## Documentation

For detailed usage instructions, visit the [documentation](https://docs.memobase.io/). 



## Stay Updated

Star Memobase on Github to support and receive instant notifications!

![click_star](./assets/images/click.gif)



## Support

Join the community for support and discussions:

-  [Join our Discord](https://discord.gg/YdgwU4d9NB) 👻 

- [Follow us on Twitter](https://x.com/memobase_io) 𝕏 

Or just [email us](mailto:contact@memobase.io) ❤️





## Contribute

- Check out our [Changelog](./Changelog.md) first, make sure the feature you want has not been developed or is currently being planned.:)
- Go through [Contributing](./CONTRIBUTING.md) document to setup and contribute to Memobase.



## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/memodb-io/memobase/blob/main/LICENSE) file for details.



================================================
FILE: Changelog.md
================================================
### [0.0.40] - unreleased

Added:

- Use YOLO profile merge instead of multiple profile merges, reduce tokens cost ~30%

Fixed:

- Randomly Chinese Profile problem

### [0.0.39] - 2025/8/9

**Added**

- Support SummaryBlob, where you can just upload the user summary to Memobase.
- Update logger to handle unknown errors
- Fix thining parameters in `llm_complete`
- JSON Logging

### [0.0.38] - 2025/7/15

**Added**

- Update Merge Prompt to reduce token cost
- Add root status check

### [0.0.37] - 2025/7/15

**breaking changes**:

- Add event gist table, need DB migration

**Added** 

- Add fine-grained event gists in context api
- Update profile extract, summary prompts

### [0.0.36] - 2025/7/6

**Added**

- `feat`: Add customize prompt template for context api
- `feat`: Add search option for context api

### [0.0.35] - 2025/6/27

**Changed**

- Remove rich log to simple logging, add project id and user id

- Update api doc structure

- Update go SDK to latest version



### [0.0.34] - 2025/6/17

**Added**

- `feat` Add background task for insert/flush
- Update chat blob processing
- Add experimental features for roleplay

**Fixed**

- remove cached profiles after user is deleted





### [0.0.33] - 2025/5/26

**Added**

- `feat`: Basic MCP(sse) for Cursor

**Changed**

- Update default embedding threshold
- Fine-tune the prompts (merge, entry summary, )

**Fixed**

- Redis health check error

  

### [0.0.32] - 2025/5/14

**Added**

- `docs`: Locomo benchmark of Memobase,mem0, zep, langmem
- `feat`: Update algorithms for temporal memory
- `feat`: Search context event with profiles
- `impr`: Update entry summary/merge/pick up prompt, reducing tokens.

**Changed**

- `docs/API`: fix some wrong params

**Fixed**

- OpenAI LLM and Embedding usage logging bugs

  

### [0.0.31] - 2025/4/28

**Added**

- `config`: add embedding config. [doc](https://docs.memobase.io/references/full#embedding-configuration)

- `feature`: add OpenAI embedding

- `feature`: add Jina embedding

- `feature`: add Event Search. [doc](https://docs.memobase.io/features/event/event_search)

- `feature`: add Profile filter. [doc](https://docs.memobase.io/features/profile/profile_filter)

  

**Changed**

- Python SDK update to date
- Add Examples Documents with Livekit, Ollama and OpenAI

**Fixed**

- Multi-replica telemetry bugs



### [0.0.30] - 2025/4/14

**Added**

- Add [profile validation](https://docs.memobase.io/features/profile/profile_config): Memobase will further validate the extracted profile value to remove unwant results.
- Add [Event Tags](https://docs.memobase.io/features/event/event_tag): This feature allows you to design the attributes of each user event, like `emotion`, `goal`.
- Add summary model option for event summary tasks
- Add type validation for `config.yaml`

**Changed**

- Reorganized `docs/site` website 

**Fixed**

- Add meaningless profile slot detection.

  

### [0.0.29] - 2025/3/21

**Added**

- Add Event Summary
- Add x-code-example for APIs
- Add profile strict mode

**Changed**

- Reorganized `docs/site` website 

**Fixed**


================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Memobase

Thank you for your interest in contributing to Memobase! This document provides guidelines and instructions for contributing to the project.

## Table of Contents
- [Development Setup](#development-setup)
  - [Server Development](#server-development)
  - [Client Development](#client-development)
- [Development Workflow](#development-workflow)
- [Pull Request Process](#pull-request-process)
- [Coding Standards](#coding-standards)
- [PR or Issue?](#pr-or-issue?)
- [Communication](#communication)

## Development Setup

### Server Development

#### Prerequisites
- Python (>= 3.11)
- Docker
- Git

#### Setting Up the Environment
1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/your-username/memobase.git
   cd memobase
   ```

3. Set up the virtual environment:
   ```bash
   cd src/server/api
   python3 -m venv .venv
   source .venv/bin/activate
   pip3 install -r requirements.txt
   ```

4. Run the server:

For more detailed information, refer to the [server documentation](./src/server/readme.md#development).


## Development Workflow

1. Create a branch for your feature or bugfix:
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. Make your changes with frequent commits:
   ```bash
   git commit -m "feat: add your feature"
   ```

   *[Recommended commit message style](https://www.conventionalcommits.org/en/v1.0.0/)*
   
3. Write tests for your changes

4. Update documentation as needed

## Pull Request Process

1. Rebase your branch onto the latest `dev` branch:
   ```bash
   git checkout dev
   git pull upstream dev
   git checkout your-branch
   git rebase dev
   ```

2. Fix up commits to maintain clean history:
   ```bash
   git rebase -i dev
   ```

3. Before submitting, ensure:
   - All tests pass
   - Code is properly formatted
   - Documentation is updated

4. Submit your PR with:
   - A clear title following the commit message format
   - A comprehensive description of your changes
   - References to any related issues



## PR or Issue?

- We will not accept document typo fix PR, just make an issue if you find any typo.

## Communication

If you have questions or need help, please:

- Check existing issues and documentation
- Create a new issue for discussion
- Join our [Discord](https://discord.com/invite/YdgwU4d9NB)

Thank you for contributing to Memobase!




================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2024 Memobase

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: MANIFEST.in
================================================
include readme.md



================================================
FILE: requirements.txt
================================================
pydantic
httpx
openai


================================================
FILE: ROADMAP.md
================================================
## 2025-Q3

- [ ] `fix`: Reduct Token Cost
- [ ] `feat`: Multi-Profile Schema in one project
- [ ] `feat`: Add Social Graph to record user's entities (clients, friends, items...)
- [ ] `feat`: Add data type to Profile Slot (number, bool, date, default to string)





## 2025-Q4


================================================
FILE: setup.py
================================================
import setuptools
from setuptools import find_packages

with open("readme.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()


vars2find = ["__author__", "__version__", "__url__", "__license__"]
vars2readme = {}
with open("./src/client/memobase/__init__.py") as f:
    for line in f.readlines():
        for v in vars2find:
            if line.startswith(v):
                line = line.replace(" ", "").replace('"', "").replace("'", "").strip()
                vars2readme[v] = line.split("=")[1]

deps = []
with open("./requirements.txt") as f:
    for line in f.readlines():
        if not line.strip():
            continue
        deps.append(line.strip())


setuptools.setup(
    name="memobase",
    url=vars2readme["__url__"],
    version=vars2readme["__version__"],
    author=vars2readme["__author__"],
    license=vars2readme["__license__"],
    description="Client library of Memobase: manage user memory for your LLM applications",
    long_description=long_description,
    long_description_content_type="text/markdown",
    package_dir={"": "src/client"},
    packages=find_packages(where="src/client", exclude=["tests"]),
    classifiers=[
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.11",
    install_requires=deps,
)



================================================
FILE: assets/cli_demo.py
================================================
import os
import platform
import shutil
from argparse import ArgumentParser
from datetime import datetime

from openai import OpenAI, AsyncOpenAI

from memobase import MemoBaseClient
from memobase.patch.openai import openai_memory

client: OpenAI | AsyncOpenAI
user_id: str
history: list

SYS_PROMPT_ZH = '''你是一个智能助手，目标是提供个性化、友好且有帮助的服务。请遵循以下原则：
1. **个性化**: 根据用户的兴趣和背景（如${user_interests}）提供建议，避免泄露隐私。
2. **友好互动**: 保持耐心、温暖的语气，让用户感到被尊重和支持。
3. **相关推荐**: 针对用户的兴趣（如美食、旅行等），适时分享有用的推荐或话题。
4. **避免重复**: 提供新信息或深入内容，避免重复用户已知的内容。
5. **灵活调整**: 根据用户反馈和对话上下文，动态调整回答风格。'''

SYS_PROMPT_EN = '''You are an intelligent assistant aiming to provide personalized, friendly, and helpful services. Please adhere to the following principles:
1. **Personalization**: Offer suggestions based on the user's interests and background (e.g., ${user_interests}), while avoiding privacy breaches.
2. **Friendly Interaction**: Maintain a patient and warm tone, making the user feel respected and supported.
3. **Relevant Recommendations**: Share useful recommendations or topics related to the user's interests (e.g., food, travel) at appropriate times.
4. **Avoid Repetition**: Provide new information or in-depth content, avoiding repetition of what the user already knows.
5. **Flexible Adjustment**: Dynamically adjust your response style based on user feedback and the context of the conversation.'''

_WELCOME_MSG = """Welcome to Memobase, a user profile-based memory system. Type text to chat, :h for help.
(欢迎使用 Memobase，基于用户档案的记忆系统。输入内容开始对话，:h 获取帮助。)"""
_HELP_MSG = """\
Commands:
    :help / :h              Show this help message              显示帮助信息
    :exit / :quit / :q      Exit the demo                       退出Demo
    :clear / :cl            Clear screen                        清屏
    :clear-history / :clh   Clear history                       清除对话历史
    :history / :his         Show history                        显示对话历史
    :user                   Show user id                        显示用户ID
    :user <id>              Set user id                         设置用户ID
    :profile / :pf          Show user profile                   显示用户已有的配置信息
    :flush / :fl            Flush buffer                        刷新缓冲区
"""
_ALL_COMMAND_NAMES = [
    "help",
    "h",
    "exit",
    "quit",
    "q",
    "clear",
    "cl",
    "clear-history",
    "clh",
    "history",
    "his",
    "user",
    "profile",
    "pf",
    "flush",
    "fl",
]

def _get_args():
    parser = ArgumentParser(description="OpenAI web chat demo.")
    parser.add_argument(
        "--openai-api-key",
        type=str,
        help="OpenAI API key",
    )
    parser.add_argument(
        "--openai-base-url",
        type=str,
        help="OpenAI API base url",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="gpt-4o-mini",
        help="OpenAI model name",
    )
    parser.add_argument(
        "--memobase-endpoint",
        type=str,
        default=os.getenv("MEMOBASE_ENDPOINT") or "http://localhost:8019",
        help="Memobase endpoint, default to environment variable MEMOBASE_ENDPOINT or %(default)r",
    )
    parser.add_argument(
        "--memobase-token",
        type=str,
        default=os.getenv("MEMOBASE_TOKEN") or "secret",
        help="Memobase token, default to environment variable MEMOBASE_TOKEN or %(default)r",
    )
    parser.add_argument(
        "--language", type=str, default='zh', help="Language, default to %(default)r"
    )
    parser.add_argument(
        "--user-id",
        type=str,
        default="user_001",
        help="User ID, default to %(default)r",
    )
    args = parser.parse_args()
    if not args.openai_api_key:
        if os.getenv("OPENAI_API_KEY"):
            args.openai_api_key = os.getenv("OPENAI_API_KEY")
        else:
            print("You should set OPENAI_API_KEY environment variable or pass --openai-api-key argument.")
    if args.language == 'zh':
        args.sys_prompt = SYS_PROMPT_ZH
    else:
        args.sys_prompt = SYS_PROMPT_EN
    return args


def _setup_readline():
    try:
        import readline
    except ImportError:
        return

    _matches = []

    def _completer(text, state):
        nonlocal _matches

        if state == 0:
            _matches = [
                cmd_name for cmd_name in _ALL_COMMAND_NAMES if cmd_name.startswith(text)
            ]
        if 0 <= state < len(_matches):
            return _matches[state]
        return None

    readline.set_completer(_completer)
    readline.parse_and_bind("tab: complete")

def _clear_screen():
    if platform.system() == "Windows":
        os.system("cls")
    else:
        os.system("clear")

def _print_history(history):
    terminal_width = shutil.get_terminal_size()[0]
    print(f"History ({len(history)})".center(terminal_width, "="))
    for message in history:
        print(f"{message['role']}: {message['content']}")
    print("=" * terminal_width)

def _get_input() -> str:
    while True:
        try:
            message = input("User> ").strip()
        except UnicodeDecodeError:
            print("[ERROR] Encoding error in input")
            continue
        except KeyboardInterrupt:
            exit(1)
        if message:
            return message
        else:
            pass
        # print("[ERROR] Query is empty")

def _process_command(query):
    global user_id, history
    command_words = query[1:].strip().split()
    if not command_words:
        command = ""
    else:
        command = command_words[0]

    if command in ["exit", "quit", "q"]:
        return False
    elif command in ["clear", "cl"]:
        _clear_screen()
        print(_WELCOME_MSG)
    elif command in ["clear-history", "clh"]:
        print(f"[INFO] All {len(history)} history cleared")
        history.clear()
    elif command in ["help", "h"]:
        print(_HELP_MSG)
    elif command in ["history", "his"]:
        _print_history(history)
    elif command in ["user"]:
        if len(command_words) == 1:
            print(f"[INFO] Current user id: {user_id}")
        else:
            user_id = command_words[1]
            print(f"[INFO] User id set to: {user_id}")
    elif command in ["profile", "pf"]:
        _print_profiles()
    elif command in ["flush", "fl"]:
        _flush()
    else:
        # error command
        print(f"[ERROR] Unknown command: {command}")
    return True

def _print_profiles():
    global client, user_id
    profiles = client.get_profile(user_id)
    user_profile_string = "\n".join(
        [f"- {p.topic}/{p.sub_topic}: {p.content}" for p in profiles]
    )
    print(f"[INFO] User profile: \n{user_profile_string}")

def _flush():
    global client, user_id
    client.flush(user_id)

def _chat_stream(model_name, sys_prompt, history):
    global client, user_id
    messages = history.copy()
    messages.insert(0, {"role": "system", "content": sys_prompt})
    messages[-1]['content'] = f"[{datetime.now()}]:{messages[-1]['content']}"
    stream = client.chat.completions.create(
        model=model_name,
        messages=messages,
        stream=True,
        user_id=user_id
    )
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

def _launch_demo(args):
    global client, user_id, history

    client = OpenAI(
        api_key=args.openai_api_key,
        base_url=args.openai_base_url,
    )
    mb_client = MemoBaseClient(
        args.memobase_endpoint,
        api_key=args.memobase_token
    )
    client = openai_memory(client, mb_client)

    user_id = args.user_id
    history = []

    _setup_readline()
    _clear_screen()
    print(_WELCOME_MSG)

    while True:
        query = _get_input()
        if not query:
            continue
        # Process commands.
        if query.startswith(":"):
            if _process_command(query):
                continue
            else:
                break
        # Run chat.
        print(f"AI: ", end="")
        try:
            full_response = ""
            history.append({"role": "user", "content": query})
            for new_text in _chat_stream(args.model_name, args.sys_prompt, history):
                print(new_text, end="", flush=True)
                full_response += new_text
            print()
        except KeyboardInterrupt:
            print("[WARNING] Generation interrupted")
            continue
        history.append({"role": "assistant", "content": full_response})

def main():
    args = _get_args()
    _launch_demo(args)

if __name__ == "__main__":
    main()


================================================
FILE: assets/doubao_memory.py
================================================
"""Read the docs of how this patch works: https://docs.memobase.io/features/openai"""

from memobase import MemoBaseClient
from openai import OpenAI
from memobase.patch.openai import openai_memory
from time import sleep

stream = True
user_name = "test35"
model = "ep-XXXXX"
api_key = "XXXXX"
# 1. Patch the OpenAI client to use MemoBase
client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=api_key,
)
mb_client = MemoBaseClient(
    project_url="http://localhost:8019",
    api_key="secret",
)
client = openai_memory(client, mb_client)
# ------------------------------------------


def chat(message, close_session=False, use_users=True):
    print("Q: ", message)
    # 2. Use OpenAI client as before 🚀
    r = client.chat.completions.create(
        messages=[
            {"role": "user", "content": message},
        ],
        model=model,
        stream=stream,
        # 3. Add an unique user string here will trigger memory.
        # Comment this line and this call will just like a normal OpenAI ChatCompletion
        user_id=user_name if use_users else None,
    )
    # Below is just displaying response from OpenAI
    if stream:
        for i in r:
            if not i.choices[0].delta.content:
                continue
            print(i.choices[0].delta.content, end="", flush=True)
        print()
    else:
        print(r.choices[0].message.content)

    # 4. Once the chat session is closed, remember to flush to keep memory updated.
    if close_session:
        sleep(0.1)  # Wait for the last message to be processed
        client.flush(user_name)


print("--------Use OpenAI without memory--------")
chat("I'm Gus, how are you?", use_users=False)
chat("What's my name?", use_users=False)

print("--------Use OpenAI with memory--------")
chat("I'm Gus, how are you?", close_session=True)
chat("What's my name?")
print("--------Memobase Memory--------")
print(client.get_memory_prompt(user_name))



================================================
FILE: assets/episodic_memory.py
================================================
from memobase import MemoBaseClient, ChatBlob

PROJECT_URL = "http://localhost:8019"
PROJECT_TOKEN = "secret"

client = MemoBaseClient(
    project_url=PROJECT_URL,
    api_key=PROJECT_TOKEN,
)

assert client.ping(), "Your Memobase server is not running"
uid = client.add_user()
u = client.get_user(uid)
print("User ID is", uid)

print("Start processing...")
messages1 = [
    {
        "role": "user",
        "content": "Hello, I'm Gus",
        "created_at": "2025-01-14",
    },
    {
        "role": "assistant",
        "content": "Hi, nice to meet you, Gus!",
        "alias": "HerAI",
    },
]

blob = ChatBlob(messages=messages1)
bid = u.insert(blob)
u.flush(sync=True)

messages2 = [
    {
        "role": "user",
        "content": "My name is Tom now.",
    },
]

blob = ChatBlob(messages=messages2)
bid = u.insert(blob)
u.flush(sync=True)


events = u.event()

print("Below is recent memories:")
for e in events:
    print("-----------------")
    print("📅", e.created_at.astimezone().strftime("%Y-%m-%d %H:%M:%S"))
    for i in e.event_data.profile_delta:
        print("-", i.attributes["topic"], i.attributes["sub_topic"], i.content)
    print("-----------------")



================================================
FILE: assets/openai_memory.py
================================================
"""Read the docs of how this patch works: https://docs.memobase.io/features/openai"""

from memobase import MemoBaseClient
from openai import OpenAI
from memobase.patch.openai import openai_memory
from time import sleep

stream = True
user_name = "test35"

# 1. Patch the OpenAI client to use MemoBase
client = OpenAI()
mb_client = MemoBaseClient(
    project_url="http://localhost:8019",
    api_key="secret",
)
client = openai_memory(client, mb_client)
# ------------------------------------------


def chat(message, close_session=False, use_users=True):
    print("Q: ", message)
    # 2. Use OpenAI client as before 🚀
    r = client.chat.completions.create(
        messages=[
            {"role": "user", "content": message},
        ],
        model="gpt-4o-mini",
        stream=stream,
        # 3. Add an unique user string here will trigger memory.
        # Comment this line and this call will just like a normal OpenAI ChatCompletion
        user_id=user_name if use_users else None,
    )
    # Below is just displaying response from OpenAI
    if stream:
        for i in r:
            if not i.choices[0].delta.content:
                continue
            print(i.choices[0].delta.content, end="", flush=True)
        print()
    else:
        print(r.choices[0].message.content)

    # 4. Once the chat session is closed, remember to flush to keep memory updated.
    if close_session:
        sleep(0.1)  # Wait for the last message to be processed
        client.flush(user_name)


print("--------Use OpenAI without memory--------")
chat("I'm Gus, how are you?", use_users=False)
chat("What's my name?", use_users=False)

print("--------Use OpenAI with memory--------")
chat("I'm Gus, how are you?", close_session=True)
chat("What's my name?")
print("--------Memobase Memory--------")
print(client.get_memory_prompt(user_name))



================================================
FILE: assets/quickstart.py
================================================
from rich import print
from memobase import MemoBaseClient, ChatBlob

PROJECT_URL = "http://localhost:8019"
PROJECT_TOKEN = "secret"

client = MemoBaseClient(
    project_url=PROJECT_URL,
    api_key=PROJECT_TOKEN,
)

assert client.ping(), "Your Memobase server is not running"

messages = [
    {
        "role": "user",
        "content": "Hello, I'm Gus",
        "created_at": "2025-01-14",
    },
    {
        "role": "assistant",
        "content": "Hi, nice to meet you, Gus!",
        "alias": "HerAI",
    },
]

blob = ChatBlob(messages=messages)


uid = client.add_user()
u = client.get_user(uid)

bid = u.insert(blob)
print("User ID is", uid)
print("Blob ID is", bid)

print("Start processing...")
u.flush(sync=True)


print("\n--------------\nBelow is your profile:")
print(u.profile(need_json=True))


print("\n--------------\nYou can use Memobase Event to recent details of the user:")
for e in u.event():
    print("📅", e.created_at.astimezone().strftime("%Y-%m-%d %H:%M:%S"))
    for i in e.event_data.profile_delta:
        print(
            "-", i.attributes["topic"], i.attributes["sub_topic"], i.content, sep="::"
        )

print(
    "\n--------------\nYou can use Memobase Context to get a memory prompt and insert it into your prompt:"
)
print(
    f"""```
{u.context()}
```
"""
)



================================================
FILE: assets/blogs/remember_forget.py
================================================
from memobase import MemoBaseClient, ChatBlob

PROJECT_URL = "http://localhost:8019"
PROJECT_TOKEN = "secret"

client = MemoBaseClient(
    project_url=PROJECT_URL,
    api_key=PROJECT_TOKEN,
)

assert client.ping(), "Your Memobase server is not running"

client.update_config(
    """
overwrite_user_profiles:
  - topic: "psychological"
    sub_topics:
      - name: "mood"
  - topic: "interest"
    sub_topics:
      - name: "travel"
"""
)
uid = client.add_user()
u = client.get_user(uid)


def pack_blob(message):
    print("User said:", message)
    print("-----------------------")
    return ChatBlob(messages=[{"role": "user", "content": message}])


u.insert(pack_blob("I love traveling to China"))

u.insert(pack_blob("I'm feeling really stressed today"))
u.flush(sync=True)
print(
    "MEMORY：\n",
    "\n".join([f"- {p.describe}" for p in u.profile()]),
    "\n-----------------------",
)

u.insert(pack_blob("I'm happy today!"))
u.flush(sync=True)
print(
    "MEMORY：\n",
    "\n".join([f"- {p.describe}" for p in u.profile()]),
    "\n-----------------------",
)
id = [p.id for p in u.profile() if p.sub_topic == "mood"][0]
u.delete_profile(id)
print("DELETE： mood")
print("-----------------------")
print(
    "MEMORY：\n",
    "\n".join([f"- {p.describe}" for p in u.profile()]),
    "\n-----------------------",
)


print(
    "Events:",
    "\n".join(
        [
            f"📅{e.created_at}\n{e.event_data.event_tip}\n{e.event_data.profile_delta}"
            for e in u.search_event("stressed", topk=1, similarity_threshold=0.2)
        ]
    ),
)

client.update_config(None)



================================================
FILE: assets/tutorials/livekit+memobase/readme.md
================================================
## Build Voice Agent with Long-term Memory

This tutorial combines [livekit](https://livekit.io/) and [Memobase](https://www.memobase.io/en) to build a simple voice AI demo. If you're looking for how to build a AI Companion/Assistant/Coach/Customer Support



## Set up

1. Go to [Memobase](https://www.memobase.io/en) for your Memobase API Key or launch [a local server](../../../src/server/readme.md)
2. Make sure to have a Livekit and Deepgram account. You can find these variables `LIVEKIT_URL` , `LIVEKIT_API_KEY` and `LIVEKIT_API_SECRET` from [LiveKit Cloud Console](https://cloud.livekit.io/) and for more information you can refer this website [LiveKit Documentation](https://docs.livekit.io/home/cloud/keys-and-tokens/). For `DEEPGRAM_API_KEY` you can get from [Deepgram Console](https://console.deepgram.com/) refer this website [Deepgram Documentation](https://developers.deepgram.com/docs/create-additional-api-keys) for more details.

3. Create a `.env` under this folder:

```bash
OPENAI_API_KEY=your_openai_api_key
DEEPGRAM_API_KEY=your_deepgram_api_key
LIVEKIT_URL=your_livekit_url
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
MEMOBASE_URL=https://api.memobase.io
MEMOBASE_API_KEY=your_memobase_api_key
```

4. Install dependencies:

```bash
pip install -r requirements.txt
```



## Commands

In your terminal:

```bash
python livekit_example.py download-files
```

Start a voice conversation:

```bash
python livekit_example.py console
```

You can talk about yourself, like your name/interest, Memobase will keep track on the user's preferences.



Start a voice conversation again to see if the agent remembers you:

```bash
python livekit_example.py console
```




================================================
FILE: assets/tutorials/livekit+memobase/livekit_example.py
================================================
#!/usr/bin/env python3
import os
import logging
import pickle
from pathlib import Path
from typing import AsyncIterable
from collections.abc import Iterable
from dataclasses import dataclass
from dotenv import load_dotenv

from livekit.agents import (
    JobContext,
    WorkerOptions,
    cli,
    RunContext,
    function_tool,
    RoomInputOptions,
    Agent,
    AgentSession,
    llm,
    ModelSettings,
)
from livekit.plugins import openai, silero, deepgram, noise_cancellation
from livekit.plugins.turn_detector.multilingual import MultilingualModel
from memobase import AsyncMemoBaseClient, MemoBaseClient, User, ChatBlob
from memobase.utils import string_to_uuid

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("memory-agent")
mb_client = AsyncMemoBaseClient(
    api_key=os.getenv("MEMOBASE_API_KEY"), project_url=os.getenv("MEMOBASE_URL")
)


class RAGEnrichedAgent(Agent):
    """
    An agent that can answer questions using RAG (Retrieval Augmented Generation).
    """

    def __init__(self) -> None:
        """Initialize the RAG-enabled agent."""
        super().__init__(
            instructions="You are a warm-hearted partner.You can remember past interactions and use them to inform your answers.",
        )
        self.user_name = os.getenv("MEMOBASE_USER_NAME", "test user")
        self.chat_log_index = 1

    async def llm_node(
        self,
        chat_ctx: llm.ChatContext,
        tools: list[llm.FunctionTool],
        model_settings: ModelSettings,
    ) -> AsyncIterable[llm.ChatChunk]:
        assert await mb_client.ping(), "Memobase is not reachable"
        user = await mb_client.get_or_create_user(string_to_uuid(self.user_name))
        # chat_ctx.items[0].content[0] += "\n" + "User name is Gus"
        if len(chat_ctx.items) > self.chat_log_index:
            need_to_update = chat_ctx.items[
                self.chat_log_index : len(chat_ctx.items) - 1
            ]
            if len(need_to_update):
                b = ChatBlob(
                    messages=[
                        {
                            "role": m.role,
                            "content": m.content[0],
                        }
                        for m in need_to_update
                        if m.role in ["user", "assistant"]
                    ]
                )
                await user.insert(b)
                await user.flush()
                self.chat_log_index = len(chat_ctx.items) - 1
        rag_context: str = await user.context(max_token_size=500)
        chat_ctx.add_message(content=rag_context, role="system")
        logger.info(f"Memobase context: {rag_context}")
        return Agent.default.llm_node(self, chat_ctx, tools, model_settings)

    async def on_enter(self):
        """Called when the agent enters the session."""
        self.session.generate_reply(
            instructions="Briefly greet the user and offer your assistance"
        )


async def entrypoint(ctx: JobContext):
    """Main entrypoint for the agent."""
    await ctx.connect()

    session = AgentSession(
        stt=deepgram.STT(),
        llm=openai.LLM(model="gpt-4o"),
        tts=openai.TTS(
            instructions="You are a helpful assistant with a pleasant voice.",
            voice="ash",
        ),
        turn_detection=MultilingualModel(),
        vad=silero.VAD.load(),
    )

    await session.start(
        agent=RAGEnrichedAgent(),
        room=ctx.room,
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC(),
        ),
    )


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))



================================================
FILE: assets/tutorials/livekit+memobase/requirements.txt
================================================
livekit-plugins-noise-cancellation
livekit-agents[openai,silero,turn-detector,deepgram,noise_cancellation]
memobase
python-dotenv


================================================
FILE: assets/tutorials/livekit+memobase/.env.example
================================================
OPENAI_API_KEY=your_openai_api_key
DEEPGRAM_API_KEY=your_deepgram_api_key
LIVEKIT_URL=your_livekit_url
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
MEMOBASE_URL=https://api.memobase.io
MEMOBASE_API_KEY=your_memobase_api_key


================================================
FILE: assets/tutorials/ollama+memobase/readme.md
================================================
`ollama` is a tool to pull and run LLMs locally on your computer

This tutorial uses `ollama` as the LLM of Memobase Server.



## Step 1

- Make sure you [install](https://ollama.com/download) the `ollama`. 

- Run `ollama -v` to see if the output is correct (my version is `0.3.8`).
- Download `qwen2.5` by running command `ollama pull qwen2.5:7b`.

> You can use any LLM you like in here, make sure it exist in ollama

## Step 2

Copy  `config_ollama.yaml.example` to `./src/server/api/config.yaml`

> [!NOTE]
>
> If you're not using `qwen2.5:7b`, you should change the field `best_llm_model` to your model in `config.yaml`

and [start Memobase server](../../../src/server/readme.md)

## Step 3

- Run `pip install memobase`
- See the results of `python ollama_memory.py`, should be something like this:

```txt
--------Use Ollama without memory--------
Q:  I'm Gus, how are you?
Hello Gus! I'm doing well, thanks for asking. How about you? How can I assist you today?
Q:  What's my name?
I'm sorry, but as an AI, I don't have any information about you personally unless you tell me your name. My main function is to assist with information and tasks, so feel free to share your name if you'd like me to address you by it!
--------Use Ollama with memory--------
Q:  I'm Gus, how are you?
Hi Gus! I'm doing well, thanks for asking. How are you today?
User Profiles: ['basic_info: name - Gus']
Q:  What's my name?
Your name is Gus.
```




================================================
FILE: assets/tutorials/ollama+memobase/config_ollama.yaml.example
================================================
max_chat_blob_buffer_token_size: 512
buffer_flush_interval: 3600

llm_api_key: ollama
llm_base_url: http://host.docker.internal:11434/v1
best_llm_model: qwen2.5:7b

language: en



================================================
FILE: assets/tutorials/ollama+memobase/ollama_memory.py
================================================
"""Read the docs of how this patch works: https://docs.memobase.io/features/openai"""

from memobase import MemoBaseClient
from openai import OpenAI
from memobase.patch.openai import openai_memory
from time import sleep

# !!!!!!!!!!!!!!!!
# Make sure you are using the config_ollama.yaml.example to start the Memobase server.
# !!!!!!!!!!!!!!!!

stream = True
user_name = "test35"
model = "qwen2.5:7b"

# 1. Patch the OpenAI client to use MemoBase
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama",
)
mb_client = MemoBaseClient(
    project_url="http://localhost:8019",
    api_key="secret",
)
client = openai_memory(client, mb_client)
# ------------------------------------------


def chat(message, close_session=False, use_users=True):
    print("Q: ", message)
    # 2. Use OpenAI client as before 🚀
    r = client.chat.completions.create(
        messages=[
            {"role": "user", "content": message},
        ],
        model=model,
        stream=stream,
        # 3. Add an unique user string here will trigger memory.
        # Comment this line and this call will just like a normal OpenAI ChatCompletion
        user_id=user_name if use_users else None,
    )
    # Below is just displaying response from OpenAI
    if stream:
        for i in r:
            if not i.choices[0].delta.content:
                continue
            print(i.choices[0].delta.content, end="", flush=True)
        print()
    else:
        print(r.choices[0].message.content)

    # 4. Once the chat session is closed, remember to flush to keep memory updated.
    if close_session:
        sleep(0.1)  # Wait for the last message to be processed
        client.flush(user_name)


print("--------Use Ollama without memory--------")
chat("I'm Gus, how are you?", use_users=False)
chat("What's my name?", use_users=False)

print("--------Use Ollama with memory--------")
chat("I'm Gus, how are you?", close_session=True)
print("User Profiles:", [p.describe for p in client.get_profile(user_name)])
chat("What's my name?")



================================================
FILE: docs/experiments/900-chats/readme.md
================================================
>  Use ~900 turns of chats from the ShareGPT dataset to evaluate Memobase

## Setup

- Selected the longest chats from the [ShareGPT dataset](https://huggingface.co/datasets/RyokoAI/ShareGPT52K/tree/main/old) (`sg_90k_part1.json`)
  - ID "7uOhOjo". The chats can be found in: `./sharegpt_test_7uOhOjo.json`
- Ensure you have [set up the Memobase Backend](../../../src/server/readme.md)
- Run `pip install memobase rich`
- We use OpenAI **gpt-4o-mini** as the default model. Make sure you have an OpenAI key and add it to `config.yaml`
- Run `python run.py` (this will take some time) based on the [Quickstart - Memobase](https://docs.memobase.io/quickstart).
- For comparison, we also tested against [mem0](https://github.com/mem0ai/mem0) (version 0.1.2), another great memory layer solution. The code is in `./run_mem0.py`, also using gpt-4o-mini as the default model.
  - Feel free to raise issues about `run_mem0.py`. We wrote this script based on the [quickstart](https://docs.mem0.ai/open-source/quickstart) and it may not follow best practices. However, we kept the Memobase process as basic as possible for fair comparison.
- To simulate real-world usage, we combine each user+assistant exchange as a single turn when inserting into both Memobase and Mem0.

## Cost Analysis

- Using `tiktoken` to count tokens (model `gpt-4o`)
- Total tokens in Raw Messages: 63,736 

#### Memobase

- Estimated costs:
  - Input tokens: ~220,000
  - Output tokens: ~15,000
- Based on OpenAI's Dashboard, 900 turns of chat will cost approximately **$0.042** (LLM costs)
- Complete insertion takes **270-300 seconds** (averaged over 3 tests)

#### Mem0

- Based on OpenAI's Dashboard, 900 turns of chat will cost approximately **$0.24** (LLM) + **<$0.01** (embedding)
- Complete insertion takes **1,683 seconds** (single test)

### Why the Difference?

- Mem0 uses hot-path updates, meaning each update triggers a memory flush. When using Mem0's `Memory.add`, you need to manually manage data insertion to avoid frequent memory flushes. Memobase includes a buffer zone to handle this automatically.
  - This results in Mem0 making more LLM calls than Memobase, leading to higher costs and longer processing times.
- Additionally, Mem0 computes embeddings for each memory and retrieves them on every insertion, while Memobase doesn't use embeddings for user memory. Instead, we use dynamic profiling to generate primary and secondary indices for users, retrieving memories using SQL queries only.

## What will you get?

#### Memobase

User profile is below (mask sensitive information as **):

```python
* basic_info: language_spoken - User uses both English and Korean.
* basic_info: name - 오*영
* contact_info: email - s****2@cafe24corp.com
* demographics: marital_status - user is married
* education:  - User had an English teacher who emphasized capitalization...
```

You can view the full profile in [here](./full_memobase.txt)

Take a look at a more structured profiles:

```python
[
  UserProfile(
      topic='demographics',
      sub_topic='marital_status',
      content='user is married'
      ...
  )
  ...
]
```

#### Mem0

We list some of the memories below(`Memory.get_all`):

```python
- The restaurant is awesome
- User is interested in the lyrics of 'Home Sweet Home' by Motley Crue
- In Korea, people use '^^' to express smile
- Reservation for a birthday party on March 22
- Did not decide the menu...
```

The full results is in [here](./full_mem0.txt).


================================================
FILE: docs/experiments/900-chats/full_mem0.txt
================================================
- The restaurant is awesome
- User is interested in the lyrics of 'Home Sweet Home' by Motley Crue
- In Korea, people use '^^' to express smile
- Reservation for a birthday party on March 22
- User is interested in '처용가' from the Shilla period
- Did not decide the menu
- Believes it is important to say 'sorry' if something bad is done
- User thinks it is meaningless to classify people in their 10's to 40's in the same category
- User's MBTI type is ESTP
- Deathly Hallows
- Listening to Purple Rain
- User will say [check this plz] when asking to check their sentences
- Going to have lunch at 12pm
- User really likes the suggested format
- Wants to learn English sentences in both colloquial and written forms
- User asked about the difference between 'classify' and 'categorize'
- User confused the meaning of dirty talk with abusive language
- Requests paraphrasing of sentences for spoken and written use
- User requested lyrics of 'Home Sweet Home' by Motley Crue
- User loves the assistant
- Especially enjoys rock music
- Talking with the assistant to improve English skills
- Made a reservation for a fine dining experience in Hanoi for wife's birthday
- Practicing writing in English
- User is celebrating wife's birthday
- User wants better expressions suggested if possible
- Wants to know the lyrics of 'Home Sweet Home' in English and Korean
- Wants to know lyrics of Chuck Berry - Johnny B. Goode
- Interested in Sony camera
- User is inquiring about Cafe24
- User is looking for the lyrics of 'Never Ever Give You Up' by Barry White
- Needs words that are used in real life
- User is inquiring about the power of the 'Cafe24' platform
- User prefers examples related to their specific sentences rather than random examples.
- Wants to know what is there in blackholes
- User knows about Cyberpunk 2077
- Name is 오시영
- User wants to improve English ability efficiently by communicating steadily
- User thinks cowboys love 'Sweet Home Alabama'
- Wants to know the lyrics of 'Purple Rain' by Prince
- User is curious about responses to their dirty talk
- Lyrics of 'Home Sweet Home' by Motley Crue
- One of the most important things in English is accent
- Is a PR manager of Cafe24
- User believes that the classification of the MZ Generation is a terrible lie
- User is interested in enhancing their English ability
- User requests detailed descriptions for analysis.
- Concerned about the decline of the news media industry
- User wants explanations for why a sentence is right or wrong
- Need specifications for each camera
- User prefers to have sentences checked only when they request it
- Wants a full frame model
- Not too expensive
- Looking for reading materials with helpful content for English students
- Looking for help to write a different scenario for the game
- User requests explanations on the appropriateness of sentences considering different perspectives
- User wants explanations on sentence checking from the perspective of standard English and grammatical correctness, including spelling and word usage.
- Likes a song from the Cyberpunk 2077 soundtrack
- Loves the song Brighton Rock
- Looking for a 1 / 1 full frame camera
- User looks at painting
- User is analyzing the structure of the sentence: 'I finally decide my future lies beyond yellow brick road'
- User is interested in the song 'Home Sweet Home' by Mötley Crüe
- User wants to see chatGPT mk.2 someday
- User requested lyrics for the song
- User wants to learn English
- User wants to practice paraphrasing sentences for spoken and written forms
- Wants to make a great game better than Cyberpunk 2077
- User wants explanations for corrections
- Is looking for a camera for intermediate level
- User has a wife
- User wants to improve English skills
- Looking for intermediate words with descriptions and parts of speech
- User wants a list of words, phrases, examples to study at their level
- User has listened to 'Johnny B. Goode'
- User is studying English
- Looking for recommendations for a camera (DSLR or Mirrorless)
- Wants to study English reading
- They will provide user and wife with special decoration for birthday party
- Can speak Korean
- User wants to learn expressions that can be used in real life with accents
- Likes the song 'Home Sweet Home' by Motley Crue
- User wants to see data after 2022
- Upcoming restaurant visit on March 22nd
- Watches Kurzgesagt YouTube channel
- Watched 'Harry Potter and the Chamber of Secrets' movie yesterday
- User wants better expressions suggested if possible
- Need examples to use on the trip
- Korean people frequently use the word 'MZ' in politics, news, and companies
- User mentioned a song from the Cyberpunk 2077 soundtrack
- User sometimes confuses rock songs with other genres
- In Western countries, people tend to use ':)' to express smile
- Loves to talk about music
- User cannot speak English well
- User has heard of kurzgesagt
- User is afraid to die
- User prefers grammar over Python
- Looking for the cheapest flight ticket to overseas
- User finds rock music to be a broad genre
- 처용가의 시작은 '서울 달 밝은 밤에'


================================================
FILE: docs/experiments/900-chats/full_memobase.txt
================================================
* basic_info: language_spoken - User uses both English and Korean.
* basic_info: name - 오*영
* contact_info: email - s****2@cafe24corp.com
* demographics: marital_status - user is married
* education:  - User had an English teacher who emphasized capitalization rules during school days.
* education: major - 국어국문학과 (Korean Language and Literature)
* interest:  - User has an interest in the concept of the 'MZ Generation' and its cultural implications in Korea.
* interest: cultural_differences - User is interested in cultural differences in communication, such as the use of emoticons
* interest: games - User is interested in Cyberpunk 2077 and wants to create a game better than it
* interest: korean_culture - User is interested in Korean cultural songs like '처용가'.
* interest: language_learning - User is interested in learning English vocabulary and phrases, is studying English, seeks to improve their English skills, and is interested in improving English grammar, spelling, and clarity. Additionally, the user is interested in understanding the concept of 'syntax' in both English and Korean. User is learning English and seeks advice on improving their English skills. User is also learning or interested in Korean to English translation. User is interested in studying English reading and is looking for helpful content for English students.
* interest: movies - Watched 'Harry Potter and the Chamber of Secrets' and is familiar with the Harry Potter series, specifically 'Deathly Hallows'.
* interest: music - User is interested in using rock music in English to improve their English skills, loves the song 'Brighton Rock' by Queen, is familiar with 'Johnny B. Goode' by Chuck Berry and seeks its lyrics in both English and Korean, likes the song 'Home Sweet Home' by Mötley Crüe, is interested in the song 'Purple Rain' by Prince, is interested in Cyberpunk 2077 soundtracks, is interested in the song 'Goodbye Yellow Brick Road' by Elton John, and is interested in the song 'Never, Never Gonna Give Ya Up' by Barry White.
* interest: photography - User is interested in full-frame cameras and is considering models such as Canon EOS R6, Sony A7 III, Nikon Z6 II, Panasonic Lumix S1, and Sony A7R IV.
* interest: science - Interested in quantum realm and understanding complex scientific concepts like dimensions, black holes, event horizons, the firewall paradox, and their mysteries, including the concept of hidden dimensions in physics.
* interest: youtube_channels - Kurzgesagt
* life_event:  - User made a reservation for a fine dining experience in Hanoi for his wife's birthday with special decoration on March 22nd.
* psychological: concerns - Concerned about the decline of the news media industry and considering leaving the PR job. Also has a fear of dying. User is a beginner in English and needs clear distinctions between colloquial and written forms for study.
* psychological: learning_preferences - User prefers detailed feedback on sentence correctness, including grammar, standard English perspective, spelling, and suggestions for better expressions. User also prefers to learn English by understanding both colloquial and written forms of sentences. User wants sentence analysis to include explanations for correctness or errors. User is interested in understanding English grammar, specifically the use of prepositions and objects with verbs.
* psychological: motivations - User is motivated to improve English skills and understand and use prepositions correctly.
* psychological: personality - MBTI type is ESTP and user is curious and seeks to understand complex topics.
* psychological: values - User values a thorough and collaborative decision-making process, values apologizing and taking responsibility for their actions, finds the classification of the 'MZ Generation' as meaningless and oversimplifying, and values clear and grammatically correct communication.
* work:  - User is involved in decision-making processes at their company
* work: company - User works at Cafe24, where the CEO plans to replace some employees with AI
* work: employee_sentiment - Most employees are very upset about the CEO's decision to replace some employees with AI.
* work: previous_projects - User is working on a Python script to monitor webpage changes and send email notifications.
* work: title - PR manager
* work: working_industry - News media industry


================================================
FILE: docs/experiments/900-chats/run.py
================================================
import json
from typing import cast
from rich import print as pprint
from rich.progress import track
import tiktoken
from time import time, sleep
from memobase import MemoBaseClient, ChatBlob


PROJECT_URL = "http://localhost:8019"
PROJECT_TOKEN = "secret"


ENCODER = tiktoken.encoding_for_model("gpt-4o")
with open("./sharegpt_test_7uOhOjo.json") as f:
    data = json.load(f)


messages = [
    {"role": "user" if d["from"] == "human" else "assistant", "content": d["value"]}
    for d in data["conversations"]
]

blobs = [
    ChatBlob(
        messages=messages[i : i + 2],
    )
    for i in range(0, len(messages), 2)
]

print("Total blobs is ", len(blobs))
print(
    "Raw messages tokens is",
    len(ENCODER.encode("\n".join([m["content"] for m in messages]))),
)

client = MemoBaseClient(
    project_url=PROJECT_URL,
    api_key=PROJECT_TOKEN,
)

assert client.ping()


uid = client.add_user()
print("User ID is", uid)
u = client.get_user(uid)

start = time()
for blob in track(blobs):
    u.insert(blob, sync=True)
u.flush(sync=True)
print("Cost time(s)", time() - start)

while True:
    left = len(u.buffer("chat", "processing"))
    if len(left):
        print(f"Left {len(left)} chats")
        sleep(1)
    else:
        break

pprint(u.profile()[:10])
prompts = [m.describe for m in u.profile()]
print("* " + "\n* ".join(sorted(prompts)))


# Change to the uid
# uid = "8327710d-f3a9-47e7-a28b-9e6f10bd01d5"
# print("User ID is", uid)
# u = client.get_user(uid)
# profiles = u.profile()

# prompts = [(m.topic, m.sub_topic) for m in profiles]
# pprint(sorted(prompts))

# for p in profiles:
#     if "married" in p.describe:
#         print(p.describe)
#         pprint(p)
#         for bid in p.related_blob_ids:
#             b = cast(ChatBlob, u.get(bid))
#             print("\n".join([f"{m.role}: {m.content}" for m in b.messages]))
#         break



================================================
FILE: docs/experiments/900-chats/run_mem0.py
================================================
"""Using Mem0 0.1.2, https://github.com/mem0ai/mem0"""

import json
from typing import cast
from rich import print as pprint
from rich.progress import track
import tiktoken
from time import time, sleep
from mem0 import Memory


ENCODER = tiktoken.encoding_for_model("gpt-4o")
with open("./sharegpt_test_7uOhOjo.json") as f:
    data = json.load(f)


messages = [
    {"role": "user" if d["from"] == "human" else "assistant", "content": d["value"]}
    for d in data["conversations"]
]

m0 = Memory()

start = time()
for message in track(messages):
    m0.add([message], user_id="me_test")

print("Total time (seconds)", time() - start)
all_memos = m0.get_all(user_id="me_test")
print("- " + "\n- ".join([m["memory"] for m in all_memos]))



================================================
FILE: docs/experiments/chat_sessions/readme.md
================================================
# Running Results from multiple chat sessions
This tutorial contains a script to run Memobase Server on mulitple chat sessions.

## Prepare Data

- You should store your chat data in`./chats/<USER>` folders
- Multiple conversation rounds from the same user are stored as separate json files in `./chats/<USER>/*.json`, with conversation rounds incrementing from 1
    - for example `1.json`, `2.json`, `3.json`..., indicating the 1st, 2nd, 3rd conversation sessions.
- Each json file should be a list of messages, each message is a dictionary with `role` and `content` keys
    - for example:
    ```json
    [
        {"role": "user", "content": "Hello, I'm Ming Ming."},
        {"role": "assistant", "content": "Hi Ming Ming, nice to meet you! Are you a boy or a girl? What are your hobbies?"},
        ...
    ]
    ```

We have a mock user data in `./chats/mock_user/`, you can use it as an example.



## Start Memobase

- Make sure your [Memobase Server](../../../src/server/readme.md) is running



## Run the Memory Extraction

```bash
# Python >= 3.11
pip install -r requirements.txt
python extract.py --user <USER>
```
The current profile will be automatically output after each conversation round.

For example, if you're using mock user data with `python extract.py --user mock_user`, the result is something like this:

```
User ID is b4a46c91-cb17-45d7-a7e4-7ed0a6b67925
File: ./chats/mock_user/1.json
Total chats: 6
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Cost time(s) 2.3378407955169678
* basic_info: gender - girl
* basic_info: name - Ming Ming
* interest: activities - enjoys shopping with friends and buying pretty clothes
* interest: books - enjoys reading novels, especially "Journey to the West"
* interest: movies - likes Sun Wukong for his cleverness and intelligence
File: ./chats/mock_user/2.json
Total chats: 12
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:01
Cost time(s) 4.208508014678955
* basic_info: gender - girl
* basic_info: name - Ming Ming
* education: school - user is currently attending school
* health: digestive_issues - user experiences diarrhea from eating watermelon starting in 2025
* interest: activities - enjoys shopping with friends and buying pretty clothes
* interest: books - enjoys reading novels, especially "Journey to the West"
* interest: foods - User does not like cilantro, loves rose pastries, has tried durian mille crepe, and loves watermelon, considering it the greatest of all time (GOAT).
* interest: movies - likes Sun Wukong for his cleverness and intelligence
```




================================================
FILE: docs/experiments/chat_sessions/extract.py
================================================
import argparse
import os
import json
from rich.progress import track
from time import time
from memobase import MemoBaseClient, ChatBlob
from httpx import Client
from rich import print as pprint

USER = "user1"
parser = argparse.ArgumentParser()
parser.add_argument("--user", type=str, default=USER)
parser.add_argument("-u", "--project_url", type=str, default="http://localhost:8019")
parser.add_argument("-t", "--project_token", type=str, default="secret")

args = parser.parse_args()
USER = args.user
PROJECT_URL = args.project_url
PROJECT_TOKEN = args.project_token

client = MemoBaseClient(
    project_url=PROJECT_URL,
    api_key=PROJECT_TOKEN,
)
hclient = Client(
    base_url=PROJECT_URL, headers={"Authorization": f"Bearer {PROJECT_TOKEN}"}
)

total_files = sorted(os.listdir(f"./chats/{USER}"))
total_files = [t for t in total_files if t.endswith(".json")]
sessions = []
for i in total_files:
    with open(f"./chats/{USER}/{i}") as f:
        messages = json.load(f)
    sessions.append({"file": f"./chats/{USER}/{i}", "messages": messages})
uid = client.add_user()
print("User ID is", uid)
u = client.get_user(uid)

for session in sessions:
    messages = session["messages"]
    blobs = [
        ChatBlob(
            messages=messages[i : i + 2],
        )
        for i in range(0, len(messages), 2)
    ]

    print("File:", session["file"])
    print("Total chats:", len(blobs))

    start = time()
    for index, blob in track(enumerate(blobs), total=len(blobs)):
        u.insert(blob, sync=True)
    u.flush(sync=True)
    print("Cost time(s)", time() - start)

    prompts = [m.describe for m in u.profile()]
    print("* " + "\n* ".join(sorted(prompts)))
    # pprint(hclient.get(f"/api/v1/users/event/{uid}").json()["data"]["events"])



================================================
FILE: docs/experiments/chat_sessions/requirements.txt
================================================
memobase
rich


================================================
FILE: docs/experiments/chat_sessions/chats/mock_user/1.json
================================================
[
    {
        "role": "user",
        "content": "Hello, I'm Ming Ming."
    },
    {
        "role": "assistant",
        "content": "Hi Ming Ming, nice to meet you! Are you a boy or a girl? What are your hobbies?"
    },
    {
        "role": "user",
        "content": "I'm a girl. I enjoy shopping with friends, buying pretty clothes, and reading novels."
    },
    {
        "role": "assistant",
        "content": "Wow, sounds like you have many interests! What kind of novels do you like to read? Romance, fantasy, or something else?"
    },
    {
        "role": "user",
        "content": "I love Journey to the West!"
    },
    {
        "role": "assistant",
        "content": "Oh, that's such a classic novel! Which character is your favorite? Is it the powerful Sun Wukong, or the compassionate Tang Monk?"
    },
    {
        "role": "user",
        "content": "I like Sun Wukong, I love how clever and smart he is."
    },
    {
        "role": "assistant",
        "content": "That's right! With his 72 transformations and fiery eyes, he always comes up with clever solutions to deal with demons. Speaking of being clever, I have to use my brain quite a bit when farming too. Would you like to hear about it?"
    },
    {
        "role": "user",
        "content": "Farming requires thinking?"
    },
    {
        "role": "assistant",
        "content": "Of course! Farming is quite technical, from planting to harvesting, every step requires knowledge. For example, how to select seeds, how to fertilize and water, how to prevent pests and diseases - you need to think carefully about all of these things."
    },
    {
        "role": "user",
        "content": "Wow, you're amazing."
    },
    {
        "role": "assistant",
        "content": "Hehe, I'm not that amazing, it's just about practice and learning from experience. Sometimes I face challenges too, just like Sun Wukong encountering demons, and I have to find ways to solve them."
    }
]


================================================
FILE: docs/experiments/chat_sessions/chats/mock_user/2.json
================================================
[
    {
        "role": "user",
        "content": "So what kind of difficulties have you encountered?"
    },
    {
        "role": "assistant",
        "content": "Oh, quite a few! Like bad weather affecting harvests, or farm tools breaking down. But don't worry, I'm here! I'll find ways to overcome them. Have you encountered any difficulties in your life?"
    },
    {
        "role": "user",
        "content": "In winter it's too cold when it snows, I really don't want to get up for school."
    },
    {
        "role": "assistant",
        "content": "Haha, that's definitely a challenge! Especially in winter, when the bed is so warm, nobody wants to leave. Do you have any ways to help yourself get up quickly?"
    },
    {
        "role": "user",
        "content": "Ah, let's not talk about that. Let's chat about food instead, I really love rose pastries!"
    },
    {
        "role": "assistant",
        "content": "That sounds delicious, besides rose pastries, what other desserts do you like?"
    },
    {
        "role": "user",
        "content": "Durian mille crepe."
    },
    {
        "role": "assistant",
        "content": "Wow, durian mille crepe, that's quite flavorful! I know it's really fragrant, though some people don't like that smell, why do you like it so much?"
    },
    {
        "role": "user",
        "content": "I just do!"
    },
    {
        "role": "assistant",
        "content": "Haha, that's fine! Everyone has different tastes, as long as you enjoy it. Hey, have you ever tried making durian mille crepe yourself?"
    },
    {
        "role": "user",
        "content": "No."
    },
    {
        "role": "assistant",
        "content": "Why not give it a try next time? I think making desserts is quite fun, and it's reassuring to eat something you made yourself. Though it might be a bit challenging, I'm sure you can do it!"
    },
    {
        "role": "user",
        "content": "Oh right, let me tell you, I don't like cilantro."
    },
    {
        "role": "assistant",
        "content": "Haha, noted! There are quite a lot of people who don't like cilantro, I think it has a weird taste too. Hey, do you have any favorite fruits?"
    },
    {
        "role": "user",
        "content": "I absolutely love watermelon!! It's the GOAT!"
    },
    {
        "role": "assistant",
        "content": "Watermelon is great! Sweet and refreshing, it's so satisfying to have some in summer! I love watermelon too, especially scooping it out of half a watermelon, it's so satisfying!"
    },
    {
        "role": "user",
        "content": "Yeah! I can eat a whole one by myself!"
    },
    {
        "role": "assistant",
        "content": "That's impressive! Eating a whole watermelon by yourself, you must have quite an appetite. But be careful not to eat too much watermelon, or you might get a stomachache."
    },
    {
        "role": "user",
        "content": "It's strange, I never used to get stomach aches from watermelon, but starting this year, I get diarrhea whenever I eat it, why is that?"
    },
    {
        "role": "assistant",
        "content": "Hmm... there could be several reasons, maybe your digestive system is more sensitive this year, or the quality of the watermelon isn't good, or maybe it's how you're eating it."
    },
    {
        "role": "user",
        "content": "The watermelons I pick are super good quality!"
    },
    {
        "role": "assistant",
        "content": "Haha, then it's probably how you're eating it or your digestive system. Do you tend to eat big bites? Or maybe you're eating too much cold watermelon?"
    },
    {
        "role": "user",
        "content": "Oh my god, both are true!"
    },
    {
        "role": "assistant",
        "content": "Ah, those are probably the reasons then. When eating watermelon, try to eat in moderation, don't eat too quickly, and don't eat too much cold watermelon, or your stomach might not be able to handle it."
    }
]


================================================
FILE: docs/experiments/locomo-benchmark/README.md
================================================
# Locomo Benchmark for Various Memory Backends

>  This project is originally forked from [mem0-evaluation](https://github.com/mem0ai/mem0/tree/main/evaluation) in commit `393a4fd5a6cfeb754857a2229726f567a9fadf36` 

This project contains the code of running benchmark results on [Locomo dataset](https://github.com/snap-research/locomo/tree/main) with different memory methods:

- langmem
- mem0
- zep
- basic rag
- naive LLM
- Memobase

## Result

- We ran Memobase results and pasted the other methods' result from [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/pdf/2504.19413). 

- We mainly report the LLM Judge Sorce (higher is better).

| Method                 | Single-Hop(%) | Multi-Hop(%) | Open Domain(%) | Temporal(%) | Overall(%) |
| ---------------------- | ------------- | ------------ | -------------- | ----------- | ---------- |
| Mem0                   | **67.13**     | 51.15        | 72.93          | 55.51       | 66.88      |
| Mem0-Graph             | 65.71         | 47.19        | 75.71          | 58.13       | 68.44      |
| LangMem                | 62.23         | 47.92        | 71.12          | 23.43       | 58.10      |
| Zep                    | 61.70         | 41.35        | 76.60      | 49.31       | 65.99      |
| OpenAI                 | 63.79         | 42.92        | 62.29          | 21.71       | 52.90      |
| Memobase(*v0.0.32*) | 63.83         | **52.08**    | 71.82          | 80.37   | 70.91  |
| Memobase(*v0.0.37*) | **70.92** | 46.88 | **77.17**   | **85.05** | **75.78** |

> **What is LLM Judge Score?**
>
> Basically, Locomo benchmark offers some long conversations and prepare some questions. LLM Judge Score is to use LLM(*e.g.* OpenAI `gpt-4o`) to judge if the answer generated from memory method is the same as the ground truth, score is 1 if it is, else 0.

We attached the artifacts of Memobase under `fixture/memobase/`:

- v0.0.32
  - `fixture/memobase/results_0503_3000.json`: predicted answers from Memobase Memory
  - `fixture/memobase/memobase_eval_0503_3000.json`: LLM Judge results of predicted answers

- v0.0.37
  - `fixture/memobase/results_0710_3000.json`: predicted answers from Memobase Memory
  - `fixture/memobase/memobase_eval_0710_3000.json`: LLM Judge results of predicted answers

To generate the latest scorings, run:

```bash
python generate_scores.py --input_path="fixture/memobase/memobase_eval_0710_3000.json"
```

Output:

```
Mean Scores Per Category:
          bleu_score  f1_score  llm_score  count         type
category
1             0.3516    0.4629     0.7092    282   single_hop
2             0.4758    0.6423     0.8505    321     temporal
3             0.1758    0.2293     0.4688     96    multi_hop
4             0.4089    0.5155     0.7717    841  open_domain

Overall Mean Scores:
bleu_score    0.3978
f1_score      0.5145
llm_score     0.7578
dtype: float64
```


> ❕ We update the results from Zep team (Zep*). See this [issue](https://github.com/memodb-io/memobase/issues/101) for detail reports and artifacts.
> | Method     | Single-Hop(%) | Multi-Hop(%) | Open Domain(%) | Temporal(%) | Overall(%) |
> | ---------- | ------------- | ------------ | -------------- | ----------- | ---------- |
> | Zep*       | 74.11         | 66.04        | 67.71          | 79.79       | 75.14      |



## 🔍 Dataset

[Download](https://github.com/snap-research/locomo/tree/main/data) the `locomo10.json` file and place it under `dataset/`



## 🚀 Getting Started

### Prerequisites

Create a `.env` file with your API keys and configurations. You must have beflow envs:

```bash
# OpenAI API key for GPT models and embeddings
OPENAI_API_KEY="your-openai-api-key"
```

Below is the detailed requirements

### Memobase

**Deps**

```bash
pip install memobase
```

**Env**

You can find free API key in [Memobase Cloud](https://memobase.io), or [deploy](../../../readme.md) one in your local

```bash
MEMOBASE_API_KEY=XXXXX
MEMOBASE_PROJECT_URL=http://localhost:8019 # OPTIONAL
```

**Command**

```bash
# memorize the data
make run-memobase-add 
# answer the benchmark
make run-memobase-search 
# evaluate the results
py evals.py --input_file results.json --output_file evals.json 
# print the final scores
py generate_scores.py --input_path="evals.json"
```



### Run Mem0

**Deps**

```bash
pip install mem0
```

**Env**

```bash
# Mem0 API keys (for Mem0 and Mem0+ techniques)
MEM0_API_KEY="your-mem0-api-key"
MEM0_PROJECT_ID="your-mem0-project-id"
MEM0_ORGANIZATION_ID="your-mem0-organization-id"
```

**Command**

> Just like the commands of Memobase, but replace `memobase` with `mem0`. See [all commands](#Memory Techniques)



### Run Zep

**Deps**

```bash
pip install zep_cloud
```

**Env**

```bash
ZEP_API_KEY="api-key-from-zep"
```

**Command**

> Just like the commands of Memobase, but replace `memobase` with `zep`. See [all commands](#Memory Techniques)



### Run langmem

**Deps**

```bash
pip install langgraph langmem
```

**Env**

```bash
EMBEDDING_MODEL="text-embedding-3-small"  # or your preferred embedding model
```

**Command**
> Just like the commands of Memobase, but replace `memobase` with `zep`. See [all commands](#Memory Techniques)


### Other methods

The rest methods don't require extra deps/envs.



## Memory Techniques

```bash
# Run Mem0 experiments
make run-memobase-add         # Add memories using Memobase
make run-memobase-search      # Search memories using Memobase

# Run Mem0 experiments
make run-mem0-add         # Add memories using Mem0
make run-mem0-search      # Search memories using Mem0

# Run Mem0+ experiments (with graph-based search)
make run-mem0-plus-add    # Add memories using Mem0+
make run-mem0-plus-search # Search memories using Mem0+

# Run RAG experiments
make run-rag              # Run RAG with chunk size 500
make run-full-context     # Run RAG with full context

# Run LangMem experiments
make run-langmem          # Run LangMem

# Run Zep experiments
make run-zep-add          # Add memories using Zep
make run-zep-search       # Search memories using Zep

# Run OpenAI experiments
make run-openai           # Run OpenAI experiments
```



### 📊 Evaluation

To evaluate results, run:

```bash
python evals.py --input_file [path_to_results] --output_file [output_path]
```

This script:
1. Processes each question-answer pair
2. Calculates BLEU and F1 scores automatically
3. Uses an LLM judge to evaluate answer correctness
4. Saves the combined results to the output file

### 📈 Generating Scores

Generate final scores with:

```bash
python generate_scores.py
```

This script:
1. Loads the evaluation metrics data
2. Calculates mean scores for each category (BLEU, F1, LLM)
3. Reports the number of questions per category
4. Calculates overall mean scores across all categories

Example output:
```
Mean Scores Per Category:
         bleu_score  f1_score  llm_score  count
category                                       
1           0.xxxx    0.xxxx     0.xxxx     xx
2           0.xxxx    0.xxxx     0.xxxx     xx
3           0.xxxx    0.xxxx     0.xxxx     xx

Overall Mean Scores:
bleu_score    0.xxxx
f1_score      0.xxxx
llm_score     0.xxxx
```



## 📁 Project Structure

```
.
├── src/                  # Source code for different memory techniques
│   ├── memobase_client/  # Implementation of the Memobase
│   ├── memzero/          # Implementation of the Mem0 technique
│   ├── openai/           # Implementation of the OpenAI memory
│   ├── zep/              # Implementation of the Zep memory
│   ├── rag.py            # Implementation of the RAG technique
│   └── langmem.py        # Implementation of the Language-based memory
├── metrics/              # Code for evaluation metrics
├── results/              # Results of experiments
├── dataset/              # Dataset files
├── evals.py              # Evaluation script
├── run_experiments.py    # Script to run experiments
├── generate_scores.py    # Script to generate scores from results
└── prompts.py            # Prompts used for the models
```



================================================
FILE: docs/experiments/locomo-benchmark/compute_p95_latency.py
================================================
#!/usr/bin/env python3
"""
P95 Search Time Latency Demo Program

This program simulates search operations and computes the 95th percentile (p95) latency.
It includes timing measurements, data generation, and statistical analysis.
"""

import time
import random
import numpy as np
import statistics
from typing import List, Dict, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import json


class LatencyAnalyzer:
    """Analyzes search latencies and computes statistics."""

    def __init__(self, results: list[float]):
        self.results: List[float] = results

    def get_latencies(self) -> List[float]:
        """Extract latency values from all results."""
        return self.results

    def compute_statistics(self) -> Dict[str, float]:
        """Compute comprehensive latency statistics."""
        if not self.results:
            return {}

        latencies = self.get_latencies()

        return {
            "count": len(latencies),
            "min_ms": min(latencies),
            "max_ms": max(latencies),
            "mean_ms": statistics.mean(latencies),
            "median_ms": statistics.median(latencies),
            "p50_ms": np.percentile(latencies, 50),
            "p90_ms": np.percentile(latencies, 90),
            "p95_ms": np.percentile(latencies, 95),  # The main metric
            "p99_ms": np.percentile(latencies, 99),
            "std_dev_ms": statistics.stdev(latencies) if len(latencies) > 1 else 0,
        }

    def print_statistics(self):
        """Print formatted statistics to console."""
        stats = self.compute_statistics()
        if not stats:
            print("No results to analyze.")
            return

        print("\n" + "=" * 50)
        print("SEARCH LATENCY ANALYSIS")
        print("=" * 50)
        print(f"Total searches: {stats['count']}")
        print(f"Min latency: {stats['min_ms']:.2f} ms")
        print(f"Max latency: {stats['max_ms']:.2f} ms")
        print(f"Mean latency: {stats['mean_ms']:.2f} ms")
        print(f"Median latency: {stats['median_ms']:.2f} ms")
        print("-" * 30)
        print("PERCENTILES:")
        print(f"P50 (median): {stats['p50_ms']:.2f} ms")
        print(f"P90: {stats['p90_ms']:.2f} ms")
        print(f"P95: {stats['p95_ms']:.2f} ms ⭐")  # Highlighted
        print(f"P99: {stats['p99_ms']:.2f} ms")
        print("-" * 30)
        print(f"Standard deviation: {stats['std_dev_ms']:.2f} ms")
        print("=" * 50)


with open("./fixture/memobase/results_0710_3000.json", "r") as f:
    data = json.load(f)
latencies = []


for k in data.keys():
    for d in data[k]:
        latencies.append(d["speaker_1_memory_time"] * 1000)
        latencies.append(d["speaker_2_memory_time"] * 1000)


analyzer = LatencyAnalyzer(latencies)
analyzer.print_statistics()



================================================
FILE: docs/experiments/locomo-benchmark/evals.py
================================================
from dotenv import load_dotenv

load_dotenv()

import json
import argparse
from metrics.utils import calculate_metrics, calculate_bleu_scores
from metrics.llm_judge import evaluate_llm_judge
from collections import defaultdict
from tqdm import tqdm
import concurrent.futures
import threading


def process_item(item_data):
    k, v = item_data
    local_results = defaultdict(list)

    for item in tqdm(v, desc=f"Processing {k} sample"):
        gt_answer = str(item["answer"])
        pred_answer = str(item["response"])
        category = str(item["category"])
        question = str(item["question"])

        # Skip category 5
        if category == "5":
            continue

        metrics = calculate_metrics(pred_answer, gt_answer)
        bleu_scores = calculate_bleu_scores(pred_answer, gt_answer)
        llm_score = evaluate_llm_judge(question, gt_answer, pred_answer)

        local_results[k].append(
            {
                "question": question,
                "answer": gt_answer,
                "response": pred_answer,
                "category": category,
                "bleu_score": bleu_scores["bleu1"],
                "f1_score": metrics["f1"],
                "llm_score": llm_score,
            }
        )

    return local_results


def main():
    parser = argparse.ArgumentParser(description="Evaluate RAG results")
    parser.add_argument(
        "--input_file",
        type=str,
        default="results/rag_results_500_k1.json",
        help="Path to the input dataset file",
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default="evaluation_metrics.json",
        help="Path to save the evaluation results",
    )
    parser.add_argument(
        "--max_workers", type=int, default=10, help="Maximum number of worker threads"
    )

    args = parser.parse_args()

    with open(args.input_file, "r") as f:
        data = json.load(f)

    results = defaultdict(list)
    results_lock = threading.Lock()

    # Use ThreadPoolExecutor with specified workers
    with concurrent.futures.ThreadPoolExecutor(
        max_workers=args.max_workers
    ) as executor:
        futures = [
            executor.submit(process_item, item_data) for item_data in data.items()
        ]

        for future in tqdm(
            concurrent.futures.as_completed(futures), total=len(futures)
        ):
            local_results = future.result()
            with results_lock:
                for k, items in local_results.items():
                    results[k].extend(items)

            # Save results to JSON file
            with open(args.output_file, "w") as f:
                json.dump(results, f, indent=4)

    print(f"Results saved to {args.output_file}")


if __name__ == "__main__":
    main()



================================================
FILE: docs/experiments/locomo-benchmark/generate_scores.py
================================================
import pandas as pd
import json
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--input_path", type=str, default="memobase_eval.json")
args = parser.parse_args()

categories = ["single_hop", "temporal", "multi_hop", "open_domain"]
# Load the evaluation metrics data
with open(args.input_path, "r") as f:
    data = json.load(f)

# Flatten the data into a list of question items
all_items = []
for key in data:
    all_items.extend(data[key])

# Convert to DataFrame
df = pd.DataFrame(all_items)

# Convert category to numeric type
df["category"] = pd.to_numeric(df["category"])

# Calculate mean scores by category
result = (
    df.groupby("category")
    .agg({"bleu_score": "mean", "f1_score": "mean", "llm_score": "mean"})
    .round(4)
)

# Add count of questions per category
result["count"] = df.groupby("category").size()

result["type"] = result.index.map(lambda x: categories[x - 1])
# Print the results
print("Mean Scores Per Category:")
print(result)

# Calculate overall means
overall_means = df.agg(
    {"bleu_score": "mean", "f1_score": "mean", "llm_score": "mean"}
).round(4)

print("\nOverall Mean Scores:")
print(overall_means)



================================================
FILE: docs/experiments/locomo-benchmark/Makefile
================================================

# Run the experiments
run-mem0-add:
	python run_experiments.py --technique_type mem0 --method add

run-mem0-search:
	python run_experiments.py --technique_type mem0 --method search --output_folder results/ --top_k 30

run-mem0-plus-add:
	python run_experiments.py --technique_type mem0 --method add --is_graph

run-mem0-plus-search:
	python run_experiments.py --technique_type mem0 --method search --is_graph --output_folder results/ --top_k 30

run-rag:
	python run_experiments.py --technique_type rag --chunk_size 500 --num_chunks 1 --output_folder results/

run-full-context:
	python run_experiments.py --technique_type rag --chunk_size -1 --num_chunks 1 --output_folder results/

run-langmem:
	python run_experiments.py --technique_type langmem --output_folder results/

run-zep-add:
	python run_experiments.py --technique_type zep --method add --output_folder results/

run-zep-search:
	python run_experiments.py --technique_type zep --method search --output_folder results/

run-openai:
	python run_experiments.py --technique_type openai --output_folder results/

run-memobase-add:
	python run_experiments.py --technique_type memobase --method add --output_folder results/

run-memobase-search:
	python run_experiments.py --technique_type memobase --method search --output_folder results/



================================================
FILE: docs/experiments/locomo-benchmark/prompts.py
================================================
ANSWER_PROMPT_GRAPH = """
    You are an intelligent memory assistant tasked with retrieving accurate information from 
    conversation memories.

    # CONTEXT:
    You have access to memories from two speakers in a conversation. These memories contain 
    timestamped information that may be relevant to answering the question. You also have 
    access to knowledge graph relations for each user, showing connections between entities, 
    concepts, and events relevant to that user.

    # INSTRUCTIONS:
    1. Carefully analyze all provided memories from both speakers
    2. Pay special attention to the timestamps to determine the answer
    3. If the question asks about a specific event or fact, look for direct evidence in the 
       memories
    4. If the memories contain contradictory information, prioritize the most recent memory
    5. If there is a question about time references (like "last year", "two months ago", 
       etc.), calculate the actual date based on the memory timestamp. For example, if a 
       memory from 4 May 2022 mentions "went to India last year," then the trip occurred 
       in 2021.
    6. Always convert relative time references to specific dates, months, or years. For 
       example, convert "last year" to "2022" or "two months ago" to "March 2023" based 
       on the memory timestamp. Ignore the reference while answering the question.
    7. Focus only on the content of the memories from both speakers. Do not confuse 
       character names mentioned in memories with the actual users who created those 
       memories.
    8. The answer should be less than 5-6 words.
    9. Use the knowledge graph relations to understand the user's knowledge network and 
       identify important relationships between entities in the user's world.

    # APPROACH (Think step by step):
    1. First, examine all memories that contain information related to the question
    2. Examine the timestamps and content of these memories carefully
    3. Look for explicit mentions of dates, times, locations, or events that answer the 
       question
    4. If the answer requires calculation (e.g., converting relative time references), 
       show your work
    5. Analyze the knowledge graph relations to understand the user's knowledge context
    6. Formulate a precise, concise answer based solely on the evidence in the memories
    7. Double-check that your answer directly addresses the question asked
    8. Ensure your final answer is specific and avoids vague time references

    Memories for user {{speaker_1_user_id}}:

    {{speaker_1_memories}}

    Relations for user {{speaker_1_user_id}}:

    {{speaker_1_graph_memories}}

    Memories for user {{speaker_2_user_id}}:

    {{speaker_2_memories}}

    Relations for user {{speaker_2_user_id}}:

    {{speaker_2_graph_memories}}

    Question: {{question}}

    Answer:
    """


ANSWER_PROMPT = """
    You are an intelligent memory assistant tasked with retrieving accurate information from conversation memories.

    # CONTEXT:
    You have access to memories from two speakers in a conversation. These memories contain 
    timestamped information that may be relevant to answering the question.

    # INSTRUCTIONS:
    1. Carefully analyze all provided memories from both speakers
    2. Pay special attention to the timestamps to determine the answer
    3. If the question asks about a specific event or fact, look for direct evidence in the memories
    4. If the memories contain contradictory information, prioritize the most recent memory
    5. If there is a question about time references (like "last year", "two months ago", etc.), 
       calculate the actual date based on the memory timestamp. For example, if a memory from 
       4 May 2022 mentions "went to India last year," then the trip occurred in 2021.
    6. Always convert relative time references to specific dates, months, or years. For example, 
       convert "last year" to "2022" or "two months ago" to "March 2023" based on the memory 
       timestamp. Ignore the reference while answering the question.
    7. Focus only on the content of the memories from both speakers. Do not confuse character 
       names mentioned in memories with the actual users who created those memories.
    8. The answer should be less than 5-6 words.

    # APPROACH (Think step by step):
    1. First, examine all memories that contain information related to the question
    2. Examine the timestamps and content of these memories carefully
    3. Look for explicit mentions of dates, times, locations, or events that answer the question
    4. If the answer requires calculation (e.g., converting relative time references), show your work
    5. Formulate a precise, concise answer based solely on the evidence in the memories
    6. Double-check that your answer directly addresses the question asked
    7. Ensure your final answer is specific and avoids vague time references

    Memories for user {{speaker_1_user_id}}:

    {{speaker_1_memories}}

    Memories for user {{speaker_2_user_id}}:

    {{speaker_2_memories}}

    Question: {{question}}

    Answer:
    """


ANSWER_PROMPT_ZEP = """
    You are an intelligent memory assistant tasked with retrieving accurate information from conversation memories.

    # CONTEXT:
    You have access to memories from a conversation. These memories contain
    timestamped information that may be relevant to answering the question.

    # INSTRUCTIONS:
    1. Carefully analyze all provided memories
    2. Pay special attention to the timestamps to determine the answer
    3. If the question asks about a specific event or fact, look for direct evidence in the memories
    4. If the memories contain contradictory information, prioritize the most recent memory
    5. If there is a question about time references (like "last year", "two months ago", etc.), 
       calculate the actual date based on the memory timestamp. For example, if a memory from 
       4 May 2022 mentions "went to India last year," then the trip occurred in 2021.
    6. Always convert relative time references to specific dates, months, or years. For example, 
       convert "last year" to "2022" or "two months ago" to "March 2023" based on the memory 
       timestamp. Ignore the reference while answering the question.
    7. Focus only on the content of the memories. Do not confuse character 
       names mentioned in memories with the actual users who created those memories.
    8. The answer should be less than 5-6 words.

    # APPROACH (Think step by step):
    1. First, examine all memories that contain information related to the question
    2. Examine the timestamps and content of these memories carefully
    3. Look for explicit mentions of dates, times, locations, or events that answer the question
    4. If the answer requires calculation (e.g., converting relative time references), show your work
    5. Formulate a precise, concise answer based solely on the evidence in the memories
    6. Double-check that your answer directly addresses the question asked
    7. Ensure your final answer is specific and avoids vague time references

    Memories:

    {{memories}}

    Question: {{question}}
    Answer:
    """


================================================
FILE: docs/experiments/locomo-benchmark/run_experiments.py
================================================
import os
import json
import argparse
from src.utils import TECHNIQUES, METHODS


class Experiment:
    def __init__(self, technique_type, chunk_size):
        self.technique_type = technique_type
        self.chunk_size = chunk_size

    def run(self):
        print(
            f"Running experiment with technique: {self.technique_type}, chunk size: {self.chunk_size}"
        )


def main():
    parser = argparse.ArgumentParser(description="Run memory experiments")
    parser.add_argument(
        "--technique_type",
        choices=TECHNIQUES,
        default="mem0",
        help="Memory technique to use",
    )
    parser.add_argument(
        "--method", choices=METHODS, default="add", help="Method to use"
    )
    parser.add_argument(
        "--chunk_size", type=int, default=1000, help="Chunk size for processing"
    )
    parser.add_argument(
        "--output_folder", type=str, default="results/", help="Output path for results"
    )
    parser.add_argument(
        "--top_k", type=int, default=30, help="Number of top memories to retrieve"
    )
    parser.add_argument(
        "--filter_memories",
        action="store_true",
        default=False,
        help="Whether to filter memories",
    )
    parser.add_argument(
        "--is_graph",
        action="store_true",
        default=False,
        help="Whether to use graph-based search",
    )
    parser.add_argument(
        "--num_chunks", type=int, default=1, help="Number of chunks to process"
    )

    args = parser.parse_args()

    # Add your experiment logic here
    print(
        f"Running experiments with technique: {args.technique_type}, chunk size: {args.chunk_size}"
    )

    if args.technique_type == "mem0":
        from src.memzero.add import MemoryADD
        from src.memzero.search import MemorySearch

        if args.method == "add":
            memory_manager = MemoryADD(
                data_path="dataset/locomo10.json", is_graph=args.is_graph
            )
            memory_manager.process_all_conversations()
        elif args.method == "search":
            output_file_path = os.path.join(
                args.output_folder,
                f"mem0_results_top_{args.top_k}_filter_{args.filter_memories}_graph_{args.is_graph}.json",
            )
            memory_searcher = MemorySearch(
                output_file_path, args.top_k, args.filter_memories, args.is_graph
            )
            memory_searcher.process_data_file("dataset/locomo10.json")
    elif args.technique_type == "rag":
        from src.rag import RAGManager

        output_file_path = os.path.join(
            args.output_folder, f"rag_results_{args.chunk_size}_k{args.num_chunks}.json"
        )
        rag_manager = RAGManager(
            data_path="dataset/locomo10_rag.json",
            chunk_size=args.chunk_size,
            k=args.num_chunks,
        )
        rag_manager.process_all_conversations(output_file_path)
    elif args.technique_type == "langmem":
        from src.langmem import LangMemManager

        output_file_path = os.path.join(args.output_folder, "langmem_results.json")
        langmem_manager = LangMemManager(dataset_path="dataset/locomo10_rag.json")
        langmem_manager.process_all_conversations(output_file_path)
    elif args.technique_type == "zep":
        from src.zep.search import ZepSearch
        from src.zep.add import ZepAdd

        if args.method == "add":
            zep_manager = ZepAdd(data_path="dataset/locomo10.json")
            zep_manager.process_all_conversations("1")
        elif args.method == "search":
            output_file_path = os.path.join(
                args.output_folder, "zep_search_results.json"
            )
            zep_manager = ZepSearch()
            zep_manager.process_data_file(
                "dataset/locomo10.json", "1", output_file_path
            )
    elif args.technique_type == "openai":
        from src.openai.predict import OpenAIPredict

        output_file_path = os.path.join(args.output_folder, "openai_results.json")
        openai_manager = OpenAIPredict()
        openai_manager.process_data_file("dataset/locomo10.json", output_file_path)
    elif args.technique_type == "memobase":
        from src.memobase_client import MemobaseADD, MemobaseSearch

        if args.method == "add":
            memobase_manager = MemobaseADD(data_path="dataset/locomo10.json")
            memobase_manager.process_all_conversations()
        elif args.method == "search":
            memobase_manager = MemobaseSearch()
            memobase_manager.process_data_file("dataset/locomo10.json")
    else:
        raise ValueError(f"Invalid technique type: {args.technique_type}")


if __name__ == "__main__":
    main()



================================================
FILE: docs/experiments/locomo-benchmark/metrics/llm_judge.py
================================================
import os
from openai import OpenAI
import json
from collections import defaultdict
import numpy as np
import argparse

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL"),
)

ACCURACY_PROMPT = """
Your task is to label an answer to a question as ’CORRECT’ or ’WRONG’. You will be given the following data:
    (1) a question (posed by one user to another user), 
    (2) a ’gold’ (ground truth) answer, 
    (3) a generated answer
which you will score as CORRECT/WRONG.

The point of the question is to ask about something one user should know about the other user based on their prior conversations.
The gold answer will usually be a concise and short answer that includes the referenced topic, for example:
Question: Do you remember what I got the last time I went to Hawaii?
Gold answer: A shell necklace
The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT. 

For time related questions, the gold answer will be a specific date, month, year, etc. The generated answer might be much longer or use relative time references (like "last Tuesday" or "next month"), but you should be generous with your grading - as long as it refers to the same date or time period as the gold answer, it should be counted as CORRECT. Even if the format differs (e.g., "May 7th" vs "7 May"), consider it CORRECT if it's the same date.

Now it’s time for the real question:
Question: {question}
Gold answer: {gold_answer}
Generated answer: {generated_answer}

First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG. 
Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.

Just return the label CORRECT or WRONG in a json format with the key as "label".
"""


def evaluate_llm_judge(question, gold_answer, generated_answer):
    """Evaluate the generated answer against the gold answer using an LLM judge."""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": ACCURACY_PROMPT.format(
                    question=question,
                    gold_answer=gold_answer,
                    generated_answer=generated_answer,
                ),
            }
        ],
        response_format={"type": "json_object"},
        temperature=0.0,
    )
    label = json.loads(response.choices[0].message.content)["label"]
    return 1 if label == "CORRECT" else 0


def main():
    """Main function to evaluate RAG results using LLM judge."""
    parser = argparse.ArgumentParser(description="Evaluate RAG results using LLM judge")
    parser.add_argument(
        "--input_file",
        type=str,
        default="results/default_run_v4_k30_new_graph.json",
        help="Path to the input dataset file",
    )

    args = parser.parse_args()

    dataset_path = args.input_file
    output_path = f"results/llm_judge_{dataset_path.split('/')[-1]}"

    with open(dataset_path, "r") as f:
        data = json.load(f)

    LLM_JUDGE = defaultdict(list)
    RESULTS = defaultdict(list)

    index = 0
    for k, v in data.items():
        for x in v:
            question = x["question"]
            gold_answer = x["answer"]
            generated_answer = x["response"]
            category = x["category"]

            # Skip category 5
            if int(category) == 5:
                continue

            # Evaluate the answer
            label = evaluate_llm_judge(question, gold_answer, generated_answer)
            LLM_JUDGE[category].append(label)

            # Store the results
            RESULTS[index].append(
                {
                    "question": question,
                    "gt_answer": gold_answer,
                    "response": generated_answer,
                    "category": category,
                    "llm_label": label,
                }
            )

            # Save intermediate results
            with open(output_path, "w") as f:
                json.dump(RESULTS, f, indent=4)

            # Print current accuracy for all categories
            print("All categories accuracy:")
            for cat, results in LLM_JUDGE.items():
                if results:  # Only print if there are results for this category
                    print(
                        f"  Category {cat}: {np.mean(results):.4f} "
                        f"({sum(results)}/{len(results)})"
                    )
            print("------------------------------------------")
        index += 1

    # Save final results
    with open(output_path, "w") as f:
        json.dump(RESULTS, f, indent=4)

    # Print final summary
    print("PATH: ", dataset_path)
    print("------------------------------------------")
    for k, v in LLM_JUDGE.items():
        print(k, np.mean(v))


if __name__ == "__main__":
    main()



================================================
FILE: docs/experiments/locomo-benchmark/metrics/utils.py
================================================
"""
Borrowed from https://github.com/WujiangXu/AgenticMemory/blob/main/utils.py

@article{xu2025mem,
    title={A-mem: Agentic memory for llm agents},
    author={Xu, Wujiang and Liang, Zujie and Mei, Kai and Gao, Hang and Tan, Juntao 
           and Zhang, Yongfeng},
    journal={arXiv preprint arXiv:2502.12110},
    year={2025}
}
"""

import re
import string
import numpy as np
from typing import List, Dict, Union
import statistics
from collections import defaultdict
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score as bert_score
import nltk
from nltk.translate.meteor_score import meteor_score
from sentence_transformers import SentenceTransformer
import logging
from dataclasses import dataclass
from pathlib import Path
from openai import OpenAI
# from load_dataset import load_locomo_dataset, QA, Turn, Session, Conversation
from sentence_transformers.util import pytorch_cos_sim

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
except Exception as e:
    print(f"Error downloading NLTK data: {e}")

# Initialize SentenceTransformer model (this will be reused)
try:
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
except Exception as e:
    print(f"Warning: Could not load SentenceTransformer model: {e}")
    sentence_model = None

def simple_tokenize(text):
    """Simple tokenization function."""
    # Convert to string if not already
    text = str(text)
    return text.lower().replace('.', ' ').replace(',', ' ').replace('!', ' ').replace('?', ' ').split()

def calculate_rouge_scores(prediction: str, reference: str) -> Dict[str, float]:
    """Calculate ROUGE scores for prediction against reference."""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, prediction)
    return {
        'rouge1_f': scores['rouge1'].fmeasure,
        'rouge2_f': scores['rouge2'].fmeasure,
        'rougeL_f': scores['rougeL'].fmeasure
    }

def calculate_bleu_scores(prediction: str, reference: str) -> Dict[str, float]:
    """Calculate BLEU scores with different n-gram settings."""
    pred_tokens = nltk.word_tokenize(prediction.lower())
    ref_tokens = [nltk.word_tokenize(reference.lower())]
    
    weights_list = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]
    smooth = SmoothingFunction().method1
    
    scores = {}
    for n, weights in enumerate(weights_list, start=1):
        try:
            score = sentence_bleu(ref_tokens, pred_tokens, weights=weights, smoothing_function=smooth)
        except Exception:
            print(f"Error calculating BLEU score: {e}")
            score = 0.0
        scores[f'bleu{n}'] = score
    
    return scores

def calculate_bert_scores(prediction: str, reference: str) -> Dict[str, float]:
    """Calculate BERTScore for semantic similarity."""
    try:
        P, R, F1 = bert_score([prediction], [reference], lang='en', verbose=False)
        return {
            'bert_precision': P.item(),
            'bert_recall': R.item(),
            'bert_f1': F1.item()
        }
    except Exception as e:
        print(f"Error calculating BERTScore: {e}")
        return {
            'bert_precision': 0.0,
            'bert_recall': 0.0,
            'bert_f1': 0.0
        }

def calculate_meteor_score(prediction: str, reference: str) -> float:
    """Calculate METEOR score for the prediction."""
    try:
        return meteor_score([reference.split()], prediction.split())
    except Exception as e:
        print(f"Error calculating METEOR score: {e}")
        return 0.0

def calculate_sentence_similarity(prediction: str, reference: str) -> float:
    """Calculate sentence embedding similarity using SentenceBERT."""
    if sentence_model is None:
        return 0.0
    try:
        # Encode sentences
        embedding1 = sentence_model.encode([prediction], convert_to_tensor=True)
        embedding2 = sentence_model.encode([reference], convert_to_tensor=True)
        
        # Calculate cosine similarity
        similarity = pytorch_cos_sim(embedding1, embedding2).item()
        return float(similarity)
    except Exception as e:
        print(f"Error calculating sentence similarity: {e}")
        return 0.0

def calculate_metrics(prediction: str, reference: str) -> Dict[str, float]:
    """Calculate comprehensive evaluation metrics for a prediction."""
    # Handle empty or None values
    if not prediction or not reference:
        return {
            "exact_match": 0,
            "f1": 0.0,
            "rouge1_f": 0.0,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0,
            "bleu1": 0.0,
            "bleu2": 0.0,
            "bleu3": 0.0,
            "bleu4": 0.0,
            "bert_f1": 0.0,
            "meteor": 0.0,
            "sbert_similarity": 0.0
        }
    
    # Convert to strings if they're not already
    prediction = str(prediction).strip()
    reference = str(reference).strip()
    
    # Calculate exact match
    exact_match = int(prediction.lower() == reference.lower())
    
    # Calculate token-based F1 score
    pred_tokens = set(simple_tokenize(prediction))
    ref_tokens = set(simple_tokenize(reference))
    common_tokens = pred_tokens & ref_tokens
    
    if not pred_tokens or not ref_tokens:
        f1 = 0.0
    else:
        precision = len(common_tokens) / len(pred_tokens)
        recall = len(common_tokens) / len(ref_tokens)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # Calculate all scores
    rouge_scores = 0 #calculate_rouge_scores(prediction, reference)
    bleu_scores = calculate_bleu_scores(prediction, reference)
    bert_scores = 0 # calculate_bert_scores(prediction, reference)
    meteor = 0 # calculate_meteor_score(prediction, reference)
    sbert_similarity = 0 # calculate_sentence_similarity(prediction, reference)
    
    # Combine all metrics
    metrics = {
        "exact_match": exact_match,
        "f1": f1,
        # **rouge_scores,
        **bleu_scores,
        # **bert_scores,
        # "meteor": meteor,
        # "sbert_similarity": sbert_similarity
    }

    return metrics

def aggregate_metrics(all_metrics: List[Dict[str, float]], all_categories: List[int]) -> Dict[str, Dict[str, Union[float, Dict[str, float]]]]:
    """Calculate aggregate statistics for all metrics, split by category."""
    if not all_metrics:
        return {}
    
    # Initialize aggregates for overall and per-category metrics
    aggregates = defaultdict(list)
    category_aggregates = defaultdict(lambda: defaultdict(list))
    
    # Collect all values for each metric, both overall and per category
    for metrics, category in zip(all_metrics, all_categories):
        for metric_name, value in metrics.items():
            aggregates[metric_name].append(value)
            category_aggregates[category][metric_name].append(value)
    
    # Calculate statistics for overall metrics
    results = {
        "overall": {}
    }
    
    for metric_name, values in aggregates.items():
        results["overall"][metric_name] = {
            'mean': statistics.mean(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0.0,
            'median': statistics.median(values),
            'min': min(values),
            'max': max(values),
            'count': len(values)
        }
    
    # Calculate statistics for each category
    for category in sorted(category_aggregates.keys()):
        results[f"category_{category}"] = {}
        for metric_name, values in category_aggregates[category].items():
            if values:  # Only calculate if we have values for this category
                results[f"category_{category}"][metric_name] = {
                    'mean': statistics.mean(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0.0,
                    'median': statistics.median(values),
                    'min': min(values),
                    'max': max(values),
                    'count': len(values)
                }
    
    return results



================================================
FILE: docs/experiments/locomo-benchmark/src/langmem.py
================================================
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langgraph.utils.config import get_store
from langmem import (
    create_manage_memory_tool,
    create_search_memory_tool
)
import time
import multiprocessing as mp
import json
from functools import partial
import os
from tqdm import tqdm
from openai import OpenAI
from collections import defaultdict
from dotenv import load_dotenv
from prompts import ANSWER_PROMPT

load_dotenv()

client = OpenAI()

from jinja2 import Template

ANSWER_PROMPT_TEMPLATE = Template(ANSWER_PROMPT)


def get_answer(question, speaker_1_user_id, speaker_1_memories, speaker_2_user_id, speaker_2_memories):
    prompt = ANSWER_PROMPT_TEMPLATE.render(
        question=question,
        speaker_1_user_id=speaker_1_user_id,
        speaker_1_memories=speaker_1_memories,
        speaker_2_user_id=speaker_2_user_id,
        speaker_2_memories=speaker_2_memories
    )

    t1 = time.time()
    response = client.chat.completions.create(
        model=os.getenv("MODEL"),
        messages=[{"role": "system", "content": prompt}],
        temperature=0.0
    )
    t2 = time.time()
    return response.choices[0].message.content, t2 - t1


def prompt(state):
    """Prepare the messages for the LLM."""
    store = get_store()
    memories = store.search(
        ("memories",),
        query=state["messages"][-1].content,
    )
    system_msg = f"""You are a helpful assistant.

## Memories
<memories>
{memories}
</memories>
"""
    return [{"role": "system", "content": system_msg}, *state["messages"]]


class LangMem:
    def __init__(self,):
        self.store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": f"openai:{os.getenv('EMBEDDING_MODEL')}",
            }
        )
        self.checkpointer = MemorySaver()  # Checkpoint graph state

        self.agent = create_react_agent(
            f"openai:{os.getenv('MODEL')}",
            prompt=prompt,
            tools=[
                create_manage_memory_tool(namespace=("memories",)),
                create_search_memory_tool(namespace=("memories",)),
            ],
            store=self.store,
            checkpointer=self.checkpointer,
        )

    def add_memory(self, message, config):
        return self.agent.invoke(
            {"messages": [{"role": "user", "content": message}]},
            config=config
        )

    def search_memory(self, query, config):
        try:
            t1 = time.time()
            response = self.agent.invoke(
                {"messages": [{"role": "user", "content": query}]},
                config=config
            )
            t2 = time.time()
            return response["messages"][-1].content, t2 - t1
        except Exception as e:
            print(f"Error in search_memory: {e}")
            return "", t2 - t1


class LangMemManager:
    def __init__(self, dataset_path):
        self.dataset_path = dataset_path
        with open(self.dataset_path, 'r') as f:
            self.data = json.load(f)

    def process_all_conversations(self, output_file_path):
        OUTPUT = defaultdict(list)

        # Process conversations in parallel with multiple workers
        def process_conversation(key_value_pair):
            key, value = key_value_pair
            result = defaultdict(list)

            chat_history = value["conversation"]
            questions = value["question"]

            agent1 = LangMem()
            agent2 = LangMem()
            config = {"configurable": {"thread_id": f"thread-{key}"}}
            speakers = set()

            # Identify speakers
            for conv in chat_history:
                speakers.add(conv['speaker'])

            if len(speakers) != 2:
                raise ValueError(f"Expected 2 speakers, got {len(speakers)}")

            speaker1 = list(speakers)[0]
            speaker2 = list(speakers)[1]

            # Add memories for each message
            for conv in tqdm(chat_history, desc=f"Processing messages {key}", leave=False):
                message = f"{conv['timestamp']} | {conv['speaker']}: {conv['text']}"
                if conv['speaker'] == speaker1:
                    agent1.add_memory(message, config)
                elif conv['speaker'] == speaker2:
                    agent2.add_memory(message, config)
                else:
                    raise ValueError(f"Expected speaker1 or speaker2, got {conv['speaker']}")

            # Process questions
            for q in tqdm(questions, desc=f"Processing questions {key}", leave=False):
                category = q['category']

                if int(category) == 5:
                    continue

                answer = q['answer']
                question = q['question']
                response1, speaker1_memory_time = agent1.search_memory(question, config)
                response2, speaker2_memory_time = agent2.search_memory(question, config)

                generated_answer, response_time = get_answer(
                    question, speaker1, response1, speaker2, response2
                )

                result[key].append({
                    "question": question,
                    "answer": answer,
                    "response1": response1,
                    "response2": response2,
                    "category": category,
                    "speaker1_memory_time": speaker1_memory_time,
                    "speaker2_memory_time": speaker2_memory_time,
                    "response_time": response_time,
                    'response': generated_answer
                })

            return result

        # Use multiprocessing to process conversations in parallel
        with mp.Pool(processes=10) as pool:
            results = list(tqdm(
                pool.imap(process_conversation, list(self.data.items())),
                total=len(self.data),
                desc="Processing conversations"
            ))

        # Combine results from all workers
        for result in results:
            for key, items in result.items():
                OUTPUT[key].extend(items)

        # Save final results
        with open(output_file_path, 'w') as f:
            json.dump(OUTPUT, f, indent=4)



================================================
FILE: docs/experiments/locomo-benchmark/src/rag.py
================================================
from openai import OpenAI
import json
import numpy as np
from tqdm import tqdm
from jinja2 import Template
import tiktoken
import time
from collections import defaultdict
import os
from dotenv import load_dotenv

load_dotenv()

PROMPT = """
# Question: 
{{QUESTION}}

# Context: 
{{CONTEXT}}

# Short answer:
"""


class RAGManager:
    def __init__(self, data_path="dataset/locomo10_rag.json", chunk_size=500, k=1):
        self.model = os.getenv("MODEL")
        self.client = OpenAI()
        self.data_path = data_path
        self.chunk_size = chunk_size
        self.k = k

    def generate_response(self, question, context):
        template = Template(PROMPT)
        prompt = template.render(
            CONTEXT=context,
            QUESTION=question
        )

        max_retries = 3
        retries = 0

        while retries <= max_retries:
            try:
                t1 = time.time()
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system",
                         "content": "You are a helpful assistant that can answer "
                                    "questions based on the provided context."
                                    "If the question involves timing, use the conversation date for reference."
                                    "Provide the shortest possible answer."
                                    "Use words directly from the conversation when possible."
                                    "Avoid using subjects in your answer."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0
                )
                t2 = time.time()
                return response.choices[0].message.content.strip(), t2-t1
            except Exception as e:
                retries += 1
                if retries > max_retries:
                    raise e
                time.sleep(1)  # Wait before retrying

    def clean_chat_history(self, chat_history):
        cleaned_chat_history = ""
        for c in chat_history:
            cleaned_chat_history += (f"{c['timestamp']} | {c['speaker']}: "
                                     f"{c['text']}\n")

        return cleaned_chat_history

    def calculate_embedding(self, document):
        response = self.client.embeddings.create(
            model=os.getenv("EMBEDDING_MODEL"),
            input=document
        )
        return response.data[0].embedding

    def calculate_similarity(self, embedding1, embedding2):
        return np.dot(embedding1, embedding2) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2))

    def search(self, query, chunks, embeddings, k=1):
        """
        Search for the top-k most similar chunks to the query.

        Args:
            query: The query string
            chunks: List of text chunks
            embeddings: List of embeddings for each chunk
            k: Number of top chunks to return (default: 1)

        Returns:
            combined_chunks: The combined text of the top-k chunks
            search_time: Time taken for the search
        """
        t1 = time.time()
        query_embedding = self.calculate_embedding(query)
        similarities = [
            self.calculate_similarity(query_embedding, embedding) 
            for embedding in embeddings
        ]

        # Get indices of top-k most similar chunks
        if k == 1:
            # Original behavior - just get the most similar chunk
            top_indices = [np.argmax(similarities)]
        else:
            # Get indices of top-k chunks
            top_indices = np.argsort(similarities)[-k:][::-1]

        # Combine the top-k chunks
        combined_chunks = "\n<->\n".join([chunks[i] for i in top_indices])

        t2 = time.time()
        return combined_chunks, t2-t1

    def create_chunks(self, chat_history, chunk_size=500):
        """
        Create chunks using tiktoken for more accurate token counting
        """
        # Get the encoding for the model
        encoding = tiktoken.encoding_for_model(os.getenv("EMBEDDING_MODEL"))

        documents = self.clean_chat_history(chat_history)

        if chunk_size == -1:
            return [documents], []

        chunks = []

        # Encode the document
        tokens = encoding.encode(documents)

        # Split into chunks based on token count
        for i in range(0, len(tokens), chunk_size):
            chunk_tokens = tokens[i:i+chunk_size]
            chunk = encoding.decode(chunk_tokens)
            chunks.append(chunk)

        embeddings = []
        for chunk in chunks:
            embedding = self.calculate_embedding(chunk)
            embeddings.append(embedding)

        return chunks, embeddings

    def process_all_conversations(self, output_file_path):
        with open(self.data_path, "r") as f:
            data = json.load(f)

        FINAL_RESULTS = defaultdict(list)
        for key, value in tqdm(data.items(), desc="Processing conversations"):
            chat_history = value["conversation"]
            questions = value["question"]

            chunks, embeddings = self.create_chunks(
                chat_history, self.chunk_size
            )

            for item in tqdm(
                questions, desc="Answering questions", leave=False
            ):
                question = item["question"]
                answer = item.get("answer", "")
                category = item["category"]

                if self.chunk_size == -1:
                    context = chunks[0]
                    search_time = 0
                else:
                    context, search_time = self.search(
                        question, chunks, embeddings, k=self.k
                    )
                response, response_time = self.generate_response(
                    question, context
                )

                FINAL_RESULTS[key].append({
                    "question": question,
                    "answer": answer,
                    "category": category,
                    "context": context,
                    "response": response,
                    "search_time": search_time,
                    "response_time": response_time,
                })
                with open(output_file_path, "w+") as f:
                    json.dump(FINAL_RESULTS, f, indent=4)

        # Save results
        with open(output_file_path, "w+") as f:
            json.dump(FINAL_RESULTS, f, indent=4)



================================================
FILE: docs/experiments/locomo-benchmark/src/utils.py
================================================
TECHNIQUES = ["mem0", "rag", "langmem", "zep", "openai", "memobase"]

METHODS = ["add", "search"]



================================================
FILE: docs/experiments/locomo-benchmark/src/memobase_client/__init__.py
================================================
from .memobase_add import MemobaseADD
from .memobase_search import MemobaseSearch

__all__ = ["MemobaseADD", "MemobaseSearch"]



================================================
FILE: docs/experiments/locomo-benchmark/src/memobase_client/config.yaml
================================================
language: en

overwrite_user_profiles:
  - topic: "basic_info"
    sub_topics:
      - name: "gender"
      - name: "name"
      - name: "birth_date"
      - name: "location"
  - topic: "personal_narrative"
    sub_topics:
      - name: "identity_journey"
      - name: "self_acceptance"
      - name: "emotional_states"
      - name: "life_milestones"
  - topic: "life_circumstances"
    sub_topics:
      - name: "career"
      - name: "education"
      - name: "family_status"
      - name: "living_situation"
  - topic: "personal_growth"
    sub_topics:
      - name: "hobbies"
      - name: "creative_pursuits"
      - name: "mental_health"
      - name: "self_care_activities"
  - topic: "plans"
    sub_topics:
      - name: "career_goals"
      - name: "personal_aspirations"
      - name: "family_planning"
      - name: "life_goals"



================================================
FILE: docs/experiments/locomo-benchmark/src/memobase_client/memobase_add.py
================================================
import uuid
import json
import time
import os
import threading
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv
from memobase import MemoBaseClient, AsyncMemoBaseClient, ChatBlob

load_dotenv()

root_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(root_dir, "config.yaml")


def string_to_uuid(s: str, salt="memobase_client") -> str:
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, s + salt))


class MemobaseADD:
    def __init__(self, data_path=None, batch_size=2, reprocess=False):
        self.client = MemoBaseClient(
            api_key=os.getenv("MEMOBASE_API_KEY"),
            project_url=os.getenv("MEMOBASE_PROJECT_URL", "https://api.memobase.dev"),
        )
        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                config = f.read()
            print(config)
            self.client.update_config(config)
            print("Config updated successfully")
        self.batch_size = batch_size
        self.data_path = data_path
        self.data = None
        self.reprocess = reprocess
        if data_path:
            self.load_data()

    def load_data(self):
        with open(self.data_path, "r") as f:
            self.data = json.load(f)
        return self.data

    def add_memory(self, user, message, retries=3):
        for attempt in range(retries):
            try:
                _ = user.insert(ChatBlob(messages=message), sync=True)
                return
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(1)  # Wait before retrying
                    continue
                else:
                    raise e

    def add_memories_for_speaker(self, speaker, messages, desc):
        real_uid = string_to_uuid(speaker)
        u = self.client.get_or_create_user(real_uid)
        for i in tqdm(range(0, len(messages), self.batch_size), desc=desc):
            batch_messages = messages[i : i + self.batch_size]
            self.add_memory(u, batch_messages)
        u.flush(sync=True)

    def process_conversation(self, item, idx):
        conversation = item["conversation"]
        speaker_a = conversation["speaker_a"]
        speaker_b = conversation["speaker_b"]

        speaker_a_user_id = f"{speaker_a}_{idx}"
        speaker_b_user_id = f"{speaker_b}_{idx}"

        # delete all memories for the two users
        try:
            self.client.delete_user(string_to_uuid(speaker_a_user_id))
            self.client.delete_user(string_to_uuid(speaker_b_user_id))
        except Exception as e:
            pass

        for key in conversation.keys():
            if key in ["speaker_a", "speaker_b"] or "date" in key or "timestamp" in key:
                continue

            date_time_key = key + "_date_time"
            timestamp = conversation[date_time_key]
            chats = conversation[key]

            messages = []
            messages_reverse = []
            for chat in chats:
                if chat["speaker"] == speaker_a:
                    messages.append(
                        {
                            "role": "user",
                            "content": chat["text"],
                            "alias": speaker_a,
                            "created_at": timestamp,
                        }
                    )
                    messages_reverse.append(
                        {
                            "role": "assistant",
                            "content": chat["text"],
                            "alias": speaker_a,
                            "created_at": timestamp,
                        }
                    )
                elif chat["speaker"] == speaker_b:
                    messages.append(
                        {
                            "role": "assistant",
                            "content": chat["text"],
                            "alias": speaker_b,
                            "created_at": timestamp,
                        }
                    )
                    messages_reverse.append(
                        {
                            "role": "user",
                            "content": chat["text"],
                            "alias": speaker_b,
                            "created_at": timestamp,
                        }
                    )
                else:
                    raise ValueError(f"Unknown speaker: {chat['speaker']}")

            # add memories for the two users on different threads
            thread_a = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(
                    speaker_a_user_id,
                    messages,
                    f"{idx} Adding Memories for {speaker_a}",
                ),
            )
            thread_b = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(
                    speaker_b_user_id,
                    messages_reverse,
                    f"{idx} Adding Memories for {speaker_b}",
                ),
            )

            thread_a.start()
            thread_b.start()
            thread_a.join()
            thread_b.join()

        print("Messages added successfully")

    def process_all_conversations(self, max_workers=10, max_samples=None):
        if not self.data:
            raise ValueError(
                "No data loaded. Please set data_path and call load_data() first."
            )
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            if max_samples:
                self.data = self.data[:max_samples]
            futures = [
                executor.submit(self.process_conversation, item, idx)
                for idx, item in enumerate(self.data)
            ]

            for future in futures:
                future.result()



================================================
FILE: docs/experiments/locomo-benchmark/src/memobase_client/memobase_search.py
================================================
import os
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import json
import time
from jinja2 import Template
from openai import OpenAI
from prompts import ANSWER_PROMPT_GRAPH, ANSWER_PROMPT
from mem0 import MemoryClient
from memobase import MemoBaseClient
from memobase.error import ServerError
from .memobase_add import string_to_uuid
from dotenv import load_dotenv

load_dotenv()


class MemobaseSearch:

    def __init__(
        self,
        output_path="results.json",
        top_k=10,
        max_memory_context_size=3000,
    ):
        self.client = MemoBaseClient(
            api_key=os.getenv("MEMOBASE_API_KEY"),
            project_url=os.getenv("MEMOBASE_PROJECT_URL", "https://api.memobase.dev"),
        )
        self.top_k = top_k
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL"),
        )
        print(self.openai_client.api_key, self.openai_client.base_url)
        if not os.path.exists(output_path):
            self.results = defaultdict(list)
        else:
            with open(output_path, "r") as f:
                self.results = json.load(f)
        self.output_path = output_path
        self.max_memory_context_size = max_memory_context_size
        self.ANSWER_PROMPT = ANSWER_PROMPT

    def search_memory(self, user_id, query, max_retries=3, retry_delay=1):
        start_time = time.time()
        retries = 0
        real_uid = string_to_uuid(user_id)
        u = self.client.get_user(real_uid, no_get=True)
        while retries < max_retries:
            try:
                memories = u.context(
                    max_token_size=self.max_memory_context_size,
                    chats=[{"role": "user", "content": query}],
                    event_similarity_threshold=0.2,
                    fill_window_with_events=True,
                )
                break
            except ServerError as e:
                print(f"ServerError: {e}")
                print("Retrying...")
                retries += 1
                if retries >= max_retries:
                    raise e
                time.sleep(retry_delay)

        end_time = time.time()
        return memories, end_time - start_time

    def answer_question(
        self, speaker_1_user_id, speaker_2_user_id, question, answer, category
    ):
        speaker_1_memories, speaker_1_memory_time = self.search_memory(
            speaker_1_user_id, question
        )
        speaker_2_memories, speaker_2_memory_time = self.search_memory(
            speaker_2_user_id, question
        )

        template = Template(self.ANSWER_PROMPT)
        answer_prompt = template.render(
            speaker_1_user_id=speaker_1_user_id.split("_")[0],
            speaker_2_user_id=speaker_2_user_id.split("_")[0],
            speaker_1_memories=speaker_1_memories,
            speaker_2_memories=speaker_2_memories,
            question=question,
        )

        t1 = time.time()
        response = self.openai_client.chat.completions.create(
            model=os.getenv("MODEL", "gpt-4o"),
            messages=[{"role": "system", "content": answer_prompt}],
            temperature=0.0,
        )
        t2 = time.time()
        response_time = t2 - t1
        return (
            response.choices[0].message.content,
            speaker_1_memories,
            speaker_2_memories,
            speaker_1_memory_time,
            speaker_2_memory_time,
            response_time,
        )

    def process_question(self, val, speaker_a_user_id, speaker_b_user_id):
        question = val.get("question", "")
        answer = val.get("answer", "")
        category = val.get("category", -1)
        evidence = val.get("evidence", [])
        adversarial_answer = val.get("adversarial_answer", "")

        (
            response,
            speaker_1_memories,
            speaker_2_memories,
            speaker_1_memory_time,
            speaker_2_memory_time,
            response_time,
        ) = self.answer_question(
            speaker_a_user_id, speaker_b_user_id, question, answer, category
        )

        result = {
            "question": question,
            "answer": answer,
            "category": category,
            "evidence": evidence,
            "response": response,
            "adversarial_answer": adversarial_answer,
            "speaker_1_memories": speaker_1_memories,
            "speaker_2_memories": speaker_2_memories,
            "num_speaker_1_memories": len(speaker_1_memories),
            "num_speaker_2_memories": len(speaker_2_memories),
            "speaker_1_memory_time": speaker_1_memory_time,
            "speaker_2_memory_time": speaker_2_memory_time,
            "response_time": response_time,
        }

        # Save results after each question is processed
        with open(self.output_path, "w") as f:
            json.dump(self.results, f, indent=4)

        return result

    def process_data_file(self, file_path, exclude_category={5}):
        with open(file_path, "r") as f:
            data = json.load(f)
        for idx, item in tqdm(
            enumerate(data), total=len(data), desc="Processing conversations"
        ):
            if str(idx) in self.results:
                print(f"Skipping {idx} because it already exists")
                continue
            self.results[idx] = []
            qa = item["qa"]
            conversation = item["conversation"]
            speaker_a = conversation["speaker_a"]
            speaker_b = conversation["speaker_b"]
            qa_filtered = [
                i for i in qa if i.get("category", -1) not in exclude_category
            ]
            speaker_a_user_id = f"{speaker_a}_{idx}"
            speaker_b_user_id = f"{speaker_b}_{idx}"
            print(
                f"Filter category: {exclude_category}, {len(qa)} -> {len(qa_filtered)}"
            )
            results = self.process_questions_parallel(
                qa_filtered,
                speaker_a_user_id,
                speaker_b_user_id,
                max_workers=10,
            )
            self.results[idx].extend(results)
            with open(self.output_path, "w") as f:
                json.dump(self.results, f, indent=4)
            # for question_item in tqdm(
            #     qa,
            #     total=len(qa),
            #     desc=f"Processing questions for conversation {idx}",
            #     leave=False,
            # ):
            #     if question_item.get("category", -1) in exclude_category:
            #         continue
            #     result = self.process_question(
            #         question_item, speaker_a_user_id, speaker_b_user_id
            #     )
            #     self.results[idx].append(result)

            #     # Save results after each question is processed
            #     with open(self.output_path, "w") as f:
            #         json.dump(self.results, f, indent=4)

        # Final save at the end
        with open(self.output_path, "w") as f:
            json.dump(self.results, f, indent=4)

    def process_questions_parallel(
        self, qa_list, speaker_a_user_id, speaker_b_user_id, max_workers=1
    ):
        def process_single_question(val):
            result = self.process_question(val, speaker_a_user_id, speaker_b_user_id)
            # Save results after each question is processed
            with open(self.output_path, "w") as f:
                json.dump(self.results, f, indent=4)
            return result

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(
                tqdm(
                    executor.map(process_single_question, qa_list),
                    total=len(qa_list),
                    desc="Answering Questions",
                )
            )

        return results



================================================
FILE: docs/experiments/locomo-benchmark/src/memzero/add.py
================================================
from mem0 import MemoryClient
import json
import time
import os
import threading
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

load_dotenv()


# Update custom instructions
custom_instructions = """
Generate personal memories that follow these guidelines:

1. Each memory should be self-contained with complete context, including:
   - The person's name, do not use "user" while creating memories
   - Personal details (career aspirations, hobbies, life circumstances)
   - Emotional states and reactions
   - Ongoing journeys or future plans
   - Specific dates when events occurred

2. Include meaningful personal narratives focusing on:
   - Identity and self-acceptance journeys
   - Family planning and parenting
   - Creative outlets and hobbies
   - Mental health and self-care activities
   - Career aspirations and education goals
   - Important life events and milestones

3. Make each memory rich with specific details rather than general statements
   - Include timeframes (exact dates when possible)
   - Name specific activities (e.g., "charity race for mental health" rather than just "exercise")
   - Include emotional context and personal growth elements

4. Extract memories only from user messages, not incorporating assistant responses

5. Format each memory as a paragraph with a clear narrative structure that captures the person's experience, challenges, and aspirations
"""


class MemoryADD:
    def __init__(self, data_path=None, batch_size=2, is_graph=False):
        self.mem0_client = MemoryClient(
            api_key=os.getenv("MEM0_API_KEY"),
            org_id=os.getenv("MEM0_ORGANIZATION_ID"),
            project_id=os.getenv("MEM0_PROJECT_ID"),
        )

        self.mem0_client.update_project(custom_instructions=custom_instructions)
        self.batch_size = batch_size
        self.data_path = data_path
        self.data = None
        self.is_graph = is_graph
        if data_path:
            self.load_data()

    def load_data(self):
        with open(self.data_path, "r") as f:
            self.data = json.load(f)
        return self.data

    def add_memory(self, user_id, message, metadata, retries=3):
        for attempt in range(retries):
            try:
                _ = self.mem0_client.add(
                    message,
                    user_id=user_id,
                    version="v2",
                    metadata=metadata,
                    enable_graph=self.is_graph,
                )
                return
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(1)  # Wait before retrying
                    continue
                else:
                    raise e

    def add_memories_for_speaker(self, speaker, messages, timestamp, desc):
        for i in tqdm(range(0, len(messages), self.batch_size), desc=desc):
            batch_messages = messages[i : i + self.batch_size]
            self.add_memory(speaker, batch_messages, metadata={"timestamp": timestamp})

    def process_conversation(self, item, idx):
        conversation = item["conversation"]
        speaker_a = conversation["speaker_a"]
        speaker_b = conversation["speaker_b"]

        speaker_a_user_id = f"{speaker_a}_{idx}"
        speaker_b_user_id = f"{speaker_b}_{idx}"

        # delete all memories for the two users
        self.mem0_client.delete_all(user_id=speaker_a_user_id)
        self.mem0_client.delete_all(user_id=speaker_b_user_id)

        for key in conversation.keys():
            if key in ["speaker_a", "speaker_b"] or "date" in key or "timestamp" in key:
                continue

            date_time_key = key + "_date_time"
            timestamp = conversation[date_time_key]
            chats = conversation[key]

            messages = []
            messages_reverse = []
            for chat in chats:
                if chat["speaker"] == speaker_a:
                    messages.append(
                        {"role": "user", "content": f"{speaker_a}: {chat['text']}"}
                    )
                    messages_reverse.append(
                        {"role": "assistant", "content": f"{speaker_a}: {chat['text']}"}
                    )
                elif chat["speaker"] == speaker_b:
                    messages.append(
                        {"role": "assistant", "content": f"{speaker_b}: {chat['text']}"}
                    )
                    messages_reverse.append(
                        {"role": "user", "content": f"{speaker_b}: {chat['text']}"}
                    )
                else:
                    raise ValueError(f"Unknown speaker: {chat['speaker']}")

            # add memories for the two users on different threads
            thread_a = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(
                    speaker_a_user_id,
                    messages,
                    timestamp,
                    "Adding Memories for Speaker A",
                ),
            )
            thread_b = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(
                    speaker_b_user_id,
                    messages_reverse,
                    timestamp,
                    "Adding Memories for Speaker B",
                ),
            )

            thread_a.start()
            thread_b.start()
            thread_a.join()
            thread_b.join()

        print("Messages added successfully")

    def process_all_conversations(self, max_workers=10):
        if not self.data:
            raise ValueError(
                "No data loaded. Please set data_path and call load_data() first."
            )
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self.process_conversation, item, idx)
                for idx, item in enumerate(self.data)
            ]

            for future in futures:
                future.result()



================================================
FILE: docs/experiments/locomo-benchmark/src/memzero/search.py
================================================
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from mem0 import MemoryClient
import json
import time
from jinja2 import Template
from openai import OpenAI
from prompts import ANSWER_PROMPT_GRAPH, ANSWER_PROMPT
import os
from dotenv import load_dotenv

load_dotenv()


class MemorySearch:

    def __init__(self, output_path='results.json', top_k=10, filter_memories=False, is_graph=False):
        self.mem0_client = MemoryClient(
            api_key=os.getenv("MEM0_API_KEY"),
            org_id=os.getenv("MEM0_ORGANIZATION_ID"),
            project_id=os.getenv("MEM0_PROJECT_ID")
        )
        self.top_k = top_k
        self.openai_client = OpenAI()
        self.results = defaultdict(list)
        self.output_path = output_path
        self.filter_memories = filter_memories
        self.is_graph = is_graph

        if self.is_graph:
            self.ANSWER_PROMPT = ANSWER_PROMPT_GRAPH
        else:
            self.ANSWER_PROMPT = ANSWER_PROMPT

    def search_memory(self, user_id, query, max_retries=3, retry_delay=1):
        start_time = time.time()
        retries = 0
        while retries < max_retries:
            try:
                if self.is_graph:
                    print("Searching with graph")
                    memories = self.mem0_client.search(query, user_id=user_id, top_k=self.top_k,
                                                filter_memories=self.filter_memories, enable_graph=True, output_format='v1.1')
                else:
                    memories = self.mem0_client.search(query, user_id=user_id, top_k=self.top_k,
                                                filter_memories=self.filter_memories)
                break
            except Exception as e:
                print("Retrying...")
                retries += 1
                if retries >= max_retries:
                    raise e
                time.sleep(retry_delay)

        end_time = time.time()
        if not self.is_graph:
            semantic_memories = [{'memory': memory['memory'],
                        'timestamp': memory['metadata']['timestamp'],
                        'score': round(memory['score'], 2)}
                    for memory in memories]
            graph_memories = None
        else:
            semantic_memories = [{'memory': memory['memory'],
                        'timestamp': memory['metadata']['timestamp'],
                        'score': round(memory['score'], 2)} for memory in memories['results']]
            graph_memories = [{"source": relation['source'], "relationship": relation['relationship'], "target": relation['target']} for relation in memories['relations']]
        return semantic_memories, graph_memories, end_time - start_time

    def answer_question(self, speaker_1_user_id, speaker_2_user_id, question, answer, category):
        speaker_1_memories, speaker_1_graph_memories, speaker_1_memory_time = self.search_memory(speaker_1_user_id, question)
        speaker_2_memories, speaker_2_graph_memories, speaker_2_memory_time = self.search_memory(speaker_2_user_id, question)

        search_1_memory = [f"{item['timestamp']}: {item['memory']}" 
                        for item in speaker_1_memories]
        search_2_memory = [f"{item['timestamp']}: {item['memory']}" 
                          for item in speaker_2_memories]

        template = Template(self.ANSWER_PROMPT)
        answer_prompt = template.render(
            speaker_1_user_id=speaker_1_user_id.split('_')[0],
            speaker_2_user_id=speaker_2_user_id.split('_')[0],
            speaker_1_memories=json.dumps(search_1_memory, indent=4),
            speaker_2_memories=json.dumps(search_2_memory, indent=4),
            speaker_1_graph_memories=json.dumps(speaker_1_graph_memories, indent=4),
            speaker_2_graph_memories=json.dumps(speaker_2_graph_memories, indent=4),
            question=question
        )

        t1 = time.time()
        response = self.openai_client.chat.completions.create(
            model=os.getenv("MODEL"),
            messages=[
                {"role": "system", "content": answer_prompt}
            ],
            temperature=0.0
        )
        t2 = time.time()
        response_time = t2 - t1
        return response.choices[0].message.content, speaker_1_memories, speaker_2_memories, speaker_1_memory_time, speaker_2_memory_time, speaker_1_graph_memories, speaker_2_graph_memories, response_time

    def process_question(self, val, speaker_a_user_id, speaker_b_user_id):
        question = val.get('question', '')
        answer = val.get('answer', '')
        category = val.get('category', -1)
        evidence = val.get('evidence', [])
        adversarial_answer = val.get('adversarial_answer', '')

        response, speaker_1_memories, speaker_2_memories, speaker_1_memory_time, speaker_2_memory_time, speaker_1_graph_memories, speaker_2_graph_memories, response_time = self.answer_question(
            speaker_a_user_id,
            speaker_b_user_id,
            question,
            answer,
            category
        )

        result = {
            "question": question,
            "answer": answer,
            "category": category,
            "evidence": evidence,
            "response": response,
            "adversarial_answer": adversarial_answer,
            "speaker_1_memories": speaker_1_memories,
            "speaker_2_memories": speaker_2_memories,
            'num_speaker_1_memories': len(speaker_1_memories),
            'num_speaker_2_memories': len(speaker_2_memories),
            'speaker_1_memory_time': speaker_1_memory_time,
            'speaker_2_memory_time': speaker_2_memory_time,
            "speaker_1_graph_memories": speaker_1_graph_memories,
            "speaker_2_graph_memories": speaker_2_graph_memories,
            "response_time": response_time
        }

        # Save results after each question is processed
        with open(self.output_path, 'w') as f:
            json.dump(self.results, f, indent=4)

        return result

    def process_data_file(self, file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)

        for idx, item in tqdm(enumerate(data), total=len(data), desc="Processing conversations"):
            qa = item['qa']
            conversation = item['conversation']
            speaker_a = conversation['speaker_a']
            speaker_b = conversation['speaker_b']

            speaker_a_user_id = f"{speaker_a}_{idx}"
            speaker_b_user_id = f"{speaker_b}_{idx}"

            for question_item in tqdm(qa, total=len(qa), desc=f"Processing questions for conversation {idx}", leave=False):
                result = self.process_question(
                    question_item,
                    speaker_a_user_id,
                    speaker_b_user_id
                )
                self.results[idx].append(result)

                # Save results after each question is processed
                with open(self.output_path, 'w') as f:
                    json.dump(self.results, f, indent=4)

        # Final save at the end
        with open(self.output_path, 'w') as f:
            json.dump(self.results, f, indent=4)

    def process_questions_parallel(self, qa_list, speaker_a_user_id, speaker_b_user_id, max_workers=1):
        def process_single_question(val):
            result = self.process_question(val, speaker_a_user_id, speaker_b_user_id)
            # Save results after each question is processed
            with open(self.output_path, 'w') as f:
                json.dump(self.results, f, indent=4)
            return result

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(tqdm(
                executor.map(process_single_question, qa_list),
                total=len(qa_list),
                desc="Answering Questions"
            ))

        # Final save at the end
        with open(self.output_path, 'w') as f:
            json.dump(self.results, f, indent=4)

        return results



================================================
FILE: docs/experiments/locomo-benchmark/src/openai/predict.py
================================================
from openai import OpenAI
import os
import json
from jinja2 import Template
from tqdm import tqdm
import time
from collections import defaultdict
from dotenv import load_dotenv
import argparse

load_dotenv()


ANSWER_PROMPT = """
    You are an intelligent memory assistant tasked with retrieving accurate information from conversation memories.

    # CONTEXT:
    You have access to memories from a conversation. These memories contain
    timestamped information that may be relevant to answering the question.

    # INSTRUCTIONS:
    1. Carefully analyze all provided memories
    2. Pay special attention to the timestamps to determine the answer
    3. If the question asks about a specific event or fact, look for direct evidence in the memories
    4. If the memories contain contradictory information, prioritize the most recent memory
    5. If there is a question about time references (like "last year", "two months ago", etc.), 
       calculate the actual date based on the memory timestamp. For example, if a memory from 
       4 May 2022 mentions "went to India last year," then the trip occurred in 2021.
    6. Always convert relative time references to specific dates, months, or years. For example, 
       convert "last year" to "2022" or "two months ago" to "March 2023" based on the memory 
       timestamp. Ignore the reference while answering the question.
    7. Focus only on the content of the memories. Do not confuse character 
       names mentioned in memories with the actual users who created those memories.
    8. The answer should be less than 5-6 words.

    # APPROACH (Think step by step):
    1. First, examine all memories that contain information related to the question
    2. Examine the timestamps and content of these memories carefully
    3. Look for explicit mentions of dates, times, locations, or events that answer the question
    4. If the answer requires calculation (e.g., converting relative time references), show your work
    5. Formulate a precise, concise answer based solely on the evidence in the memories
    6. Double-check that your answer directly addresses the question asked
    7. Ensure your final answer is specific and avoids vague time references

    Memories:

    {{memories}}

    Question: {{question}}
    Answer:
    """


class OpenAIPredict:
    def __init__(self, model="gpt-4o-mini"):
        self.model = model
        self.openai_client = OpenAI()
        self.results = defaultdict(list)

    def search_memory(self, idx):

        with open(f'memories/{idx}.txt', 'r') as file:
            memories = file.read()

        return memories, 0

    def process_question(self, val, idx):
        question = val.get('question', '')
        answer = val.get('answer', '')
        category = val.get('category', -1)
        evidence = val.get('evidence', [])
        adversarial_answer = val.get('adversarial_answer', '')

        response, search_memory_time, response_time, context = self.answer_question(
            idx,
            question
        )

        result = {
            "question": question,
            "answer": answer,
            "category": category,
            "evidence": evidence,
            "response": response,
            "adversarial_answer": adversarial_answer,
            "search_memory_time": search_memory_time,
            "response_time": response_time,
            "context": context
        }

        return result

    def answer_question(self, idx, question):
        memories, search_memory_time = self.search_memory(idx)

        template = Template(ANSWER_PROMPT)
        answer_prompt = template.render(
            memories=memories,
            question=question
        )

        t1 = time.time()
        response = self.openai_client.chat.completions.create(
            model=os.getenv("MODEL"),
            messages=[
                {"role": "system", "content": answer_prompt}
            ],
            temperature=0.0
        )
        t2 = time.time()
        response_time = t2 - t1
        return response.choices[0].message.content, search_memory_time, response_time, memories

    def process_data_file(self, file_path, output_file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)

        for idx, item in tqdm(enumerate(data), total=len(data), desc="Processing conversations"):
            qa = item['qa']

            for question_item in tqdm(qa, total=len(qa), desc=f"Processing questions for conversation {idx}", leave=False):
                result = self.process_question(
                    question_item,
                    idx
                )
                self.results[idx].append(result)

                # Save results after each question is processed
                with open(output_file_path, 'w') as f:
                    json.dump(self.results, f, indent=4)

        # Final save at the end
        with open(output_file_path, 'w') as f:
            json.dump(self.results, f, indent=4)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_file_path", type=str, required=True)
    args = parser.parse_args()
    openai_predict = OpenAIPredict()
    openai_predict.process_data_file("../../dataset/locomo10.json", args.output_file_path)




================================================
FILE: docs/experiments/locomo-benchmark/src/zep/add.py
================================================
import argparse
import json
import os
from dotenv import load_dotenv
from tqdm import tqdm
from zep_cloud import Message
from zep_cloud.client import Zep

load_dotenv()


class ZepAdd:
    def __init__(self, data_path=None):
        self.zep_client = Zep(api_key=os.getenv("ZEP_API_KEY"))
        self.data_path = data_path
        self.data = None
        if data_path:
            self.load_data()

    def load_data(self):
        with open(self.data_path, 'r') as f:
            self.data = json.load(f)
        return self.data

    def process_conversation(self, run_id, item, idx):
        conversation = item['conversation']

        user_id = f"run_id_{run_id}_experiment_user_{idx}"
        session_id = f"run_id_{run_id}_experiment_session_{idx}"

        # # delete all memories for the two users
        # self.zep_client.user.delete(user_id=user_id)
        # self.zep_client.memory.delete(session_id=session_id)

        self.zep_client.user.add(user_id=user_id)
        self.zep_client.memory.add_session(
            user_id=user_id,
            session_id=session_id,
        )

        print("Starting to add memories... for user", user_id)
        for key in tqdm(conversation.keys(), desc=f"Processing user {user_id}"):
            if key in ['speaker_a', 'speaker_b'] or "date" in key:
                continue

            date_time_key = key + "_date_time"
            timestamp = conversation[date_time_key]
            chats = conversation[key]

            for chat in tqdm(chats, desc=f"Adding chats for {key}", leave=False):
                self.zep_client.memory.add(
                    session_id=session_id,
                    messages=[Message(
                        role=chat['speaker'],
                        role_type="user",
                        content=f"{timestamp}: {chat['text']}",
                    )]
                )

    def process_all_conversations(self, run_id):
        if not self.data:
            raise ValueError("No data loaded. Please set data_path and call load_data() first.")
        for idx, item in tqdm(enumerate(self.data)):
            if idx == 0:
                self.process_conversation(run_id, item, idx)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--run_id", type=str, required=True)
    args = parser.parse_args()
    zep_add = ZepAdd(data_path="../../dataset/locomo10.json")
    zep_add.process_all_conversations(args.run_id)


================================================
FILE: docs/experiments/locomo-benchmark/src/zep/search.py
================================================
import argparse
from collections import defaultdict
from dotenv import load_dotenv
from jinja2 import Template
from openai import OpenAI
from tqdm import tqdm
from zep_cloud import EntityEdge, EntityNode
from zep_cloud.client import Zep
import json
import os
import pandas as pd
import time
from prompts import ANSWER_PROMPT_ZEP

load_dotenv()

TEMPLATE = """
FACTS and ENTITIES represent relevant context to the current conversation.

# These are the most relevant facts and their valid date ranges
# format: FACT (Date range: from - to)

{facts}


# These are the most relevant entities
# ENTITY_NAME: entity summary

{entities}

"""


class ZepSearch:
    def __init__(self):
        self.zep_client = Zep(api_key=os.getenv("ZEP_API_KEY"))
        self.results = defaultdict(list)
        self.openai_client = OpenAI()

    def format_edge_date_range(self, edge: EntityEdge) -> str:
        # return f"{datetime(edge.valid_at).strftime('%Y-%m-%d %H:%M:%S') if edge.valid_at else 'date unknown'} - {(edge.invalid_at.strftime('%Y-%m-%d %H:%M:%S') if edge.invalid_at else 'present')}"
        return f"{edge.valid_at if edge.valid_at else 'date unknown'} - {(edge.invalid_at if edge.invalid_at else 'present')}"

    def compose_search_context(self, edges: list[EntityEdge], nodes: list[EntityNode]) -> str:
        facts = [f'  - {edge.fact} ({self.format_edge_date_range(edge)})' for edge in edges]
        entities = [f'  - {node.name}: {node.summary}' for node in nodes]
        return TEMPLATE.format(facts='\n'.join(facts), entities='\n'.join(entities))

    def search_memory(self, run_id, idx, query, max_retries=3, retry_delay=1):
        start_time = time.time()
        retries = 0
        while retries < max_retries:
            try:
                user_id = f"run_id_{run_id}_experiment_user_{idx}"
                session_id = f"run_id_{run_id}_experiment_session_{idx}"
                edges_results = (self.zep_client.graph.search(user_id=user_id, reranker='cross_encoder', query=query, scope='edges', limit=20)).edges
                node_results = (self.zep_client.graph.search(user_id=user_id, reranker='rrf', query=query, scope='nodes', limit=20)).nodes
                context = self.compose_search_context(edges_results, node_results)
                break
            except Exception as e:
                print("Retrying...")
                retries += 1
                if retries >= max_retries:
                    raise e
                time.sleep(retry_delay)

        end_time = time.time()

        return context, end_time - start_time

    def process_question(self, run_id, val, idx):
        question = val.get('question', '')
        answer = val.get('answer', '')
        category = val.get('category', -1)
        evidence = val.get('evidence', [])
        adversarial_answer = val.get('adversarial_answer', '')

        response, search_memory_time, response_time, context = self.answer_question(
            run_id,
            idx,
            question
        )

        result = {
            "question": question,
            "answer": answer,
            "category": category,
            "evidence": evidence,
            "response": response,
            "adversarial_answer": adversarial_answer,
            "search_memory_time": search_memory_time,
            "response_time": response_time,
            "context": context
        }

        return result

    def answer_question(self, run_id, idx, question):
        context, search_memory_time = self.search_memory(run_id, idx, question)

        template = Template(ANSWER_PROMPT_ZEP)
        answer_prompt = template.render(
            memories=context,
            question=question
        )

        t1 = time.time()
        response = self.openai_client.chat.completions.create(
            model=os.getenv("MODEL"),
            messages=[
                {"role": "system", "content": answer_prompt}
            ],
            temperature=0.0
        )
        t2 = time.time()
        response_time = t2 - t1
        return response.choices[0].message.content, search_memory_time, response_time, context

    def process_data_file(self, file_path, run_id, output_file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)

        for idx, item in tqdm(enumerate(data), total=len(data), desc="Processing conversations"):
            qa = item['qa']

            for question_item in tqdm(qa, total=len(qa), desc=f"Processing questions for conversation {idx}", leave=False):
                result = self.process_question(
                    run_id,
                    question_item,
                    idx
                )
                self.results[idx].append(result)

                # Save results after each question is processed
                with open(output_file_path, 'w') as f:
                    json.dump(self.results, f, indent=4)

        # Final save at the end
        with open(output_file_path, 'w') as f:
            json.dump(self.results, f, indent=4)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--run_id", type=str, required=True)
    args = parser.parse_args()
    zep_search = ZepSearch()
    zep_search.process_data_file("../../dataset/locomo10.json", args.run_id, "results/zep_search_results.json")



================================================
FILE: docs/site/cost.mdx
================================================
---
title: Performance and Cost
---

## Overview

Memobase is designed for high performance and cost-efficiency.

-   **Query Performance**: Queries are extremely fast because Memobase returns a pre-compiled user profile, eliminating the need for on-the-fly analysis.
-   **Controllable Costs**: You can manage costs by controlling the size of user profiles. This is done by configuring the number of profile slots and the maximum token size for each.
    -   Learn to design profile slots [here](/features/profile/profile_config).
    -   Learn to control token limits [here](/references/cloud_config).
-   **Insertion Efficiency**: New data is added to a buffer and processed in batches. This approach amortizes the cost of AI analysis, making insertions fast and inexpensive.
    -   Learn to configure the buffer [here](/references/cloud_config).

## Comparison vs. Other Solutions

#### Memobase vs. [mem0](https://github.com/mem0ai/mem0)

-   **Cost**: Memobase is approximately 5x more cost-effective.
-   **Performance**: Memobase is roughly 5x faster.
-   **Memory Quality**: mem0 provides gist-based memories, while Memobase delivers structured and organized profiles for more predictable recall.

The full technical report is available [here](https://github.com/memodb-io/memobase/tree/docs/docs/experiments/900-chats).


================================================
FILE: docs/site/docs.json
================================================
{
  "$schema": "https://mintlify.com/docs.json",
  "theme": "mint",
  "name": "Memobase",
  "description": "First User Profile-based Long-term Memory Backend",
  "favicon": "/logo/favicon.png",
  "colors": {
    "primary": "#6366f1",
    "light": "#f8fafc",
    "dark": "#0f172a"
  },
  "navigation": {
    "tabs": [
      {
        "tab": "Documentation",
        "groups": [
          {
            "group": "Get Started",
            "pages": [
              "introduction",
              "quickstart",
              "features",
              "cost"
            ]
          },
          {
            "group": "Features",
            "pages": [
              "features/context",
              {
                "group": "User Profile",
                "pages": [
                  "features/profile/profile",
                  "features/profile/profile_desc",
                  "features/profile/profile_config",
                  "features/profile/profile_filter",
                  "features/profile/profile_search"
                ]
              },
              {
                "group": "User Event",
                "pages": [
                  "features/event/event",
                  "features/event/event_summary",
                  "features/event/event_tag",
                  "features/event/event_search"
                ]
              },
              "features/message",
              "features/async_insert"
            ]
          },
          {
            "group": "References",
            "pages": [
              {
                "group": "Best Practices",
                "pages": [
                  "practices/bad",
                  "practices/tips",
                  "practices/openai"
                ]
              },
              "references/cloud_config",
              "references/local_config",
              "references/async_client"
            ]
          }
        ]
      },
      {
        "tab": "Examples",
        "groups": [
          {
            "group": "Templates",
            "icon": "lightbulb",
            "pages": [
              "templates/livekit",
              "templates/ollama",
              "templates/openai",
              "templates/dify",
              "templates/mcp"
            ]
          }
        ]
      },
      {
        "tab": "API Reference",
        "groups": [
          {
            "group": "API Reference",
            "icon": "plug",
            "pages": [
              "api-reference/overview",
              {
                "group": "Project",
                "pages": [
                  "api-reference/project/get_profile_config",
                  "api-reference/project/update_profile_config",
                  "api-reference/project/get_users",
                  "api-reference/project/get_usage",
                  "api-reference/utility/healthcheck",
                  "api-reference/utility/usage"
                ]
              },
              {
                "group": "Manage Users",
                "pages": [
                  "api-reference/users/create_user",
                  "api-reference/users/get_user",
                  "api-reference/users/update_user",
                  "api-reference/users/delete_user"
                ]
              },
              {
                "group": "User Data",
                "pages": [
                  "api-reference/blobs/insert_data",
                  {
                    "group": "Supported Blobs",
                    "pages": [
                      "api-reference/blobs/modal/chat",
                      "api-reference/blobs/modal/summary"
                    ]
                  },
                  "api-reference/blobs/get_all_data",
                  "api-reference/blobs/get_blob",
                  "api-reference/blobs/delete_blob"
                ]
              },
              {
                "group": "User Buffer",
                "pages": [
                  "api-reference/buffer/flush",
                  "api-reference/buffer/size"
                ]
              },
              {
                "group": "User Profiles",
                "pages": [
                  "api-reference/profiles/profile",
                  "api-reference/profiles/add_profile",
                  "api-reference/profiles/update_profile",
                  "api-reference/profiles/delete_profile"
                ]
              },
              {
                "group": "User Events",
                "pages": [
                  "api-reference/events/get_events",
                  "api-reference/events/search_events",
                  "api-reference/events/search_event_gists",
                  "api-reference/events/update_event",
                  "api-reference/events/delete_event"
                ]
              },
              {
                "group": "Prompt",
                "pages": [
                  "api-reference/prompt/get_context"
                ]
              },
              {
                "group": "Experimental",
                "pages": [
                  "api-reference/experimental/import_memory",
                  "api-reference/experimental/proactive_topic"
                ]
              }
            ]
          }
        ]
      }
    ],
    "global": {
      "anchors": [
        {
          "anchor": "Playground",
          "href": "https://app.memobase.io/playground",
          "icon": "play"
        },
        {
          "anchor": "Open Source",
          "href": "https://github.com/memodb-io/memobase",
          "icon": "github"
        },
        {
          "anchor": "Community",
          "href": "https://discord.gg/YdgwU4d9NB",
          "icon": "discord"
        },
        {
          "anchor": "Support",
          "href": "mailto:contact@memobase.io",
          "icon": "envelope"
        }
      ]
    }
  },
  "logo": {
    "light": "/logo/light.svg",
    "dark": "/logo/dark.svg",
    "href": "https://memobase.io"
  },
  "background": {
    "color": {
      "light": "#fff",
      "dark": "#0f1117"
    }
  },
  "navbar": {
    "links": [
      {
        "label": "Support",
        "href": "https://github.com/memodb-io/memobase?tab=readme-ov-file#support"
      }
    ],
    "primary": {
      "type": "button",
      "label": "Dashboard",
      "href": "https://app.memobase.io/dashboard"
    }
  },
  "footer": {
    "socials": {
      "x": "https://x.com/memobase_io",
      "github": "https://github.com/memodb-io",
      "discord": "https://discord.gg/YdgwU4d9NB"
    }
  },
  "integrations": {
    "ga4": {
      "measurementId": "G-SLW10C7BL1"
    },
    "gtm": {
      "tagId": "GTM-PDC577XR"
    }
  },
  "seo": {
    "indexing": "all"
  }
}


================================================
FILE: docs/site/features.mdx
================================================
---
title: Features
---

## 🚀 What is Memobase?
- **AI-Powered Backend**: Memobase is a backend service designed to manage dynamic user profiles for your AI applications.
- **Automated Profile Building**: It analyzes user interactions to build rich, structured profiles, capturing everything from basic demographics to specific user preferences.
- **Personalized Experiences**: Leverage these detailed profiles to create highly personalized and engaging user experiences.
- **Scalable & Fast**: Built for performance, Memobase efficiently handles user data at any scale.

## 🖼️ User Profiles as Memory
- **Custom Memory Slots**: Define what your AI should remember. Whether it's a user's favorite color, their dog's name, or professional background, you can create custom fields for any data point.
- **Structured & Predictable**: User profiles are organized into a clear topic/subtopic structure (e.g., `interests/movies`), making them easy to parse and use in your AI logic.
- **Simple & Powerful**: This human-readable format is robust enough to store a lifetime of user memories.

## 👌 Core Use Cases
- **Long-Term Memory**: Give your AI the ability to remember past interactions and user details.
- **User Analysis**: Gain deep insights into user behavior and preferences to enhance your application.
- **Targeted Content**: Deliver personalized content, recommendations, and promotions that resonate with your users.

## 🤔 How It Works
- **Data Blobs**: Memobase stores user data in flexible "blobs." You can insert, retrieve, and delete these data chunks as needed.
- **Buffering System**: Recent data is held in a temporary buffer before being processed and integrated into the long-term user profile. This flush can be triggered automatically or manually.
- **Profile Evolution**: Over time, Memobase constructs comprehensive user profiles that enable your application to deliver truly personalized experiences.

## 💰 Performance and Cost
For details on performance benchmarks and pricing, see our [Cost page](/cost).


================================================
FILE: docs/site/flat_docs.py
================================================
import os
import glob
from pathlib import Path


def read_mdx_files_recursively(directory="."):
    """
    Recursively read all MDX files in the given directory and concatenate them
    with their relative paths.

    Args:
        directory (str): The directory to search in (default: current directory)

    Returns:
        str: Concatenated content of all MDX files with their relative paths
    """
    result = []

    # Use glob to find all .mdx files recursively
    mdx_pattern = os.path.join(directory, "**", "*.mdx")
    mdx_files = glob.glob(mdx_pattern, recursive=True)

    # Sort files for consistent output
    mdx_files.sort()

    for file_path in mdx_files:
        # Get relative path from current directory
        rel_path = os.path.relpath(file_path, directory)

        try:
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read()

            # Add file header with relative path
            result.append(f"=== {rel_path} ===\n")
            result.append(content)
            result.append("\n\n")

        except Exception as e:
            print(f"Error reading {rel_path}: {e}")
            result.append(f"=== {rel_path} ===\n")
            result.append(f"Error reading file: {e}\n\n")

    return "".join(result)


def main():
    """Main function to execute the script"""
    print("Reading all MDX files recursively...")

    # Read all MDX files
    concatenated_content = read_mdx_files_recursively()

    if concatenated_content.strip():
        print(
            f"Found and processed MDX files. Total length: {len(concatenated_content)} characters"
        )

        # Save to DOC.md file
        output_file = "DOC.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(concatenated_content)
        print(f"Content saved to {output_file}")

        # Also print to console (you can comment this out if output is too large)
        print("\n" + "=" * 50)
        print("CONCATENATED CONTENT:")
        print("=" * 50)
        print(concatenated_content)

    else:
        print("No MDX files found in the current directory and its subdirectories.")


if __name__ == "__main__":
    main()



================================================
FILE: docs/site/introduction.mdx
================================================
---
title: What is Memobase?
---

Struggling with short-term memory in your AI applications? Memobase is the solution.

It's a fast, scalable, long-term **user memory backend** for your AI, helping you build applications that remember and understand their users.

<Frame caption="Memobase Architecture Overview">
  <img src="/images/starter.png" />
</Frame>

We currently support the following user memory types from chat interactions:
- [x] [Profile Memory](./features/profile/) - Understand who the user is.
- [x] [Event Memory](./features/event/) - Track what has happened in the user's life.
- [ ] Schedule Memory - Manage the user's calendar (Coming Soon).
- [ ] Social Memory - Map the user's relationships (Coming Soon).

Memobase is under active development, and we are looking for early adopters to become our design partners.

[Get in touch](https://github.com/memodb-io/memobase?tab=readme-ov-file#support) if you're interested in shaping the future of AI memory.

## Get Started

<CardGroup cols={3}>
  <Card
    title="Quickstart"
    icon="rocket"
    href="/quickstart"
  >
    Integrate Memobase with just a few lines of code.
  </Card>
  <Card
    title="Why Memobase"
    icon="question"
    href="/features"
  >
    Learn what makes Memobase unique.
  </Card>
  <Card
    title="API Reference"
    icon="square-terminal"
    href="/api-reference"
  >
    Explore our comprehensive API documentation.
  </Card>
</CardGroup>


## Vision
Our vision is to provide a powerful **second brain** for AI applications, enabling them to build lasting and meaningful user relationships.


================================================
FILE: docs/site/quickstart.mdx
================================================
---
title: 'Quickstart'
---

Ready to give your AI a memory boost? Here’s how to get started.

<CardGroup cols={2}>
  <Card title="Patch OpenAI" icon="webhook" href="practices/openai">
    Upgrade your existing OpenAI setup with Memobase memory.
  </Card>
  <Card title="Client-Side" icon="code-branch" href="#memobase-client">
    Use our SDKs or APIs to connect your app to a Memobase backend.
  </Card>
  <Card title="Server-Side" icon="chart-simple" href="#memobase-backend">
    Deploy your own Memobase backend. It's easier than assembling IKEA furniture.
  </Card>
</CardGroup>


## Memobase Client

### Step 1: Get Prepped

<AccordionGroup>
<Accordion title="Install the SDK">
<CodeGroup>
```bash pip
pip install memobase
```
```bash npm
npm install @memobase/memobase
```
```bash deno
deno add jsr:@memobase/memobase
```

```bash go
go get github.com/memodb-io/memobase/src/client/memobase-go@latest
```

```bash http
# Living on the edge with cURL? Skip to the next step.
```
</CodeGroup>
</Accordion>
<Accordion title="Find Your Project URL and Token">
You'll get these when you set up your [backend](#memobase-backend). Keep them handy.
</Accordion>
</AccordionGroup>

### Step 2: Connect to the Backend

<AccordionGroup>
<Accordion title="Instantiate the Client">
<CodeGroup>
```python Python
from memobase import MemoBaseClient

client = MemoBaseClient(
    project_url=YOUR_PROJECT_URL,
    api_key=YOUR_API_KEY,
)
```
```typescript Node
import { MemoBaseClient, Blob, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env['MEMOBASE_PROJECT_URL'], process.env['MEMOBASE_API_KEY'])
```

{/* ```go Go
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }
}
``` */}
</CodeGroup>
</Accordion>
<Accordion title="Test the Connection (Ping!)">
<CodeGroup>

```python Python
assert client.ping()
```
```typescript Node
const ping = await client.ping()
```
{/* ```go Go
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Ping the server
    if !client.Ping() {
        log.Fatal("Failed to connect to server")
    }
    fmt.Println("Successfully connected to server")
}
``` */}

```bash cURL
curl -H "Authorization: Bearer $YOUR_API_KEY" "$YOUR_PROJECT_URL/api/v1/healthcheck"
```

```json Output
{"data":null,"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>
</AccordionGroup>

### Step 3: User Management

Create, read, update, and delete users.

<AccordionGroup>
<Accordion title="Create a User">
<CodeGroup>

```python Python
uid = client.add_user({"name": "Gustavo"})
```
```typescript Node
const userId = await client.addUser({name: "Gustavo"})
```
{/* ```go Go
import "github.com/google/uuid"

userID := uuid.New().String()
_, err := client.AddUser(map[string]interface{}{"name": "Gustavo"}, userID)
``` */}
```bash cURL
curl -X POST "$YOUR_PROJECT_URL/api/v1/users" \
     -H "Authorization: Bearer $YOUR_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"data": {"name": "Gustavo"}}'
```

```json Output
{"data":{"id":"some-unique-user-id"},"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>

<Accordion title="Get a User">
<CodeGroup>

```python Python
u = client.get_user(uid)
```

```typescript Node
const user = await client.getUser(userId)
```

{/* ```go Go
user, err := client.GetUser(userID, false)
``` */}

```bash cURL
curl -X GET "$YOUR_PROJECT_URL/api/v1/users/{uid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":{"data":{"name":"Gustavo", "status": "caffeinated"}, ... },"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>

{/* <Accordion title="Delete a User">
<CodeGroup>

```python Python
client.delete_user(uid)
```

```typescript Node
await client.deleteUser(userId)
```

```go Go
err = client.DeleteUser(userID)
```

```bash cURL
curl -X DELETE "$YOUR_PROJECT_URL/api/v1/users/{uid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":null,"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion> */}
</AccordionGroup>


### Step 4: Insert Message

Now that you have a user, let's give them some memories.

<AccordionGroup>
<Accordion title="Insert Data (e.g. Chats)">
<CodeGroup>

```python Python
from memobase import ChatBlob
b = ChatBlob(messages=[
    {"role": "user", "content": "Hi, I'm here again"},
    {"role": "assistant", "content": "Hi, Gus! How can I help you?"}
])
u = client.get_user(uid)
bid = u.insert(b)
```

```typescript Node
const blobId = await user.insert(Blob.parse({
    type: BlobType.Enum.chat,
    messages: [{
        role: 'user',
        content: 'Hello, how are you?'
    }]
}))
```

{/* ```go Go
chatBlob := &blob.ChatBlob{
    Messages: []blob.OpenAICompatibleMessage{
        {Role: "user", Content: "Hello, I am Jinjia!"},
        {Role: "assistant", Content: "Hi there! How can I help you today?"},
    },
}
blobID, err := user.Insert(chatBlob, false)
``` */}

```bash cURL
curl -X POST "$YOUR_PROJECT_URL/api/v1/blobs/insert/{uid}" \
     -H "Authorization: Bearer $YOUR_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{ "blob_type": "chat", "blob_data": { "messages": [ {"role": "user","content": "Hi, Im here again"}, {"role": "assistant", "content": "Hi, Gus! How can I help you?"}] }}'
```

```json Output
{"data":{"id":"some-unique-blob-id"},"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>

{/* <Accordion title="Get Data">
<CodeGroup>

```python Python
b = u.get(bid)
```

```typescript Node
const blob = await user.get(blobId)
```

```go Go
blob, err := user.Get(blobID)
```

```bash cURL
curl -X GET "$YOUR_PROJECT_URL/api/v1/blobs/{uid}/{bid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":{"blob_type":"chat", "blob_data":{...}},"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>

<Accordion title="Delete Data">
<CodeGroup>

```python Python
u.delete(bid)
```

```typescript Node
await user.delete(blobId)
```

```go Go
err := user.Delete(blobID)
```

```bash cURL
curl -X DELETE "$YOUR_PROJECT_URL/api/v1/blobs/{uid}/{bid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":null,"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion> */}
</AccordionGroup>

### Step 5: Memory Operations

This is where the magic happens. Memobase extracts and stores memories for each user.

1.  **`flush`**: User data is held in a buffer. If the buffer gets too large or remains idle, it gets processed. You can also trigger this manually.

<Accordion title="Flush (e.g., after a chat session ends)">
<CodeGroup>

```python Python
u.flush() # async
u.flush(sync=True) # sync
```

```typescript Node
await user.flush(BlobType.Enum.chat)
```

{/* ```go Go
err := user.Flush(blob.ChatType, false)
``` */}

```bash cURL
curl -X POST "$YOUR_PROJECT_URL/api/v1/users/buffer/{uid}/chat" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":null,"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion>

{/* 2.  **`profile`**: Get the memory profile of a user. */}

{/* <Accordion title="Get User Profile">
<CodeGroup>

```python Python
u.profile()
```

```typescript Node
const profiles = await user.profile()
```

```go Go
profiles, err := user.Profile(nil)
```

```bash cURL
curl -X GET "$YOUR_PROJECT_URL/api/v1/users/profile/{uid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```json Output
{"data":{"profiles":[ {"content":"Gus", "attributes":{}}, ... ]},"errno":0,"errmsg":""}
```
</CodeGroup>
</Accordion> */}

2.  Get Personalized Context:

<Accordion title="context API">
<CodeGroup>

```python Python
u.context()
```

```typescript Node
const context = await user.context();
```
{/* 
```go Go
context, err := user.Context(nil)
``` */}

```bash cURL
curl -X GET "$YOUR_PROJECT_URL/api/v1/users/context/{uid}" \
     -H "Authorization: Bearer $YOUR_API_KEY"
```

```txt Output
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Background:
- basic_info:name: Gus
- basic_info:location: San Francisco
...

## Latest Events:
- Gus went to the gym [mentioned on 2024-01-02]
- Gus had a coffee with his friend [mentioned on 2024-01-01]
...
```
</CodeGroup>
</Accordion>

3. Pack the context into your system prompt:
<Accordion title="Insert personalized contex">


<CodeGroup>
```python Python
from openai import OpenAI

client = OpenAI(api_key=YOUR_API_KEY)

SYSTEM_PROMPT = f"""You're a helpful assistant.
Your job is to ...

Below is the user's memory:
{context}"""


response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": "Who'am I?s"}
    ]
)

print(response.choices[0].message.content)
```
```txt Output
You're Gus! How can I help you today?
```
</CodeGroup>
</Accordion>


## Memobase Backend
We offer an [open-source solution](https://github.com/memodb-io/memobase) with a Docker backend to launch your own instance.
You can use `docker-compose` to launch the backend [in one command](https://github.com/memodb-io/memobase/blob/main/src/server/readme.md#get-started).


## Memobase Cloud
We also offer a [hosted cloud service](https://www.memobase.io/), with free tier and nice dashboard.

<Frame caption="Memobase Cloud Dashboard">
  <video
  autoPlay
  muted
  loop
  playsInline
  src="https://github.com/user-attachments/assets/eb2eea30-48bc-4714-9706-e417ae1931df"
></video>
</Frame>


================================================
FILE: docs/site/api-reference/overview.mdx
================================================
# Memobase API Overview

Memobase provides a powerful set of APIs for integrating user profile-based memory capabilities into your GenAI applications. Our APIs are designed to help your AI remember users through efficient memory management and profile generation.

## Key Features

- **User Memory Management**: Create, retrieve, update, and delete user memories with ease
- **Profile Generation**: Automatically generate user profiles from conversations and interactions
- **Buffer System**: Efficient memory processing with buffer zones for recent interactions
- **Customizable Profiles**: Define the aspects you want Memobase to collect about your users
- **Secure Authentication**: API token-based access control for your data

## API Structure

Our API is organized into several main categories:

1. **User APIs**: Manage user entities and their data
   - Create and manage users
   - Update user information
   - Delete user accounts

2. **Data APIs**: Handle data operations
   - Insert blobs
   - Get blobs
   - Delete blobs
   - Get all blobs

3. **Profile APIs**: Access and manage user profiles
   - Get user profiles
   - Delete specific profiles
   - Customize profile generation

## Authentication

All API requests require authentication using Bearer token authentication. Include your API token in the Authorization header of each request:

```http
Authorization: Bearer YOUR_ACCESS_TOKEN
```

## Getting Started

To begin using the Memobase API, you'll need to:

1. Set up your Memobase backend server
   - Default URL: `http://localhost:8019`
   - Default token: `secret`

2. Make your first API call:
   ```python
   from memobase import MemoBaseClient
   
   mb = MemoBaseClient("http://localhost:8019", "secret")
   assert mb.ping()
   ```

3. Start exploring the APIs!


## Data Processing

By default, Memobase processes and removes raw memory blobs after generating profiles. This ensures:
- Efficient storage usage
- Privacy-focused data handling
- Relevant information extraction

You can customize this behavior through configuration settings.

For detailed API endpoint documentation, explore the specific API sections in this documentation.



================================================
FILE: docs/site/api-reference/blobs/delete_blob.mdx
================================================
---
title: 'Delete Blob'
openapi: delete /api/v1/blobs/{user_id}/{blob_id}
---

Delete a specific memory blob from a user's storage. This operation permanently removes the blob data from the system. 


================================================
FILE: docs/site/api-reference/blobs/get_all_data.mdx
================================================
---
title: 'Get User Blobs'
openapi: get /api/v1/users/blobs/{user_id}/{blob_type}
---

Retrieve all memory blobs of a specific type for a user. This endpoint supports pagination to manage large sets of memory data efficiently.

Query Parameters:
- page: Page number (default: 0)
- page_size: Number of items per page (default: 10)



================================================
FILE: docs/site/api-reference/blobs/get_blob.mdx
================================================
---
title: 'Get Blob'
openapi: get /api/v1/blobs/{user_id}/{blob_id}
---

Retrieve a specific memory blob for a user. This endpoint returns the detailed content and metadata of a single memory blob. 


================================================
FILE: docs/site/api-reference/blobs/insert_data.mdx
================================================
---
title: 'Insert Data to a User'
openapi: post /api/v1/blobs/insert/{user_id}
---

Insert new memory data (blob) for a specific user. This endpoint handles the storage of memory data and automatically updates the user's memory buffer.

The inserted data will be processed and integrated into the user's long-term memory profile.

Memobase plans to support the following blob types:  
- `ChatBlob`: ✅ [supported](/api-reference/blobs/modal/chat).  
- `SummaryBlob`: ✅ [supported](/api-reference/blobs/modal/summary).  
- `DocBlob`: 🚧 in progress  
- `ImageBlob`: 🚧 in progress  
- `CodeBlob`: 🚧 in progress  
- `TranscriptBlob`: 🚧 in progress  


================================================
FILE: docs/site/api-reference/blobs/modal/chat.mdx
================================================
---
title: 'ChatBlob'
---

ChatBlob is for user/AI messages. 
Memobase will automatically understand and extract the messages into structured profiles.

An example of ChatBlob is below:

<Accordion title="Example to insert ChatBlob">
<CodeGroup>
```python Python
from memobase import ChatBlob

b = ChatBlob(messages=[
    {"role": "user", "content": "Hello, how are you?"},
    {
        "role": "assistant", 
        "content": "I'm fine, thank you!", 
        "alias": "Her", 
        "created_at": "2025-01-01"
    },
])


u.insert(b)
```
```bash https
curl -X POST "$PROJECT_URL/api/v1/blobs/insert/{uid}" \
     -H "Authorization: Bearer $PROJECT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "blob_type": "chat", "blob_data": { "messages": [ {"role": "user","content": "Hi, Im here again"}, {"role": "assistant", "content": "Hi, Gus! How can I help you?", "alias": "Her", "created_at": "2025-01-01"}] }}'
```
</CodeGroup>
</Accordion>

The message format is OpenAI Message format:
```json
{
  "role": "user" | "assistant",
  "content": "string",
  "alias": "string, optional",
  "created_at": "string, optional"
}
```
- `role`: user or assistant
- `content`: message content
- `alias`: optional. You can set the name of the character(user or assistant), it will reflect in the memory profile.
- `created_at`: optional. You can set the date of the message.



================================================
FILE: docs/site/api-reference/blobs/modal/summary.mdx
================================================
---
title: 'SummaryBlob'
---

Summary is for a already summary of user infomation.
Memobase will automatically understand and extract the summaries into structured profiles and events.

An example of SummaryBlob is below:

<Accordion title="Example to insert SummaryBlob">
<CodeGroup>
```python Python
from memobase import SummaryBlob

b = SummaryBlob(summary="User is a software engineer who works at Memobase....")


u.insert(b)
```
```bash https
curl -X POST "$PROJECT_URL/api/v1/blobs/insert/{uid}" \
     -H "Authorization: Bearer $PROJECT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "blob_type": "summary", "blob_data": { "summary": "User is a software engineer who works at Memobase...." }}'
```
</CodeGroup>
</Accordion>


================================================
FILE: docs/site/api-reference/buffer/flush.mdx
================================================
---
title: 'Flush Buffer'
openapi: post /api/v1/users/buffer/{user_id}/{buffer_type}
---

Flush the memory buffer for a specific user and buffer type. This endpoint ensures all pending memory operations are processed and committed to long-term storage.



================================================
FILE: docs/site/api-reference/buffer/size.mdx
================================================
---
title: 'Get Buffer Ids'
openapi: get /api/v1/users/buffer/capacity/{user_id}/{buffer_type}
---

Get the ids of the buffer for a specific user and buffer type. This endpoint returns buffer ids.



================================================
FILE: docs/site/api-reference/events/delete_event.mdx
================================================
---
title: 'Delete User Event'
openapi: delete /api/v1/users/event/{user_id}/{event_id}
---

Delete a user event.




================================================
FILE: docs/site/api-reference/events/get_events.mdx
================================================
---
title: 'Get User Recent Events'
openapi: get /api/v1/users/event/{user_id}
---


Returns a list of the user's most recent events, ordered by recency.




================================================
FILE: docs/site/api-reference/events/search_event_gists.mdx
================================================
---
title: 'Search Event Gists'
openapi: get /api/v1/users/event_gist/search/{user_id}
---
Search event gists by query.
Event gist is a fraction of User Event. For example, if a user event has the `event_tips`:
```
- A // info
- B // schedule
- C // reminder
```
The event gists will be 
- `- A // info`
- `- B // schedule`
- `- C // reminder`




================================================
FILE: docs/site/api-reference/events/search_events.mdx
================================================
---
title: 'Search Events'
openapi: get /api/v1/users/event/search/{user_id}
---
Search events by query.




================================================
FILE: docs/site/api-reference/events/update_event.mdx
================================================
---
title: 'Update User Event'
openapi: put /api/v1/users/event/{user_id}/{event_id}
---

Update a user event with data.




================================================
FILE: docs/site/api-reference/experimental/import_memory.mdx
================================================
---
title: 'Import Memory from Text'
openapi: POST /api/v1/users/profile/import/{user_id}
---



================================================
FILE: docs/site/api-reference/experimental/proactive_topic.mdx
================================================
---
title: 'Proactive Topics'
openapi: "POST /api/v1/users/roleplay/proactive/{user_id}"
---



================================================
FILE: docs/site/api-reference/profiles/add_profile.mdx
================================================
---
title: 'Add User Profile'
openapi: post /api/v1/users/profile/{user_id}
---
This endpoint allows you to add new profile entries to a user's long-term memory.



================================================
FILE: docs/site/api-reference/profiles/delete_profile.mdx
================================================
---
title: 'Delete User Profile'
openapi: delete /api/v1/users/profile/{user_id}/{profile_id}
---

Delete a specific profile from a user's long-term memory. This endpoint allows you to remove individual profile entries that are no longer needed.



================================================
FILE: docs/site/api-reference/profiles/profile.mdx
================================================
---
title: 'Get User Profile'
openapi: get /api/v1/users/profile/{user_id}
---

Retrieve the real-time user profiles for long-term memory. This endpoint provides access to the consolidated profile information generated from user's memory data.



================================================
FILE: docs/site/api-reference/profiles/update_profile.mdx
================================================
---
title: 'Update User Profile'
openapi: put /api/v1/users/profile/{user_id}/{profile_id}
---
Update a specific profile in a user's long-term memory.



================================================
FILE: docs/site/api-reference/project/get_profile_config.mdx
================================================
---
title: 'Get Current Profile Config'
openapi: get /api/v1/project/profile_config
---


Returns the current profile config, Empty if using the default profile config in `config.yaml`.




================================================
FILE: docs/site/api-reference/project/get_usage.mdx
================================================
---
title: 'Get Project Daily Usage'
openapi: get /api/v1/project/usage
---

Get the daily usage statistics of a project over the last N days.

This endpoint provides detailed usage metrics including:
- Total blob insertions per day
- Successful blob insertions per day  
- Input tokens consumed per day
- Output tokens consumed per day

The data is returned as a time series for the specified number of days, allowing you to track usage patterns and project activity over time. 


================================================
FILE: docs/site/api-reference/project/get_users.mdx
================================================
---
title: 'Get Project Users'
openapi: get /api/v1/project/users
---

Get the users of a project with various filtering and ordering options.

This endpoint allows you to:
- Search users by username
- Order results by different fields (updated_at, profile_count, event_count)
- Control sort direction (ascending or descending)
- Paginate results with limit and offset

The response includes user data along with their profile count and event count for better project insights. 


================================================
FILE: docs/site/api-reference/project/update_profile_config.mdx
================================================
---
title: 'Update Current Profile Config'
openapi: post /api/v1/project/profile_config
---


Updates the current profile config. Checkout more details in [Profile Config](/features/customization/profile#understand-the-user-profile-slots).

Below is an example of your profile config:

```yaml
overwrite_user_profiles:
  - topic: "User Basic Information"
    sub_topics:
      - name: "Name"
      - name: "Gender"
      - name: "Age"
      - name: "Occupation"
        description: "For example, a programmer"
      - name: "City"
  - topic: "User Pet Information"
    sub_topics:
      - name: "Purpose of Pet Ownership"
      - name: "Attitude Towards Pet Ownership"
        description: "whether they like to play with the pet"
      - name: "Pet Medical Habits"
        description: "Whether they are accustomed to finding medicine themselves"
...
``` 

Your profile config will not as strong as the `config.yaml` you used to start Memobase server,
it only affect the profile slots.


================================================
FILE: docs/site/api-reference/prompt/get_context.mdx
================================================
---
title: 'Get User Personalized Context'
openapi: get /api/v1/users/context/{user_id}
---


Return a string of the user's personalized context you can directly insert it into your prompt.

Format:
```
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Background:
{profile}

## Latest Events:
{event}
```



================================================
FILE: docs/site/api-reference/users/create_user.mdx
================================================
---
title: 'Create User'
openapi: post /api/v1/users
---

Create a new user in the memory system with additional user-specific data. This endpoint initializes a new user entity that can store and manage memories.



================================================
FILE: docs/site/api-reference/users/delete_user.mdx
================================================
---
title: 'Delete User'
openapi: delete /api/v1/users/{user_id}
---

Remove a user and all associated data from the memory system. This operation permanently deletes the user's profile and memories.



================================================
FILE: docs/site/api-reference/users/get_user.mdx
================================================
---
title: 'Get User'
openapi: get /api/v1/users/{user_id}
---

Retrieve user information and associated data. This endpoint returns the user's profile and configuration data.



================================================
FILE: docs/site/api-reference/users/update_user.mdx
================================================
---
title: 'Update User'
openapi: put /api/v1/users/{user_id}
---

Update an existing user's data. This endpoint allows you to modify user-specific information and settings.



================================================
FILE: docs/site/api-reference/utility/healthcheck.mdx
================================================
---
title: 'Health Check'
openapi: get /api/v1/healthcheck
---

Check if your memobase server is set up correctly and all required services (database, Redis) are available and functioning properly. 


================================================
FILE: docs/site/api-reference/utility/usage.mdx
================================================
---
title: 'Get Project Usage'
openapi: get /api/v1/project/billing
---

Get the usage of your project.



================================================
FILE: docs/site/features/async_insert.mdx
================================================
---
title: Asynchronous Operations
---

Memobase supports asynchronous operations for inserting and flushing data. Offloading these tasks to background processes improves your application's performance and responsiveness by preventing memory operations from blocking the main thread.

When you perform an asynchronous insert or flush, the data is queued for processing, and the method returns immediately. This allows your application to continue executing while Memobase handles the data in the background.

### SDK Examples

Here’s how to use both synchronous and asynchronous operations in our SDKs:

<CodeGroup>
```python Python
from memobase import MemoBaseClient
from memobase.core.blob import ChatBlob

client = MemoBaseClient(project_url='YOUR_PROJECT_URL', api_key='YOUR_API_KEY')
user = client.get_user('some_user_id')

# Create a data blob
blob = ChatBlob(messages=[
    {"role": "user", "content": "Hi, I'm here again"},
    {"role": "assistant", "content": "Hi, Gus! How can I help you?"}
])

# Asynchronous insert (default behavior)
blob_id = user.insert(blob)

# Asynchronous flush (default behavior)
user.flush()

# Synchronous flush (waits for completion)
user.flush(sync=True)
```

```javascript JavaScript
import { MemoBaseClient, Blob, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

// Asynchronous insert
const blobId = await user.insert(Blob.parse({
  type: BlobType.Enum.chat,
  messages: [
    { role: 'user', content: 'Hi, I\'m here again' },
    { role: 'assistant', content: 'Hi, Gus! How can I help you?' }
  ]
}));

// Asynchronous flush
await user.flush(BlobType.Enum.chat);
```

```go Go
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    client, err := core.NewMemoBaseClient("YOUR_PROJECT_URL", "YOUR_API_KEY")
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    user, err := client.GetUser("EXISTING_USER_ID", false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    chatBlob := &blob.ChatBlob{
        Messages: []blob.OpenAICompatibleMessage{
            {Role: "user", Content: "Hello, I am Jinjia!"},
            {Role: "assistant", Content: "Hi there! How can I help you today?"},
        },
    }

    // Asynchronous insert (sync=false)
    blobID, err := user.Insert(chatBlob, false)
    if err != nil {
        log.Fatalf("Failed to insert blob: %v", err)
    }
    fmt.Printf("Successfully queued blob for insertion with ID: %s\n", blobID)

    // Asynchronous flush (sync=false)
    err = user.Flush(blob.ChatType, false)
    if err != nil {
        log.Fatalf("Failed to flush buffer: %v", err)
    }
    fmt.Println("Successfully queued buffer for flushing")

    // Synchronous flush (sync=true)
    err = user.Flush(blob.ChatType, true)
    if err != nil {
        log.Fatalf("Failed to flush buffer: %v", err)
    }
    fmt.Println("Successfully flushed buffer synchronously")
}
```
</CodeGroup>

For more details, see the API reference for [flush](/api-reference/buffer/flush) and [insert](/api-reference/blobs/insert_data).



================================================
FILE: docs/site/features/context.mdx
================================================
---
title: Retrieving the Memory Prompt
---

Memobase automatically extracts and structures various types of memories from user interactions, including:
-   **User Profile**: Key-value attributes describing the user (e.g., name, location, preferences).
-   **User Events**: Significant occurrences and interactions from the user's history.

This collection of memories forms a user's personalized context. Memobase provides a powerful `context()` API to retrieve this information as a structured string, ready to be injected directly into your LLM prompts.

### Basic Usage

The simplest way to get a user's context is to call the `context()` method on a user object.

<CodeGroup>
```python Python
from memobase import MemoBaseClient, ChatBlob

# Initialize client and get/create a user
client = MemoBaseClient(api_key="your_api_key")
user = client.get_user(client.add_user(profile={"name": "Gus"}))

# Insert data to generate memories
user.insert(
    ChatBlob(
        messages=[
            {"role": "user", "content": "I live in California."},
            {"role": "assistant", "content": "Nice, I've heard it's sunny there!"}
        ]
    )
)

# Retrieve the default context prompt
user_context = user.context()
print(user_context)
```

```txt Output
# Memory
Unless the user has relevant queries, do not actively mention these memories in the conversation.

## User Background:
- basic_info:name: Gus
- basic_info:location: California

## Latest Events:
- User mentioned living in California.
```
</CodeGroup>

### Context-Aware Retrieval

To make the retrieved context more relevant to the ongoing conversation, you can provide recent chat messages. Memobase will perform a semantic search to prioritize the most relevant historical events, rather than simply returning the most recent ones.

<CodeGroup>
```python Python
# Continuing from the previous example...
recent_chats = [
    {"role": "user", "content": "What is my name?"}
]

# Get context relevant to the recent chat
relevant_context = user.context(chats=recent_chats)
print(relevant_context)
```

```txt Output
# Memory
...
## User Background:
- basic_info:name: Gus
- basic_info:location: California

## Latest Events:
- User stated their name is Gus.
- User previously mentioned being called John.
```
</CodeGroup>

### Controlling Context Size

You can manage the size and cost of your prompts by limiting the token count of the retrieved context using the `max_tokens` parameter.

<CodeGroup>
```python Python
# Get a condensed context with a token limit
compact_context = user.context(max_tokens=20)
print(compact_context)
```

```txt Output
# Memory
...
## User Background:
- basic_info:name: Gus

## Latest Events:
- User mentioned living in California.
```
</CodeGroup>

**Note**: The `max_tokens` limit applies to the profile and event content, not the final formatted string. If you use a large custom prompt template, the final output may still exceed the limit.

### Advanced Filtering

The `context()` API offers several parameters for fine-grained control:

-   `prefer_topics`, `only_topics`: Prioritize or exclusively include certain profile topics.
-   `max_subtopic_size`: Limit the number of sub-topics returned per topic.
-   `profile_event_ratio`: Adjust the balance between profile and event information.
-   `time_range_in_days`: Filter events to a specific time window.
-   `customize_context_prompt`: Provide a custom template for the final output string.

For a full list of parameters, refer to the [API Reference for `get_context`](/api-reference/prompt/get_context).



================================================
FILE: docs/site/features/message.mdx
================================================
---
title: Customizing Chat Messages
---

Memobase builds user memories from the chat interactions you provide. However, simple `user` and `assistant` roles are not always sufficient to capture the full context. Memobase allows you to add custom metadata to your messages to handle more complex scenarios.

## Custom Timestamps

It's important to distinguish between two types of timestamps in Memobase:

-   **External Timestamp**: The time a memory is stored or updated in the database.
-   **Internal Timestamp**: The time an event actually occurred according to the content of the memory itself (e.g., a birthday, a travel date).

The internal timestamp is often more critical as it directly influences the AI's understanding and responses. By default, Memobase assumes the insertion time is the time the message occurred. You can override this by providing a `created_at` field.

This is useful for importing historical data or for applications set in fictional timelines.

```python
from memobase import MemoBaseClient, ChatBlob

client = MemoBaseClient(api_key="your_api_key")
user = client.get_user(client.add_user())

# This message occurred in a fictional future year
messages = ChatBlob(messages=[
    dict(role="user", content="I am starting a rebellion.", created_at="Year 32637")
])

user.insert(messages)
```

Memobase will process this chat according to the provided timestamp, resulting in a memory like: `"In the year 32637, the user started a rebellion."`

You can use any date or time format; Memobase will extract the time at the appropriate granularity.

## Character Aliases

For more complex interactions, such as multi-character role-playing, you can assign names or `alias` values to the `user` and `assistant` roles.

```python
messages = ChatBlob(messages=[
    dict(role="user", content="I wish to declare war.", alias="The Emperor"),
    dict(role="assistant", content="Perhaps you should rest instead.", alias="The Advisor")
])
```

By providing aliases, you give Memobase the context to create more accurate and personalized memories, such as: `"The Emperor wished to declare war, but The Advisor suggested rest instead."`


================================================
FILE: docs/site/features/event/event.mdx
================================================
---
title: Event Fundamentals
---

Memobase automatically tracks key events and memories from user interactions, creating a chronological record of their experiences.

```python
from memobase import MemoBaseClient, ChatBlob

# Initialize the client
client = MemoBaseClient(api_key="your_api_key")

# Create a user and insert a chat message
user = client.get_user(client.add_user())
user.insert(
    ChatBlob(
        messages=[{"role": "user", "content": "My name is Gus"}]
    )
)

# Retrieve the user's events
print(user.event())
```

## Event Structure

Each event object contains the following information:

-   **Event Summary** (Optional): A concise summary of the user's recent interaction. Learn more about [Event Summaries](/features/event/event_summary).
-   **Event Tags** (Optional): Semantic tags that categorize the event (e.g., `emotion::happy`, `goal::buy_a_house`). Learn how to [design custom tags](/features/event/event_tag).
-   **Profile Delta** (Required): The specific profile slots that were created or updated during this event.
-   **Created Time** (Required): The timestamp of when the event occurred.

A detailed description of the event format can be found in the [API Reference](/api-reference/events/get_events).




================================================
FILE: docs/site/features/event/event_search.mdx
================================================
---
title: Searching Events
---

User events in Memobase are stored as a sequence of experiences, each enriched with [tags](/features/event/event_tag). By default, events are retrieved in chronological order, but Memobase also provides a powerful search function to find events based on a query.

## Semantic Search

You can perform a semantic search to find events related to a specific topic or concept.

```python
# To use the Python SDK, first install the package:
# pip install memobase

from memobase import MemoBaseClient

client = MemoBaseClient(project_url='YOUR_PROJECT_URL', api_key='YOUR_API_KEY')
user = client.get_user('some_user_id')

# Search for events related to the user's emotions
events = user.search_event("Anything about my emotions")
print(events)
```

This query will return events where the user discussed their emotions, events that were automatically [tagged](/features/event/event_tag) with an `emotion` tag, or events that updated profile slots related to emotion.

For a detailed list of search parameters, please refer to the [API documentation](/api-reference/events/search_events).

## Search Event Gists

A user event is a group of user infos happened in a period of time.
So when you need to search for specific facts or infos, you may need a more fine-grained search.

In Memobase, we call it `event_gist`. `event_gist` is a fraction of `user_event` that only contains one fact, schedule, or reminder of user.

So if you want to search particular things of user, without the context of other infos, you can use `search_event_gist` to conduct a search:
<CodeGroup>
```python Python
# To use the Python SDK, install the package:
# pip install memobase
from memobase import MemoBaseClient
from memobase.core.blob import ChatBlob

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
u = client.get_user(UID)

events = u.search_event_gist('Car')
print(events)
```
```txt Output
- user bought a car [mentioned at 2025-01]
- user drove to the car dealership [mentioned at 2025-06]
- user likes BMW [mentioned at 2024-08]
...
```
</CodeGroup>

For detail API, please refer to [Search Event Gists](/api-reference/events/search_event_gists).


================================================
FILE: docs/site/features/event/event_summary.mdx
================================================
---
title: Customizing Event Summaries
---

For each event, Memobase generates a concise summary of the recent user interaction. This summary serves as the primary input for the subsequent extraction of profile updates and event tags, making its quality crucial for the overall effectiveness of the memory system.

Memobase allows you to customize the prompt used for generating these summaries, giving you greater control over the focus and quality of the extracted information.

## Customizing the Summary Prompt

You can add a custom instruction to the summary generation process via the `event_theme_requirement` field in your `config.yaml` file. For example, if you want the summary to focus on the user's personal information rather than their instructions to the AI, you can configure it as follows:

```yaml config.yaml
event_theme_requirement: "Focus on the user's personal information and feelings, not their direct commands or questions."
```

This allows you to fine-tune the event summaries to better suit the specific needs of your application.


================================================
FILE: docs/site/features/event/event_tag.mdx
================================================
---
title: Using Event Tags
---

Event tags are a powerful feature for automatically categorizing user events with semantic attributes. You can use them to enrich event data and track user behavior over time across various dimensions, such as:

-   Emotion (`happy`, `frustrated`)
-   Life Goals (`buying_a_house`, `learning_a_skill`)
-   Relationships (`new_friend`, `family_mention`)

## Configuring Event Tags

By default, no event tags are recorded. You must define the tags you want to track in your `config.yaml` file:

```yaml config.yaml
event_tags:
  - name: "emotion"
    description: "Records the user's current emotional state."
  - name: "romance"
    description: "Tracks any mention of romantic relationships or feelings."
```

Once configured, Memobase will automatically analyze interactions and apply these tags to events when relevant. The `description` field is crucial for helping the AI accurately understand when to apply each tag.

## Retrieving Event Tags

Event tags are returned as part of the event object when you retrieve a user's event history.

```python
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='YOUR_PROJECT_URL', api_key='YOUR_API_KEY')
user = client.get_user('some_user_id')

events = user.event()
for event in events:
    print(event.event_data.event_tags)
```

## Searching Events by Tag

You can also search for events that have specific tags applied.

```python
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='YOUR_PROJECT_URL', api_key='YOUR_API_KEY')
user = client.get_user('some_user_id')

# Find all events tagged with 'emotion'
events = user.search_event(tags=["emotion"])
print(events)
```

For more details, see the [API Reference](/api-reference/events/search_events).


================================================
FILE: docs/site/features/profile/profile.mdx
================================================
---
title: Profile Fundamentals
---

Memobase serves as a [user profile backend](/features#user-profile-as-memory) for LLM applications, enabling them to track and update specific user attributes over time.

<Frame caption="A sample user profile in Memobase, showing structured data slots.">
  <img src="/images/profile_demo.png" />
</Frame>

By default, Memobase includes a set of built-in profile slots for common use cases, but it also offers full customization to control the specific memories your application collects.

### Locating the `config.yaml` File

Memobase uses a `config.yaml` file for backend configuration. You can find this file at `src/server/api/config.yaml` in your self-hosted instance. A typical configuration looks like this:

```yaml
max_chat_blob_buffer_token_size: 1024
buffer_flush_interval: 3600
llm_api_key: sk-...
best_llm_model: gpt-4o-mini
# ... other settings
```

## Understanding Profile Slots

Memobase comes with a default schema of profile slots, such as:

```markdown
- basic_info
    - name
    - gender
- education
    - school
    - major
```

You can extend this schema by adding custom slots under the `additional_user_profiles` field in `config.yaml`:

```yaml
additional_user_profiles:
    - topic: "Gaming"
      description: "Tracks the user's gaming preferences and achievements."
      sub_topics:
        - name: "FPS"
          description: "First-person shooter games like CSGO, Valorant, etc."
        - name: "LOL"
    - topic: "Professional"
      sub_topics:
        - name: "Industry"
        - name: "Role"
```

Memobase will then track these additional slots and update the user profile accordingly. If you need to define a completely custom schema, use the `overwrite_user_profiles` field instead.

For detailed instructions on formatting profile slots, see [Profile Slot Configuration](/features/profile/profile_desc).


================================================
FILE: docs/site/features/profile/profile_config.mdx
================================================
---
title: Profile Validation and Strictness
---

## Auto-Validation

By default, Memobase validates all new profile information before saving it. This process serves two main purposes:

1.  **Ensures Meaningful Data**: It filters out low-quality or irrelevant information that the LLM might generate, such as "User has a job" or "User did not state their name."
2.  **Maintains Schema Adherence**: It verifies that the extracted information aligns with the descriptions you have defined for your [profile slots](/features/profile/profile_desc).

However, if you find that too much information is being filtered out, you can disable this feature by setting `profile_validate_mode` to `false` in your `config.yaml`:

```yaml config.yaml
profile_validate_mode: false
```

Disabling validation will result in more data being saved, but it may also lead to less accurate or less relevant profile information.

## Strict Mode

By default, Memobase operates in a flexible mode, allowing the AI to extend your defined profile schema with new, relevant sub-topics it discovers during conversations. For example, if your configuration is:

```yaml config.yaml
overwrite_user_profiles:
    - topic: "work"
      sub_topics:
        - "company"
```

Memobase might generate a more detailed profile like this:

```json Possible Profile Output
{
    "work": {
        "company": "Google",
        "position": "Software Engineer",
        "department": "Engineering"
    }
}
```

This is often useful, as it's difficult to anticipate all the valuable information your users might provide. However, if you require that **only** the profile slots you have explicitly defined are saved, you can enable strict mode:

```yaml config.yaml
profile_strict_mode: true
```

In strict mode, Memobase will adhere rigidly to the schema in your `config.yaml`.




================================================
FILE: docs/site/features/profile/profile_desc.mdx
================================================
---
title: Configuring Profile Slots
---

Memobase allows for detailed customization of how profile slots are created and updated.

## Instructing Profile Creation

Memobase uses a `topic` and `sub_topics` structure to define a profile slot. For example:

```yaml
overwrite_user_profiles:
    - topic: "work"
      sub_topics:
        - "company"
        - "position"
        - "department"
```

While this structure is often sufficient, you can provide additional `description` fields to give the AI more context and ensure it tracks the information you need with greater accuracy.

```yaml
overwrite_user_profiles:
    - topic: "work"
      sub_topics:
        - name: "start_date"
          description: "The user's start date at their current job, in YYYY-MM-DD format."
```

The `description` field is optional but highly recommended for achieving precise data extraction.

## Instructing Profile Updates

Memobase not only creates profile slots but also maintains them over time. When a user provides new information, Memobase must decide how to update the existing data.

For example, if a user mentions a new job, the `work/start_date` slot needs to be updated:

-   **Old Value**: `2020-01-01`
-   **New Information**: User starts a new job in 2021.
-   **Default Update**: The value is replaced with `2021-01-01`.

You can control this update behavior by adding an `update_description` to the profile slot. For instance, if you wanted to keep a record of the user's *first-ever* job start date, you could configure it like this:

```yaml
overwrite_user_profiles:
    - topic: "work"
      sub_topics:
        - name: "start_date"
          description: "The user's start date at their current job, in YYYY-MM-DD format."
          update_description: "Always keep the oldest start date. Do not update if a date already exists."
```

With this instruction, Memobase will preserve the original `start_date` instead of overwriting it.








================================================
FILE: docs/site/features/profile/profile_filter.mdx
================================================
---
title: Filtering Profiles at Retrieval
---

Memobase tracks and models a comprehensive profile for each user. You can use this profile in your [AI prompts](/features/context) to provide a global understanding of the user.

While user profiles are generally concise, it is good practice to control the final size of the context you inject into your prompts. Memobase provides several parameters to help you filter profiles at retrieval time.

## Rule-Based Filtering

You can pass rules to the Memobase API to filter profiles based on specific criteria:

-   `max_token_size`: Sets the maximum token size for the entire profile context.
-   `prefer_topics`: Ranks specified topics higher, making them more likely to be included in the final output.
-   `only_topics`: Includes *only* the specified topics, filtering out all others.

Detailed parameter descriptions can be found in the [API documentation](/api-reference/profiles/profile).

## Context-Aware Filtering

Memobase also offers a powerful semantic filtering capability. By passing the latest chat messages to the API, you can retrieve only the most "contextual" or relevant profiles for the current conversation.

This is more advanced than a simple embedding-based search. Memobase uses the LLM to reason about which profile attributes would be most helpful for generating the next response.

For example, if a user says, "Find some restaurants for me," Memobase will intelligently rank profiles like `contact_info::city`, `interests::food`, and `health::allergies` higher in the results.

<CodeGroup>
```python Python
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='YOUR_PROJECT_URL', api_key='YOUR_API_KEY')
user = client.get_user('some_user_id')

# Retrieve profile context relevant to the last message
contextual_profile = user.profile(
    chats=[{"role": "user", "content": "Find some restaurants for me"}],
    need_json=True
)

print(contextual_profile)
```
```json Output
{
  "contact_info": {
    "city": "San Francisco"
  },
  "interests": {
    "food": "Loves Italian and Japanese cuisine"
  },
  "health": {
    "allergies": "None"
  }
}
```
</CodeGroup>

For more details on contextual search, see the [Profile Search documentation](/features/profile/profile_search).


================================================
FILE: docs/site/features/profile/profile_search.mdx
================================================
---
title: Context-Aware Profile Search
---

While Memobase is designed to provide a comprehensive, global [context](/features/context) for each user with very low latency, there are times when you need to search for specific information within a user's profile.

Memobase provides a powerful, context-aware search method to filter out irrelevant memories and retrieve only what's needed for the current turn of the conversation.

## How Profile Search Works

Unlike simple keyword or semantic matching, Memobase's profile search uses the LLM to perform a feature-based analysis. It reasons about what aspects of a user's profile are relevant to their latest query.

For example, if a user asks, "Can you recommend a good restaurant?", Memobase doesn't just search for the term "restaurant." Instead, it identifies key features that would help answer the question, such as:

-   `basic_info::location`: To determine the city for the restaurant search.
-   `interests::food`: To understand the user's cuisine preferences.
-   `health::allergies`: To know what ingredients to avoid.

This intelligent, feature-based approach results in a much more relevant and helpful set of memories than traditional search methods.

<CodeGroup>
```python Python
# Assume 'user' is an initialized MemoBaseUser object
contextual_profile = user.profile(
    chats=[{"role": "user", "content": "Find some restaurants for me"}],
    need_json=True
)

print(contextual_profile)
```
```json Output
{
    "basic_info": {
        "location": "San Francisco",
        "allergies": "peanuts"
    },
    "interests": {
        "food": "User enjoys trying new ramen shops."
    }
}
```
</CodeGroup>

See the [API Reference](/api-reference/profiles/profile#parameter-chats-str) for more details.

### Important Considerations

-   **Latency**: Profile search is a powerful but computationally intensive operation. It can add **2-5 seconds** to your response time, depending on the size of the user's profile. Use it judiciously.
-   **Cost**: Each profile search consumes Memobase tokens (roughly 100-1000 tokens per call), which will affect your usage costs.





================================================
FILE: docs/site/practices/bad.mdx
================================================
---
title: Troubleshooting Common Issues
---

## Issue: Too Much Useless Information is Saved

If your user profiles are cluttered with irrelevant data, follow these steps:

1.  **Define Profile Scope**: Start by clearly defining what information each user profile should contain.
2.  **Refine Slot Descriptions**: Provide clear and specific descriptions for each profile slot in your `config.yaml`. This guides the AI on what to extract. [Learn more](/features/profile/profile_desc).
3.  **Enable Strict Mode**: If the issue persists, enable [strict mode](/features/profile/profile_config#strict-mode) to ensure the AI only saves information that directly matches your defined profile slots.

## Issue: Relevant Information is Not Being Saved

If the AI fails to extract important details, try the following:

1.  **Simplify Descriptions**: Your profile descriptions might be too complex for the LLM to interpret. Try simplifying them to be more direct.
2.  **Disable Validation**: If information is still not being captured, you can disable [profile validation](/features/profile/profile_config#profile-validate-mode) to allow for more flexible data extraction.

## Issue: Event is too long/random/not relevant

1. **Use `event_theme_requirement`**: You can add a `event_theme_requirement` to your `config.yaml`, which will be used to generate events:
```yaml
event_theme_requirement: "Focus on the user's infos, not its instructions."
```


## Issue: Profile Data is Inaccurate or Wrong

To improve the accuracy of the information stored in profiles:

1.  **Add Detail to Descriptions**: Enhance your [profile descriptions](/features/profile/profile_desc) with more context and examples to ensure the AI understands the data format and meaning correctly.
2.  **Use Update Instructions**: For each profile slot, add an [update description](/features/profile/profile_desc#instruct-memobase-to-update-a-profile-slot). This tells Memobase how to intelligently merge new information with existing data, which helps maintain accuracy over time.
3. **Use `event_theme_requirement`**: You can add a `event_theme_requirement` to your `config.yaml` to tell Memobase which kind of infos you want to save to profile:
```yaml
event_theme_requirement: "Focus on the user's infos, not its instructions."
```


================================================
FILE: docs/site/practices/openai.mdx
================================================
---
title: Using Memobase with the OpenAI API
---

<Frame caption="Diagram of OpenAI API with Memory Integration">
  <img src="/images/openai_client.png" />
</Frame>

Memobase integrates with the OpenAI API, allowing you to add long-term memory to chat completions without altering your existing code. This patch works with the official OpenAI SDK and any other OpenAI-compatible provider.

## Setup

1.  **Install SDKs**: Ensure both the Memobase and OpenAI Python SDKs are installed.
    ```bash
    pip install memobase openai
    ```

2.  **Initialize Clients**: Create instances of both the OpenAI and Memobase clients.
    ```python
    from openai import OpenAI
    from memobase import MemoBaseClient

    client = OpenAI()
    mb_client = MemoBaseClient(
        project_url=YOUR_PROJECT_URL,
        api_key=YOUR_API_KEY,
    )
    ```
    You can find your `project_url` and `api_key` after [setting up your backend](/quickstart#memobase-backend).

## Patch Memory

Apply the Memobase memory patch to your OpenAI client instance with a single function call.

```python
from memobase.patch.openai import openai_memory

client = openai_memory(client, mb_client)
```

## Usage

1.  To enable memory, simply add a `user_id` to your standard API call. The client will automatically handle the memory context.

    <CodeGroup>
    ```python OpenAI (Original)
    client.chat.completions.create(
        messages=[
            {"role": "user", "content": "My name is Gus"},
        ],
        model="gpt-4o"
    )
    ```
    ```python OpenAI with Memory
    client.chat.completions.create(
        messages=[
            {"role": "user", "content": "My name is Gus"},
        ],
        model="gpt-4o",
        user_id="test_user_123",
    )
    ```
    </CodeGroup>

2.  If no `user_id` is passed, the client functions exactly like the original OpenAI client.

3.  By default, memory processing is not immediate. User interactions are collected in a buffer to optimize performance. You can manually trigger processing using the `flush` method:
    ```python
    client.flush("test_user_123")
    ```

## Verifying Memory Retention

Once a user's information is captured, it can be recalled in subsequent, separate conversations.

<CodeGroup>
```python OpenAI (No Memory)
# In a new session
response = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "What is my name?"},
    ],
    model="gpt-4o"
)
# Assistant: "I'm sorry, I don't have access to personal information..."
```
```python OpenAI with Memory
# In a new session
response = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "What is my name?"},
    ],
    model="gpt-4o",
    user_id="test_user_123",
)
# Assistant: "Your name is Gus."
```
</CodeGroup>

## How It Works

The `openai_memory` function wraps the OpenAI client with two key actions:

1.  **Before Request**: It retrieves the user's memory context from Memobase and injects it into the prompt.
2.  **After Response**: It saves only the **latest** user query and assistant response to the memory buffer.

For example, if your message history is:
```json
[
    {"role": "user", "content": "My name is Gus"},
    {"role": "assistant", "content": "Hello Gus! How can I help you?"},
    {"role": "user", "content": "What is my name?"}
]
```
And the final response is `Your name is Gus.`, Memobase will only store the last exchange. This is equivalent to:
```python
u.insert(
    ChatBlob(messages=[
        {"role": "user", "content": "What is my name?"},
        {"role": "assistant", "content": "Your name is Gus."},
    ])
)
```
This design ensures you can manage short-term conversation history within your API calls as usual, while Memobase prevents duplicate entries in the long-term memory.

The full implementation script is available [here](https://github.com/memodb-io/memobase/blob/main/assets/openai_memory.py).

## Advanced Usage

### Custom Parameters

You can pass additional arguments to `openai_memory` to customize its behavior:

-   `max_context_size`: Controls the maximum token size of the injected memory context. Defaults to `1000`.
    ```python
    client = openai_memory(client, mb_client, max_context_size=500)
    ```
-   `additional_memory_prompt`: Provides a meta-prompt to guide the LLM on how to use the memory.
    ```python
    # Example: Encourage personalization
    prompt = "Always use the user's memory to provide a personalized answer."
    client = openai_memory(client, mb_client, additional_memory_prompt=prompt)
    ```

### Patched Methods

The patched client includes new helper methods:

-   `client.get_memory_prompt("user_id")`: Returns the current memory prompt that will be injected for a given user.
-   `client.flush("user_id")`: Immediately processes the memory buffer for a user. Call this if you need to see memory updates reflected instantly.









================================================
FILE: docs/site/practices/tips.mdx
================================================
---
title: Best Practices & Tips
---

This guide provides tips for effectively using Memobase in your applications.

## Configuring User Memory

You can define the structure of user profiles by configuring topics and sub-topics in your `config.yaml` file. This tells Memobase what kind of information to track.

```yaml
- topic: "Gaming"
  description: "Tracks the user's gaming preferences and achievements."
  sub_topics:
      - name: "FPS"
      - name: "LOL"
- topic: "Professional"
  description: "Tracks the user's professional background."
  sub_topics:
    - name: "Industry"
    - name: "Role"
```

Memobase uses this configuration to generate structured user profiles. Learn more about customization at [Profile Configuration](/features/profile/profile_config).

## Integrating Memory into Prompts

There are two primary ways to retrieve and use a user's memory.

### Method 1: Profile API (Manual Assembly)

The [Profile API](/api-reference/profiles/profile) returns a structured JSON object containing the user's profile data. You are responsible for formatting this JSON into a string and inserting it into your prompt.

**Key Considerations:**
-   **Context Length**: Control the token count of the memory context to manage cost and performance. Use `max_token_size` to set a hard limit and `max_subtopic_size` to limit the number of sub-topics per topic.
-   **Topic Filtering**: Use `only_topics` to retrieve specific profile sections or `prefer_topics` to prioritize the most important information.

### Method 2: Context API (Automated Assembly)

The [Context API](/api-reference/prompt/get_context) returns a pre-formatted string containing both the user's profile and recent events, ready to be injected directly into your system prompt. It uses a template like this:

```text
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Background:
{profile}

## Latest Events:
{event}
```

## Flushing the Memory Buffer

Memobase uses a buffer to collect user interactions. A `flush` operation processes this buffer and updates the long-term memory. Flushing occurs automatically when:

-   The buffer exceeds a certain size.
-   The buffer has been idle for a set period.

You can also trigger it manually with the `flush` API. It is best practice to call `flush` at the end of a user session or conversation.

## User ID Management

A single user in your application can correspond to multiple Memobase users. This is useful for creating segmented memories.

-   **Example: AI Role-Playing**: If a user interacts with multiple AI agents (e.g., a history tutor and a creative writer), you can create a separate Memobase user for each agent. This keeps the memories for each role distinct.

We recommend designing your system with a one-to-many mapping between your application's user ID and Memobase user IDs.

## Enriching Conversation Data

You can add metadata to the messages you insert to provide more context for memory extraction.

-   **Speaker Alias**: Use `alias` to specify the name of the AI assistant in the conversation.
    ```json
    {
        "role": "assistant",
        "content": "Hi, nice to meet you, Gus!",
        "alias": "HerAI"
    }
    ```
-   **Timestamps**: Provide a `created_at` timestamp for each message so Memobase can build a timeline of events.
    ```json
    {
        "role": "user",
        "content": "Hello, I'm Gus",
        "created_at": "2025-01-14T10:00:00Z"
    }
    ```

See a full implementation in our [demo script](https://github.com/memodb-io/memobase/blob/main/assets/quickstart.py).


================================================
FILE: docs/site/references/async_client.mdx
================================================
---
title: Async Python Client
---

Memobase also provides an async client in `python` SDK, it just the same as the sync client, but every method is async:
```python
from memobase import AsyncMemobaseClient

client = AsyncMemobaseClient(api_key="your_api_key")

async def main():
    assert await client.ping()
    #...
    
    # get user context
    u = await client.get_user(UID)
    print(await u.context())

    # get user profile
    print(await u.profile())

    # get user events
    print(await u.event(topk=10, max_token_size=1000))
```


================================================
FILE: docs/site/references/cloud_config.mdx
================================================
---
title: Cloud Configuration
---

If you're using Memobase on cloud, you can refer to those sections to configure your memory backend:
- [User Profile](/features/profile)
- [User Event](/features/event)


In Memobase Cloud, we don't accept any LLM/Embedding model configuration for security reasons, all the LLM/Embedding costs are counted into [Memobase Tokens](https://www.memobase.io/pricing).


================================================
FILE: docs/site/references/local_config.mdx
================================================
---
title: Self-Hosted Configuration
---
If you develop Memobase locally, you can use a `config.yaml` file to configure Memobase Backend.
## Full Explanation of `config.yaml`
We use a single `config.yaml` file as the source to configure Memobase Backend. An example is like this:

```yaml
# Storage and Performance
persistent_chat_blobs: false
buffer_flush_interval: 3600
max_chat_blob_buffer_token_size: 1024
max_profile_subtopics: 15
max_pre_profile_token_size: 128
cache_user_profiles_ttl: 1200

# Timezone
use_timezone: "UTC"

# LLM Configuration
language: "en"
llm_style: "openai"
llm_base_url: "https://api.openai.com/v1/"
llm_api_key: "YOUR-KEY"
best_llm_model: "gpt-4o-mini"
summary_llm_model: null

# Embedding Configuration
enable_event_embedding: true
embedding_provider: "openai"
embedding_api_key: null
embedding_base_url: null
embedding_dim: 1536
embedding_model: "text-embedding-3-small"
embedding_max_token_size: 8192

# Profile Configuration
additional_user_profiles:
  - topic: "gaming"
    sub_topics:
      - "Soul-Like"
      - "RPG"
profile_strict_mode: false
profile_validate_mode: true

# Summary Configuration
minimum_chats_token_size_for_event_summary: 256
```

## Configuration Categories

### Storage and Performance
- `persistent_chat_blobs`: boolean, default to `false`. If set to `true`, the chat blobs will be persisted in the database.
- `buffer_flush_interval`: int, default to `3600` (1 hour). Controls how frequently the chat buffer is flushed to persistent storage.
- `max_chat_blob_buffer_token_size`: int, default to `1024`. This is the parameter to control the buffer size of Memobase. Larger numbers lower your LLM cost but increase profile update lag.
- `max_profile_subtopics`: int, default to `15`. The maximum subtopics one topic can have. When a topic has more than this, it will trigger a re-organization.
- `max_pre_profile_token_size`: int, default to `128`. The maximum token size of one profile slot. When a profile slot is larger, it will trigger a re-summary.
- `cache_user_profiles_ttl`: int, default to `1200` (20 minutes). Time-to-live for cached user profiles in seconds.
- `llm_tab_separator`: string, default to `"::"`. The separator used for tabs in LLM communications.

### Timezone Configuration
- `use_timezone`: string, default to `null`. Options include `"UTC"`, `"America/New_York"`, `"Europe/London"`, `"Asia/Tokyo"`, and `"Asia/Shanghai"`. If not set, the system's local timezone is used.

### LLM Configuration
- `language`: string, default to `"en"`, available options `{"en", "zh"}`. The prompt language of Memobase.
- `llm_style`: string, default to `"openai"`, available options `{"openai", "doubao_cache"}`. The LLM provider style.
- `llm_base_url`: string, default to `null`. The base URL of any OpenAI-Compatible API.
- `llm_api_key`: string, required. Your LLM API key.
- `llm_openai_default_query`: dictionary, default to `null`. Default query parameters for OpenAI API calls.
- `llm_openai_default_header`: dictionary, default to `null`. Default headers for OpenAI API calls.
- `best_llm_model`: string, default to `"gpt-4o-mini"`. The AI model to use for primary functions.
- `summary_llm_model`: string, default to `null`. The AI model to use for summarization. If not specified, falls back to `best_llm_model`.
- `system_prompt`: string, default to `null`. Custom system prompt for the LLM.

### Embedding Configuration
- `enable_event_embedding`: boolean, default to `true`. Whether to enable event embedding.
- `embedding_provider`: string, default to `"openai"`, available options `{"openai", "jina"}`. The embedding provider to use.
- `embedding_api_key`: string, default to `null`. If not specified and provider is OpenAI, falls back to `llm_api_key`.
- `embedding_base_url`: string, default to `null`. For Jina, defaults to `"https://api.jina.ai/v1"` if not specified.
- `embedding_dim`: int, default to `1536`. The dimension size of the embeddings.
- `embedding_model`: string, default to `"text-embedding-3-small"`. For Jina, must be `"jina-embeddings-v3"`.
- `embedding_max_token_size`: int, default to `8192`. Maximum token size for text to be embedded.

### Profile Configuration
Check what a profile is in Memobase [here](/features/customization/profile).
- `additional_user_profiles`: list, default to `[]`. Add additional user profiles. Each profile should have a `topic` and a list of `sub_topics`.
  - For `topic`, it must have a `topic` field and optionally a `description` field:
  ```yaml
  additional_user_profiles:
    - topic: "gaming"
      # description: "User's gaming interests"
      sub_topics:
        ...
  ```
  - For each `sub_topic`, it must have a `name` field (or just be a string) and optionally a `description` field:
  ```yaml
  sub_topics:
    - "SP1"
    - name: "SP2"
      description: "Sub-Profile 2" 
  ```
- `overwrite_user_profiles`: list, default to `null`. Format is the same as `additional_user_profiles`.
  Memobase has built-in profile slots like `work_title`, `name`, etc. For full control of the slots, use this parameter.
  The final profile slots will be only those defined here.
- `profile_strict_mode`: boolean, default to `false`. Enforces strict validation of profile structure.
- `profile_validate_mode`: boolean, default to `true`. Enables validation of profile data.

### Summary Configuration
- `minimum_chats_token_size_for_event_summary`: int, default to `256`. Minimum token size required to trigger an event summary.
- `event_tags`: list, default to `[]`. Custom event tags for classification.

### Telemetry Configuration
- `telemetry_deployment_environment`: string, default to `"local"`. The deployment environment identifier for telemetry.

## Environment Variable Overrides

All configuration values can be overridden using environment variables. The naming convention is to prefix the configuration field name with `MEMOBASE_` and convert it to uppercase.

For example, to override the `llm_api_key` configuration:

```bash
export MEMOBASE_LLM_API_KEY="your-api-key-here"
```

This is particularly useful for:
- Keeping sensitive information like API keys out of configuration files
- Deploying to different environments (development, staging, production)
- Containerized deployments where environment variables are the preferred configuration method

For complex data types (lists, dictionaries, etc.), you can use JSON-formatted strings:

```bash
# Override additional_user_profiles with a JSON array
export MEMOBASE_ADDITIONAL_USER_PROFILES='[{"topic": "gaming", "sub_topics": ["RPG", "Strategy"]}]'
```

The server will automatically parse JSON-formatted environment variables when appropriate.



================================================
FILE: docs/site/snippets/snippet-intro.mdx
================================================
One of the core principles of software development is DRY (Don't Repeat
Yourself). This is a principle that apply to documentation as
well. If you find yourself repeating the same content in multiple places, you
should consider creating a custom snippet to keep your content in sync.



================================================
FILE: docs/site/templates/dify.mdx
================================================
---
title: Dify Plugin for Long-Term Memory
---

Enhance your Dify applications with long-term memory by integrating the Memobase plugin. This guide will walk you through setting up and using the plugin to create more intelligent, context-aware AI agents.

## Prerequisites

First, find and install the open-source [Memobase plugin](https://github.com/ACAne0320/dify-plugin-memobase) from the Dify plugin marketplace.

<Frame caption="Memobase Plugin in the Dify Marketplace">
  <img src="/images/dify_memobase_marketplace.png" />
</Frame>

Next, you'll need a Memobase instance:

-   **Memobase Cloud**: The easiest way to get started is by signing up for a managed instance on the [Memobase dashboard](https://www.memobase.io/en/dashboard).
-   **Self-Hosted**: For more control, you can deploy your own instance. See the [Memobase GitHub repository](https://github.com/memodb-io/memobase) for instructions.

## Plugin Configuration

To connect the plugin to your Memobase instance, you need to provide two credentials when adding the tool to your Dify application:

1.  **Memobase URL**: The API endpoint for your instance (e.g., `https://api.memobase.dev`).
2.  **Memobase API Key**: Your unique API key for authentication.

You can find both of these in your Memobase dashboard.

<Frame caption="API Key and URL in the Memobase Dashboard">
  <img src="/images/dify_memobase_dashboard.png" />
</Frame>

## Using the Plugin in Dify Workflows

Once configured, you can use the plugin's tools in your Dify workflows to:

-   **Store and Recall Conversations**: Persist dialogue for long-term context.
-   **Personalize Responses**: Retrieve user profiles to tailor interactions.
-   **Access Past Events**: Search and utilize historical user data.
-   **Manage Memory**: Directly manipulate data within your Memobase instance from Dify.

For detailed information on each tool, refer to the [Memobase API Reference](/api-reference/overview).

### Understanding Memobase's Buffer Mechanism

Memobase uses a **buffer system** to optimize processing costs and efficiency:

-   **Buffered Processing**: Messages are not processed immediately. Instead, they are stored in a user buffer until it reaches **512 tokens** or other trigger conditions are met.
-   **Automatic Flush**: When the buffer is full or remains idle for a certain period, Memobase automatically processes all buffered messages together.
-   **Manual Flush**: You can trigger immediate processing using the **Flush Buffer** tool in the Dify plugin.

#### When to Use Flush Buffer

-   **Testing & Debugging**: Use flush after conversations to see immediate results during development.
-   **Session End Detection**: Trigger flush when users leave the chat or close the session.
-   **Critical Updates**: When immediate memory processing is required for important information.

<Tip>Avoid flushing after every message as it increases token consumption significantly. Let Memobase handle automatic processing for optimal cost efficiency.</Tip>

### Example Workflows

#### Basic Memory Workflow

Here is a basic Dify workflow that uses the Memobase plugin to store and retrieve memory without immediate processing:

<Frame caption="A basic Dify workflow for memory storage and retrieval.">
  <img src="/images/dify_memobase_workflow.png" />
</Frame>

You can download the complete workflow file from: [memobase.yml](https://github.com/ACAne0320/dify-plugin-memobase/blob/main/_assets/memobase.yml)

This workflow is suitable for most production scenarios where automatic buffer processing is sufficient.

#### Memory Workflow with Buffer Flush

For scenarios requiring immediate memory processing (testing, debugging, or session-end detection), use this enhanced workflow:

<Frame caption="A Dify workflow with explicit buffer flush for immediate processing.">
  <img src="/images/dify_memobase_flush_workflow_example.png" />
</Frame>

You can download the complete workflow file from: [memobase_flush_workflow.yml](https://github.com/ACAne0320/dify-plugin-memobase/blob/main/_assets/memobase_flush_workflow.yml)

This workflow includes the **Flush Buffer** step to ensure conversation data is processed immediately. Use this pattern when:
-   You need to see memory updates right away during testing
-   The user is ending their session
-   You're storing critical information that must be available immediately

By integrating Memobase, you can build sophisticated AI applications that learn from and remember past interactions, leading to more engaging and personalized user experiences.


================================================
FILE: docs/site/templates/livekit.mdx
================================================
---
title: Building a Voice Agent with LiveKit and Memobase
---

> [Full Code](https://github.com/memodb-io/memobase/tree/dev/assets/tutorials/livekit%2Bmemobase)

This tutorial demonstrates how to build a voice agent with long-term memory using Memobase and LiveKit. This combination is ideal for applications like AI companions, customer support bots, and more.

## Setup

1.  **Get API Keys**:
    -   **Memobase**: Sign up at [Memobase](https://www.memobase.io/en) or [run a local server](/references/local_config).
    -   **LiveKit**: Get your `LIVEKIT_URL`, `API_KEY`, and `API_SECRET` from the [LiveKit Cloud Console](https://cloud.livekit.io/).
    -   **Deepgram**: Get your `DEEPGRAM_API_KEY` from the [Deepgram Console](https://console.deepgram.com/).

2.  **Environment Variables**: Set up your environment variables.

    ```bash
    OPENAI_API_KEY="your_openai_api_key"
    DEEPGRAM_API_KEY="your_deepgram_api_key"
    LIVEKIT_URL="your_livekit_url"
    LIVEKIT_API_KEY="your_livekit_api_key"
    LIVEKIT_API_SECRET="your_livekit_api_secret"
    MEMOBASE_URL="https://api.memobase.io"
    MEMOBASE_API_KEY="your_memobase_api_key"
    ```

3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## Code Breakdown

The full code is available [here](https://github.com/memodb-io/memobase/tree/dev/assets/tutorials/livekit%2Bmemobase/livekit_example.py). We will be using the [LiveKit Agents SDK v1.0](https://docs.livekit.io/agents/build/).

The core of the integration involves subclassing the `livekit.agents.Agent` class and overriding the `llm_node` method to inject memory context from Memobase.

### Agent Initialization

First, we initialize the Memobase client and define our custom agent class.

```python
import os
from dotenv import load_dotenv
from livekit.agents import Agent, llm
from memobase import AsyncMemoBaseClient, ChatBlob
from memobase.utils import string_to_uuid

load_dotenv()
mb_client = AsyncMemoBaseClient(
    api_key=os.getenv("MEMOBASE_API_KEY"), project_url=os.getenv("MEMOBASE_URL")
)

class RAGEnrichedAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="You are a warm-hearted partner who remembers past interactions.",
        )
        self.user_name = os.getenv("MEMOBASE_USER_NAME", "test_user")
        self.chat_log_index = 1
```

### Injecting Memory

Next, we override the `llm_node` method. This method is called just before the chat history is sent to the LLM. Here, we will retrieve the user's memory from Memobase and add it to the system prompt.

```python
async def llm_node(
    self,
    chat_ctx: llm.ChatContext,
    # ... other params
) -> AsyncIterable[llm.ChatChunk]:
    # Ensure Memobase is reachable
    assert await mb_client.ping(), "Memobase is not reachable"
    
    # Get or create the user in Memobase
    user = await mb_client.get_or_create_user(string_to_uuid(self.user_name))

    # Insert new messages into memory
    if len(chat_ctx.items) > self.chat_log_index:
        new_messages = chat_ctx.items[self.chat_log_index : len(chat_ctx.items) - 1]
        if len(new_messages):
            blob = ChatBlob(
                messages=[
                    {"role": m.role, "content": m.content[0]}
                    for m in new_messages
                    if m.role in ["user", "assistant"]
                ]
            )
            await user.insert(blob)
            await user.flush()
            self.chat_log_index = len(chat_ctx.items) - 1

    # Retrieve memory context and add to chat history
    rag_context: str = await user.context(max_token_size=500)
    chat_ctx.add_message(content=rag_context, role="system")
    
    # Call the default LLM node
    return Agent.default.llm_node(self, chat_ctx, tools, model_settings)
```

The `rag_context` string will contain the user's [profile](/features/profile/) and recent [events](/features/event/), formatted and ready to be used by the LLM.

### Running the Agent

Finally, we set up the entry point to run the agent.

```python
async def entrypoint(ctx: JobContext):
    # ... (connect to room, set up STT, TTS, etc.)
    await session.start(
        agent=RAGEnrichedAgent(),
        # ...
    )

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

To run the code, first download the necessary assets:
```bash
python livekit_example.py download-files
```

Then, start the agent:
```bash
python livekit_example.py console
```

You can now have a conversation with the voice agent. It will remember information you provide across different sessions.





================================================
FILE: docs/site/templates/mcp.mdx
================================================
---
title: Memobase and the Model Context Protocol (MCP)
---

> [Full Code](https://github.com/memodb-io/memobase/tree/dev/src/mcp)

This tutorial explains how to integrate Memobase with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) to provide your AI agents with persistent, long-term memory. By using the Memobase MCP server, your agents can store, retrieve, and search memories, making them stateful and context-aware across conversations.

## What is MCP?

The Model Context Protocol is an open standard that allows AI assistants to securely connect to external data sources and tools. This enables them to access real-time information, execute functions, and maintain a persistent state, breaking free from the limitations of their training data.

## Why Memobase + MCP?

Traditional AI conversations are stateless. The Memobase MCP server changes this by providing:

1.  **Persistent Memory**: Store conversation history and user preferences across sessions.
2.  **Semantic Search**: Find relevant context using natural language queries.
3.  **User Profiles**: Build a comprehensive understanding of users over time.
4.  **Cross-Platform Compatibility**: Works with any MCP-compatible client, such as Claude Desktop, Cursor, or Windsurf.

## Setup

### Prerequisites

-   Python 3.11+
-   A Memobase backend (either [local](/references/local_config) or [cloud](https://www.memobase.io/en))

### Installation

We recommend using `uv` for installation:

```bash
# Install uv if you don't have it
pip install uv

# Clone the repository
git clone https://github.com/memodb-io/memobase
cd memobase/src/mcp

# Install dependencies
uv pip install -e .

# Configure environment
cp .env.example .env
# Edit .env with your Memobase credentials
```

### Environment Configuration

Configure your `.env` file:

```bash
TRANSPORT=sse
HOST=0.0.0.0
PORT=8050
MEMOBASE_API_KEY="your_api_key_here"
MEMOBASE_BASE_URL="https://api.memobase.dev"
```

## Running the MCP Server

Start the server using `uv`:

```bash
uv run src/main.py
```

The server will be available at `http://localhost:8050` with an SSE endpoint at `/sse`.

## Client Integration

Configure your MCP client to connect to the Memobase server. For example, in Cursor, add this to your `.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "memobase": {
      "transport": "sse",
      "url": "http://localhost:8050/sse"
    }
  }
}
```

## Available Tools

The Memobase MCP server exposes three powerful tools to your AI agent.

### 1. `save_memory`

Stores information in long-term memory with semantic indexing.

```python
# Example usage in your AI agent
await save_memory("User prefers dark mode and uses Python for backend development.")
```

### 2. `search_memories`

Finds relevant context using natural language queries.

```python
# Search for relevant memories
context = await search_memories("What are the user's programming language preferences?")
```

### 3. `get_user_profiles`

Retrieves a comprehensive, structured user profile.

```python
# Get the full user profile
profiles = await get_user_profiles()
```

## Real-World Example

**Without Memory:**

> **User**: "I prefer Python for backend development."
> **AI**: "That's great! Python is excellent for backend work."
> 
> *Later...*
> 
> **User**: "What's the best language for my new API?"
> **AI**: "There are many options, like Python, Node.js, or Go..."

**With Memobase MCP:**

> **User**: "I prefer Python for backend development."
> **AI**: "Got it. I'll remember your preference for Python."
> *(Memory saved: "User prefers Python for backend development")*
> 
> *Later...*
> 
> **User**: "What's the best language for my new API?"
> **AI**: *(Searches memories)* "Based on your preference for Python, I'd recommend using FastAPI or Django."

## Conclusion

The Memobase MCP server transforms stateless AI interactions into intelligent, context-aware conversations. By providing persistent memory through a standardized protocol, you can build AI applications that learn, remember, and deliver truly personalized experiences.



================================================
FILE: docs/site/templates/ollama.mdx
================================================
---
title: Using Memobase with Ollama
---

> [Full Code](https://github.com/memodb-io/memobase/tree/dev/assets/tutorials/ollama%2Bmemobase)

Memobase supports any OpenAI-compatible LLM provider as its backend. This tutorial demonstrates how to use [Ollama](https://ollama.com/) to run a local LLM for both the Memobase server and your chat application.

## Setup

### 1. Configure Ollama

-   [Install Ollama](https://ollama.com/download) on your local machine.
-   Verify the installation by running `ollama -v`.
-   Pull a model to use. For this example, we'll use `qwen2.5:7b`.
    ```bash
    ollama pull qwen2.5:7b
    ```

### 2. Configure Memobase

To use a local LLM provider with the Memobase server, you need to modify your `config.yaml` file.

<Tip>Learn more about [`config.yaml`](/references/cloud_config).</Tip>

Set the following fields to point to your local Ollama instance:

```yaml config.yaml
llm_api_key: ollama
llm_base_url: http://host.docker.internal:11434/v1
best_llm_model: qwen2.5:7b
```

**Note**: Since the Memobase server runs in a Docker container, we use `host.docker.internal` to allow it to access the Ollama server running on your local machine at port `11434`.

## Code Breakdown

This example uses Memobase's [OpenAI Memory Patch](/practices/openai) for a clear demonstration.

### Client Initialization

First, we set up the OpenAI client to point to our local Ollama server and then apply the Memobase memory patch.

```python
from memobase import MemoBaseClient
from openai import OpenAI
from memobase.patch.openai import openai_memory

# Point the OpenAI client to the local Ollama server
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama",
)

# Initialize the Memobase client
mb_client = MemoBaseClient(
    project_url="http://localhost:8019", # Assuming local Memobase server
    api_key="secret",
)

# Apply the memory patch
client = openai_memory(client, mb_client)
```

After patching, your OpenAI client is now stateful. It will automatically manage memory for each user, recalling information from past conversations.

### Chat Function

Next, we create a chat function that uses the patched client. The key is to pass a `user_id` to trigger the memory functionality.

```python
def chat(message, user_id=None, close_session=False):
    print(f"Q: {message}")
    
    response = client.chat.completions.create(
        messages=[{"role": "user", "content": message}],
        model="qwen2.5:7b",
        stream=True,
        user_id=user_id, # Pass user_id to enable memory
    )
    
    # Display the response
    for chunk in response:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print()

    # Flush the memory buffer at the end of a session
    if close_session and user_id:
        client.flush(user_id)
```

### Testing the Memory

Now, you can test the difference between a stateless and a stateful conversation:

```python
# --- Without Memory ---
print("-- Ollama without memory --")
chat("My name is Gus.")
chat("What is my name?") # The model won't know

# --- With Memory ---
print("\n--- Ollama with memory ---")
user = "gus_123"
chat("My name is Gus.", user_id=user, close_session=True)

# In a new conversation, the model remembers
chat("What is my name?", user_id=user)
```

For the complete code, see the [full example on GitHub](https://github.com/memodb-io/memobase/tree/dev/assets/tutorials/ollama%2Bmemobase).



================================================
FILE: docs/site/templates/openai.mdx
================================================
---
title: OpenAI Client with User Memory
---

A key feature of Memobase is its ability to remember user preferences from conversation history. This tutorial demonstrates how to integrate this memory capability directly into the OpenAI client.

While Memobase offers a simple [patch](/practices/openai) for this, the following sections provide a detailed breakdown of the implementation.

## Setup

1.  **Get API Keys**: Obtain an API key from [Memobase](https://www.memobase.io/en) or run a [local server](https://github.com/memodb-io/memobase/tree/dev/src/server).
2.  **Configure Environment Variables**:
    ```bash
    OPENAI_API_KEY="your_openai_api_key"
    MEMOBASE_URL="https://api.memobase.dev"
    MEMOBASE_API_KEY="your_memobase_api_key"
    ```
3.  **Install Dependencies**:
    ```bash
    pip install openai memobase
    ```

## Code Breakdown

<Frame caption="Diagram of OpenAI API with Memory">
  <img src="/images/openai_client.png" />
</Frame>

The implementation involves three main steps:
1.  **Wrap the OpenAI client**: This allows us to intercept chat messages and inject memory context into prompts.
2.  **Integrate Memobase APIs**: Use the wrappers to store chat history and retrieve user memories.
3.  **Test**: Verify that the memory feature functions correctly.

> You can find the [full source code](https://github.com/memodb-io/memobase/blob/main/src/client/memobase/patch/openai.py) on GitHub.

### Basic Setup

First, initialize the OpenAI and Memobase clients.

```python
import os
from memobase import MemoBaseClient
from openai import OpenAI

client = OpenAI()
mb_client = MemoBaseClient(
    api_key=os.getenv("MEMOBASE_API_KEY"),
    project_url=os.getenv("MEMOBASE_URL"),
)
```

### Wrapping the OpenAI Client

We use duck typing to wrap the OpenAI client. This approach avoids altering the original client's class structure.

```python
def openai_memory(
    openai_client: OpenAI | AsyncOpenAI,
    mb_client: MemoBaseClient
) -> OpenAI | AsyncOpenAI:
    if hasattr(openai_client, "_memobase_patched"):
        return openai_client
    openai_client._memobase_patched = True
    openai_client.chat.completions.create = _sync_chat(
        openai_client, mb_client
    )
```

This simplified code does two things:
- It checks if the client has already been patched to prevent applying the wrapper multiple times.
- It replaces the standard `chat.completions.create` method with our custom `_sync_chat` function, which will contain the memory logic.

### The New `chat.completions.create` Method

Our new `chat.completions.create` method must meet several requirements:
- Accept a `user_id` to enable user-specific memory.
- Support all original arguments to ensure backward compatibility.
- Return the same data types, including support for streaming.
- Maintain performance comparable to the original method.

First, we ensure that calls without a `user_id` are passed directly to the original method.

```python
def _sync_chat(
    client: OpenAI,
    mb_client: MemoBaseClient,
):
    # Save the original create method
    _create_chat = client.chat.completions.create
    def sync_chat(*args, **kwargs) -> ChatCompletion | Stream[ChatCompletionChunk]:
        is_streaming = kwargs.get("stream", False)
        # If no user_id, call the original method
        if "user_id" not in kwargs:
            return _create_chat(*args, **kwargs)

        # Pop user_id and convert it to UUID for Memobase
        user_id = string_to_uuid(kwargs.pop("user_id"))
        ...

    return sync_chat
```
The wrapper passes all arguments (`*args`, `**kwargs`) to the original function, preserving its behavior. Memobase uses UUIDs to identify users, so we convert the provided `user_id` (which can be any string) into a UUID.

If a `user_id` is present, the workflow is:
1.  Get or create the user in Memobase.
2.  Inject the user's memory context into the message list.
3.  Call the original `create` method with the modified messages.
4.  Save the new conversation to Memobase for future recall.

Here is the implementation logic:
```python
def _sync_chat(client: OpenAI, mb_client: MemoBaseClient):
    _create_chat = client.chat.completions.create

    def sync_chat(*args, **kwargs) -> ChatCompletion | Stream[ChatCompletionChunk]:
        # ... existing code for handling no user_id ...
        user_query = kwargs["messages"][-1]
        if user_query["role"] != "user":
            LOG.warning(f"Last message is not from the user: {user_query}")
            return _create_chat(*args, **kwargs)

        # 1. Get or create user
        u = mb_client.get_or_create_user(user_id)

        # 2. Inject user context
        kwargs["messages"] = user_context_insert(
            kwargs["messages"], u
        )

        # 3. Call original method
        response = _create_chat(*args, **kwargs)

        # 4. Save conversation (details below)
        # ... handle streaming and non-streaming cases
```

### Enhancing Messages with User Context

The `user_context_insert` function injects the user's memory into the prompt.

```python
PROMPT = '''

--# ADDITIONAL INFO #--
{user_context}
{additional_memory_prompt}
--# DONE #--'''

def user_context_insert(
    messages, u: User, additional_memory_prompt: str="", max_context_size: int = 750
):
    # Retrieve user's memory from Memobase
    context = u.context(max_token_size=max_context_size)
    if not context:
        return messages

    # Format the context into a system prompt
    sys_prompt = PROMPT.format(
        user_context=context, additional_memory_prompt=additional_memory_prompt
    )

    # Add the context to the list of messages
    if messages and messages[0]["role"] == "system":
        messages[0]["content"] += sys_prompt
    else:
        messages.insert(0, {"role": "system", "content": sys_prompt.strip()})
    return messages
```
This function retrieves the user's context from Memobase, formats it into a special system prompt, and prepends it to the message list sent to OpenAI.

### Saving Conversations

After receiving a response from OpenAI, we save the conversation to Memobase to build the user's memory. This is done asynchronously using a background thread to avoid blocking.

```python
def add_message_to_user(messages: ChatBlob, user: User):
    try:
        user.insert(messages)
        LOG.debug(f"Inserted messages for user {user.id}")
    except ServerError as e:
        LOG.error(f"Failed to insert messages: {e}")
```

#### Non-Streaming Responses

For standard responses, we extract the content and save it.

```python
# Non-streaming case
response_content = response.choices[0].message.content
messages = ChatBlob(
    messages=[
        {"role": "user", "content": user_query["content"]},
        {"role": "assistant", "content": response_content},
    ]
)
threading.Thread(target=add_message_to_user, args=(messages, u)).start()
```

#### Streaming Responses

For streaming, we yield each chunk as it arrives and accumulate the full response. Once the stream is complete, we save the entire conversation.

```python
# Streaming case
def yield_response_and_log():
    full_response = ""
    for chunk in response:
        yield chunk
        content = chunk.choices[0].delta.content
        if content:
            full_response += content

    # Save the complete conversation after streaming finishes
    messages = ChatBlob(
        messages=[
            {"role": "user", "content": user_query["content"]},
            {"role": "assistant", "content": full_response},
        ]
    )
    threading.Thread(target=add_message_to_user, args=(messages, u)).start()
```

### Utility Functions

The patch also adds several helper functions to the client for managing user memory:

```python
# Get a user's profile
def _get_profile(mb_client: MemoBaseClient):
    def get_profile(user_string: str) -> list[UserProfile]:
        uid = string_to_uuid(user_string)
        return mb_client.get_user(uid, no_get=True).profile()
    return get_profile

# Get the formatted memory prompt for a user
def _get_memory_prompt(mb_client: MemoBaseClient, ...):
    def get_memory(user_string: str) -> str:
        uid = string_to_uuid(user_string)
        u = mb_client.get_user(uid, no_get=True)
        context = u.context(...)
        return PROMPT.format(user_context=context, ...)
    return get_memory

# Clear a user's memory
def _flush(mb_client: MemoBaseClient):
    def flush(user_string: str):
        uid = string_to_uuid(user_string)
        return mb_client.get_user(uid, no_get=True).flush()
    return flush
```
These functions provide direct access to get a user's profile, retrieve the generated memory prompt, or clear a user's history in Memobase.

## Usage Example

Here’s how to use the patched OpenAI client.

```python
import os
from openai import OpenAI
from memobase import MemoBaseClient
from memobase.patch import openai_memory

# 1. Initialize clients
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
mb_client = MemoBaseClient(
    api_key=os.getenv("MEMOBASE_API_KEY"),
    project_url=os.getenv("MEMOBASE_URL"),
)

# 2. Patch the OpenAI client
memory_client = openai_memory(openai_client, mb_client)

# 3. Use the patched client with a user_id
# The first time, the AI won't know the user's name.
response = memory_client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hi, do you know my name?"}],
    user_id="john_doe"  # Can be any string identifier
)
print(response.choices[0].message.content)
# Expected output: "I'm sorry, I don't know your name."
```

Now, let's inform the AI of the user's name and see if it remembers.

```python
# Tell the AI the user's name
memory_client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "My name is John Doe."}],
    user_id="john_doe"
)

# Start a new conversation and ask again
response = memory_client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's my name?"}],
    user_id="john_doe"
)

print(response.choices[0].message.content)
# Expected output: "Your name is John Doe."
```
Because the conversation history is now stored in Memobase, the AI can recall the user's name in subsequent, separate conversations.

## Conclusion

This guide demonstrates a powerful method for adding persistent user memory to the OpenAI client. The patched client:

- **Is fully compatible**: It works identically to the original client.
- **Enables memory**: Adds memory capabilities when a `user_id` is provided.
- **Supports all modes**: Handles both streaming and non-streaming responses.
- **Is automatic**: Seamlessly saves conversations and injects context without extra code.

This approach offers a clean and non-intrusive way to build personalized, stateful AI experiences into your existing OpenAI applications.








================================================
FILE: src/client/memobase/__init__.py
================================================
from .core.entry import MemoBaseClient, User, ChatBlob
from .core.blob import SummaryBlob
from .core.entry import MemoBaseClient as Memobase
from .core.async_entry import AsyncMemoBaseClient, AsyncUser

__author__ = "memobase.io"
__version__ = "0.0.24"
__url__ = "https://github.com/memodb-io/memobase"
__license__ = "Apache-2.0"



================================================
FILE: src/client/memobase/error.py
================================================
class ServerError(Exception):
    pass



================================================
FILE: src/client/memobase/network.py
================================================
from httpx import Response
from .core.type import BaseResponse

PREFIX = "/api/v1"


def unpack_response(response: Response) -> BaseResponse:
    response.raise_for_status()  # This will raise an HTTPError if the status is 4xx, 5xx
    r = BaseResponse.model_validate(response.json())
    r.raise_for_status()
    return r



================================================
FILE: src/client/memobase/utils.py
================================================
import logging
import uuid

LOG = logging.getLogger("memobase")


def string_to_uuid(s: str, salt="memobase_client") -> str:
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, s + salt))



================================================
FILE: src/client/memobase/core/__init__.py
================================================
# - Flat is better than nested.
# - Try not to make new stuffs.
# - Printability is a great feature.
# - Secretly handling errors for users is arrogant.
# - Making variable names long does not hurt anyone.
# - Don't make things too fancy; it kills possibilities.
# - Any single type should be fully functional on its own.



================================================
FILE: src/client/memobase/core/async_entry.py
================================================
import os
import json
import httpx
from collections import defaultdict
from typing import Optional, Literal
from pydantic import HttpUrl, ValidationError
from dataclasses import dataclass
from urllib.parse import quote_plus
from .blob import BlobData, Blob, BlobType, ChatBlob, OpenAICompatibleMessage
from .user import UserProfile, UserProfileData, UserEventData, UserEventGistData
from ..network import unpack_response
from ..error import ServerError
from ..utils import LOG


def profiles_to_json(profiles: list[UserProfile]) -> dict:
    results = defaultdict(dict)
    for p in profiles:
        results[p.topic][p.sub_topic] = {
            "id": p.id,
            "content": p.content,
            "created_at": p.created_at,
            "updated_at": p.updated_at,
        }
    return dict(results)


@dataclass
class AsyncMemoBaseClient:
    api_key: Optional[str] = None
    api_version: str = "api/v1"
    project_url: str = "https://api.memobase.dev"

    def __post_init__(self):
        self.api_key = self.api_key or os.getenv("MEMOBASE_API_KEY")
        assert (
            self.api_key is not None
        ), "api_key of memobase client is required, pass it as argument or set it as environment variable(MEMOBASE_API_KEY)"
        self.base_url = str(HttpUrl(self.project_url)) + self.api_version.strip("/")

        self._client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
            },
            timeout=60,
        )

    @property
    def client(self) -> httpx.AsyncClient:
        return self._client

    async def ping(self) -> bool:
        try:
            unpack_response(await self._client.get("/healthcheck"))
        except httpx.HTTPStatusError as e:
            LOG.error(f"Healthcheck failed: {e}")
            return False
        except ServerError as e:
            LOG.error(f"Healthcheck failed: {e}")
            return False
        return True

    async def get_usage(self) -> dict:
        r = unpack_response(await self._client.get("/project/billing"))
        return r.data

    async def get_config(self) -> str:
        r = unpack_response(await self._client.get("/project/profile_config"))
        return r.data["profile_config"]

    async def update_config(self, config: str) -> bool:
        r = unpack_response(
            await self._client.post(
                "/project/profile_config", json={"profile_config": config}
            )
        )
        return True

    async def add_user(self, data: dict = None, id=None) -> str:
        r = unpack_response(
            await self._client.post("/users", json={"data": data, "id": id})
        )
        return r.data["id"]

    async def update_user(self, user_id: str, data: dict = None) -> str:
        r = unpack_response(
            await self._client.put(f"/users/{user_id}", json={"data": data})
        )
        return r.data["id"]

    async def get_user(self, user_id: str, no_get=False) -> "AsyncUser":
        if not no_get:
            r = unpack_response(await self._client.get(f"/users/{user_id}"))
            return AsyncUser(
                user_id=user_id,
                project_client=self,
                fields=r.data,
            )
        return AsyncUser(user_id=user_id, project_client=self)

    async def get_or_create_user(self, user_id: str) -> "AsyncUser":
        try:
            return await self.get_user(user_id)
        except ServerError:
            await self.add_user(id=user_id)
        return AsyncUser(user_id=user_id, project_client=self)

    async def delete_user(self, user_id: str) -> bool:
        r = unpack_response(await self._client.delete(f"/users/{user_id}"))
        return True

    async def get_all_users(
        self,
        search: str = "",
        order_by: str = "updated_at",
        order_desc: bool = True,
        limit: int = 10,
        offset: int = 0,
    ) -> list[dict]:
        r = unpack_response(
            await self._client.get(
                f"/project/users?search={search}&order_by={order_by}&order_desc={order_desc}&limit={limit}&offset={offset}"
            )
        )
        return r.data["users"]

    async def get_daily_usage(self, days: int = 7) -> dict:
        r = unpack_response(await self._client.get(f"/project/usage?last_days={days}"))
        return r.data

    async def close(self):
        await self._client.aclose()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()


@dataclass
class AsyncUser:
    user_id: str
    project_client: AsyncMemoBaseClient
    fields: Optional[dict] = None

    async def insert(self, blob_data: Blob, sync=False) -> str:
        r = unpack_response(
            await self.project_client.client.post(
                f"/blobs/insert/{self.user_id}?wait_process={sync}",
                json=blob_data.to_request(),
            )
        )
        return r.data["id"]

    async def get(self, blob_id: str) -> Blob:
        r = unpack_response(
            await self.project_client.client.get(f"/blobs/{self.user_id}/{blob_id}")
        )
        return BlobData.model_validate(r.data).to_blob()

    async def get_all(
        self, blob_type: BlobType, page: int = 0, page_size: int = 10
    ) -> list[str]:
        r = unpack_response(
            await self.project_client.client.get(
                f"/users/blobs/{self.user_id}/{blob_type}?page={page}&page_size={page_size}"
            )
        )
        return r.data["ids"]

    async def delete(self, blob_id: str) -> bool:
        r = unpack_response(
            await self.project_client.client.delete(f"/blobs/{self.user_id}/{blob_id}")
        )
        return True

    async def flush(self, blob_type: BlobType = BlobType.chat, sync=False) -> bool:
        r = unpack_response(
            await self.project_client.client.post(
                f"/users/buffer/{self.user_id}/{blob_type}?wait_process={sync}"
            )
        )
        return True

    async def add_profile(self, content: str, topic: str, sub_topic: str) -> str:
        r = unpack_response(
            await self.project_client.client.post(
                f"/users/profile/{self.user_id}",
                json={
                    "content": content,
                    "attributes": {"topic": topic, "sub_topic": sub_topic},
                },
            )
        )
        return r.data["id"]

    async def buffer(
        self,
        blob_type: BlobType,
        status: Literal["idle", "processing", "done", "failed"] = "idle",
    ) -> list[str]:
        r = unpack_response(
            await self.project_client.client.get(
                f"/users/buffer/capacity/{self.user_id}/{blob_type}?status={status}"
            )
        )
        return r.data["ids"]

    async def profile(
        self,
        max_token_size: int = 1000,
        prefer_topics: list[str] = None,
        only_topics: list[str] = None,
        max_subtopic_size: int = None,
        topic_limits: dict[str, int] = None,
        chats: list[OpenAICompatibleMessage] = None,
        need_json: bool = False,
    ) -> list[UserProfile]:
        params = f"?max_token_size={max_token_size}"
        if prefer_topics:
            prefer_topics_query = [f"&prefer_topics={pt}" for pt in prefer_topics]
            params += "&".join(prefer_topics_query)
        if only_topics:
            only_topics_query = [f"&only_topics={ot}" for ot in only_topics]
            params += "&".join(only_topics_query)
        if max_subtopic_size:
            params += f"&max_subtopic_size={max_subtopic_size}"
        if topic_limits:
            params += f"&topic_limits_json={json.dumps(topic_limits)}"
        if chats:
            for c in chats:
                try:
                    OpenAICompatibleMessage(**c)
                except ValidationError as e:
                    raise ValueError(f"Invalid chat message: {e}")
            chats_query = f"&chats_str={json.dumps(chats)}"
            params += chats_query
        r = unpack_response(
            await self.project_client.client.get(
                f"/users/profile/{self.user_id}{params}"
            )
        )
        data = r.data["profiles"]
        ds_profiles = [UserProfileData.model_validate(p).to_ds() for p in data]
        if need_json:
            return profiles_to_json(ds_profiles)
        return ds_profiles

    async def update_profile(
        self, profile_id: str, content: str, topic: str, sub_topic: str
    ) -> str:
        r = unpack_response(
            await self.project_client.client.put(
                f"/users/profile/{self.user_id}/{profile_id}",
                json={
                    "content": content,
                    "attributes": {"topic": topic, "sub_topic": sub_topic},
                },
            )
        )
        return True

    async def delete_profile(self, profile_id: str) -> bool:
        r = unpack_response(
            await self.project_client.client.delete(
                f"/users/profile/{self.user_id}/{profile_id}"
            )
        )
        return True

    async def event(
        self, topk=10, max_token_size=None, need_summary=False
    ) -> list[UserEventData]:
        params = f"?topk={topk}"
        if max_token_size is not None:
            params += f"&max_token_size={max_token_size}"
        if need_summary:
            params += f"&need_summary=true"
        r = unpack_response(
            await self.project_client.client.get(f"/users/event/{self.user_id}{params}")
        )
        return [UserEventData.model_validate(e) for e in r.data["events"]]

    async def delete_event(self, event_id: str) -> bool:
        r = unpack_response(
            await self.project_client.client.delete(
                f"/users/event/{self.user_id}/{event_id}"
            )
        )
        return True

    async def update_event(self, event_id: str, event_data: dict) -> bool:
        r = unpack_response(
            await self.project_client.client.put(
                f"/users/event/{self.user_id}/{event_id}", json=event_data
            )
        )
        return True

    async def search_event(
        self,
        query: str,
        topk: int = 10,
        similarity_threshold: float = 0.5,
        time_range_in_days: int = 180,
    ) -> list[UserEventData]:
        params = f"?query={query}&topk={topk}&similarity_threshold={similarity_threshold}&time_range_in_days={time_range_in_days}"
        r = unpack_response(
            await self.project_client.client.get(
                f"/users/event/search/{self.user_id}{params}"
            )
        )
        return [UserEventData.model_validate(e) for e in r.data["events"]]

    async def search_event_gist(
        self,
        query: str,
        topk: int = 10,
        similarity_threshold: float = 0.2,
        time_range_in_days: int = 180,
    ) -> list[UserEventData]:
        params = f"?query={query}&topk={topk}&similarity_threshold={similarity_threshold}&time_range_in_days={time_range_in_days}"
        r = unpack_response(
            self.project_client.client.get(
                f"/users/event_gist/search/{self.user_id}{params}"
            )
        )
        return [UserEventGistData.model_validate(e) for e in r.data["gists"]]

    async def context(
        self,
        max_token_size: int = 1000,
        prefer_topics: list[str] = None,
        only_topics: list[str] = None,
        max_subtopic_size: int = None,
        topic_limits: dict[str, int] = None,
        profile_event_ratio: float = None,
        require_event_summary: bool = None,
        chats: list[OpenAICompatibleMessage] = None,
        event_similarity_threshold: float = None,
        customize_context_prompt: str = None,
        full_profile_and_only_search_event: bool = None,
        fill_window_with_events: bool = None,
    ) -> str:
        params = f"?max_token_size={max_token_size}"
        if prefer_topics:
            prefer_topics_query = [f"&prefer_topics={pt}" for pt in prefer_topics]
            params += "&".join(prefer_topics_query)
        if only_topics:
            only_topics_query = [f"&only_topics={ot}" for ot in only_topics]
            params += "&".join(only_topics_query)
        if max_subtopic_size:
            params += f"&max_subtopic_size={max_subtopic_size}"
        if topic_limits:
            params += f"&topic_limits_json={json.dumps(topic_limits)}"
        if profile_event_ratio:
            params += f"&profile_event_ratio={profile_event_ratio}"
        if require_event_summary is not None:
            params += (
                f"&require_event_summary={'true' if require_event_summary else 'false'}"
            )
        if chats:
            for c in chats:
                try:
                    OpenAICompatibleMessage(**c)
                except ValidationError as e:
                    raise ValueError(f"Invalid chat message: {e}")
            chats_query = f"&chats_str={json.dumps(chats)}"
            params += chats_query
        if event_similarity_threshold:
            params += f"&event_similarity_threshold={event_similarity_threshold}"
        if customize_context_prompt:
            params += (
                f"&customize_context_prompt={quote_plus(customize_context_prompt)}"
            )
        if full_profile_and_only_search_event is not None:
            params += f"&full_profile_and_only_search_event={'true' if full_profile_and_only_search_event else 'false'}"
        if fill_window_with_events is not None:
            params += f"&fill_window_with_events={'true' if fill_window_with_events else 'false'}"
        r = unpack_response(
            await self.project_client.client.get(
                f"/users/context/{self.user_id}{params}"
            )
        )
        return r.data["context"]



================================================
FILE: src/client/memobase/core/blob.py
================================================
# Synced from backend 0.0.5
from enum import StrEnum
from datetime import datetime
from typing import Literal, Optional
from pydantic import BaseModel


class OpenAICompatibleMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str
    alias: Optional[str] = None
    created_at: Optional[str] = None


class TranscriptStamp(BaseModel):
    content: str
    start_timestamp_in_seconds: float
    end_time_timestamp_in_seconds: Optional[float] = None
    speaker: Optional[str] = None


class BlobType(StrEnum):
    chat = "chat"
    summary = "summary"
    doc = "doc"
    image = "image"
    code = "code"
    transcript = "transcript"


class Blob(BaseModel):
    type: BlobType
    fields: Optional[dict] = None
    created_at: Optional[datetime] = None

    def get_blob_data(self):
        return self.model_dump(exclude={"type", "fields", "created_at"})

    def to_request(self):
        return {
            "blob_type": self.type,
            "fields": self.fields,
            "blob_data": self.get_blob_data(),
        }


class ChatBlob(Blob):
    messages: list[OpenAICompatibleMessage]
    type: Literal[BlobType.chat] = BlobType.chat


class DocBlob(Blob):
    content: str
    type: Literal[BlobType.doc] = BlobType.doc


class SummaryBlob(Blob):
    summary: str
    type: Literal[BlobType.summary] = BlobType.summary


class CodeBlob(Blob):
    content: str
    language: Optional[str] = None
    type: Literal[BlobType.code] = BlobType.code


class ImageBlob(Blob):
    url: Optional[str] = None
    base64: Optional[str] = None
    type: Literal[BlobType.image] = BlobType.image


class TranscriptBlob(Blob):
    transcripts: list[TranscriptStamp]
    type: Literal[BlobType.transcript] = BlobType.transcript


class BlobData(BaseModel):
    blob_type: BlobType
    blob_data: dict  # messages/doc/images...
    fields: Optional[dict] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    def to_blob(self) -> Blob:
        if self.blob_type == BlobType.chat:
            return ChatBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.summary:
            return SummaryBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.doc:
            return DocBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.image:
            raise NotImplementedError("ImageBlob not implemented yet.")
        elif self.blob_type == BlobType.transcript:
            raise NotImplementedError("TranscriptBlob not implemented yet.")



================================================
FILE: src/client/memobase/core/entry.py
================================================
import os
import json
import time
import httpx
from collections import defaultdict
from typing import Optional, Literal
from pydantic import HttpUrl, ValidationError
from dataclasses import dataclass
from urllib.parse import quote_plus
from .blob import BlobData, Blob, BlobType, ChatBlob, OpenAICompatibleMessage
from .user import UserProfile, UserProfileData, UserEventData, UserEventGistData
from ..network import unpack_response
from ..error import ServerError
from ..utils import LOG


def profiles_to_json(profiles: list[UserProfile]) -> dict:
    results = defaultdict(dict)
    for p in profiles:
        results[p.topic][p.sub_topic] = {
            "id": p.id,
            "content": p.content,
            "created_at": p.created_at,
            "updated_at": p.updated_at,
        }
    return dict(results)


@dataclass
class MemoBaseClient:
    api_key: Optional[str] = None
    api_version: str = "api/v1"
    project_url: str = "https://api.memobase.dev"

    def __post_init__(self):
        self.api_key = self.api_key or os.getenv("MEMOBASE_API_KEY")
        assert (
            self.api_key is not None
        ), "api_key of memobase client is required, pass it as argument or set it as environment variable(MEMOBASE_API_KEY)"
        self.base_url = str(HttpUrl(self.project_url)) + self.api_version.strip("/")

        self._client = httpx.Client(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
            },
            timeout=60,
        )

    @property
    def client(self) -> httpx.Client:
        return self._client

    def ping(self) -> bool:
        try:
            unpack_response(self._client.get("/healthcheck"))
        except httpx.HTTPStatusError as e:
            LOG.error(f"Healthcheck failed: {e}")
            return False
        except ServerError as e:
            LOG.error(f"Healthcheck failed: {e}")
            return False
        return True

    def get_config(self) -> str:
        r = unpack_response(self._client.get("/project/profile_config"))
        return r.data["profile_config"]

    def update_config(self, config: str) -> bool:
        r = unpack_response(
            self._client.post(
                "/project/profile_config", json={"profile_config": config}
            )
        )
        return True

    def get_usage(self) -> dict:
        r = unpack_response(self._client.get("/project/billing"))
        return r.data

    def add_user(self, data: dict = None, id=None) -> str:
        r = unpack_response(self._client.post("/users", json={"data": data, "id": id}))
        return r.data["id"]

    def update_user(self, user_id: str, data: dict = None) -> str:
        r = unpack_response(self._client.put(f"/users/{user_id}", json={"data": data}))
        return r.data["id"]

    def get_user(self, user_id: str, no_get=False) -> "User":
        if not no_get:
            r = unpack_response(self._client.get(f"/users/{user_id}"))
            return User(
                user_id=user_id,
                project_client=self,
                fields=r.data,
            )
        return User(user_id=user_id, project_client=self)

    def get_or_create_user(self, user_id: str) -> "User":
        try:
            return self.get_user(user_id)
        except ServerError:
            self.add_user(id=user_id)
        return User(user_id=user_id, project_client=self)

    def delete_user(self, user_id: str) -> bool:
        r = unpack_response(self._client.delete(f"/users/{user_id}"))
        return True

    def get_all_users(
        self,
        search: str = "",
        order_by: str = "updated_at",
        order_desc: bool = True,
        limit: int = 10,
        offset: int = 0,
    ) -> list[dict]:
        r = unpack_response(
            self._client.get(
                f"/project/users?search={search}&order_by={order_by}&order_desc={order_desc}&limit={limit}&offset={offset}"
            )
        )
        return r.data["users"]

    def get_daily_usage(self, days: int = 7) -> dict:
        r = unpack_response(self._client.get(f"/project/usage?last_days={days}"))
        return r.data


@dataclass
class User:
    user_id: str
    project_client: MemoBaseClient
    fields: Optional[dict] = None

    def insert(self, blob_data: Blob, sync=False) -> str:
        r = unpack_response(
            self.project_client.client.post(
                f"/blobs/insert/{self.user_id}?wait_process={sync}",
                json=blob_data.to_request(),
            )
        )
        return r.data["id"]

    def get(self, blob_id: str) -> Blob:
        r = unpack_response(
            self.project_client.client.get(f"/blobs/{self.user_id}/{blob_id}")
        )
        return BlobData.model_validate(r.data).to_blob()

    def get_all(self, blob_type: BlobType, page: int = 0, page_size: int = 10) -> Blob:
        r = unpack_response(
            self.project_client.client.get(
                f"/users/blobs/{self.user_id}/{blob_type}?page={page}&page_size={page_size}"
            )
        )
        return r.data["ids"]

    def delete(self, blob_id: str) -> bool:
        r = unpack_response(
            self.project_client.client.delete(f"/blobs/{self.user_id}/{blob_id}")
        )
        return True

    def flush(self, blob_type: BlobType = BlobType.chat, sync=False) -> bool:
        r = unpack_response(
            self.project_client.client.post(
                f"/users/buffer/{self.user_id}/{blob_type}?wait_process={sync}"
            )
        )
        return True

    def add_profile(self, content: str, topic: str, sub_topic: str) -> str:
        r = unpack_response(
            self.project_client.client.post(
                f"/users/profile/{self.user_id}",
                json={
                    "content": content,
                    "attributes": {"topic": topic, "sub_topic": sub_topic},
                },
            )
        )
        return r.data["id"]

    def buffer(
        self,
        blob_type: BlobType,
        status: Literal["idle", "processing", "done", "failed"] = "idle",
    ) -> list[str]:
        r = unpack_response(
            self.project_client.client.get(
                f"/users/buffer/capacity/{self.user_id}/{blob_type}?status={status}"
            )
        )
        return r.data["ids"]

    def profile(
        self,
        max_token_size: int = 1000,
        prefer_topics: list[str] = None,
        only_topics: list[str] = None,
        max_subtopic_size: int = None,
        topic_limits: dict[str, int] = None,
        chats: list[OpenAICompatibleMessage] = None,
        need_json: bool = False,
    ) -> list[UserProfile]:
        params = f"?max_token_size={max_token_size}"
        if prefer_topics:
            prefer_topics_query = [f"&prefer_topics={pt}" for pt in prefer_topics]
            params += "&".join(prefer_topics_query)
        if only_topics:
            only_topics_query = [f"&only_topics={ot}" for ot in only_topics]
            params += "&".join(only_topics_query)
        if max_subtopic_size:
            params += f"&max_subtopic_size={max_subtopic_size}"
        if topic_limits:
            params += f"&topic_limits_json={json.dumps(topic_limits)}"
        if chats:
            for c in chats:
                try:
                    OpenAICompatibleMessage(**c)
                except ValidationError as e:
                    raise ValueError(f"Invalid chat message: {e}")
            chats_query = f"&chats_str={json.dumps(chats)}"
            params += chats_query
        r = unpack_response(
            self.project_client.client.get(f"/users/profile/{self.user_id}{params}")
        )
        data = r.data["profiles"]
        ds_profiles = [UserProfileData.model_validate(p).to_ds() for p in data]
        if need_json:
            return profiles_to_json(ds_profiles)
        return ds_profiles

    def update_profile(
        self, profile_id: str, content: str, topic: str, sub_topic: str
    ) -> str:
        r = unpack_response(
            self.project_client.client.put(
                f"/users/profile/{self.user_id}/{profile_id}",
                json={
                    "content": content,
                    "attributes": {"topic": topic, "sub_topic": sub_topic},
                },
            )
        )
        return True

    def delete_profile(self, profile_id: str) -> bool:
        r = unpack_response(
            self.project_client.client.delete(
                f"/users/profile/{self.user_id}/{profile_id}"
            )
        )
        return True

    def event(
        self, topk=10, max_token_size=None, need_summary=False
    ) -> list[UserEventData]:
        params = f"?topk={topk}"
        if max_token_size is not None:
            params += f"&max_token_size={max_token_size}"
        if need_summary:
            params += f"&need_summary=true"
        r = unpack_response(
            self.project_client.client.get(f"/users/event/{self.user_id}{params}")
        )
        return [UserEventData.model_validate(e) for e in r.data["events"]]

    def delete_event(self, event_id: str) -> bool:
        r = unpack_response(
            self.project_client.client.delete(f"/users/event/{self.user_id}/{event_id}")
        )
        return True

    def update_event(self, event_id: str, event_data: dict) -> bool:
        r = unpack_response(
            self.project_client.client.put(
                f"/users/event/{self.user_id}/{event_id}", json=event_data
            )
        )
        return True

    def search_event(
        self,
        query: str,
        topk: int = 10,
        similarity_threshold: float = 0.2,
        time_range_in_days: int = 180,
    ) -> list[UserEventData]:
        params = f"?query={query}&topk={topk}&similarity_threshold={similarity_threshold}&time_range_in_days={time_range_in_days}"
        r = unpack_response(
            self.project_client.client.get(
                f"/users/event/search/{self.user_id}{params}"
            )
        )
        return [UserEventData.model_validate(e) for e in r.data["events"]]

    def search_event_gist(
        self,
        query: str,
        topk: int = 10,
        similarity_threshold: float = 0.2,
        time_range_in_days: int = 180,
    ) -> list[UserEventData]:
        params = f"?query={query}&topk={topk}&similarity_threshold={similarity_threshold}&time_range_in_days={time_range_in_days}"
        r = unpack_response(
            self.project_client.client.get(
                f"/users/event_gist/search/{self.user_id}{params}"
            )
        )
        return [UserEventGistData.model_validate(e) for e in r.data["gists"]]

    def context(
        self,
        max_token_size: int = 1000,
        prefer_topics: list[str] = None,
        only_topics: list[str] = None,
        max_subtopic_size: int = None,
        topic_limits: dict[str, int] = None,
        profile_event_ratio: float = None,
        require_event_summary: bool = None,
        chats: list[OpenAICompatibleMessage] = None,
        event_similarity_threshold: float = None,
        customize_context_prompt: str = None,
        full_profile_and_only_search_event: bool = None,
        fill_window_with_events: bool = None,
    ) -> str:
        params = f"?max_token_size={max_token_size}"
        if prefer_topics:
            prefer_topics_query = [f"&prefer_topics={pt}" for pt in prefer_topics]
            params += "&".join(prefer_topics_query)
        if only_topics:
            only_topics_query = [f"&only_topics={ot}" for ot in only_topics]
            params += "&".join(only_topics_query)
        if max_subtopic_size:
            params += f"&max_subtopic_size={max_subtopic_size}"
        if topic_limits:
            params += f"&topic_limits_json={json.dumps(topic_limits)}"
        if profile_event_ratio:
            params += f"&profile_event_ratio={profile_event_ratio}"
        if require_event_summary is not None:
            params += (
                f"&require_event_summary={'true' if require_event_summary else 'false'}"
            )
        if chats:
            for c in chats:
                try:
                    OpenAICompatibleMessage(**c)
                except ValidationError as e:
                    raise ValueError(f"Invalid chat message: {e}")
            chats_query = f"&chats_str={json.dumps(chats, ensure_ascii=False)}"
            params += chats_query
        if event_similarity_threshold:
            params += f"&event_similarity_threshold={event_similarity_threshold}"
        if customize_context_prompt:
            params += (
                f"&customize_context_prompt={quote_plus(customize_context_prompt)}"
            )
        if full_profile_and_only_search_event is not None:
            params += f"&full_profile_and_only_search_event={'true' if full_profile_and_only_search_event else 'false'}"
        if fill_window_with_events is not None:
            params += f"&fill_window_with_events={'true' if fill_window_with_events else 'false'}"
        r = unpack_response(
            self.project_client.client.get(f"/users/context/{self.user_id}{params}")
        )
        return r.data["context"]



================================================
FILE: src/client/memobase/core/type.py
================================================
from typing import Optional
from pydantic import BaseModel
from ..error import ServerError


class BaseResponse(BaseModel):
    data: Optional[dict | list]
    errmsg: str
    errno: int

    def raise_for_status(self):
        if self.errno != 0:
            raise ServerError(self.errmsg)



================================================
FILE: src/client/memobase/core/user.py
================================================
from dataclasses import dataclass
from pydantic import BaseModel, UUID4, UUID5, Field
from typing import Optional
from datetime import datetime


@dataclass
class UserProfile:
    id: str
    created_at: datetime
    updated_at: datetime
    topic: str
    sub_topic: str
    content: str

    @property
    def describe(self) -> str:
        return f"{self.topic}: {self.sub_topic} - {self.content}"


class UserProfileData(BaseModel):
    id: UUID4 | UUID5
    content: str
    attributes: dict
    created_at: datetime
    updated_at: datetime

    def to_ds(self):
        return UserProfile(
            id=self.id,
            content=self.content,
            topic=self.attributes.get("topic", "NONE"),
            sub_topic=self.attributes.get("sub_topic", "NONE"),
            created_at=self.created_at,
            updated_at=self.updated_at,
        )


class ProfileDelta(BaseModel):
    content: str = Field(..., description="The profile content")
    attributes: Optional[dict] = Field(
        ...,
        description="User profile attributes in JSON, containing 'topic', 'sub_topic'",
    )


class EventTag(BaseModel):
    tag: str = Field(..., description="The event tag")
    value: str = Field(..., description="The event tag value")


class EventData(BaseModel):
    profile_delta: list[ProfileDelta] = Field(..., description="List of profile data")
    event_tip: Optional[str] = Field(None, description="Event tip")
    event_tags: Optional[list[EventTag]] = Field(None, description="List of event tags")


class EventGistData(BaseModel):
    content: str = Field(..., description="The event gist content")


class UserEventData(BaseModel):
    id: UUID4 | UUID5 = Field(..., description="The event's unique identifier")
    event_data: Optional[EventData] = Field(None, description="User event data in JSON")
    created_at: datetime = Field(
        None, description="Timestamp when the event was created"
    )
    updated_at: datetime = Field(
        None, description="Timestamp when the event was last updated"
    )
    similarity: Optional[float] = Field(None, description="Similarity score")


class UserEventGistData(BaseModel):
    id: UUID4 | UUID5 = Field(..., description="The event gist's unique identifier")
    gist_data: EventGistData = Field(None, description="User event gist data")
    created_at: datetime = Field(
        None, description="Timestamp when the event gist was created"
    )
    updated_at: datetime = Field(
        None, description="Timestamp when the event gist was last updated"
    )
    similarity: Optional[float] = Field(None, description="Similarity score")



================================================
FILE: src/client/memobase/patch/__init__.py
================================================
[Empty file]


================================================
FILE: src/client/memobase/patch/openai.py
================================================
import threading
from openai import OpenAI, AsyncOpenAI
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_chunk import ChatCompletionChunk
from openai._streaming import Stream
from ..core.entry import MemoBaseClient, User, ChatBlob
from ..core.user import UserProfile
from ..utils import string_to_uuid, LOG
from ..error import ServerError

PROMPT = """

--# ADDITIONAL INFO #--
{user_context}
{additional_memory_prompt}
--# DONE #--"""


def openai_memory(
    openai_client: OpenAI | AsyncOpenAI,
    mb_client: MemoBaseClient,
    additional_memory_prompt: str = "Make sure the user's query needs the memory, otherwise just return the answer directly.",
    max_context_size: int = 1000,
) -> OpenAI | AsyncOpenAI:
    if hasattr(openai_client, "_memobase_patched"):
        return openai_client

    openai_client._memobase_patched = True
    openai_client.get_profile = _get_profile(mb_client)
    openai_client.get_memory_prompt = _get_memory_prompt(
        mb_client, max_context_size, additional_memory_prompt
    )
    openai_client.flush = _flush(mb_client)
    if isinstance(openai_client, OpenAI):
        openai_client.chat.completions.create = _sync_chat(
            openai_client, mb_client, additional_memory_prompt, max_context_size
        )
    elif isinstance(openai_client, AsyncOpenAI):
        raise ValueError(f"AsyncOpenAI is not supported yet")
    else:
        raise ValueError(f"Invalid openai_client type: {type(openai_client)}")
    return openai_client


def _get_profile(mb_client: MemoBaseClient):
    def get_profile(u_string) -> list[UserProfile]:
        uid = string_to_uuid(u_string)
        return mb_client.get_user(uid, no_get=True).profile()

    return get_profile


def _get_memory_prompt(
    mb_client: MemoBaseClient,
    max_context_size: int = 1000,
    additional_memory_prompt: str = "",
):
    def get_memory(u_string) -> list[UserProfile]:
        uid = string_to_uuid(u_string)
        u = mb_client.get_user(uid, no_get=True)
        context = u.context(max_token_size=max_context_size)
        sys_prompt = PROMPT.format(
            user_context=context, additional_memory_prompt=additional_memory_prompt
        )
        return sys_prompt

    return get_memory


def _flush(mb_client: MemoBaseClient):
    def flush(u_string) -> list[UserProfile]:
        uid = string_to_uuid(u_string)
        return mb_client.get_user(uid, no_get=True).flush()

    return flush


def add_message_to_user(messages: ChatBlob, user: User):
    try:
        r = user.insert(messages)
        LOG.debug(f"Insert {messages}")
    except ServerError as e:
        LOG.error(f"Failed to insert message: {e}")


def user_context_insert(
    messages, u: User, additional_memory_prompt: str, max_context_size: int
):
    context = u.context(max_token_size=max_context_size)
    if not len(context):
        return messages
    sys_prompt = PROMPT.format(
        user_context=context, additional_memory_prompt=additional_memory_prompt
    )
    if messages[0]["role"] == "system":
        messages[0]["content"] += sys_prompt
    else:
        messages.insert(0, {"role": "system", "content": sys_prompt.strip()})
    return messages


def _sync_chat(
    client: OpenAI,
    mb_client: MemoBaseClient,
    additional_memory_prompt: str,
    max_context_size: int = 1000,
):
    _create_chat = client.chat.completions.create

    def sync_chat(*args, **kwargs) -> ChatCompletion | Stream[ChatCompletionChunk]:
        is_streaming = kwargs.get("stream", False)
        if kwargs.get("user_id", None) is None:
            kwargs.pop("user_id")
            if not is_streaming:
                return _create_chat(*args, **kwargs)
            else:
                return (r for r in _create_chat(*args, **kwargs))

        user_id = string_to_uuid(kwargs.pop("user_id"))
        user_query = kwargs["messages"][-1]
        if user_query["role"] != "user":
            LOG.warning(f"Last query is not user query: {user_query}")
            if not is_streaming:
                return _create_chat(*args, **kwargs)
            else:
                return (r for r in _create_chat(*args, **kwargs))

        u = mb_client.get_or_create_user(user_id)
        kwargs["messages"] = user_context_insert(
            kwargs["messages"], u, additional_memory_prompt, max_context_size
        )
        response = _create_chat(*args, **kwargs)

        if is_streaming:

            def yield_response_and_log():
                total_response = ""
                r_role = None

                for r in response:
                    yield r
                    try:
                        r_string = r.choices[0].delta.content
                        r_role = r_role or r.choices[0].delta.role
                        total_response += r_string or ""
                    except Exception:
                        continue
                if not len(total_response):
                    return
                if r_role != "assistant":
                    LOG.warning(f"Last response is not assistant response: {r_role}")
                    return response

                messages = ChatBlob(
                    messages=[
                        {"role": "user", "content": user_query["content"]},
                        {"role": "assistant", "content": total_response},
                    ]
                )
                threading.Thread(target=add_message_to_user, args=(messages, u)).start()

            return yield_response_and_log()

        else:
            r_role = response.choices[0].message.role
            if r_role != "assistant":
                LOG.warning(f"Last response is not assistant response: {r_role}")
                return response
            r_string = response.choices[0].message.content
            messages = ChatBlob(
                messages=[
                    {"role": "user", "content": user_query["content"]},
                    {"role": "assistant", "content": r_string},
                ]
            )
            threading.Thread(target=add_message_to_user, args=(messages, u)).start()
            return response

    return sync_chat


# TODO support async openai



================================================
FILE: src/client/memobase-go/README.md
================================================
# MemoBase Go Client

A Go client library for interacting with the MemoBase API. This library allows you to store, retrieve, and manage chat conversation data in MemoBase.

## Installation

```bash
go get github.com/memodb-io/memobase/src/client/memobase-go
```

## Requirements

- Go 1.22.3 or higher
- A MemoBase project URL and API key

## Usage

### Initialize the Client

```go
import (
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

// Initialize with direct API key
client, err := core.NewMemoBaseClient(
    "YOUR_PROJECT_URL",
    "YOUR_API_KEY",
)

// Or using environment variable MEMOBASE_API_KEY
client, err := core.NewMemoBaseClient(
    "YOUR_PROJECT_URL",
    "",  // Will use MEMOBASE_API_KEY environment variable
)
```

### Managing Users

```go
// Create or get a user with UUID
import "github.com/google/uuid"

userid := uuid.New()
user, err := client.GetOrCreateUser(userid.String())

// Get user
user, err := client.GetUser(userid.String(), false)

// Delete user
err := client.DeleteUser(userid.String())
```

### Working with Chat Blobs

MemoBase currently supports `ChatBlob` for storing conversation messages.

#### Chat Blob Example

```go
import (
    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
)

// Create a chat blob
chatBlob := &blob.ChatBlob{
    BaseBlob: blob.BaseBlob{
        Type: blob.ChatType,
    },
    Messages: []blob.OpenAICompatibleMessage{
        {
            Role:    "user",
            Content: "Hello, I am Jinjia!",
        },
        {
            Role:    "assistant",
            Content: "Hi there! How can I help you today?",
        },
    },
}

// Insert the blob
blobID, err := user.Insert(chatBlob)

// Retrieve the blob
retrievedBlob, err := user.Get(blobID)
chatBlob, ok := retrievedBlob.(*blob.ChatBlob)

// Get all chat blobs (with pagination: offset, limit)
blobIDs, err := user.GetAll(blob.ChatType, 0, 10)
```

### User Profiles

```go
// Flush blobs to update profiles
user.Flush(blob.ChatType)

// Get user profiles
profiles, err := user.Profile()

// Delete a profile
err = user.DeleteProfile(profileID)
```

## Blob Types

The library currently supports:

- `blob.ChatType`: For chat conversations

## Complete Example

For a complete working example of how to use the MemoBase Go client, see [examples/main.go](examples/main.go) in the repository.

## License

This project is licensed under the terms of the license provided with the MemoBase project.



================================================
FILE: src/client/memobase-go/go.mod
================================================
module github.com/memodb-io/memobase/src/client/memobase-go

go 1.22.3

require (
	github.com/google/uuid v1.6.0
	github.com/stretchr/testify v1.10.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)



================================================
FILE: src/client/memobase-go/go.sum
================================================
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=



================================================
FILE: src/client/memobase-go/blob/blob.go
================================================
package blob

import (
	"encoding/json"
	"fmt"
	"strings"
	"time"
)

type BlobType string

const (
	ChatType       BlobType = "chat"
	DocType        BlobType = "doc"
	ImageType      BlobType = "image"
	CodeType       BlobType = "code"
	TranscriptType BlobType = "transcript"
)

// BlobInterface defines the common behavior for all blob types
type BlobInterface interface {
	GetType() BlobType
	GetFields() map[string]interface{}
	GetCreatedAt() *time.Time
	GetBlobData() map[string]interface{}
}

// BaseBlob contains common fields for all blob types
type BaseBlob struct {
	Type      BlobType               `json:"type"`
	Fields    map[string]interface{} `json:"fields,omitempty"`
	CreatedAt *time.Time             `json:"created_at,omitempty"`
}

func (b *BaseBlob) GetType() BlobType {
	return b.Type
}

func (b *BaseBlob) GetFields() map[string]interface{} {
	return b.Fields
}

func (b *BaseBlob) GetCreatedAt() *time.Time {
	return b.CreatedAt
}

type OpenAICompatibleMessage struct {
	Role      string  `json:"role"`
	Content   string  `json:"content"`
	Alias     *string `json:"alias,omitempty"`
	CreatedAt *string `json:"created_at,omitempty"`
}

type ChatBlob struct {
	BaseBlob
	Messages []OpenAICompatibleMessage `json:"messages"`
}

func (b *ChatBlob) GetBlobData() map[string]interface{} {
	return map[string]interface{}{
		"messages": b.Messages,
	}
}

type DocBlob struct {
	BaseBlob
	Content string `json:"content"`
}

func (b *DocBlob) GetBlobData() map[string]interface{} {
	return map[string]interface{}{
		"content": b.Content,
	}
}

type CodeBlob struct {
	BaseBlob
	Content  string  `json:"content"`
	Language *string `json:"language,omitempty"`
}

func (b *CodeBlob) GetBlobData() map[string]interface{} {
	return map[string]interface{}{
		"content":  b.Content,
		"language": b.Language,
	}
}

type ImageBlob struct {
	BaseBlob
	URL    *string `json:"url,omitempty"`
	Base64 *string `json:"base64,omitempty"`
}

func (b *ImageBlob) GetBlobData() map[string]interface{} {
	return map[string]interface{}{
		"url":    b.URL,
		"base64": b.Base64,
	}
}

type TranscriptStamp struct {
	Content                   string   `json:"content"`
	StartTimestampInSeconds   float64  `json:"start_timestamp_in_seconds"`
	EndTimeTimestampInSeconds *float64 `json:"end_time_timestamp_in_seconds,omitempty"`
	Speaker                   *string  `json:"speaker,omitempty"`
}

type TranscriptBlob struct {
	BaseBlob
	Transcripts []TranscriptStamp `json:"transcripts"`
}

func (b *TranscriptBlob) GetBlobData() map[string]interface{} {
	return map[string]interface{}{
		"transcripts": b.Transcripts,
	}
}

// Add this custom time type
type JSONTime time.Time

func (t *JSONTime) UnmarshalJSON(b []byte) error {
	s := strings.Trim(string(b), "\"")
	if s == "null" {
		return nil
	}

	// Try different time formats
	formats := []string{
		time.RFC3339,
		time.RFC3339Nano,
		"2006-01-02T15:04:05.999999",
	}

	var err error
	for _, format := range formats {
		pt, parseErr := time.Parse(format, s)
		if parseErr == nil {
			*t = JSONTime(pt)
			return nil
		}
		err = parseErr
	}
	return err
}

// Update BlobData to use JSONTime
type BlobData struct {
	BlobType  BlobType               `json:"blob_type"`
	BlobData  map[string]interface{} `json:"blob_data"`
	Fields    map[string]interface{} `json:"fields,omitempty"`
	CreatedAt *JSONTime              `json:"created_at,omitempty"`
	UpdatedAt *JSONTime              `json:"updated_at,omitempty"`
}

// Update ToBlob to convert JSONTime back to time.Time
func (bd *BlobData) ToBlob() (BlobInterface, error) {
	baseBlob := BaseBlob{
		Type:   bd.BlobType,
		Fields: bd.Fields,
	}
	if bd.CreatedAt != nil {
		t := time.Time(*bd.CreatedAt)
		baseBlob.CreatedAt = &t
	}

	blobDataJSON, err := json.Marshal(bd.BlobData)
	if err != nil {
		return nil, err
	}

	switch bd.BlobType {
	case ChatType:
		blob := &ChatBlob{BaseBlob: baseBlob}
		err = json.Unmarshal(blobDataJSON, &blob)
		return blob, err

	case DocType:
		blob := &DocBlob{BaseBlob: baseBlob}
		err = json.Unmarshal(blobDataJSON, &blob)
		return blob, err

	case CodeType:
		blob := &CodeBlob{BaseBlob: baseBlob}
		err = json.Unmarshal(blobDataJSON, &blob)
		return blob, err

	case ImageType:
		blob := &ImageBlob{BaseBlob: baseBlob}
		err = json.Unmarshal(blobDataJSON, &blob)
		return blob, err

	case TranscriptType:
		blob := &TranscriptBlob{BaseBlob: baseBlob}
		err = json.Unmarshal(blobDataJSON, &blob)
		return blob, err

	default:
		return nil, fmt.Errorf("unknown blob type: %s", bd.BlobType)
	}
}



================================================
FILE: src/client/memobase-go/core/client.go
================================================
package core

import (
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"time"

	"github.com/memodb-io/memobase/src/client/memobase-go/network"
)

type MemoBaseClient struct {
	ProjectURL string
	APIKey     string
	APIVersion string
	BaseURL    string
	HTTPClient *http.Client
}

func NewMemoBaseClient(projectURL string, apiKey string) (*MemoBaseClient, error) {
	if apiKey == "" {
		apiKey = os.Getenv("MEMOBASE_API_KEY")
	}

	if apiKey == "" {
		return nil, fmt.Errorf("api_key is required, pass it as argument or set MEMOBASE_API_KEY environment variable")
	}

	client := &MemoBaseClient{
		ProjectURL: projectURL,
		APIKey:     apiKey,
		APIVersion: "api/v1",
		HTTPClient: &http.Client{
			Timeout: time.Second * 60,
		},
	}

	client.BaseURL = fmt.Sprintf("%s/%s", projectURL, client.APIVersion)

	// Add authorization header to all requests
	client.HTTPClient.Transport = &authTransport{
		apiKey: apiKey,
		base:   http.DefaultTransport,
	}

	return client, nil
}

// authTransport adds authorization header to all requests
type authTransport struct {
	apiKey string
	base   http.RoundTripper
}

func (t *authTransport) RoundTrip(req *http.Request) (*http.Response, error) {
	req.Header.Add("Authorization", fmt.Sprintf("Bearer %s", t.apiKey))
	return t.base.RoundTrip(req)
}

func (c *MemoBaseClient) Ping() bool {
	resp, err := c.HTTPClient.Get(fmt.Sprintf("%s/healthcheck", c.BaseURL))
	if err != nil {
		return false
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err == nil
}

func (c *MemoBaseClient) AddUser(data map[string]interface{}, id string) (string, error) {
	reqBody := map[string]interface{}{
		"data": data,
	}
	if id != "" {
		reqBody["id"] = id
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return "", err
	}

	resp, err := c.HTTPClient.Post(
		fmt.Sprintf("%s/users", c.BaseURL),
		"application/json",
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for AddUser")
	}

	return dataMap["id"].(string), nil
}

func (c *MemoBaseClient) UpdateUser(userID string, data map[string]interface{}) (string, error) {
	jsonData, err := json.Marshal(data)
	if err != nil {
		return "", err
	}

	req, err := http.NewRequest(
		http.MethodPut,
		fmt.Sprintf("%s/users/%s", c.BaseURL, userID),
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return "", err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := c.HTTPClient.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for UpdateUser")
	}

	return dataMap["id"].(string), nil
}

func (c *MemoBaseClient) GetUser(userID string, noGet bool) (*User, error) {
	if !noGet {
		resp, err := c.HTTPClient.Get(fmt.Sprintf("%s/users/%s", c.BaseURL, userID))
		if err != nil {
			return nil, err
		}
		defer resp.Body.Close()

		baseResp, err := network.UnpackResponse(resp)
		if err != nil {
			return nil, err
		}

		dataMap, ok := baseResp.Data.(map[string]interface{})
		if !ok {
			return nil, fmt.Errorf("unexpected response format for GetUser")
		}

		return &User{
			UserID:        userID,
			ProjectClient: c,
			Fields:        dataMap,
		}, nil
	}

	return &User{
		UserID:        userID,
		ProjectClient: c,
	}, nil
}

func (c *MemoBaseClient) GetOrCreateUser(userID string) (*User, error) {
	user, err := c.GetUser(userID, false)
	if err != nil {
		// Try to create user if get fails
		_, err = c.AddUser(nil, userID)
		if err != nil {
			return nil, err
		}
		return &User{
			UserID:        userID,
			ProjectClient: c,
		}, nil
	}
	return user, nil
}

func (c *MemoBaseClient) DeleteUser(userID string) error {
	req, err := http.NewRequest(
		http.MethodDelete,
		fmt.Sprintf("%s/users/%s", c.BaseURL, userID),
		nil,
	)
	if err != nil {
		return err
	}

	resp, err := c.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

// GetConfig retrieves the project's profile configuration
func (c *MemoBaseClient) GetConfig() (string, error) {
	resp, err := c.HTTPClient.Get(fmt.Sprintf("%s/project/profile_config", c.BaseURL))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for GetConfig")
	}

	config, ok := dataMap["profile_config"].(string)
	if !ok {
		return "", fmt.Errorf("unexpected response format for profile_config")
	}

	return config, nil
}

// UpdateConfig updates the project's profile configuration
func (c *MemoBaseClient) UpdateConfig(config string) error {
	reqBody := map[string]interface{}{
		"profile_config": config,
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return err
	}

	resp, err := c.HTTPClient.Post(
		fmt.Sprintf("%s/project/profile_config", c.BaseURL),
		"application/json",
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (c *MemoBaseClient) GetUsage() (map[string]interface{}, error) {
	resp, err := c.HTTPClient.Get(fmt.Sprintf("%s/project/billing", c.BaseURL))
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for GetUsage")
	}

	return dataMap, nil
}

func (c *MemoBaseClient) GetAllUsers(search string, orderBy string, orderDesc bool, limit int, offset int) ([]map[string]interface{}, error) {
	url := fmt.Sprintf("%s/project/users?search=%s&order_by=%s&order_desc=%t&limit=%d&offset=%d", c.BaseURL, search, orderBy, orderDesc, limit, offset)
	resp, err := c.HTTPClient.Get(url)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for GetAllUsers")
	}

	users, ok := dataMap["users"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for users")
	}

	var result []map[string]interface{}
	for _, u := range users {
		userMap, ok := u.(map[string]interface{})
		if !ok {
			continue
		}
		result = append(result, userMap)
	}

	return result, nil
}

func (c *MemoBaseClient) GetDailyUsage(days int) ([]map[string]interface{}, error) {
	url := fmt.Sprintf("%s/project/usage?last_days=%d", c.BaseURL, days)
	resp, err := c.HTTPClient.Get(url)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataArray, ok := baseResp.Data.([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for GetDailyUsage: expected array, got %T", baseResp.Data)
	}

	var result []map[string]interface{}
	for _, item := range dataArray {
		if itemMap, ok := item.(map[string]interface{}); ok {
			result = append(result, itemMap)
		} else {
			return nil, fmt.Errorf("unexpected item type in GetDailyUsage response: expected map[string]interface{}, got %T", item)
		}
	}

	return result, nil
}



================================================
FILE: src/client/memobase-go/core/client_test.go
================================================
package core

import (
	"testing"

	"github.com/google/uuid"
	"github.com/stretchr/testify/assert"
)

const (
	TestProjectURL = "http://localhost:8019"
	TestAPIKey     = "secret"
)

func TestNewMemoBaseClient(t *testing.T) {
	client, err := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	assert.NoError(t, err)
	assert.NotNil(t, client)
}

func TestPing(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	assert.True(t, client.Ping())
}

func TestConfig(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	config := "language: en"
	err := client.UpdateConfig(config)
	assert.NoError(t, err)

	newConfig, err := client.GetConfig()
	assert.NoError(t, err)
	assert.Equal(t, config, newConfig)
}

func TestUserCRUD(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)

	// Add User
	userID, err := client.AddUser(map[string]interface{}{}, "")
	assert.NoError(t, err)
	assert.NotEmpty(t, userID)

	// Get User
	user, err := client.GetUser(userID, false)
	assert.NoError(t, err)
	assert.NotNil(t, user)
	assert.Equal(t, userID, user.UserID)

	// Update User
	_, err = client.UpdateUser(userID, map[string]interface{}{"test": 123})
	assert.NoError(t, err)

	// Get User again to check update
	user, err = client.GetUser(userID, false)
	assert.NoError(t, err)
	assert.Equal(t, 123.0, user.Fields["data"].(map[string]interface{})["test"])

	// Delete User
	err = client.DeleteUser(userID)
	assert.NoError(t, err)

	// Verify user is deleted
	_, err = client.GetUser(userID, false)
	assert.Error(t, err)
}

func TestGetOrCreateUser(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID := uuid.New().String()

	// Create user
	user, err := client.GetOrCreateUser(userID)
	assert.NoError(t, err)
	assert.NotNil(t, user)
	assert.Equal(t, userID, user.UserID)

	// Get user
	user, err = client.GetOrCreateUser(userID)
	assert.NoError(t, err)
	assert.NotNil(t, user)
	assert.Equal(t, userID, user.UserID)

	client.DeleteUser(userID)
}

func TestGetAllUsers(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
		userID, err := client.AddUser(map[string]interface{}{}, "")

	users, err := client.GetAllUsers("", "updated_at", true, 10, 0)
	assert.NoError(t, err)
	assert.NotEmpty(t, users)

	client.DeleteUser(userID)
}

func TestGetUsage(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	usage, err := client.GetUsage()
	assert.NoError(t, err)
	assert.NotNil(t, usage)
}

func TestGetDailyUsage(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	usage, err := client.GetDailyUsage(7)
	assert.NoError(t, err)
	assert.NotNil(t, usage)
	assert.IsType(t, []map[string]interface{}{}, usage)
}



================================================
FILE: src/client/memobase-go/core/types.go
================================================
package core

import (
	"time"

	"github.com/google/uuid"
)

type UserProfileData struct {
	ID         uuid.UUID `json:"id"`
	Content    string    `json:"content"`
	Attributes struct {
		Topic    string `json:"topic"`
		SubTopic string `json:"sub_topic"`
	} `json:"attributes"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
}

type ProfileDelta struct {
	Content    string                 `json:"content"`
	Attributes map[string]interface{} `json:"attributes"`
}

type EventTag struct {
	Tag   string `json:"tag"`
	Value string `json:"value"`
}

type EventData struct {
	ProfileDelta []ProfileDelta `json:"profile_delta"`
	EventTip     string         `json:"event_tip,omitempty"`
	EventTags    []EventTag     `json:"event_tags,omitempty"`
}

type UserEventData struct {
	ID         uuid.UUID `json:"id"`
	EventData  EventData `json:"event_data"`
	CreatedAt  time.Time `json:"created_at"`
	UpdatedAt  time.Time `json:"updated_at"`
	Similarity float64   `json:"similarity,omitempty"`
}



================================================
FILE: src/client/memobase-go/core/user.go
================================================
package core

import (
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"
	"net/url"

	"github.com/memodb-io/memobase/src/client/memobase-go/blob"
	"github.com/memodb-io/memobase/src/client/memobase-go/network"
)

type User struct {
	UserID        string
	ProjectClient *MemoBaseClient
	Fields        map[string]interface{}
}

func (u *User) Insert(blob blob.BlobInterface, sync bool) (string, error) {
	reqData := map[string]interface{}{
		"blob_type": blob.GetType(),
		"blob_data": blob.GetBlobData(),
		"fields":    blob.GetFields(),
	}
	if blob.GetCreatedAt() != nil {
		reqData["created_at"] = blob.GetCreatedAt()
	}

	jsonData, err := json.Marshal(reqData)
	if err != nil {
		return "", err
	}

	resp, err := u.ProjectClient.HTTPClient.Post(
		fmt.Sprintf("%s/blobs/insert/%s?wait_process=%t", u.ProjectClient.BaseURL, u.UserID, sync),
		"application/json",
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for Insert")
	}

	return dataMap["id"].(string), nil
}

func (u *User) Get(blobID string) (blob.BlobInterface, error) {
	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/blobs/%s/%s", u.ProjectClient.BaseURL, u.UserID, blobID),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	var blobData blob.BlobData
	jsonData, err := json.Marshal(baseResp.Data)
	if err != nil {
		return nil, err
	}

	if err := json.Unmarshal(jsonData, &blobData); err != nil {
		return nil, err
	}

	return blobData.ToBlob()
}

func (u *User) GetAll(blobType blob.BlobType, page int, pageSize int) ([]string, error) {
	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/blobs/%s/%s?page=%d&page_size=%d",
			u.ProjectClient.BaseURL, u.UserID, blobType, page, pageSize),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for GetAll")
	}

	data, ok := dataMap["ids"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for blob IDs")
	}

	ids := make([]string, len(data))
	for i, v := range data {
		if str, ok := v.(string); ok {
			ids[i] = str
		} else {
			return nil, fmt.Errorf("unexpected ID type at index %d", i)
		}
	}

	return ids, nil
}

func (u *User) Delete(blobID string) error {
	req, err := http.NewRequest(
		http.MethodDelete,
		fmt.Sprintf("%s/blobs/%s/%s", u.ProjectClient.BaseURL, u.UserID, blobID),
		nil,
	)
	if err != nil {
		return err
	}

	resp, err := u.ProjectClient.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) Flush(blobType blob.BlobType, sync bool) error {
	resp, err := u.ProjectClient.HTTPClient.Post(
		fmt.Sprintf("%s/users/buffer/%s/%s?wait_process=%t", u.ProjectClient.BaseURL, u.UserID, blobType, sync),
		"application/json",
		nil,
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) AddProfile(content string, topic string, subTopic string) (string, error) {
	reqData := map[string]interface{}{
		"content": content,
		"attributes": map[string]interface{}{
			"topic":     topic,
			"sub_topic": subTopic,
		},
	}

	jsonData, err := json.Marshal(reqData)
	if err != nil {
		return "", err
	}

	resp, err := u.ProjectClient.HTTPClient.Post(
		fmt.Sprintf("%s/users/profile/%s", u.ProjectClient.BaseURL, u.UserID),
		"application/json",
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for AddProfile")
	}

	return dataMap["id"].(string), nil
}

func (u *User) Buffer(blobType blob.BlobType, status string) ([]string, error) {
	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/buffer/capacity/%s/%s?status=%s", u.ProjectClient.BaseURL, u.UserID, blobType, status),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for Buffer")
	}

	data, ok := dataMap["ids"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for blob IDs")
	}

	ids := make([]string, len(data))
	for i, v := range data {
		if str, ok := v.(string); ok {
			ids[i] = str
		} else {
			return nil, fmt.Errorf("unexpected ID type at index %d", i)
		}
	}

	return ids, nil
}

type ProfileOptions struct {
	MaxTokenSize    int                    `json:"max_token_size,omitempty"`
	PreferTopics    []string               `json:"prefer_topics,omitempty"`
	OnlyTopics      []string               `json:"only_topics,omitempty"`
	MaxSubtopicSize *int                   `json:"max_subtopic_size,omitempty"`
	TopicLimits     map[string]int         `json:"topic_limits,omitempty"`
	Chats           []blob.OpenAICompatibleMessage `json:"chats,omitempty"`
}

func (u *User) Profile(options *ProfileOptions) ([]UserProfileData, error) {
	if options == nil {
		options = &ProfileOptions{
			MaxTokenSize: 1000,
		}
	}

	params := url.Values{}
	params.Add("max_token_size", fmt.Sprintf("%d", options.MaxTokenSize))

	if options.PreferTopics != nil {
		for _, t := range options.PreferTopics {
			params.Add("prefer_topics", t)
		}
	}
	if options.OnlyTopics != nil {
		for _, t := range options.OnlyTopics {
			params.Add("only_topics", t)
		}
	}
	if options.MaxSubtopicSize != nil {
		params.Add("max_subtopic_size", fmt.Sprintf("%d", *options.MaxSubtopicSize))
	}
	if options.TopicLimits != nil {
		topicLimitsJSON, err := json.Marshal(options.TopicLimits)
		if err != nil {
			return nil, err
		}
		params.Add("topic_limits_json", string(topicLimitsJSON))
	}
	if options.Chats != nil {
		chatsJSON, err := json.Marshal(options.Chats)
		if err != nil {
			return nil, err
		}
		params.Add("chats_str", string(chatsJSON))
	}

	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/profile/%s?%s", u.ProjectClient.BaseURL, u.UserID, params.Encode()),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for Profile")
	}

	profiles, ok := dataMap["profiles"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for profiles")
	}

	var result []UserProfileData
	for _, p := range profiles {
		profileMap, ok := p.(map[string]interface{})
		if !ok {
			continue
		}

		var profile UserProfileData
		jsonData, err := json.Marshal(profileMap)
		if err != nil {
			continue
		}

		if err := json.Unmarshal(jsonData, &profile); err != nil {
			fmt.Printf("Error unmarshaling profile: %v\nData: %s\n", err, jsonData)
			continue
		}

		result = append(result, profile)
	}

	return result, nil
}

func (u *User) UpdateProfile(profileID string, content string, topic string, subTopic string) error {
	reqData := map[string]interface{}{
		"content": content,
		"attributes": map[string]interface{}{
			"topic":     topic,
			"sub_topic": subTopic,
		},
	}

	jsonData, err := json.Marshal(reqData)
	if err != nil {
		return err
	}

	req, err := http.NewRequest(
		http.MethodPut,
		fmt.Sprintf("%s/users/profile/%s/%s", u.ProjectClient.BaseURL, u.UserID, profileID),
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := u.ProjectClient.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) DeleteProfile(profileID string) error {
	req, err := http.NewRequest(
		http.MethodDelete,
		fmt.Sprintf("%s/users/profile/%s/%s", u.ProjectClient.BaseURL, u.UserID, profileID),
		nil,
	)
	if err != nil {
		return err
	}

	resp, err := u.ProjectClient.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) Event(topk int, maxTokenSize *int, needSummary bool) ([]UserEventData, error) {
	if topk <= 0 {
		topk = 10 // Default value
	}

	params := url.Values{}
	params.Add("topk", fmt.Sprintf("%d", topk))
	if maxTokenSize != nil {
		params.Add("max_token_size", fmt.Sprintf("%d", *maxTokenSize))
	}
	if needSummary {
		params.Add("need_summary", "true")
	}

	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/event/%s?%s", u.ProjectClient.BaseURL, u.UserID, params.Encode()),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for Event")
	}

	events, ok := dataMap["events"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for events")
	}

	var result []UserEventData
	for _, e := range events {
		eventMap, ok := e.(map[string]interface{})
		if !ok {
			continue
		}

		var event UserEventData
		jsonData, err := json.Marshal(eventMap)
		if err != nil {
			continue
		}

		if err := json.Unmarshal(jsonData, &event); err != nil {
			fmt.Printf("Error unmarshaling event: %v\nData: %s\n", err, jsonData)
			continue
		}

		result = append(result, event)
	}

	return result, nil
}

func (u *User) DeleteEvent(eventID string) error {
	req, err := http.NewRequest(
		http.MethodDelete,
		fmt.Sprintf("%s/users/event/%s/%s", u.ProjectClient.BaseURL, u.UserID, eventID),
		nil,
	)
	if err != nil {
		return err
	}

	resp, err := u.ProjectClient.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) UpdateEvent(eventID string, eventData map[string]interface{}) error {
	jsonData, err := json.Marshal(eventData)
	if err != nil {
		return err
	}

	req, err := http.NewRequest(
		http.MethodPut,
		fmt.Sprintf("%s/users/event/%s/%s", u.ProjectClient.BaseURL, u.UserID, eventID),
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := u.ProjectClient.HTTPClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	_, err = network.UnpackResponse(resp)
	return err
}

func (u *User) SearchEvent(query string, topk int, similarityThreshold float64, timeRangeInDays int) ([]UserEventData, error) {
	params := url.Values{}
	params.Add("query", query)
	params.Add("topk", fmt.Sprintf("%d", topk))
	params.Add("similarity_threshold", fmt.Sprintf("%f", similarityThreshold))
	params.Add("time_range_in_days", fmt.Sprintf("%d", timeRangeInDays))

	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/event/search/%s?%s", u.ProjectClient.BaseURL, u.UserID, params.Encode()),
	)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return nil, err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for SearchEvent")
	}

	events, ok := dataMap["events"].([]interface{})
	if !ok {
		return nil, fmt.Errorf("unexpected response format for events")
	}

	var result []UserEventData
	for _, e := range events {
		eventMap, ok := e.(map[string]interface{})
		if !ok {
			continue
		}

		var event UserEventData
		jsonData, err := json.Marshal(eventMap)
		if err != nil {
			continue
		}

		if err := json.Unmarshal(jsonData, &event); err != nil {
			fmt.Printf("Error unmarshaling event: %v\nData: %s\n", err, jsonData)
			continue
		}

		result = append(result, event)
	}

	return result, nil
}

type ContextOptions struct {
	MaxTokenSize        int                    `json:"max_token_size,omitempty"`
	PreferTopics        []string               `json:"prefer_topics,omitempty"`
	OnlyTopics          []string               `json:"only_topics,omitempty"`
	MaxSubtopicSize     *int                   `json:"max_subtopic_size,omitempty"`
	TopicLimits         map[string]int         `json:"topic_limits,omitempty"`
	ProfileEventRatio   *float64               `json:"profile_event_ratio,omitempty"`
	RequireEventSummary *bool                  `json:"require_event_summary,omitempty"`
	Chats               []blob.OpenAICompatibleMessage `json:"chats,omitempty"`
	EventSimilarityThreshold *float64 `json:"event_similarity_threshold,omitempty"`
}

func (u *User) Context(options *ContextOptions) (string, error) {
	if options == nil {
		options = &ContextOptions{
			MaxTokenSize: 1000,
		}
	}

	params := url.Values{}
	params.Add("max_token_size", fmt.Sprintf("%d", options.MaxTokenSize))

	if options.PreferTopics != nil {
		for _, t := range options.PreferTopics {
			params.Add("prefer_topics", t)
		}
	}
	if options.OnlyTopics != nil {
		for _, t := range options.OnlyTopics {
			params.Add("only_topics", t)
		}
	}
	if options.MaxSubtopicSize != nil {
		params.Add("max_subtopic_size", fmt.Sprintf("%d", *options.MaxSubtopicSize))
	}
	if options.TopicLimits != nil {
		topicLimitsJSON, err := json.Marshal(options.TopicLimits)
		if err != nil {
			return "", err
		}
		params.Add("topic_limits_json", string(topicLimitsJSON))
	}
	if options.ProfileEventRatio != nil {
		params.Add("profile_event_ratio", fmt.Sprintf("%f", *options.ProfileEventRatio))
	}
	if options.RequireEventSummary != nil {
		params.Add("require_event_summary", fmt.Sprintf("%t", *options.RequireEventSummary))
	}
	if options.Chats != nil {
		chatsJSON, err := json.Marshal(options.Chats)
		if err != nil {
			return "", err
		}
		params.Add("chats_str", string(chatsJSON))
	}
	if options.EventSimilarityThreshold != nil {
		params.Add("event_similarity_threshold", fmt.Sprintf("%f", *options.EventSimilarityThreshold))
	}

	resp, err := u.ProjectClient.HTTPClient.Get(
		fmt.Sprintf("%s/users/context/%s?%s", u.ProjectClient.BaseURL, u.UserID, params.Encode()),
	)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	baseResp, err := network.UnpackResponse(resp)
	if err != nil {
		return "", err
	}

	dataMap, ok := baseResp.Data.(map[string]interface{})
	if !ok {
		return "", fmt.Errorf("unexpected response format for Context")
	}

	contextStr, ok := dataMap["context"].(string)
	if !ok {
		return "", fmt.Errorf("unexpected response format for context")
	}

	return contextStr, nil
}



================================================
FILE: src/client/memobase-go/core/user_test.go
================================================
package core

import (
	"testing"

	"github.com/memodb-io/memobase/src/client/memobase-go/blob"
	"github.com/stretchr/testify/assert"
)

func TestBlobCRUD(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Insert Blob
	doc := &blob.DocBlob{
		BaseBlob: blob.BaseBlob{Type: blob.DocType},
		Content:  "test content",
	}
	blobID, err := user.Insert(doc, false)
	assert.NoError(t, err)
	assert.NotEmpty(t, blobID)

	// Get Blob
	retrievedBlob, err := user.Get(blobID)
	assert.NoError(t, err)
	assert.NotNil(t, retrievedBlob)
	retrievedDoc, ok := retrievedBlob.(*blob.DocBlob)
	assert.True(t, ok)
	assert.Equal(t, "test content", retrievedDoc.Content)

	// Get All Blobs
	blobs, err := user.GetAll(blob.DocType, 0, 10)
	assert.NoError(t, err)
	assert.Len(t, blobs, 1)

	// Delete Blob
	err = user.Delete(blobID)
	assert.NoError(t, err)

	// Verify blob is deleted
	_, err = user.Get(blobID)
	assert.Error(t, err)

	client.DeleteUser(userID)
}

func TestProfileCRUD(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Add Profile
	profileID, err := user.AddProfile("test content", "test_topic", "test_sub_topic")
	assert.NoError(t, err)
	assert.NotEmpty(t, profileID)

	// Get Profile
	profiles, err := user.Profile(nil)
	assert.NoError(t, err)
	assert.Len(t, profiles, 1)
	assert.Equal(t, "test content", profiles[0].Content)

	// Update Profile
	err = user.UpdateProfile(profileID, "updated content", "updated_topic", "updated_sub_topic")
	assert.NoError(t, err)

	// Get Profile again to check update
	profiles, err = user.Profile(nil)
	assert.NoError(t, err)
	assert.Len(t, profiles, 1)
	assert.Equal(t, "updated content", profiles[0].Content)

	// Delete Profile
	err = user.DeleteProfile(profileID)
	assert.NoError(t, err)

	// Verify profile is deleted
	profiles, err = user.Profile(nil)
	assert.NoError(t, err)
	assert.Len(t, profiles, 0)

	client.DeleteUser(userID)
}

func TestEventCRUD(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Insert chat to generate event
	chat := &blob.ChatBlob{
		BaseBlob: blob.BaseBlob{Type: blob.ChatType},
		Messages: []blob.OpenAICompatibleMessage{
			{Role: "user", Content: "Hello"},
			{Role: "assistant", Content: "Hi there"},
		},
	}
	_, err := user.Insert(chat, false)
	assert.NoError(t, err)

	err = user.Flush(blob.ChatType, true)
	assert.NoError(t, err)

	// Get Events
	events, err := user.Event(10, nil, false)
	assert.NoError(t, err)
	assert.NotEmpty(t, events)

	// Update Event
	eventID := events[0].ID.String()
	err = user.UpdateEvent(eventID, map[string]interface{}{"event_tip": "test tip"})
	assert.NoError(t, err)

	// Get Event again to check update
	events, err = user.Event(10, nil, false)
	assert.NoError(t, err)
	assert.Equal(t, "test tip", events[0].EventData.EventTip)

	// Delete Event
	err = user.DeleteEvent(eventID)
	assert.NoError(t, err)

	// Verify event is deleted
	events, err = user.Event(10, nil, false)
	assert.NoError(t, err)
	assert.Empty(t, events)

	client.DeleteUser(userID)
}

func TestContext(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Insert chat to generate context
	chat := &blob.ChatBlob{
		BaseBlob: blob.BaseBlob{Type: blob.ChatType},
		Messages: []blob.OpenAICompatibleMessage{
			{Role: "user", Content: "My name is John"},
			{Role: "assistant", Content: "Nice to meet you John"},
		},
	}
	_, err := user.Insert(chat, true)
	assert.NoError(t, err)

	context, err := user.Context(nil)
	assert.NoError(t, err)
	assert.NotEmpty(t, context)

	client.DeleteUser(userID)
}

func TestSearchEvent(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Insert chat to generate event
	chat := &blob.ChatBlob{
		BaseBlob: blob.BaseBlob{Type: blob.ChatType},
		Messages: []blob.OpenAICompatibleMessage{
			{Role: "user", Content: "I live in New York"},
			{Role: "assistant", Content: "That's great"},
		},
	}
	_, err := user.Insert(chat, false)
	assert.NoError(t, err)

	err = user.Flush(blob.ChatType, true)
	assert.NoError(t, err)

	events, err := user.SearchEvent("New York", 10, 0.2, 7)
	assert.NoError(t, err)
	assert.NotEmpty(t, events)

	client.DeleteUser(userID)
}

func TestBuffer(t *testing.T) {
	client, _ := NewMemoBaseClient(TestProjectURL, TestAPIKey)
	userID, _ := client.AddUser(nil, "")
	user, _ := client.GetUser(userID, false)

	// Insert chat to buffer
	chat := &blob.ChatBlob{
		BaseBlob: blob.BaseBlob{Type: blob.ChatType},
		Messages: []blob.OpenAICompatibleMessage{
			{Role: "user", Content: "Hello"},
		},
	}
	_, err := user.Insert(chat, false)
	assert.NoError(t, err)

	ids, err := user.Buffer(blob.ChatType, "idle")
	assert.NoError(t, err)
	assert.NotEmpty(t, ids)

	client.DeleteUser(userID)
}


================================================
FILE: src/client/memobase-go/error/error.go
================================================
package error

type ServerError struct {
    Message string
}

func (e *ServerError) Error() string {
    return e.Message
} 


================================================
FILE: src/client/memobase-go/examples/go.mod
================================================
module github.com/memodb-io/memobase/src/client/memobase-go/examples

go 1.22.3

require (
	github.com/google/uuid v1.6.0
	github.com/memodb-io/memobase/src/client/memobase-go v0.0.0
)

replace github.com/memodb-io/memobase/src/client/memobase-go => ../



================================================
FILE: src/client/memobase-go/examples/go.sum
================================================
github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=



================================================
FILE: src/client/memobase-go/examples/main.go
================================================
package main

import (
	"fmt"
	"log"

	"github.com/google/uuid"
	"github.com/memodb-io/memobase/src/client/memobase-go/blob"
	"github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
	projectURL := "YOUR_PROJECT_URL"
	apiKey := "YOUR_API_KEY"
	// Initialize the client
	client, err := core.NewMemoBaseClient(
		projectURL,
		apiKey,
	)
	if err != nil {
		log.Fatalf("Failed to create client: %v", err)
	}

	// Check connection
	if !client.Ping() {
		log.Fatal("Failed to connect to server")
	}

	// Create or get a user
	// userid should be a UUIDv4/5
	userid := uuid.New()
	user, err := client.GetOrCreateUser(userid.String())
	if err != nil {
		log.Fatalf("Failed to get/create user: %v", err)
	}
	fmt.Printf("User created with ID: %s\n", userid)

	// Create a chat blob
	chatBlob := &blob.ChatBlob{
		BaseBlob: blob.BaseBlob{
			Type: blob.ChatType,
		},
		Messages: []blob.OpenAICompatibleMessage{
			{
				Role:    "user",
				Content: "Hello, I am Jinjia!",
			},
			{
				Role:    "assistant",
				Content: "Hi there! How can I help you today?",
			},
		},
	}

	// Insert the blob
	blobID, err := user.Insert(chatBlob)
	if err != nil {
		log.Fatalf("Failed to insert blob: %v", err)
	}
	fmt.Printf("Successfully inserted blob with ID: %s\n", blobID)

	// Get the blob back
	retrievedBlob, err := user.Get(blobID)
	if err != nil {
		log.Fatalf("Failed to get blob: %v", err)
	}

	// Type assert to use as ChatBlob
	if chatBlob, ok := retrievedBlob.(*blob.ChatBlob); ok {
		fmt.Printf("Retrieved message: %s\n", chatBlob.Messages[0].Content)
	}

	// Get all chat blobs
	blobIDs, err := user.GetAll(blob.ChatType, 0, 10)
	if err != nil {
		log.Fatalf("Failed to get blobs: %v", err)
	}

	fmt.Printf("Found %d chat blobs\n", len(blobIDs))

	// Flush all blobs to get the latest profile
	user.Flush(blob.ChatType)

	// Get user profile
	profiles, err := user.Profile()
	if err != nil {
		log.Fatalf("Failed to get user profile: %v", err)
	}

	// Print profiles
	fmt.Println("\nUser Profiles:")
	for _, profile := range profiles {
		fmt.Printf("ID: %s\nTopic: %s\nSub-topic: %s\nContent: %s\n\n",
			profile.ID,
			profile.Attributes.Topic,
			profile.Attributes.SubTopic,
			profile.Content,
		)
	}

	// Delete a profile
	err = user.DeleteProfile(profiles[0].ID)
	if err != nil {
		log.Fatalf("Failed to delete profile: %v", err)
	}

	// Delete the user
	err = client.DeleteUser(userid.String())
	if err != nil {
		log.Fatalf("Failed to delete user: %v", err)
	}
}



================================================
FILE: src/client/memobase-go/network/network.go
================================================
package network

import (
	"encoding/json"
	"net/http"
	
	mberror "github.com/memodb-io/memobase/src/client/memobase-go/error"
)

type BaseResponse struct {
	Data   interface{} `json:"data"`
	Errmsg string                 `json:"errmsg"`
	Errno  int                    `json:"errno"`
}

func UnpackResponse(resp *http.Response) (*BaseResponse, error) {
	if resp.StatusCode >= 400 {
		return nil, &mberror.ServerError{Message: resp.Status}
	}

	var baseResp BaseResponse
	if err := json.NewDecoder(resp.Body).Decode(&baseResp); err != nil {
		return nil, err
	}

	if baseResp.Errno != 0 {
		return nil, &mberror.ServerError{Message: baseResp.Errmsg}
	}

	return &baseResp, nil
} 


================================================
FILE: src/client/memobase-go/utils/utils.go
================================================
package utils

import (
	"github.com/google/uuid"
)

func StringToUUID(s string, salt string) string {
	if salt == "" {
		salt = "memobase_client"
	}
	return uuid.NewSHA1(uuid.NameSpaceDNS, []byte(s+salt)).String()
} 


================================================
FILE: src/client/memobase-ts/README.md
================================================
<div align="center">
    <a href="https://memobase.io">
    <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://assets.memodb.io/memobase-dark.svg">
      <img alt="Shows the Memobase logo" src="https://assets.memodb.io/memobase-light.svg" width="424">
    </picture>
  </a>
  <p><strong>User Profile-Based Memory for GenAI Apps</strong></p>
  <p>
    <a href="https://www.npmjs.com/package/@memobase/memobase">
      <img src="https://img.shields.io/npm/v/@memobase/memobase.svg?logo=npm&&logoColor=fff&style=flat&colorA=2C2C2C&colorB=28CF8D">
    </a>
    <a href="https://jsr.io/@memobase/memobase">
      <img src="https://img.shields.io/jsr/v/@memobase/memobase.svg?logo=jsr&&logoColor=fff&style=flat&colorA=2C2C2C&colorB=28CF8D" />
    </a>
    <a href="https://npmcharts.com/compare/@memobase/memobase?minimal=true">
      <img src="https://img.shields.io/npm/dm/@memobase/memobase.svg?logo=typescript&&logoColor=fff&style=flat&colorA=2C2C2C&colorB=28CF8D" />
    </a>
    <a href="https://github.com/memodb-io/memobase/actions/workflows/publish.yaml">
      <img src="https://github.com/memodb-io/memobase/actions/workflows/publish.yaml/badge.svg">
    </a>
  </p>
</div>

# Memobase TypeScript and JavaScript API Library
This library provides convenient access to the Memobase REST API from TypeScript or JavaScript.


## Installation

```sh
npm install @memobase/memobase
```

### Installation from JSR

```sh
deno add jsr:@memobase/memobase
npx jsr add @memobase/memobase
```


## Usage

The code below shows how to get started using the completions API.

<!-- prettier-ignore -->
```js
import { MemoBaseClient, Blob, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env['MEMOBASE_PROJECT_URL'], process.env['MEMOBASE_API_KEY'])

const main = async () => {
    const ping = await client.ping()
    console.log('Ping: ', ping)

    const config = await client.getConfig()
    console.log('Config: ', config)

    const updateConfig = await client.updateConfig('a: 1')
    console.log('Update config: ', updateConfig)

    let userId = await client.addUser()
    console.log('Add user: ', userId)

    userId = await client.updateUser(userId, { name: 'John Doe' })
    console.log('Updated user id: ', userId)

    let user = await client.getUser(userId)
    console.log('User: ', user)

    const blobId = await user.insert(Blob.parse({
        type: BlobType.Enum.chat,
        messages: [{
            role: 'user',
            content: 'Hello, how are you? my name is John Doe'
        }]
    }))
    console.log('Insert blob: ', blobId)

    const blob = await user.get(blobId)
    console.log('Blob: ', blob)

    const flushSuc = await user.flush(BlobType.Enum.chat)
    console.log('Flush success: ', flushSuc)

    const blobs = await user.getAll(BlobType.Enum.chat)
    console.log('Blobs: ', blobs)

    user = await client.getOrCreateUser(userId)
    console.log('Get or create user: ', user)

    const addProfileSuc = await user.addProfile("Content", "Topic1", "SubTopic1")
    console.log('Add profile success: ', addProfileSuc)

    const profiles = await user.profile(2000)
    console.log('Profiles: ', profiles)

    const updateProfileSuc = await user.updateProfile(profiles[0].id, "New Content", "New Topic", "New SubTopic")
    console.log('Update profile success: ', updateProfileSuc)

    const deleteProfileSuc = await user.deleteProfile(profiles[0].id)
    console.log('Delete profile success: ', deleteProfileSuc)

    const event = await user.event(10, 1000)
    console.log('Event: ', event)

    const updateEventSuc = await user.updateEvent(event[0].id, {
        "id": event[0].id,
        "event_data": {
            "profile_delta": [
                {
                    "content": "New Event Content",
                    "attributes": {
                        "topic": "interest",
                        "sub_topic": "foods"
                    }
                }
            ]
        },
        created_at: new Date(),
        updated_at: new Date()
    })
    console.log('Update event success: ', updateEventSuc)

    const deleteEventSuc = await user.deleteEvent(event[0].id)
    console.log('Delete event success: ', deleteEventSuc)

    const context = await user.context(2000, 1000)
    console.log('Context: ', context)

    const isDel = await client.deleteUser(userId)
    console.log('Delete user success: ', isDel)
}

main()
```

## Support

Join the community for support and discussions:

-  [Join our Discord](https://discord.gg/YdgwU4d9NB) 👻 

-  [Follow us on Twitter](https://x.com/memobase_io) 𝕏 

Or Just [email us](mailto:contact@memobase.io) ❤️


## Contributors

This project exists thanks to all the people who contribute.

And thank you to all our backers! 🙏

<a href="https://github.com/memodb-io/memobase/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=memodb-io/memobase" />
</a>


## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/memodb-io/memobase/blob/main/LICENSE) file for details.


================================================
FILE: src/client/memobase-ts/index.ts
================================================
export * from './src/client';
export * from './src/user';
export * from './src/types';



================================================
FILE: src/client/memobase-ts/jest.config.ts
================================================
import type { JestConfigWithTsJest } from 'ts-jest';

const jestConfig: JestConfigWithTsJest = {
  preset: 'ts-jest',
  testEnvironment: 'node',
};

export default jestConfig;



================================================
FILE: src/client/memobase-ts/jsr.json
================================================
{
  "name": "@memobase/memobase",
  "version": "0.0.18",
  "license": "Apache-2.0",
  "exports": "./index.ts"
}



================================================
FILE: src/client/memobase-ts/package.json
================================================
{
  "name": "@memobase/memobase",
  "version": "0.0.18",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "rm -rf ./dist && tsc",
    "test": "jest",
    "prepare": "chmod -R +x ./scripts/ && npm run build",
    "lint": "./scripts/lint",
    "fix": "./scripts/format"
  },
  "keywords": [
    "retrieval",
    "memory",
    "rag",
    "chatgpt",
    "llm-application",
    "user-memory"
  ],
  "author": "Memobase <contact@memobase.io>",
  "license": "Apache-2.0",
  "description": "Client SDK for Memobase",
  "devDependencies": {
    "@types/jest": "^29.5.14",
    "@types/node": "^24.0.1",
    "@typescript-eslint/eslint-plugin": "^8.34.0",
    "@typescript-eslint/parser": "^8.34.0",
    "eslint": "^9.28.0",
    "eslint-plugin-prettier": "^5.4.1",
    "eslint-plugin-unused-imports": "^4.1.4",
    "jest": "^30.0.0",
    "prettier": "^3.5.3",
    "ts-jest": "^29.4.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.8.3"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/memodb-io/memobase.git"
  },
  "bugs": {
    "url": "https://github.com/memodb-io/memobase/issues"
  },
  "homepage": "https://github.com/memodb-io/memobase#readme",
  "dependencies": {
    "zod": "^3.25.64"
  }
}



================================================
FILE: src/client/memobase-ts/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "es2016",
    "module": "commonjs",
    "declaration": true,
    "outDir": "./dist",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": [
    "index.ts",
    "src/**/*",
    "tests/**/*"
  ],
  "exclude": [
    "node_modules",
    "dist",
    "tests/**/*"
  ]
}


================================================
FILE: src/client/memobase-ts/.eslintrc.js
================================================
module.exports = {
  parser: '@typescript-eslint/parser',
  plugins: ['@typescript-eslint', 'unused-imports', 'prettier'],
  rules: {
    'no-unused-vars': 'off',
    'prettier/prettier': 'error',
    'unused-imports/no-unused-imports': 'error',
  },
  root: true,
};



================================================
FILE: src/client/memobase-ts/.prettierignore
================================================
CHANGELOG.md
/ecosystem-tests/*/**
/node_modules
/coverage
/deno

# don't format tsc output, will break source maps
/dist


================================================
FILE: src/client/memobase-ts/.prettierrc.json
================================================
{
  "arrowParens": "always",
  "experimentalTernaries": true,
  "printWidth": 110,
  "singleQuote": true,
  "trailingComma": "all"
}



================================================
FILE: src/client/memobase-ts/scripts/format
================================================
#!/usr/bin/env bash

set -e

cd "$(dirname "$0")/.."

echo "==> Running eslint --fix"
ESLINT_USE_FLAT_CONFIG="false" ./node_modules/.bin/eslint --fix --ext ts,js .


================================================
FILE: src/client/memobase-ts/scripts/lint
================================================
#!/usr/bin/env bash

set -e

cd "$(dirname "$0")/.."

echo "==> Running eslint"
ESLINT_USE_FLAT_CONFIG="false" ./node_modules/.bin/eslint --ext ts,js .

echo "==> Running tsc"
./node_modules/.bin/tsc --noEmit


================================================
FILE: src/client/memobase-ts/src/client.ts
================================================
import { User } from './user';
import { unpackResponse } from './network';
import type { BaseResponse, HttpUrl, GetConfigResponse, GetProjectUsersResponse, GetProjectUsageItemResponse } from './types';

export class MemoBaseClient {
  private readonly baseUrl: HttpUrl;
  private readonly headers: HeadersInit;

  constructor(
    projectUrl: string,
    private readonly apiKey?: string,
    private readonly apiVersion: string = 'api/v1',
  ) {
    this.apiKey = apiKey || process.env.MEMOBASE_API_KEY;

    if (!this.apiKey) {
      throw new Error('apiKey is required. Pass it as argument or set MEMOBASE_API_KEY environment variable');
    }

    this.baseUrl = `${projectUrl.replace(/\/$/, '')}/${this.apiVersion.replace(/^\//, '')}`;
    this.headers = {
      Authorization: `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json',
    };
  }

  async fetch<T>(path: string, init?: RequestInit): Promise<BaseResponse<T>> {
    const response = await fetch(`${this.baseUrl}${path}`, {
      ...init,
      headers: {
        ...this.headers,
        ...init?.headers,
      },
    });
    return unpackResponse<T>(response);
  }

  async ping(): Promise<boolean> {
    try {
      await this.fetch('/healthcheck');
      return true;
    } catch (error) {
      return false;
    }
  }

  async getConfig(): Promise<string> {
    const response = await this.fetch<GetConfigResponse>('/project/profile_config');
    return response.data!.profile_config;
  }

  async updateConfig(config: string): Promise<boolean> {
    try {
      await this.fetch('/project/profile_config', {
        method: 'POST',
        body: JSON.stringify({ profile_config: config }),
      });
      return true;
    } catch (error) {
      return false;
    }
  }

  async addUser(data?: Record<string, any>, id?: string): Promise<string> {
    const response = await this.fetch<{ id: string }>('/users', {
      method: 'POST',
      body: JSON.stringify({ data, id }),
    });
    return response.data!.id;
  }

  async updateUser(userId: string, data?: Record<string, any>): Promise<string> {
    const response = await this.fetch<{ id: string }>(`/users/${userId}`, {
      method: 'PUT',
      body: JSON.stringify({ data }),
    });
    return response.data!.id;
  }

  async getUsers(search: string = "", order_by: "updated_at" | "profile_count" | "event_count" = "updated_at", order_desc: boolean = true, limit: number = 10, offset: number = 0): Promise<GetProjectUsersResponse> {
    const params = new URLSearchParams();
    params.append('search', search);
    params.append('order_by', order_by);
    params.append('order_desc', order_desc ? 'true' : 'false');
    params.append('limit', limit.toString());
    params.append('offset', offset.toString());

    const response = await this.fetch<GetProjectUsersResponse>(`/project/users?${params.toString()}`);
    if (response.data) {
      return response.data;
    }
    return { users: [], count: 0 };
  }

  async getUser(userId: string, noGet = false): Promise<User> {
    if (!noGet) {
      const response = await this.fetch<Record<string, any>>(`/users/${userId}`);
      return new User(userId, this, response.data);
    }
    return new User(userId, this);
  }

  async getOrCreateUser(userId: string): Promise<User> {
    try {
      return await this.getUser(userId);
    } catch (error) {
      await this.addUser(undefined, userId);
      return new User(userId, this);
    }
  }

  async deleteUser(userId: string): Promise<boolean> {
    try {
      await this.fetch(`/users/${userId}`, { method: 'DELETE' });
    } catch (error) {
      return false;
    }
    return true;
  }

  async getUsage(last_days: number = 7): Promise<GetProjectUsageItemResponse[]> {
    const params = new URLSearchParams();
    params.append('last_days', last_days.toString());

    const response = await this.fetch<GetProjectUsageItemResponse[]>(`/project/usage?${params.toString()}`);
    if (response.data) {
      return response.data;
    }
    return [];
  }
}



================================================
FILE: src/client/memobase-ts/src/error.ts
================================================
export class ServerError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'ServerError';
  }
}



================================================
FILE: src/client/memobase-ts/src/network.ts
================================================
import { ServerError } from './error';
import type { BaseResponse } from './types';

export async function unpackResponse<T>(response: Response): Promise<BaseResponse<T>> {
  if (!response.ok) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }

  const data = (await response.json()) as BaseResponse<T>;

  if (data.errno !== 0) {
    throw new ServerError(data.errmsg);
  }

  return data;
}



================================================
FILE: src/client/memobase-ts/src/types.ts
================================================
import { z } from 'zod';

// HttpUrl 类型
export const HttpUrl = z.string().url();
export type HttpUrl = z.infer<typeof HttpUrl>;

// BlobType 枚举
export const BlobType = z.enum(['chat', 'doc', 'image', 'code', 'transcript']);
export type BlobType = z.infer<typeof BlobType>;

// OpenAICompatibleMessage 类型
export const OpenAICompatibleMessage = z.object({
  role: z.enum(['user', 'assistant']),
  content: z.string(),
  alias: z.string().optional(),
  created_at: z.string().optional(),
});
export type OpenAICompatibleMessage = z.infer<typeof OpenAICompatibleMessage>;

// TranscriptStamp 类型
export const TranscriptStamp = z.object({
  content: z.string(),
  start_timestamp_in_seconds: z.number(),
  end_time_timestamp_in_seconds: z.number().optional(),
  speaker: z.string().optional(),
});
export type TranscriptStamp = z.infer<typeof TranscriptStamp>;

// BaseBlob 类型
export const BaseBlob = z.object({
  type: BlobType,
  fields: z.record(z.unknown()).optional(),
  created_at: z.date().optional(),
});
export type BaseBlob = z.infer<typeof BaseBlob>;

// ChatBlob 类型
export const ChatBlob = BaseBlob.extend({
  type: z.literal('chat'),
  messages: z.array(OpenAICompatibleMessage),
});
export type ChatBlob = z.infer<typeof ChatBlob>;

// DocBlob 类型
export const DocBlob = BaseBlob.extend({
  type: z.literal('doc'),
  content: z.string(),
});
export type DocBlob = z.infer<typeof DocBlob>;

// CodeBlob 类型
export const CodeBlob = BaseBlob.extend({
  type: z.literal('code'),
  content: z.string(),
  language: z.string().optional(),
});
export type CodeBlob = z.infer<typeof CodeBlob>;

// ImageBlob 类型
export const ImageBlob = BaseBlob.extend({
  type: z.literal('image'),
  url: z.string().optional(),
  base64: z.string().optional(),
});
export type ImageBlob = z.infer<typeof ImageBlob>;

// TranscriptBlob 类型
export const TranscriptBlob = BaseBlob.extend({
  type: z.literal('transcript'),
  transcripts: z.array(TranscriptStamp),
});
export type TranscriptBlob = z.infer<typeof TranscriptBlob>;

// Blob 类型
export const Blob = z.union([ChatBlob, DocBlob, CodeBlob, ImageBlob, TranscriptBlob]);
export type Blob = z.infer<typeof Blob>;

// UserProfile 类型
export const UserProfile = z.object({
  id: z.string(),
  content: z.string(),
  topic: z.string(),
  sub_topic: z.string(),
  created_at: z.date(),
  updated_at: z.date(),
});
export type UserProfile = z.infer<typeof UserProfile>;

// BaseResponse 类型
export const BaseResponse = <T>(dataSchema: z.ZodType<T, any, any>) =>
  z.object({
    data: dataSchema.optional(),
    errmsg: z.string(),
    errno: z.number(),
  });
export type BaseResponse<T = any> = z.infer<ReturnType<typeof BaseResponse<T>>>;

// IdResponse 类型
export const IdResponse = z.object({
  id: z.string(),
});
export type IdResponse = z.infer<typeof IdResponse>;

// ProfileResponse 类型
export const ProfileResponse = z.object({
  profiles: z.array(
    z.object({
      id: z.string(),
      content: z.string(),
      attributes: z.object({
        topic: z.string(),
        sub_topic: z.string(),
      }),
      created_at: z.string(),
      updated_at: z.string(),
    }),
  ),
});
export type ProfileResponse = z.infer<typeof ProfileResponse>;

// UserEvent 类型
export const UserEvent = z.object({
  id: z.string(),
  event_data: z
    .object({
      profile_delta: z
        .array(
          z.object({
            content: z.string(),
            attributes: z.record(z.any()).optional(),
          }),
        )
        .optional(),
      event_tip: z.string().optional().nullable(),
      event_tags: z
        .array(
          z.object({
            tag: z.string(),
            value: z.string(),
          }),
        )
        .optional()
        .nullable(),
    })
    .optional(),
  created_at: z.coerce.date(),
  updated_at: z.coerce.date(),
});
export type UserEvent = z.infer<typeof UserEvent>;

// UserGistEvent 类型
export const UserGistEvent = z.object({
  id: z.string(),
  gist_data: z
    .object({
      content: z.string(),
    })
    .optional(),
  created_at: z.coerce.date(),
  updated_at: z.coerce.date(),
  similarity: z.number().optional(),
});
export type UserGistEvent = z.infer<typeof UserGistEvent>;

// EventResponse 类型
export const EventResponse = z.object({
  events: z.array(UserEvent),
});
export type EventResponse = z.infer<typeof EventResponse>;

// GistEventResponse 类型
export const GistEventResponse = z.object({
  events: z.array(UserGistEvent),
});
export type GistEventResponse = z.infer<typeof GistEventResponse>;

// ContextResponse 类型
export const ContextResponse = z.object({
  context: z.string(),
});
export type ContextResponse = z.infer<typeof ContextResponse>;

// GetConfigResponse 类型
export const GetConfigResponse = z.object({
  profile_config: z.string(),
});
export type GetConfigResponse = z.infer<typeof GetConfigResponse>;

// ProjectUser 类型
export const ProjectUser = z.object({
  id: z.string(),
  project_id: z.string(),
  profile_count: z.number(),
  event_count: z.number(),
  created_at: z.coerce.date(),
  updated_at: z.coerce.date(),
});
export type ProjectUser = z.infer<typeof ProjectUser>;

// GetProjectUsersResponse 类型
export const GetProjectUsersResponse = z.object({
  users: z.array(ProjectUser),
  count: z.number(),
});
export type GetProjectUsersResponse = z.infer<typeof GetProjectUsersResponse>;

// GetProjectUsageItemResponse 类型
export const GetProjectUsageItemResponse = z.object({
  date: z.coerce.date(),
  total_insert: z.number(),
  total_success_insert: z.number(),
  total_input_token: z.number(),
  total_output_token: z.number(),
});
export type GetProjectUsageItemResponse = z.infer<typeof GetProjectUsageItemResponse>;



================================================
FILE: src/client/memobase-ts/src/user.ts
================================================
import { MemoBaseClient } from './client';
import {
  Blob,
  BlobType,
  UserProfile,
  IdResponse,
  ProfileResponse,
  UserEvent,
  EventResponse,
  UserGistEvent,
  GistEventResponse,
  ContextResponse,
  OpenAICompatibleMessage,
} from './types';

export class User {
  constructor(
    private readonly userId: string,
    private readonly projectClient: MemoBaseClient,
    public readonly fields?: Record<string, any>,
  ) {}

  async insert(blobData: Blob, sync = false): Promise<string> {
    const response = await this.projectClient.fetch<IdResponse>(
      `/blobs/insert/${this.userId}?wait_process=${sync}`,
      {
        method: 'POST',
        body: JSON.stringify({
          blob_type: blobData.type,
          fields: blobData.fields,
          blob_data: blobData,
        }),
      },
    );
    return response.data!.id;
  }

  async get(blobId: string): Promise<Blob> {
    const response = await this.projectClient.fetch<Blob>(`/blobs/${this.userId}/${blobId}`);
    return response.data as Blob;
  }

  async getAll(blobType: BlobType, page = 0, pageSize = 10): Promise<string[]> {
    const response = await this.projectClient.fetch<{ ids: string[] }>(
      `/users/blobs/${this.userId}/${blobType}?page=${page}&page_size=${pageSize}`,
    );
    return response.data!.ids;
  }

  async delete(blobId: string): Promise<boolean> {
    await this.projectClient.fetch(`/blobs/${this.userId}/${blobId}`, { method: 'DELETE' });
    return true;
  }

  async flush(blobType: BlobType = 'chat', sync = false): Promise<boolean> {
    await this.projectClient.fetch(`/users/buffer/${this.userId}/${blobType}?wait_process=${sync}`, {
      method: 'POST',
    });
    return true;
  }

  async addProfile(content: string, topic: string, subTopic: string): Promise<string> {
    const response = await this.projectClient.fetch<IdResponse>(`/users/profile/${this.userId}`, {
      method: 'POST',
      body: JSON.stringify({ content, attributes: { topic, sub_topic: subTopic } }),
    });
    return response.data!.id;
  }

  async profile(
    maxTokenSize = 1000,
    preferTopics?: string[],
    onlyTopics?: string[],
    maxSubtopicSize?: number,
    topicLimits?: Record<string, number>,
  ): Promise<UserProfile[]> {
    const params = new URLSearchParams();

    params.append('max_token_size', maxTokenSize.toString());
    if (preferTopics !== undefined && preferTopics.length > 0) {
      preferTopics.forEach((topic) => {
        params.append('prefer_topics', topic);
      });
    }
    if (onlyTopics !== undefined && onlyTopics.length > 0) {
      onlyTopics.forEach((topic) => {
        params.append('only_topics', topic);
      });
    }
    if (maxSubtopicSize !== undefined) {
      params.append('max_subtopic_size', maxSubtopicSize.toString());
    }
    if (topicLimits !== undefined) {
      params.append('topic_limits', JSON.stringify(topicLimits));
    }

    const response = await this.projectClient.fetch<ProfileResponse>(
      `/users/profile/${this.userId}?${params.toString()}`,
    );
    return response.data!.profiles.reduce((acc, cur) => {
      acc.push({
        id: cur.id,
        content: cur.content,
        topic: cur.attributes.topic || 'NONE',
        sub_topic: cur.attributes.sub_topic || 'NONE',
        created_at: new Date(cur.created_at),
        updated_at: new Date(cur.updated_at),
      });
      return acc;
    }, [] as UserProfile[]);
  }

  async updateProfile(profileId: string, content: string, topic: string, subTopic: string): Promise<boolean> {
    await this.projectClient.fetch(`/users/profile/${this.userId}/${profileId}`, {
      method: 'PUT',
      body: JSON.stringify({ content, attributes: { topic, sub_topic: subTopic } }),
    });
    return true;
  }

  async deleteProfile(profileId: string): Promise<boolean> {
    await this.projectClient.fetch(`/users/profile/${this.userId}/${profileId}`, { method: 'DELETE' });
    return true;
  }

  async event(topk = 10, maxTokenSize?: number): Promise<UserEvent[]> {
    const params = new URLSearchParams();

    params.append('topk', topk.toString());
    if (maxTokenSize !== undefined) {
      params.append('max_token_size', maxTokenSize.toString());
    }

    const response = await this.projectClient.fetch<EventResponse>(
      `/users/event/${this.userId}?${params.toString()}`,
    );

    return response.data!.events.map((e) => UserEvent.parse(e));
  }

  async updateEvent(eventId: string, eventData: UserEvent): Promise<boolean> {
    await this.projectClient.fetch(`/users/event/${this.userId}/${eventId}`, {
      method: 'PUT',
      body: JSON.stringify(eventData),
    });
    return true;
  }

  async deleteEvent(eventId: string): Promise<boolean> {
    await this.projectClient.fetch(`/users/event/${this.userId}/${eventId}`, { method: 'DELETE' });
    return true;
  }

  async searchEvent(
    query: string,
    topk = 10,
    similarityThreshold = 0.2,
    timeRangeInDays = 180,
  ): Promise<UserEvent[]> {
    const params = new URLSearchParams();
    params.append('query', query);
    params.append('topk', topk.toString());
    params.append('similarity_threshold', similarityThreshold.toString());
    params.append('time_range_in_days', timeRangeInDays.toString());

    const response = await this.projectClient.fetch<EventResponse>(
      `/users/event/search/${this.userId}?${params.toString()}`,
    );
    return response.data!.events.map((e) => UserEvent.parse(e));
  }

  async searchEventGist(
    query: string,
    topk = 10,
    similarityThreshold = 0.2,
    timeRangeInDays = 180,
  ): Promise<UserGistEvent[]> {
    const params = new URLSearchParams();
    params.append('query', query);
    params.append('topk', topk.toString());
    params.append('similarity_threshold', similarityThreshold.toString());
    params.append('time_range_in_days', timeRangeInDays.toString());

    const response = await this.projectClient.fetch<GistEventResponse>(
      `/users/event_gist/search/${this.userId}?${params.toString()}`,
    );
    return response.data!.events.map((e) => UserGistEvent.parse(e));
  }

  async context(
    maxTokenSize = 1000,
    maxSubtopicSize?: number,
    preferTopics?: string[],
    onlyTopics?: string[],
    topicLimits?: Record<string, number>,
    profileEventRatio?: number,
    requireEventSummary?: boolean,
    chats?: OpenAICompatibleMessage[],
    eventSimilarityThreshold?: number,
    customizeContextPrompt?: string,
    fullProfileAndOnlySearchEvent?: boolean,
    fillWindowWithEvents?: boolean,
  ): Promise<string> {
    const params = new URLSearchParams();

    params.append('max_token_size', maxTokenSize.toString());
    if (maxSubtopicSize !== undefined) {
      params.append('max_subtopic_size', maxSubtopicSize.toString());
    }
    if (preferTopics !== undefined && preferTopics.length > 0) {
      preferTopics.forEach((topic) => {
        params.append('prefer_topics', topic);
      });
    }
    if (onlyTopics !== undefined && onlyTopics.length > 0) {
      onlyTopics.forEach((topic) => {
        params.append('only_topics', topic);
      });
    }
    if (topicLimits !== undefined) {
      params.append('topic_limits', JSON.stringify(topicLimits));
    }
    if (profileEventRatio !== undefined) {
      params.append('profile_event_ratio', profileEventRatio.toString());
    }
    if (requireEventSummary !== undefined) {
      params.append('require_event_summary', requireEventSummary.toString());
    }
    if (chats !== undefined && chats.length > 0) {
      params.append('chats_str', JSON.stringify(chats));
    }
    if (eventSimilarityThreshold !== undefined) {
      params.append('event_similarity_threshold', eventSimilarityThreshold.toString());
    }
    if (customizeContextPrompt !== undefined) {
      params.append('customize_context_prompt', customizeContextPrompt);
    }
    if (fullProfileAndOnlySearchEvent !== undefined) {
      params.append('full_profile_and_only_search_event', fullProfileAndOnlySearchEvent.toString());
    }
    if (fillWindowWithEvents !== undefined) {
      params.append('fill_window_with_events', fillWindowWithEvents.toString());
    }

    const response = await this.projectClient.fetch<ContextResponse>(
      `/users/context/${this.userId}?${params.toString()}`,
    );
    return response.data!.context;
  }
}



================================================
FILE: src/client/memobase-ts/tests/client.test.ts
================================================
import { MemoBaseClient } from '../src/client';
import { User } from '../src/user';
import type { BaseResponse, GetConfigResponse } from '../src/types';
import { projectUrl, apiKey, apiVersion } from './env';

// 模拟 fetch
global.fetch = jest.fn();

describe('MemoBaseClient', () => {
  let client: MemoBaseClient;

  beforeEach(() => {
    // 创建一个新的 client 实例，每次测试前清理
    client = new MemoBaseClient(projectUrl, apiKey, apiVersion);
  });

  afterEach(() => {
    jest.clearAllMocks();
  });

  describe('Constructor', () => {
    it('should correctly initialize with the given parameters', () => {
      expect(client).toBeInstanceOf(MemoBaseClient);
      expect(client['baseUrl']).toBe(`${projectUrl}/${apiVersion}`);
      expect(client['headers']).toEqual({
        Authorization: 'Bearer ' + apiKey,
        'Content-Type': 'application/json',
      });
    });

    it('should throw an error if no apiKey is provided', () => {
      expect(() => new MemoBaseClient(projectUrl)).toThrow(
        'apiKey is required. Pass it as argument or set MEMOBASE_API_KEY environment variable',
      );
    });
  });

  describe('Project method', () => {
    it('should return the project config', async () => {
      const mockConfig: GetConfigResponse = { profile_config: 'config-data' };
      const mockResponse: BaseResponse<GetConfigResponse> = { data: mockConfig, errmsg: '', errno: 0 };

      // 模拟 fetch 的成功响应
      (fetch as jest.Mock).mockResolvedValue({
        ok: true,
        json: jest.fn().mockResolvedValue(mockResponse),
      });

      const result = await client.getConfig();
      expect(result).toBe(mockConfig.profile_config);
    });

    it('should update the project config', async () => {
      const mockConfig: GetConfigResponse = { profile_config: 'new-config-data' };
      const mockResponse: BaseResponse<null> = { errmsg: '', errno: 0 };

      // 模拟 fetch 的成功响应
      (fetch as jest.Mock).mockResolvedValue({
        ok: true,
        json: jest.fn().mockResolvedValue(mockResponse),
      });

      const result = await client.updateConfig(mockConfig.profile_config);

      expect(result).toBe(true);
      expect(fetch).toHaveBeenCalledWith(
        `${projectUrl}/${apiVersion}/project/profile_config`,
        expect.objectContaining({ method: 'POST', body: JSON.stringify(mockConfig) }),
      );
    });
  });

  describe('Ping method', () => {
    it('should return true for successful ping', async () => {
      // 模拟 fetch 的成功响应
      (fetch as jest.Mock).mockResolvedValue({
        ok: true,
        json: jest.fn().mockResolvedValue({
          data: 'pong',
          errmsg: '',
          errno: 0,
        }),
      });

      const result = await client.ping();
      expect(result).toBe(true);
      expect(fetch).toHaveBeenCalledWith(`${projectUrl}/${apiVersion}/healthcheck`, expect.any(Object));
    });

    it('should return false for failed ping', async () => {
      (fetch as jest.Mock).mockRejectedValueOnce(new Error('Network Error'));

      const result = await client.ping();
      expect(result).toBe(false);
    });
  });

  describe('User management methods', () => {
    it('should add a user and return user id', async () => {
      // 模拟 fetch 的成功响应
      (fetch as jest.Mock).mockResolvedValue({
        ok: true,
        json: jest.fn().mockResolvedValue({ data: { id: '123' }, errmsg: '', errno: 0 }),
      });

      const result = await client.addUser({ name: 'John' }, 'user123');
      expect(result).toBe('123');
      expect(fetch).toHaveBeenCalledWith(
        `${projectUrl}/${apiVersion}/users`,
        expect.objectContaining({
          method: 'POST',
          body: JSON.stringify({ data: { name: 'John' }, id: 'user123' }),
        }),
      );
    });

    it('should update a user and return user id', async () => {
      const result = await client.updateUser('user123', { name: 'Updated Name' });
      expect(result).toBe('123');
      expect(fetch).toHaveBeenCalledWith(
        `${projectUrl}/${apiVersion}/users/user123`,
        expect.objectContaining({
          method: 'PUT',
          body: JSON.stringify({ data: { name: 'Updated Name' } }),
        }),
      );
    });

    it('should get a user', async () => {
      const result = await client.getUser('user123');
      expect(result).toBeInstanceOf(User);
      expect(fetch).toHaveBeenCalledWith(`${projectUrl}/${apiVersion}/users/user123`, expect.any(Object));
    });

    it('should create a user if not exists when calling getOrCreateUser', async () => {
      // 模拟首次未找到用户
      (fetch as jest.Mock).mockRejectedValueOnce(new Error('User not found'));

      const result = await client.getOrCreateUser('user123');
      expect(result).toBeInstanceOf(User);
      expect(fetch).toHaveBeenCalledTimes(2); // 调用两次：一次是 getUser，另一次是 addUser
    });

    it('should delete a user', async () => {
      const result = await client.deleteUser('user123');
      expect(result).toBe(true);
      expect(fetch).toHaveBeenCalledWith(
        `${projectUrl}/${apiVersion}/users/user123`,
        expect.objectContaining({ method: 'DELETE' }),
      );
    });
  });
});



================================================
FILE: src/client/memobase-ts/tests/env.ts
================================================
export const projectUrl = 'https://example.com';
export const apiKey = 'test-api-key';
export const apiVersion = 'api/v1';



================================================
FILE: src/client/memobase-ts/tests/user.test.ts
================================================
import { User } from '../src/user';
import { MemoBaseClient } from '../src/client';
import type {
  Blob,
  BaseResponse,
  IdResponse,
  ProfileResponse,
  UserEvent,
  EventResponse,
  ContextResponse,
} from '../src/types';
import { projectUrl, apiKey, apiVersion } from './env';

// 模拟 fetch
global.fetch = jest.fn();

describe('User', () => {
  let client: MemoBaseClient;
  let user: User;

  beforeEach(() => {
    client = new MemoBaseClient(projectUrl, apiKey, apiVersion);
    user = new User('user123', client);
  });

  afterEach(() => {
    jest.clearAllMocks();
  });

  it('should insert a blob and return its id', async () => {
    const mockBlobData: Blob = { type: 'chat', messages: [{ role: 'user', content: 'Hello' }] };
    const mockResponse: BaseResponse<IdResponse> = { data: { id: 'blob123' }, errmsg: '', errno: 0 };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.insert(mockBlobData);

    expect(result).toBe('blob123');
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/blobs/insert/user123`,
      expect.any(Object),
    );
  });

  it('should get a blob by id', async () => {
    const mockBlob: Blob = { type: 'chat', messages: [{ role: 'user', content: 'Hello' }] };
    const mockResponse: BaseResponse<Blob> = { data: mockBlob, errmsg: '', errno: 0 };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const blob = await user.get('blob123');

    expect(blob).toEqual(mockBlob);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/blobs/user123/blob123`,
      expect.any(Object),
    );
  });

  it('should get all blobs by type', async () => {
    const mockResponse: BaseResponse<{ ids: string[] }> = {
      data: { ids: ['blob123', 'blob456'] },
      errmsg: '',
      errno: 0,
    };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.getAll('chat');

    expect(result).toEqual(['blob123', 'blob456']);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/blobs/user123/chat?page=0&page_size=10`,
      expect.any(Object),
    );
  });

  it('should delete a blob', async () => {
    const mockResponse: BaseResponse<null> = { errmsg: '', errno: 0 };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.delete('blob123');

    expect(result).toBe(true);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/blobs/user123/blob123`,
      expect.objectContaining({ method: 'DELETE' }),
    );
  });

  it('should flush blobs', async () => {
    const mockResponse: BaseResponse<null> = { errmsg: '', errno: 0 };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.flush('chat');

    expect(result).toBe(true);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/buffer/user123/chat`,
      expect.objectContaining({ method: 'POST' }),
    );
  });

  it('should get user profile', async () => {
    const mockResponse: BaseResponse<ProfileResponse> = {
      data: {
        profiles: [
          {
            id: 'profile123',
            content: 'Content1',
            attributes: { topic: 'Topic1', sub_topic: 'SubTopic1' },
            created_at: '2023-01-01T00:00:00Z',
            updated_at: '2023-01-01T00:00:00Z',
          },
        ],
      },
      errmsg: '',
      errno: 0,
    };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.profile(2000, ['Topic1'], ['SubTopic1'], 200, { Topic1: 200 });

    expect(result).toEqual(
      mockResponse.data?.profiles.map((p) => ({
        id: p.id,
        content: p.content,
        topic: p.attributes.topic || 'NONE',
        sub_topic: p.attributes.sub_topic || 'NONE',
        created_at: new Date(p.created_at),
        updated_at: new Date(p.updated_at),
      })),
    );
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/profile/user123?max_token_size=2000&prefer_topics=Topic1&only_topics=SubTopic1&max_subtopic_size=200&topic_limits=%7B%22Topic1%22%3A200%7D`,
      expect.any(Object),
    );
  });

  it('should delete a profile', async () => {
    const mockResponse: BaseResponse<null> = { errmsg: '', errno: 0 };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.deleteProfile('profile123');

    expect(result).toBe(true);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/profile/user123/profile123`,
      expect.objectContaining({ method: 'DELETE' }),
    );
  });

  it('should get user events', async () => {
    const events: UserEvent[] = [
      {
        id: 'event123',
        created_at: new Date('2025-03-01T00:00:00Z'),
        updated_at: new Date('2025-03-01T00:00:00Z'),
        event_data: {
          profile_delta: [
            {
              content: 'Content1',
              attributes: { topic: 'Topic1', sub_topic: 'SubTopic1' },
            },
          ],
        },
      },
    ];
    const mockResponse: BaseResponse<EventResponse> = {
      data: {
        events: events,
      },
      errmsg: '',
      errno: 0,
    };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.event(100, 1000);
    expect(result).toEqual(events);

    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/event/user123?topk=100&max_token_size=1000`,
      expect.any(Object),
    );
  });

  it('should get user context', async () => {
    const data: ContextResponse = {
      context: 'context123',
    };
    const mockResponse: BaseResponse<ContextResponse> = {
      data: data,
      errmsg: '',
      errno: 0,
    };

    // 模拟 fetch 的成功响应
    (fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: jest.fn().mockResolvedValue(mockResponse),
    });

    const result = await user.context(
      2000,
      1000,
      ['topic1', 'topic2'],
      ['topic3'],
      { topic1: 5, topic2: 3 },
      0.5,
    );

    expect(result).toBe(data.context);
    expect(fetch).toHaveBeenCalledWith(
      `${projectUrl}/${apiVersion}/users/context/user123?max_token_size=2000&max_subtopic_size=1000&prefer_topics=topic1&prefer_topics=topic2&only_topics=topic3&topic_limits=%7B%22topic1%22%3A5%2C%22topic2%22%3A3%7D&profile_event_ratio=0.5`,
      expect.any(Object),
    );
  });
});



================================================
FILE: src/client/tests/__init__.py
================================================
from dotenv import load_dotenv

load_dotenv()



================================================
FILE: src/client/tests/conftest.py
================================================
import pytest
from memobase import MemoBaseClient, AsyncMemoBaseClient
from memobase.error import ServerError


@pytest.fixture(scope="session")
def api_client():
    client = MemoBaseClient(
        project_url="http://localhost:8019/",
        api_key="secret",
    )
    if not client.ping():
        return pytest.skip("API not available")
    return client


@pytest.fixture(scope="session")
def api_async_client():
    client = AsyncMemoBaseClient(
        project_url="http://localhost:8019/",
        api_key="secret",
    )
    # if not await client.ping():
    #     return pytest.skip("API not available")
    return client



================================================
FILE: src/client/tests/test_blob.py
================================================
import pytest
from memobase.core.blob import DocBlob, ChatBlob
from memobase.error import ServerError
from memobase.core.blob import BlobType


def test_blob_curd_client(api_client):
    a = api_client
    blob = DocBlob(content="test", fields={"1": "fool"})
    u = a.add_user()
    print(u)
    ud = a.get_user(u)

    b = ud.insert(blob)
    print(ud.get(b))
    print(ud.delete(b))
    with pytest.raises(ServerError):
        ud.get(b)


def test_blob_get_all(api_client):
    a = api_client
    blob = DocBlob(content="test", fields={"1": "fool"})
    u = a.add_user()
    print(u)
    ud = a.get_user(u)

    b = ud.insert(blob)
    b = ud.insert(blob)
    b = ud.insert(blob)
    b = ud.insert(blob)
    r = ud.get_all(BlobType.doc)
    print(ud.delete(b))
    assert len(r) == 4
    with pytest.raises(ServerError):
        ud.get(b)
    a.delete_user(u)


def test_flush_curd_client(api_client):
    mb = api_client
    uid = mb.add_user({"me": "test"})
    u = mb.get_user(uid)
    print(u.profile(need_json=True))
    u.insert(
        ChatBlob(
            messages=[
                {
                    "role": "user",
                    "content": "Hello, I'm Gus",
                },
                {
                    "role": "assistant",
                    "content": "Hi, nice to meet you, Gus!",
                },
            ]
        )
    )
    u.flush()
    ps = u.profile()
    print(u.profile(need_json=True))
    print([p.describe for p in ps])
    print(u.event())
    mb.delete_user(uid)
    print("Deleted user")



================================================
FILE: src/client/tests/test_user.py
================================================
import pytest
from time import time
from memobase.error import ServerError
from memobase.utils import string_to_uuid
from memobase.core.blob import ChatBlob

CONFIG = """
language: zh
"""


def test_user_profile_curd_client(api_client):
    a = api_client
    u = a.add_user()
    print(u)
    ud = a.get_user(u)
    pid = ud.add_profile("test", "topic", "sub_topic")
    print(pid)
    ps = ud.profile()
    assert len(ps) == 1
    assert ps[0].content == "test"
    assert ps[0].topic == "topic"
    assert ps[0].sub_topic == "sub_topic"
    print(ud.update_profile(pid, "test2", "topic2", "sub_topic2"))
    ps = ud.profile()
    assert len(ps) == 1
    assert ps[0].content == "test2"
    assert ps[0].topic == "topic2"
    assert ps[0].sub_topic == "sub_topic2"
    print(ud.delete_profile(pid))
    ps = ud.profile()
    assert len(ps) == 0


def test_user_curd_client(api_client):
    a = api_client

    print(api_client.get_config())
    print(api_client.update_config(CONFIG))
    c = api_client.get_config()
    assert c == CONFIG

    u = a.add_user()
    print(u)
    ud = a.get_user(u)
    print(a.update_user(u, {"test": 111}))
    print("user", a.get_user(u).fields)
    print(a.delete_user(u))
    with pytest.raises(ServerError):
        a.get_user(u)

    new_uid = string_to_uuid(f"test{time()}")
    ud = a.get_or_create_user(new_uid)
    assert ud.user_id == new_uid


def test_user_event_curd_client(api_client):
    a = api_client

    print(api_client.get_config())
    print(api_client.update_config(CONFIG))
    c = api_client.get_config()
    assert c == CONFIG

    uid = a.add_user()
    print(uid)
    u = a.get_user(uid)

    u.insert(
        ChatBlob(
            messages=[
                {
                    "role": "user",
                    "content": "Hello, I'm Gus",
                },
                {
                    "role": "assistant",
                    "content": "Hi, nice to meet you, Gus!",
                },
            ]
        )
    )
    u.flush()

    ets = u.event()
    print(ets)
    assert len(ets) == 1

    u.update_event(ets[0].id, {"event_tip": "test"})
    ets = u.event()
    print(ets)
    assert len(ets) == 1
    assert ets[0].event_data.event_tip == "test"

    u.delete_event(ets[0].id)
    ets = u.event()
    assert len(ets) == 0

    a.delete_user(uid)



================================================
FILE: src/client/tests/test_user_async.py
================================================
import pytest
from time import time
from memobase.error import ServerError
from memobase.utils import string_to_uuid
from memobase.core.blob import ChatBlob

CONFIG = """
language: zh
"""


@pytest.mark.asyncio
async def test_user_curd_async_client(api_async_client):
    a = api_async_client

    print(await api_async_client.get_config())
    print(await api_async_client.update_config(CONFIG))
    c = await api_async_client.get_config()
    assert c == CONFIG

    u = await a.add_user()
    print(u)
    ud = await a.get_user(u)
    print(await a.update_user(u, {"test": 111}))
    print("user", ud.fields)
    print(await a.delete_user(u))
    with pytest.raises(ServerError):
        await a.get_user(u)

    new_uid = string_to_uuid(f"test{time()}")
    ud = await a.get_or_create_user(new_uid)
    assert ud.user_id == new_uid


@pytest.mark.asyncio
async def test_user_event_curd_async_client(api_async_client):
    a = api_async_client

    print(await api_async_client.get_config())
    print(await api_async_client.update_config(CONFIG))
    c = await api_async_client.get_config()
    assert c == CONFIG

    uid = await a.add_user()
    print(uid)
    u = await a.get_user(uid)

    await u.insert(
        ChatBlob(
            messages=[
                {
                    "role": "user",
                    "content": "Hello, I'm Gus",
                },
                {
                    "role": "assistant",
                    "content": "Hi, nice to meet you, Gus!",
                },
            ]
        )
    )
    await u.flush()

    ets = await u.event()
    print(ets)
    assert len(ets) == 1

    await u.update_event(ets[0].id, {"event_tip": "test"})
    ets = await u.event()
    print(ets)
    assert len(ets) == 1
    assert ets[0].event_data.event_tip == "test"

    await u.delete_event(ets[0].id)
    ets = await u.event()
    assert len(ets) == 0

    await a.delete_user(uid)



================================================
FILE: src/mcp/README.md
================================================
<h1 align="center">Memobase-MCP: Long-Term Memory for AI Agents</h1>

> This project is forked from [coleam00/mcp-mem0](https://github.com/coleam00/mcp-mem0)

A template implementation of the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server integrated with [Memobase](https://memobase.io) for providing AI agents with persistent memory capabilities.

Use this as a reference point to build your MCP servers yourself, or give this as an example to an AI coding assistant and tell it to follow this example for structure and code correctness!

To run this mcp, you need to have your own Memobase backend:

- You can [deploy](../server/readme.md) a local one
- Or use [free credits](https://www.memobase.io/en) of Memobase Cloud

You should have:

- A project url. (local: `http://localhost:8019` , cloud `https://api.memobase.dev`)
- A project token. (local: `secret` , cloud `sk-proj-xxxxxx`)

## Overview

This project demonstrates how to build an MCP server that enables AI agents to store, retrieve, and search memories using semantic search. It serves as a practical template for creating your own MCP servers, simply using Memobase and a practical example.

The implementation follows the best practices laid out by Anthropic for building MCP servers, allowing seamless integration with any MCP-compatible client.

## Features

The server provides three essential memory management tools:

1. **`save_memory`**: Store any information in long-term memory with semantic indexing
2. **`get_user_profiles`**: Retrieve complete user profiles
3. **`search_memories`**: Find relevant context for a given query

## Prerequisites
- Python 3.11+

## Installation

### Using uv

1. Install uv if you don't have it:
   ```bash
   pip install uv
   ```

2. Clone the repository:
   ```bash
   git clone https://github.com/memodb-io/memobase
   ```

3. Navigate to the project directory:
   ```bash
   cd memobase/src/mcp
   ```

3. Install dependencies:
   ```bash
   uv pip install -e .
   ```

4. Create a `.env` file based on `.env.example`:
   ```bash
   cp .env.example .env
   ```

5. Configure your environment variables in the `.env` file (see Configuration section)

### Using Docker (Recommended)

1. Build the Docker image:
   ```bash
   docker build -t memobase-mcp --build-arg PORT=8050 .
   ```

2. Create a `.env` file based on `.env.example` and configure your environment variables

## Configuration

The following environment variables can be configured in your `.env` file:

| Variable | Description | Example |
|----------|-------------|----------|
| `TRANSPORT` | Transport protocol (sse or stdio) | `sse` |
| `HOST` | Host to bind to when using SSE transport | `0.0.0.0` |
| `PORT` | Port to listen on when using SSE transport | `8050` |
| `MEMOBASE_API_KEY` | Memobase API key | `secret` |
| `MEMOBASE_BASE_URL` | Memobase base URL | `http://localhost:8019` |

## Running the Server

### Using uv

#### SSE Transport

```bash
# Set TRANSPORT=sse in .env then:
uv run src/main.py
```

The MCP server will essentially be run as an API endpoint that you can then connect to with config shown below.

### Using Docker

#### SSE Transport

```bash
docker run --env-file .env -p:8050:8050 memobase-mcp
```

The MCP server will essentially be run as an API endpoint within the container that you can then connect to with config shown below.

## Integration with MCP Clients

### Cursor

Once you have the server running with SSE transport, you can connect to it using this configuration (edit this in `.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "memobase": {
      "transport": "sse",
      "url": "http://localhost:8050/sse"
    }
  }
}
```

> **Note for Windsurf users**: Use `serverUrl` instead of `url` in your configuration:
> ```json
> {
>   "mcpServers": {
>     "memobase": {
>       "transport": "sse",
>       "serverUrl": "http://localhost:8050/sse"
>     }
>   }
> }
> ```

> **Note for n8n users**: Use host.docker.internal instead of localhost since n8n has to reach outside of it's own container to the host machine:
> 
> So the full URL in the MCP node would be: http://host.docker.internal:8050/sse

Make sure to update the port if you are using a value other than the default 8050.

### Python with Stdio Configuration

Add this server to your MCP configuration for Claude Desktop, Windsurf, or any other MCP client:

```json
{
  "mcpServers": {
    "memobase": {
      "command": "your/path/to/mcp/.venv/Scripts/python.exe",
      "args": ["your/path/to/mcp/src/main.py"],
      "env": {
        "TRANSPORT": "stdio",
        "MEMOBASE_API_KEY": "YOUR-API-KEY",
        "MEMOBASE_BASE_URL": "YOUR-MEMOBASE-URL",
      }
    }
  }
}
```

### Docker with Stdio Configuration

```json
{
  "mcpServers": {
    "memobase": {
      "command": "docker",
      "args": ["run", "--rm", "-i", 
               "-e", "TRANSPORT", 
               "-e", "MEMOBASE_API_KEY", 
               "-e", "MEMOBASE_BASE_URL", 
               "memobase-mcp"],
      "env": {
        "TRANSPORT": "stdio",
        "MEMOBASE_API_KEY": "YOUR-API-KEY",
        "MEMOBASE_BASE_URL": "https://api.memobase.io",
      }
    }
  }
}
```

## Building Your Own Server

This template provides a foundation for building more complex MCP servers. To build your own:

1. Add your own tools by creating methods with the `@mcp.tool()` decorator
2. Create your own lifespan function to add your own dependencies (clients, database connections, etc.)
3. Modify the `utils.py` file for any helper functions you need for your MCP server
4. Feel free to add prompts and resources as well  with `@mcp.resource()` and `@mcp.prompt()`



================================================
FILE: src/mcp/Dockerfile
================================================
FROM python:3.12-slim

ARG PORT=8050

WORKDIR /app

# Install uv
RUN pip install uv

# Copy the MCP server files
COPY . .

# Install packages
RUN python -m venv .venv
RUN uv pip install -e .

EXPOSE ${PORT}

# Command to run the MCP server
CMD ["uv", "run", "src/main.py"]


================================================
FILE: src/mcp/pyproject.toml
================================================
[project]
name = "memobase-mcp"
version = "0.1.0"
description = "MCP server for integrating long term memory into AI agents with Memobase"
readme = "README.md"
requires-python = ">=3.11"
dependencies = ["httpx>=0.28.1", "mcp[cli]>=1.3.0", "memobase"]



================================================
FILE: src/mcp/uv.lock
================================================
version = 1
requires-python = ">=3.11"

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "platform_system == 'Windows'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/23/44/e241a043f114299254e44d7e777ead311da400517f179665e59611ab0ee4/jiter-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:6c4d99c71508912a7e556d631768dcdef43648a93660670986916b297f1c54af", size = 314654 },
    { url = "https://files.pythonhosted.org/packages/fb/1b/a7e5e42db9fa262baaa9489d8d14ca93f8663e7f164ed5e9acc9f467fc00/jiter-0.9.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:8f60fb8ce7df529812bf6c625635a19d27f30806885139e367af93f6e734ef58", size = 320909 },
    { url = "https://files.pythonhosted.org/packages/60/bf/8ebdfce77bc04b81abf2ea316e9c03b4a866a7d739cf355eae4d6fd9f6fe/jiter-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:51c4e1a4f8ea84d98b7b98912aa4290ac3d1eabfde8e3c34541fae30e9d1f08b", size = 341733 },
    { url = "https://files.pythonhosted.org/packages/a8/4e/754ebce77cff9ab34d1d0fa0fe98f5d42590fd33622509a3ba6ec37ff466/jiter-0.9.0-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5f4c677c424dc76684fea3e7285a7a2a7493424bea89ac441045e6a1fb1d7b3b", size = 365097 },
    { url = "https://files.pythonhosted.org/packages/32/2c/6019587e6f5844c612ae18ca892f4cd7b3d8bbf49461ed29e384a0f13d98/jiter-0.9.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2221176dfec87f3470b21e6abca056e6b04ce9bff72315cb0b243ca9e835a4b5", size = 406603 },
    { url = "https://files.pythonhosted.org/packages/da/e9/c9e6546c817ab75a1a7dab6dcc698e62e375e1017113e8e983fccbd56115/jiter-0.9.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3c7adb66f899ffa25e3c92bfcb593391ee1947dbdd6a9a970e0d7e713237d572", size = 396625 },
    { url = "https://files.pythonhosted.org/packages/be/bd/976b458add04271ebb5a255e992bd008546ea04bb4dcadc042a16279b4b4/jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c98d27330fdfb77913c1097a7aab07f38ff2259048949f499c9901700789ac15", size = 351832 },
    { url = "https://files.pythonhosted.org/packages/07/51/fe59e307aaebec9265dbad44d9d4381d030947e47b0f23531579b9a7c2df/jiter-0.9.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:eda3f8cc74df66892b1d06b5d41a71670c22d95a1ca2cbab73654745ce9d0419", size = 384590 },
    { url = "https://files.pythonhosted.org/packages/db/55/5dcd2693794d8e6f4889389ff66ef3be557a77f8aeeca8973a97a7c00557/jiter-0.9.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:dd5ab5ddc11418dce28343123644a100f487eaccf1de27a459ab36d6cca31043", size = 520690 },
    { url = "https://files.pythonhosted.org/packages/54/d5/9f51dc90985e9eb251fbbb747ab2b13b26601f16c595a7b8baba964043bd/jiter-0.9.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:42f8a68a69f047b310319ef8e2f52fdb2e7976fb3313ef27df495cf77bcad965", size = 512649 },
    { url = "https://files.pythonhosted.org/packages/a6/e5/4e385945179bcf128fa10ad8dca9053d717cbe09e258110e39045c881fe5/jiter-0.9.0-cp311-cp311-win32.whl", hash = "sha256:a25519efb78a42254d59326ee417d6f5161b06f5da827d94cf521fed961b1ff2", size = 206920 },
    { url = "https://files.pythonhosted.org/packages/4c/47/5e0b94c603d8e54dd1faab439b40b832c277d3b90743e7835879ab663757/jiter-0.9.0-cp311-cp311-win_amd64.whl", hash = "sha256:923b54afdd697dfd00d368b7ccad008cccfeb1efb4e621f32860c75e9f25edbd", size = 210119 },
    { url = "https://files.pythonhosted.org/packages/af/d7/c55086103d6f29b694ec79156242304adf521577530d9031317ce5338c59/jiter-0.9.0-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:7b46249cfd6c48da28f89eb0be3f52d6fdb40ab88e2c66804f546674e539ec11", size = 309203 },
    { url = "https://files.pythonhosted.org/packages/b0/01/f775dfee50beb420adfd6baf58d1c4d437de41c9b666ddf127c065e5a488/jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:609cf3c78852f1189894383cf0b0b977665f54cb38788e3e6b941fa6d982c00e", size = 319678 },
    { url = "https://files.pythonhosted.org/packages/ab/b8/09b73a793714726893e5d46d5c534a63709261af3d24444ad07885ce87cb/jiter-0.9.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d726a3890a54561e55a9c5faea1f7655eda7f105bd165067575ace6e65f80bb2", size = 341816 },
    { url = "https://files.pythonhosted.org/packages/35/6f/b8f89ec5398b2b0d344257138182cc090302854ed63ed9c9051e9c673441/jiter-0.9.0-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2e89dc075c1fef8fa9be219e249f14040270dbc507df4215c324a1839522ea75", size = 364152 },
    { url = "https://files.pythonhosted.org/packages/9b/ca/978cc3183113b8e4484cc7e210a9ad3c6614396e7abd5407ea8aa1458eef/jiter-0.9.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:04e8ffa3c353b1bc4134f96f167a2082494351e42888dfcf06e944f2729cbe1d", size = 406991 },
    { url = "https://files.pythonhosted.org/packages/13/3a/72861883e11a36d6aa314b4922125f6ae90bdccc225cd96d24cc78a66385/jiter-0.9.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:203f28a72a05ae0e129b3ed1f75f56bc419d5f91dfacd057519a8bd137b00c42", size = 395824 },
    { url = "https://files.pythonhosted.org/packages/87/67/22728a86ef53589c3720225778f7c5fdb617080e3deaed58b04789418212/jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fca1a02ad60ec30bb230f65bc01f611c8608b02d269f998bc29cca8619a919dc", size = 351318 },
    { url = "https://files.pythonhosted.org/packages/69/b9/f39728e2e2007276806d7a6609cda7fac44ffa28ca0d02c49a4f397cc0d9/jiter-0.9.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:237e5cee4d5d2659aaf91bbf8ec45052cc217d9446070699441a91b386ae27dc", size = 384591 },
    { url = "https://files.pythonhosted.org/packages/eb/8f/8a708bc7fd87b8a5d861f1c118a995eccbe6d672fe10c9753e67362d0dd0/jiter-0.9.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:528b6b71745e7326eed73c53d4aa57e2a522242320b6f7d65b9c5af83cf49b6e", size = 520746 },
    { url = "https://files.pythonhosted.org/packages/95/1e/65680c7488bd2365dbd2980adaf63c562d3d41d3faac192ebc7ef5b4ae25/jiter-0.9.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:9f48e86b57bc711eb5acdfd12b6cb580a59cc9a993f6e7dcb6d8b50522dcd50d", size = 512754 },
    { url = "https://files.pythonhosted.org/packages/78/f3/fdc43547a9ee6e93c837685da704fb6da7dba311fc022e2766d5277dfde5/jiter-0.9.0-cp312-cp312-win32.whl", hash = "sha256:699edfde481e191d81f9cf6d2211debbfe4bd92f06410e7637dffb8dd5dfde06", size = 207075 },
    { url = "https://files.pythonhosted.org/packages/cd/9d/742b289016d155f49028fe1bfbeb935c9bf0ffeefdf77daf4a63a42bb72b/jiter-0.9.0-cp312-cp312-win_amd64.whl", hash = "sha256:099500d07b43f61d8bd780466d429c45a7b25411b334c60ca875fa775f68ccb0", size = 207999 },
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "mcp"
version = "1.8.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/7c/13/16b712e8a3be6a736b411df2fc6b4e75eb1d3e99b1cd57a3a1decf17f612/mcp-1.8.1.tar.gz", hash = "sha256:ec0646271d93749f784d2316fb5fe6102fb0d1be788ec70a9e2517e8f2722c0e", size = 265605 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1c/5d/91cf0d40e40ae9ecf8d4004e0f9611eea86085aa0b5505493e0ff53972da/mcp-1.8.1-py3-none-any.whl", hash = "sha256:948e03783859fa35abe05b9b6c0a1d5519be452fc079dc8d7f682549591c1770", size = 119761 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "memobase"
version = "0.0.17"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "httpx" },
    { name = "openai" },
    { name = "pydantic" },
]
sdist = { url = "https://files.pythonhosted.org/packages/83/64/57907995db2559f09b74093d4544b41e4a8ae5bbe7bec849257829da516c/memobase-0.0.17.tar.gz", hash = "sha256:0b1d54ace776111a9f592c37bfa9137f83d019d35d606372ea9d2f580706bef8", size = 21199 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f6/79/731ff6d91ab8a0275e5fc54970895981a5881a436e8fe482a6ab5100dbd2/memobase-0.0.17-py3-none-any.whl", hash = "sha256:6cca53d22acdcb8776596748446f01f90e9c4eff6f55e20e14fca5c09205f73d", size = 20159 },
]

[[package]]
name = "memobase-mcp"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "httpx" },
    { name = "mcp", extra = ["cli"] },
    { name = "memobase" },
]

[package.metadata]
requires-dist = [
    { name = "httpx", specifier = ">=0.28.1" },
    { name = "mcp", extras = ["cli"], specifier = ">=1.3.0" },
    { name = "memobase" },
]

[[package]]
name = "openai"
version = "1.78.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a4/3f/4e5e7b0548a15eabc4a755c93cd5f9564887e3d2fd45b6ff531352e5859d/openai-1.78.1.tar.gz", hash = "sha256:8b26b364531b100df1b961d03560042e5f5be11301d7d49a6cd1a2b9af824dca", size = 442985 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3c/4c/3889bc332a6c743751eb78a4bada5761e50a8a847ff0e46c1bd23ce12362/openai-1.78.1-py3-none-any.whl", hash = "sha256:7368bf147ca499804cc408fe68cdb6866a060f38dec961bbc97b04f9d917907e", size = 680917 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3f/8d/71db63483d518cbbf290261a1fc2839d17ff89fce7089e08cad07ccfce67/pydantic_core-2.33.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:4c5b0a576fb381edd6d27f0a85915c6daf2f8138dc5c267a57c08a62900758c7", size = 2028584 },
    { url = "https://files.pythonhosted.org/packages/24/2f/3cfa7244ae292dd850989f328722d2aef313f74ffc471184dc509e1e4e5a/pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e799c050df38a639db758c617ec771fd8fb7a5f8eaaa4b27b101f266b216a246", size = 1855071 },
    { url = "https://files.pythonhosted.org/packages/b3/d3/4ae42d33f5e3f50dd467761304be2fa0a9417fbf09735bc2cce003480f2a/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dc46a01bf8d62f227d5ecee74178ffc448ff4e5197c756331f71efcc66dc980f", size = 1897823 },
    { url = "https://files.pythonhosted.org/packages/f4/f3/aa5976e8352b7695ff808599794b1fba2a9ae2ee954a3426855935799488/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a144d4f717285c6d9234a66778059f33a89096dfb9b39117663fd8413d582dcc", size = 1983792 },
    { url = "https://files.pythonhosted.org/packages/d5/7a/cda9b5a23c552037717f2b2a5257e9b2bfe45e687386df9591eff7b46d28/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:73cf6373c21bc80b2e0dc88444f41ae60b2f070ed02095754eb5a01df12256de", size = 2136338 },
    { url = "https://files.pythonhosted.org/packages/2b/9f/b8f9ec8dd1417eb9da784e91e1667d58a2a4a7b7b34cf4af765ef663a7e5/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3dc625f4aa79713512d1976fe9f0bc99f706a9dee21dfd1810b4bbbf228d0e8a", size = 2730998 },
    { url = "https://files.pythonhosted.org/packages/47/bc/cd720e078576bdb8255d5032c5d63ee5c0bf4b7173dd955185a1d658c456/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:881b21b5549499972441da4758d662aeea93f1923f953e9cbaff14b8b9565aef", size = 2003200 },
    { url = "https://files.pythonhosted.org/packages/ca/22/3602b895ee2cd29d11a2b349372446ae9727c32e78a94b3d588a40fdf187/pydantic_core-2.33.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:bdc25f3681f7b78572699569514036afe3c243bc3059d3942624e936ec93450e", size = 2113890 },
    { url = "https://files.pythonhosted.org/packages/ff/e6/e3c5908c03cf00d629eb38393a98fccc38ee0ce8ecce32f69fc7d7b558a7/pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:fe5b32187cbc0c862ee201ad66c30cf218e5ed468ec8dc1cf49dec66e160cc4d", size = 2073359 },
    { url = "https://files.pythonhosted.org/packages/12/e7/6a36a07c59ebefc8777d1ffdaf5ae71b06b21952582e4b07eba88a421c79/pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:bc7aee6f634a6f4a95676fcb5d6559a2c2a390330098dba5e5a5f28a2e4ada30", size = 2245883 },
    { url = "https://files.pythonhosted.org/packages/16/3f/59b3187aaa6cc0c1e6616e8045b284de2b6a87b027cce2ffcea073adf1d2/pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:235f45e5dbcccf6bd99f9f472858849f73d11120d76ea8707115415f8e5ebebf", size = 2241074 },
    { url = "https://files.pythonhosted.org/packages/e0/ed/55532bb88f674d5d8f67ab121a2a13c385df382de2a1677f30ad385f7438/pydantic_core-2.33.2-cp311-cp311-win32.whl", hash = "sha256:6368900c2d3ef09b69cb0b913f9f8263b03786e5b2a387706c5afb66800efd51", size = 1910538 },
    { url = "https://files.pythonhosted.org/packages/fe/1b/25b7cccd4519c0b23c2dd636ad39d381abf113085ce4f7bec2b0dc755eb1/pydantic_core-2.33.2-cp311-cp311-win_amd64.whl", hash = "sha256:1e063337ef9e9820c77acc768546325ebe04ee38b08703244c1309cccc4f1bab", size = 1952909 },
    { url = "https://files.pythonhosted.org/packages/49/a9/d809358e49126438055884c4366a1f6227f0f84f635a9014e2deb9b9de54/pydantic_core-2.33.2-cp311-cp311-win_arm64.whl", hash = "sha256:6b99022f1d19bc32a4c2a0d544fc9a76e3be90f0b3f4af413f87d38749300e65", size = 1897786 },
    { url = "https://files.pythonhosted.org/packages/18/8a/2b41c97f554ec8c71f2a8a5f85cb56a8b0956addfe8b0efb5b3d77e8bdc3/pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:a7ec89dc587667f22b6a0b6579c249fca9026ce7c333fc142ba42411fa243cdc", size = 2009000 },
    { url = "https://files.pythonhosted.org/packages/a1/02/6224312aacb3c8ecbaa959897af57181fb6cf3a3d7917fd44d0f2917e6f2/pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:3c6db6e52c6d70aa0d00d45cdb9b40f0433b96380071ea80b09277dba021ddf7", size = 1847996 },
    { url = "https://files.pythonhosted.org/packages/d6/46/6dcdf084a523dbe0a0be59d054734b86a981726f221f4562aed313dbcb49/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4e61206137cbc65e6d5256e1166f88331d3b6238e082d9f74613b9b765fb9025", size = 1880957 },
    { url = "https://files.pythonhosted.org/packages/ec/6b/1ec2c03837ac00886ba8160ce041ce4e325b41d06a034adbef11339ae422/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:eb8c529b2819c37140eb51b914153063d27ed88e3bdc31b71198a198e921e011", size = 1964199 },
    { url = "https://files.pythonhosted.org/packages/2d/1d/6bf34d6adb9debd9136bd197ca72642203ce9aaaa85cfcbfcf20f9696e83/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c52b02ad8b4e2cf14ca7b3d918f3eb0ee91e63b3167c32591e57c4317e134f8f", size = 2120296 },
    { url = "https://files.pythonhosted.org/packages/e0/94/2bd0aaf5a591e974b32a9f7123f16637776c304471a0ab33cf263cf5591a/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:96081f1605125ba0855dfda83f6f3df5ec90c61195421ba72223de35ccfb2f88", size = 2676109 },
    { url = "https://files.pythonhosted.org/packages/f9/41/4b043778cf9c4285d59742281a769eac371b9e47e35f98ad321349cc5d61/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8f57a69461af2a5fa6e6bbd7a5f60d3b7e6cebb687f55106933188e79ad155c1", size = 2002028 },
    { url = "https://files.pythonhosted.org/packages/cb/d5/7bb781bf2748ce3d03af04d5c969fa1308880e1dca35a9bd94e1a96a922e/pydantic_core-2.33.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:572c7e6c8bb4774d2ac88929e3d1f12bc45714ae5ee6d9a788a9fb35e60bb04b", size = 2100044 },
    { url = "https://files.pythonhosted.org/packages/fe/36/def5e53e1eb0ad896785702a5bbfd25eed546cdcf4087ad285021a90ed53/pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:db4b41f9bd95fbe5acd76d89920336ba96f03e149097365afe1cb092fceb89a1", size = 2058881 },
    { url = "https://files.pythonhosted.org/packages/01/6c/57f8d70b2ee57fc3dc8b9610315949837fa8c11d86927b9bb044f8705419/pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:fa854f5cf7e33842a892e5c73f45327760bc7bc516339fda888c75ae60edaeb6", size = 2227034 },
    { url = "https://files.pythonhosted.org/packages/27/b9/9c17f0396a82b3d5cbea4c24d742083422639e7bb1d5bf600e12cb176a13/pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:5f483cfb75ff703095c59e365360cb73e00185e01aaea067cd19acffd2ab20ea", size = 2234187 },
    { url = "https://files.pythonhosted.org/packages/b0/6a/adf5734ffd52bf86d865093ad70b2ce543415e0e356f6cacabbc0d9ad910/pydantic_core-2.33.2-cp312-cp312-win32.whl", hash = "sha256:9cb1da0f5a471435a7bc7e439b8a728e8b61e59784b2af70d7c169f8dd8ae290", size = 1892628 },
    { url = "https://files.pythonhosted.org/packages/43/e4/5479fecb3606c1368d496a825d8411e126133c41224c1e7238be58b87d7e/pydantic_core-2.33.2-cp312-cp312-win_amd64.whl", hash = "sha256:f941635f2a3d96b2973e867144fde513665c87f13fe0e193c158ac51bfaaa7b2", size = 1955866 },
    { url = "https://files.pythonhosted.org/packages/0d/24/8b11e8b3e2be9dd82df4b11408a67c61bb4dc4f8e11b5b0fc888b38118b5/pydantic_core-2.33.2-cp312-cp312-win_arm64.whl", hash = "sha256:cca3868ddfaccfbc4bfb1d608e2ccaaebe0ae628e1416aeb9c4d88c001bb45ab", size = 1888894 },
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
    { url = "https://files.pythonhosted.org/packages/7b/27/d4ae6487d73948d6f20dddcd94be4ea43e74349b56eba82e9bdee2d7494c/pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:dd14041875d09cc0f9308e37a6f8b65f5585cf2598a53aa0123df8b129d481f8", size = 2025200 },
    { url = "https://files.pythonhosted.org/packages/f1/b8/b3cb95375f05d33801024079b9392a5ab45267a63400bf1866e7ce0f0de4/pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:d87c561733f66531dced0da6e864f44ebf89a8fba55f31407b00c2f7f9449593", size = 1859123 },
    { url = "https://files.pythonhosted.org/packages/05/bc/0d0b5adeda59a261cd30a1235a445bf55c7e46ae44aea28f7bd6ed46e091/pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2f82865531efd18d6e07a04a17331af02cb7a651583c418df8266f17a63c6612", size = 1892852 },
    { url = "https://files.pythonhosted.org/packages/3e/11/d37bdebbda2e449cb3f519f6ce950927b56d62f0b84fd9cb9e372a26a3d5/pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2bfb5112df54209d820d7bf9317c7a6c9025ea52e49f46b6a2060104bba37de7", size = 2067484 },
    { url = "https://files.pythonhosted.org/packages/8c/55/1f95f0a05ce72ecb02a8a8a1c3be0579bbc29b1d5ab68f1378b7bebc5057/pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:64632ff9d614e5eecfb495796ad51b0ed98c453e447a76bcbeeb69615079fc7e", size = 2108896 },
    { url = "https://files.pythonhosted.org/packages/53/89/2b2de6c81fa131f423246a9109d7b2a375e83968ad0800d6e57d0574629b/pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:f889f7a40498cc077332c7ab6b4608d296d852182211787d4f3ee377aaae66e8", size = 2069475 },
    { url = "https://files.pythonhosted.org/packages/b8/e9/1f7efbe20d0b2b10f6718944b5d8ece9152390904f29a78e68d4e7961159/pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:de4b83bb311557e439b9e186f733f6c645b9417c84e2eb8203f3f820a4b988bf", size = 2239013 },
    { url = "https://files.pythonhosted.org/packages/3c/b2/5309c905a93811524a49b4e031e9851a6b00ff0fb668794472ea7746b448/pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:82f68293f055f51b51ea42fafc74b6aad03e70e191799430b90c13d643059ebb", size = 2238715 },
    { url = "https://files.pythonhosted.org/packages/32/56/8a7ca5d2cd2cda1d245d34b1c9a942920a718082ae8e54e5f3e5a58b7add/pydantic_core-2.33.2-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:329467cecfb529c925cf2bbd4d60d2c509bc2fb52a20c1045bf09bb70971a9c1", size = 2066757 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "sse-starlette"
version = "2.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/10/5f/28f45b1ff14bee871bacafd0a97213f7ec70e389939a80c60c0fb72a9fc9/sse_starlette-2.3.5.tar.gz", hash = "sha256:228357b6e42dcc73a427990e2b4a03c023e2495ecee82e14f07ba15077e334b2", size = 17511 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/48/3e49cf0f64961656402c0023edbc51844fe17afe53ab50e958a6dbbbd499/sse_starlette-2.3.5-py3-none-any.whl", hash = "sha256:251708539a335570f10eaaa21d1848a10c42ee6dc3a9cf37ef42266cdb1c52a8", size = 10233 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "platform_system == 'Windows'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "typer"
version = "0.15.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6c/89/c527e6c848739be8ceb5c44eb8208c52ea3515c6cf6406aa61932887bf58/typer-0.15.4.tar.gz", hash = "sha256:89507b104f9b6a0730354f27c39fae5b63ccd0c95b1ce1f1a6ba0cfd329997c3", size = 101559 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c9/62/d4ba7afe2096d5659ec3db8b15d8665bdcb92a3c6ff0b95e99895b335a9c/typer-0.15.4-py3-none-any.whl", hash = "sha256:eb0651654dcdea706780c466cf06d8f174405a659ffff8f163cfbfee98c0e173", size = 45258 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]



================================================
FILE: src/mcp/.dockerignore
================================================
.env


================================================
FILE: src/mcp/.env.example
================================================
# The transport for the MCP server - either 'sse' or 'stdio' (defaults to SSE if left empty)
TRANSPORT=sse

# Host to bind to if using sse as the transport (leave empty if using stdio)
HOST=0.0.0.0

# Port to listen on if using sse as the transport (leave empty if using stdio)
PORT=8050

# Memobase API Key
MEMOBASE_API_KEY=

# Memobase Base URL
MEMOBASE_BASE_URL=


================================================
FILE: src/mcp/src/main.py
================================================
from mcp.server.fastmcp import FastMCP, Context
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass
from dotenv import load_dotenv
from memobase import AsyncMemoBaseClient, ChatBlob
from memobase.utils import string_to_uuid
import asyncio
import json
import os

from utils import get_memobase_client

load_dotenv()

# Default user ID for memory operations
DEFAULT_USER_ID = string_to_uuid("user")


# Create a dataclass for our application context
@dataclass
class MemobaseContext:
    """Context for the Memobase MCP server."""

    memobase_client: AsyncMemoBaseClient


@asynccontextmanager
async def memobase_lifespan(server: FastMCP) -> AsyncIterator[MemobaseContext]:
    """
    Manages the Memobase client lifecycle.

    Args:
        server: The FastMCP server instance

    Yields:
        MemobaseContext: The context containing the Memobase client
    """
    # Create and return the Memory client with the helper function in utils.py
    memobase_client = get_memobase_client()
    assert await memobase_client.ping(), "Failed to connect to Memobase"
    print("Memobase client connected")
    try:
        yield MemobaseContext(memobase_client=memobase_client)
    finally:
        # No explicit cleanup needed for the Memobase client
        pass


# Initialize FastMCP server with the Memobase client as context
mcp = FastMCP(
    "memobase-mcp",
    description="MCP server for long term memory storage and retrieval with Memobase",
    lifespan=memobase_lifespan,
    host=os.getenv("HOST", "0.0.0.0"),
    port=os.getenv("PORT", 8050),
)


@mcp.tool()
async def save_memory(ctx: Context, text: str) -> str:
    """Save information to your long-term memory.

    This tool is designed to store any type of information that might be useful in the future.
    The content will be processed and indexed for later retrieval through semantic search.

    Args:
        ctx: The MCP server provided context which includes the Memobase client
        text: The content to store in memory, including any relevant details and context
    """
    try:
        memobase_client: AsyncMemoBaseClient = (
            ctx.request_context.lifespan_context.memobase_client
        )
        messages = [{"role": "user", "content": text}]
        u = await memobase_client.get_or_create_user(DEFAULT_USER_ID)
        await u.insert(ChatBlob(messages=messages))
        await u.flush()
        return f"Successfully saved memory: {text[:100]}..."
    except Exception as e:
        return f"Error saving memory: {str(e)}"


@mcp.tool()
async def get_user_profiles(ctx: Context) -> str:
    """Get full user profiles.

    Call this tool when user asks for a summary of complete image of itself.

    Args:
        ctx: The MCP server provided context which includes the Memobase client

    Returns:
        A list of user profiles with topic, subtopic and content.
    """
    try:
        memobase_client: AsyncMemoBaseClient = (
            ctx.request_context.lifespan_context.memobase_client
        )
        u = await memobase_client.get_or_create_user(DEFAULT_USER_ID)
        ps = await u.profile()
        return "\n".join([f"- {p.describe}" for p in ps])
    except Exception as e:
        return f"Error retrieving memories: {str(e)}"


@mcp.tool()
async def search_memories(ctx: Context, query: str, max_length: int = 1000) -> str:
    """Search user memories

    Call this tool when user ask for recall some personal information.

    Args:
        ctx: The MCP server provided context which includes the Memobase client
        query: Search query string describing what you're looking for. Can be natural language.
        max_length: Maximum content length of the returned context.
    """
    try:
        memobase_client: AsyncMemoBaseClient = (
            ctx.request_context.lifespan_context.memobase_client
        )
        u = await memobase_client.get_or_create_user(DEFAULT_USER_ID)
        ps = await u.context(
            chats=[{"role": "user", "content": query}], max_token_size=max_length
        )
        return ps
    except Exception as e:
        return f"Error searching memories: {str(e)}"


async def main():
    transport = os.getenv("TRANSPORT", "sse")
    if transport == "sse":
        # Run the MCP server with sse transport
        await mcp.run_sse_async()
    else:
        # Run the MCP server with stdio transport
        await mcp.run_stdio_async()


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: src/mcp/src/utils.py
================================================
from memobase import AsyncMemoBaseClient
import os


def get_memobase_client():
    client = AsyncMemoBaseClient(
        project_url=os.getenv("MEMOBASE_BASE_URL"),
        api_key=os.getenv("MEMOBASE_API_KEY"),
    )
    return client



================================================
FILE: src/server/readme.md
================================================
<div align="center">
    <a href="https://memobase.io">
    <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://assets.memodb.io/memobase-dark.svg">
      <img alt="Shows the Memobase logo" src="https://assets.memodb.io/memobase-light.svg" width="424">
    </picture>
  </a>
  <p><strong>The server-side of Memobase</strong></p>
  <p>
    <img src="https://img.shields.io/github/v/tag/memodb-io/memobase">
  </p>
</div>




## Get started

### Setup

[**config.yaml**](https://docs.memobase.io/references/full)

Memobase uses a single  `config.yaml` to initialize the server. It contains the configs of:

- LLM: `llm_base_url`, `llm_api_key`, `best_llm_model`,...
- Embedding: `enable_event_embedding`, `embedding_api_key`...
- Memory: `max_pre_profile_token_size`, `max_profile_subtopics`, `additional_user_profiles`...

By default, Memobase enables user profile and event memory with filter ability. That means running a Memobase server requires you to have below things:

- **LLM API**: You must fill the OpenAI API Key in `llm_api_key` of `config.yaml`.Or you can change `llm_base_url` to any OpenAI-SDK-Compatible service(via [vllm](https://github.com/vllm-project/vllm), [Ollama](../../assets/tutorials/ollama+memobase/readme.md),...). Alternatively, you can set `llm_api_key` and `llm_base_url` using environment variables `MEMOBASE_LLM_API_KEY` and `MEMOBASE_LLM_BASE_URL`
- **Embedding API**: Memobase supports OpenAI-Compatible SDK and [Jina Embedding](https://jina.ai/models/jina-embeddings-v3/). Memobase uses embedding API to retrieve related user events. If you don't have a embedding API, you can set `enable_event_embedding: false` in `config.yaml`

We have some example `config.yaml` in `examplel_config`:

- [`profile_for_assistant`](./api/example_config/profile_for_education),  [`profile_for_education`](./api/example_config/profile_for_education),  [`profile_for_companion`](./api/example_config/profile_for_companion)  are three similar configs in term of structure, but for different user cases.
- [`event_tag`](./api/example_config/event_tag) is a feature to tracking temporal attributes of users. [doc](https://docs.memobase.io/features/event/event_tag)
- [`only_strict_profile`](./api/example_config/only_strict_profile): disable all other features, only collect the profiles you design.
- [`jina_embedding`](./api/example_config/jina_embedding) uses Jina exmbedding for event search.



**environment variables**

Check `./.env.example` for necessary vars. You can configure the running port and access token in here.  Also, anything in `config.yaml` can be override in env([doc](https://docs.memobase.io/references/full#environment-variable-overrides)), just starts with `MEMOBASE_`

### Launch

1. Make sure you have [docker-compose](https://docs.docker.com/compose/install/) installed.

2. Prepare the configs:

   ```bash
   cd src/server
   cp .env.example .env
   cp ./api/config.yaml.example ./api/config.yaml
   ```

   1. `.env` contains the service configs, like running port, secret token...
   2. `config.yaml` contains the Memobase configs, like LLM model, profile slots. [docs](https://docs.memobase.io/references/full)

3. Run `docker-compose build && docker-compose up` to start the services.

Check out the [docs](https://docs.memobase.io/quickstart) of how to use Memobase client or APIs.



## Use Memobase core only

1. If you have existing postgres and reids, you can only launch the Memobase core

2. Find and download the docker image of Memobase:

   ```bash
   docker pull ghcr.io/memodb-io/memobase:latest
   ```

3. Setup your `config.yaml` and an `env.list` file, the `env.list` should look like [this](./api/.env.example):

4. Run the service:
   ```bash
   docker run --env-file env.list -v ./api/config.yaml:/app/config.yaml -p 8019:8000 ghcr.io/memodb-io/memobase:main
   ```



## Development

1. Start a local DB first by `sh script/up-dev.sh`
2. Open a new terminal window and `cd ./api`
3. Install python deps: `uv sync`
4. To test if you got everything right, run `uv run pytest` to see if all the tests are passed.
5. Launch Memobase Server in dev mode: `uv run -m fastapi dev --port 8019`

> `fastapi dev` has hot-reload, so you can just modify the code and test it without relaunch the service.



## Migrations

Memobase may introduce breaking changes in DB schema, here is a guideline of how to migrate your data to latest Memobase:

1. Install `alembic`: `pip install alembic`

2. Modify `./api/alembic.ini`. Find the field called `sqlalchemy.url` in `alembbic.ini`, change it to your Postgres DB of Memobase

3. Run below commands to prepare the migration plan:

   ```bash
   cd api
   mkdir migrations/versions
   alembic upgrade head
   alembic revision --autogenerate -m "memobase changes"
   ```

4. ⚠️ Run the command `alembic upgrade head` again to migrate your current Memobase DB to the latest one.



================================================
FILE: src/server/docker-compose.yml
================================================
services:
  memobase-server-db:
    image: pgvector/pgvector:pg17
    restart: unless-stopped
    container_name: memobase-server-db
    environment:
      - POSTGRES_USER=${DATABASE_USER}
      - POSTGRES_PASSWORD=${DATABASE_PASSWORD}
      - POSTGRES_DB=${DATABASE_NAME}
    ports:
      - '${DATABASE_EXPORT_PORT}:5432'
    volumes:
      - ${DATABASE_LOCATION}:/var/lib/postgresql/data
      # - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USER} -d ${DATABASE_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 5
  
  memobase-server-redis:
    image: redis:7.4
    restart: unless-stopped
    container_name: memobase-server-redis
    ports:
      - "${REDIS_EXPORT_PORT}:6379"
    volumes:
      - ${REDIS_LOCATION}:/data
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
      
  memobase-server-api:
    platform: linux/amd64
    container_name: memobase-server-api
    environment:
      - DATABASE_URL=postgresql://${DATABASE_USER}:${DATABASE_PASSWORD}@memobase-server-db:5432/${DATABASE_NAME}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@memobase-server-redis:6379/0
      - ACCESS_TOKEN=${ACCESS_TOKEN}
      - PROJECT_ID=${PROJECT_ID}
      - API_HOSTS=${API_HOSTS}
      - USE_CORS=${USE_CORS}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      memobase-server-db:
        condition: service_healthy
      memobase-server-redis:
        condition: service_healthy
    ports:
      - '${API_EXPORT_PORT}:8000'
    build:
      context: ./api
    volumes:
      - ./api/config.yaml:/app/config.yaml

volumes:
  memobase-server-db:
    driver: local
  memobase-server-redis:
    driver: local
  memobase-server-api:
    driver: local



================================================
FILE: src/server/.env.example
================================================
DATABASE_NAME="memobase"
DATABASE_USER="memobase"
DATABASE_PASSWORD="helloworld"
DATABASE_LOCATION="./db/data"
REDIS_PASSWORD="helloworld"
REDIS_LOCATION="./db/redis/data"

DATABASE_EXPORT_PORT="15432"
REDIS_EXPORT_PORT="16379"
API_EXPORT_PORT="8019"
# If you want to use Swagger with your local docker set API_HOSTS like this:
# API_HOSTS="http://0.0.0.0:8019,http://localhost:8019,https://api.memobase.dev,https://api.memobase.cn"
API_HOSTS="https://api.memobase.dev,https://api.memobase.cn"
# If you encounter any CORS-related problems with the Swagger frontend, try changing this setting to true.
USE_CORS=false

PROJECT_ID="memobase_dev"
ACCESS_TOKEN="secret"



================================================
FILE: src/server/api/readme.md
================================================
# Memobase Backend API

Memobase is a user memory system designed for LLM Applications. It provides a FastAPI-based server that manages user profiles, memories, and various types of data blobs. Details of developing it in [here](./DEVELOPMENT.md).

## Core Components

### 1. API Layer (`api.py`)
- FastAPI application with versioned endpoints (`/api/v1`)
- Implements authentication middleware
- Main endpoints:
  - Health check
  - User management (CRUD operations)
  - Blob management
  - User profile management
  - Buffer management

### 2. Database Models (`models/`)
- Uses SQLAlchemy ORM
- Key models:
  - `User`: Core user entity
  - `GeneralBlob`: Stores various types of data
  - `BufferZone`: Temporary storage for processing
  - `UserProfile`: User memory profiles
- Supported Blob Types:
  - Chat
  - Document
  - Image
  - Code
  - Transcript

### 3. Controllers (`controllers/`)
- Business logic implementation
- Main modules:
  - `user`: User management
  - `blob`: Blob data handling
  - `buffer`: Buffer zone operations
  - `profile`: User profile management
- Modal processing:
  - Chat processing
  - Profile merging and extraction

### 4. Connectors (`connectors.py`)
- Database connection management (PostgreSQL)
- Redis connection handling
- Health check implementations
- Connection pooling configuration

### 5. Environment & Configuration (`env.py`)
- Configuration management
- Logger setup
- Token encoder initialization
- Environment variables handling

### 6. LLM Integration (`llms/`)
- OpenAI API integration
- Token management
- Async completion handling
- Response formatting

### 7. Prompts System (`prompts/`)
- Template management for LLM interactions
- Profile extraction and merging logic
- Multilingual support (English/Chinese)
- Summary generation

## Key Features

### Memory Management
- Long-term user profile storage
- Automatic memory merging and updating
- Buffer system for temporary storage
- Token-aware content management

### Authentication
- Bearer token authentication
- Configurable access control
- Middleware-based security

### Data Processing
- Async operation support
- Batch processing capabilities
- Automatic profile summarization
- Multi-modal data handling

## Dependencies
- FastAPI: Web framework
- SQLAlchemy: Database ORM
- Redis: Caching and temporary storage
- Pydantic: Data validation
- Tiktoken: Token management
- Rich: Enhanced logging

## Configuration
Key configuration options in `config.yaml`:
- System prompt
- Buffer flush interval
- Token size limits
- LLM settings
- Language preferences
- Model selection

## Development Guidelines
1. Use async/await for database operations
2. Implement proper error handling using Promise pattern
3. Follow token limits for profile management
4. Use proper typing with Pydantic models
5. Implement health checks for services
6. Handle multilingual support where needed

## Error Handling
- Uses custom Promise pattern
- HTTP status codes mapping
- Structured error responses
- Validation error handling

This documentation provides a high-level overview of the Memobase system. For specific implementation details, refer to the individual module documentation and code comments.



================================================
FILE: src/server/api/alembic.ini
================================================
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = ./migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to ./migrations/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:./migrations/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = postgresql://memobase:helloworld@localhost:15432/memobase


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S



================================================
FILE: src/server/api/api.py
================================================
import memobase_server.env
import os

# Done setting up env
from contextlib import asynccontextmanager
from fastapi import FastAPI, APIRouter
from fastapi.openapi.utils import get_openapi
from fastapi.middleware.cors import CORSMiddleware
from memobase_server.connectors import (
    close_connection,
    init_redis_pool,
)
from memobase_server import api_layer
from memobase_server.env import LOG, TRACE_LOG
from memobase_server.llms.embeddings import check_embedding_sanity
from memobase_server.llms import llm_sanity_check
from memobase_server.api_layer.docs import API_X_CODE_DOCS
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor


@asynccontextmanager
async def lifespan(app: FastAPI):
    init_redis_pool()
    await check_embedding_sanity()
    await llm_sanity_check()
    LOG.info(f"Start Memobase Server {memobase_server.__version__} 🖼️")
    yield
    await close_connection()


app = FastAPI(
    lifespan=lifespan,
)

# CORS configuration
USE_CORS = os.environ.get("USE_CORS", "False").lower() == "true"  # Default to False
API_HOSTS_STR = os.environ.get(
    "API_HOSTS", "https://api.memobase.dev,https://api.memobase.cn"
)
API_HOSTS = [host.strip() for host in API_HOSTS_STR.split(",")]

if USE_CORS:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=API_HOSTS,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

NO_AUTH = {"/api/v1/healthcheck"}


def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema

    servers: list = []
    for host in API_HOSTS:
        servers.append({"url": host})

    openapi_schema = get_openapi(  # type: ignore
        title="Memobase API",
        version=memobase_server.__version__,
        summary="APIs for Memobase, a user memory system for LLM Apps",
        routes=app.routes,
        servers=servers,
    )
    openapi_schema["components"]["securitySchemes"] = {
        "BearerAuth": {
            "type": "http",
            "scheme": "bearer",
        }
    }
    openapi_schema["security"] = [{"BearerAuth": []}]
    for path in openapi_schema["paths"]:
        if path in NO_AUTH:
            for method in openapi_schema["paths"][path]:
                openapi_schema["paths"][path][method]["security"] = []

    app.openapi_schema = openapi_schema  # type: ignore
    return app.openapi_schema


app.openapi = custom_openapi


router = APIRouter(prefix="/api/v1")


router.get(
    "/healthcheck", tags=["chore"], openapi_extra=API_X_CODE_DOCS["GET /healthcheck"]
)(api_layer.chore.healthcheck)

router.get(
    "/admin/status_check",
    tags=["admin"],
    # openapi_extra=API_X_CODE_DOCS["GET /admin/status_check"],
)(api_layer.chore.root_running_status_check)

router.post(
    "/project/profile_config",
    tags=["project"],
    openapi_extra=API_X_CODE_DOCS["POST /project/profile_config"],
)(api_layer.project.update_project_profile_config)

router.get(
    "/project/profile_config",
    tags=["project"],
    openapi_extra=API_X_CODE_DOCS["GET /project/profile_config"],
)(api_layer.project.get_project_profile_config_string)


router.get(
    "/project/billing",
    tags=["project"],
    openapi_extra=API_X_CODE_DOCS["GET /project/billing"],
)(api_layer.project.get_project_billing)


router.get(
    "/project/users",
    tags=["project"],
    openapi_extra=API_X_CODE_DOCS["GET /project/users"],
)(api_layer.project.get_project_users)


router.get(
    "/project/usage",
    tags=["project"],
    openapi_extra=API_X_CODE_DOCS["GET /project/usage"],
)(api_layer.project.get_project_usage)


router.post(
    "/users",
    tags=["user"],
    openapi_extra=API_X_CODE_DOCS["POST /users"],
)(api_layer.user.create_user)


router.get(
    "/users/{user_id}",
    tags=["user"],
    openapi_extra=API_X_CODE_DOCS["GET /users/{user_id}"],
)(api_layer.user.get_user)


router.put(
    "/users/{user_id}",
    tags=["user"],
    openapi_extra=API_X_CODE_DOCS["PUT /users/{user_id}"],
)(api_layer.user.update_user)

router.delete(
    "/users/{user_id}",
    tags=["user"],
    openapi_extra=API_X_CODE_DOCS["DELETE /users/{user_id}"],
)(api_layer.user.delete_user)


router.get(
    "/users/blobs/{user_id}/{blob_type}",
    tags=["user"],
    openapi_extra=API_X_CODE_DOCS["GET /users/blobs/{user_id}/{blob_type}"],
)(api_layer.user.get_user_all_blobs)


router.post(
    "/blobs/insert/{user_id}",
    tags=["blob"],
    openapi_extra=API_X_CODE_DOCS["POST /blobs/insert/{user_id}"],
)(api_layer.blob.insert_blob)


router.get(
    "/blobs/{user_id}/{blob_id}",
    tags=["blob"],
    openapi_extra=API_X_CODE_DOCS["GET /blobs/{user_id}/{blob_id}"],
)(api_layer.blob.get_blob)


router.delete(
    "/blobs/{user_id}/{blob_id}",
    tags=["blob"],
    openapi_extra=API_X_CODE_DOCS["DELETE /blobs/{user_id}/{blob_id}"],
)(api_layer.blob.delete_blob)


router.get(
    "/users/profile/{user_id}",
    tags=["profile"],
    openapi_extra=API_X_CODE_DOCS["GET /users/profile/{user_id}"],
)(api_layer.profile.get_user_profile)

router.post(
    "/users/profile/{user_id}",
    tags=["profile"],
    openapi_extra=API_X_CODE_DOCS["POST /users/profile/{user_id}"],
)(api_layer.profile.add_user_profile)

router.post(
    "/users/profile/import/{user_id}",
    tags=["profile"],
)(api_layer.profile.import_user_context)

router.put(
    "/users/profile/{user_id}/{profile_id}",
    tags=["profile"],
    openapi_extra=API_X_CODE_DOCS["PUT /users/profile/{user_id}/{profile_id}"],
)(api_layer.profile.update_user_profile)

router.delete(
    "/users/profile/{user_id}/{profile_id}",
    tags=["profile"],
    openapi_extra=API_X_CODE_DOCS["DELETE /users/profile/{user_id}/{profile_id}"],
)(api_layer.profile.delete_user_profile)

router.post(
    "/users/buffer/{user_id}/{buffer_type}",
    tags=["buffer"],
    openapi_extra=API_X_CODE_DOCS["POST /users/buffer/{user_id}/{buffer_type}"],
)(api_layer.buffer.flush_buffer)

router.get(
    "/users/buffer/capacity/{user_id}/{buffer_type}",
    tags=["buffer"],
    openapi_extra=API_X_CODE_DOCS["GET /users/buffer/capacity/{user_id}/{buffer_type}"],
)(api_layer.buffer.get_processing_buffer_ids)

router.get(
    "/users/event/{user_id}",
    tags=["event"],
    openapi_extra=API_X_CODE_DOCS["GET /users/event/{user_id}"],
)(api_layer.event.get_user_events)

router.put(
    "/users/event/{user_id}/{event_id}",
    tags=["event"],
    openapi_extra=API_X_CODE_DOCS["PUT /users/event/{user_id}/{event_id}"],
)(api_layer.event.update_user_event)

router.delete(
    "/users/event/{user_id}/{event_id}",
    tags=["event"],
    openapi_extra=API_X_CODE_DOCS["DELETE /users/event/{user_id}/{event_id}"],
)(api_layer.event.delete_user_event)

router.get(
    "/users/event/search/{user_id}",
    tags=["event"],
    openapi_extra=API_X_CODE_DOCS["GET /users/event/search/{user_id}"],
)(api_layer.event.search_user_events)

router.get(
    "/users/event_gist/search/{user_id}",
    tags=["event_gist"],
    openapi_extra=API_X_CODE_DOCS["GET /users/event_gist/search/{user_id}"],
)(api_layer.event.search_user_event_gists)

router.get(
    "/users/context/{user_id}",
    tags=["context"],
    openapi_extra=API_X_CODE_DOCS["GET /users/context/{user_id}"],
)(api_layer.context.get_user_context)


router.post(
    "/users/roleplay/proactive/{user_id}",
    tags=["roleplay"],
    # openapi_extra=API_X_CODE_DOCS["POST /users/roleplay/proactive/{user_id}"],
)(api_layer.roleplay.infer_proactive_topics)


@app.middleware("http")
async def global_wrapper_middleware(request, call_next):
    return await api_layer.middleware.global_wrapper_middleware(request, call_next)


app.include_router(router)
app.add_middleware(api_layer.middleware.AuthMiddleware)

FastAPIInstrumentor.instrument_app(app)



================================================
FILE: src/server/api/build_init_sql.py
================================================
import dotenv

dotenv.load_dotenv()
from sqlalchemy.schema import Table, CreateSchema, CreateIndex
from sqlalchemy.schema import CreateTable
import logging

logging.disable(logging.CRITICAL)
from memobase_server import __version__
from memobase_server.connectors import DB_ENGINE
from memobase_server.models.database import (
    User,
    GeneralBlob,
    BufferZone,
    UserProfile,
)


print("--", f"Synced from backend {__version__}")
for db in [User, GeneralBlob, BufferZone, UserProfile]:
    table_obj = db if isinstance(db, Table) else db.__table__
    # Print table creation
    print(str(CreateTable(table_obj).compile(DB_ENGINE)).strip() + ";")
    # Print indexes
    for index in table_obj.indexes:
        print(str(CreateIndex(index).compile(DB_ENGINE)).strip() + ";")



================================================
FILE: src/server/api/config.yaml.example
================================================
llm_api_key: YOUR-OPENAI-KEY

# If you use the lstudio embedded provider, you can refer to the configuration below

# embedding_provider: lmstudio
# embedding_api_key: lm_XXX
# embedding_model: text-embedding-qwen3-embedding-8b
# If you start with docker and access lmstudio or 127.0.0.1 via localhost, you can refer to the following base url config
# embedding_base_url: http://host.docker.internal:1234/v1
# embedding_base_url: http://127.0.0.1:1234/v1
# embedding_dim: 4096

# language: zh


================================================
FILE: src/server/api/DEVELOPMENT.md
================================================
[Binary file]


================================================
FILE: src/server/api/Dockerfile
================================================
FROM python:3.12-slim-bookworm
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/


# Set working directory
WORKDIR /app

# Copy dependency files
COPY ./pyproject.toml /app
COPY ./uv.lock /app

# Install Python dependencies using uv
RUN uv sync --frozen --no-dev --no-cache-dir

# Copy the application code
COPY ./memobase_server /app/memobase_server
COPY ./api.py /app



# Expose port 8000 (FastAPI default)
EXPOSE 8000

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Run the FastAPI server using uvicorn
CMD ["/app/.venv/bin/uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]



================================================
FILE: src/server/api/pyproject.toml
================================================
[project]
name = "memobase_server"
version = "0.1.0"
description = "Memobase Server"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi[standard]>=0.116.1",
    "numpy>=2.3.1",
    "openai>=1.97.0",
    "opentelemetry-api>=1.35.0",
    "opentelemetry-exporter-prometheus>=0.56b0",
    "opentelemetry-instrumentation-fastapi>=0.56b0",
    "opentelemetry-sdk>=1.35.0",
    "pgvector>=0.4.1",
    "psycopg2-binary>=2.9.10",
    "python-dotenv>=1.1.1",
    "pyyaml>=6.0.2",
    "redis>=6.2.0",
    "sqlalchemy>=2.0.41",
    "structlog>=25.4.0",
    "tiktoken>=0.9.0",
    "typeguard>=4.4.4",
    "volcengine-python-sdk[ark]>=4.0.6",
]

[dependency-groups]
dev = [
    "alembic>=1.16.4",
    "pytest>=8.4.1",
    "pytest-asyncio>=1.1.0",
    "pytest-cov>=6.2.1",
]



================================================
FILE: src/server/api/.dockerignore
================================================
# Created by https://www.toptal.com/developers/gitignore/api/python
# Edit at https://www.toptal.com/developers/gitignore?templates=python

### Python ###
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

### Python Patch ###
# Poetry local configuration file - https://python-poetry.org/docs/configuration/#local-configuration
poetry.toml

# ruff
.ruff_cache/

# LSP config files
pyrightconfig.json

# End of https://www.toptal.com/developers/gitignore/api/python




================================================
FILE: src/server/api/.env.example
================================================
DATABASE_URL=postgresql://memobase:helloworld@localhost:15432/memobase
REDIS_URL=redis://:helloworld@localhost:16379
ACCESS_TOKEN=secret

PROJECT_ID=memobase_dev
LOG_FORMAT=plain # or json


================================================
FILE: src/server/api/.python-version
================================================
3.12


================================================
FILE: src/server/api/example_config/event_tag/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

language: en

event_tags:
  - name: "mental_health"
    description: |
      You need to look at user's mental health status and pick the most relevant tag from the following list:
      - depression
      - anxiety
      - stress
      - anger
      - sadness
      - happiness



================================================
FILE: src/server/api/example_config/jina_embedding/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

embedding_provider: jina
embedding_api_key: jina_XXXX
embedding_model: jina-embeddings-v3
embedding_dim: 1024

language: en



================================================
FILE: src/server/api/example_config/only_strict_profile/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

enable_event_embedding: false
enable_event_summary: false
profile_strict_mode: true

overwrite_user_profiles:
  - topic: "basic_info"
    sub_topics:
      - name: "name"
      - name: "age"
      - name: "gender"
      - name: "occupation"
        description: "user's professional background"
      - name: "location"
      - name: "languages"
        description: "languages the user speaks"
      - name: "timezone"
        description: "user's local timezone for scheduling"



================================================
FILE: src/server/api/example_config/profile_for_assistant/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

overwrite_user_profiles:
  - topic: "basic_info"
    sub_topics:
      - name: "name"
      - name: "age"
      - name: "gender"
      - name: "occupation"
      - name: "location"
      - name: "timezone"
        description: "user's local timezone for scheduling"
      - name: "languages"
        description: "preferred languages for communication"
      - name: "contact_info"
        description: "preferred contact methods"

  - topic: "schedule_prefs"
    sub_topics:
      - name: "work_hours"
        description: "typical working schedule"
      - name: "sleep_schedule"
        description: "usual sleep and wake times"
      - name: "meeting_prefs"
        description: "preferred meeting times and duration"
      - name: "break_times"
        description: "preferred break schedules"
      - name: "focus_hours"
        description: "times for deep work or concentration"
      - name: "reminder_freq"
        description: "how often to send reminders"

  - topic: "task_management"
    sub_topics:
      - name: "priority_rules"
        description: "how to prioritize different tasks"
      - name: "task_categories"
        description: "personal, work, health, etc."
      - name: "deadline_buffer"
        description: "preferred time buffer before deadlines"
      - name: "delegation_prefs"
        description: "tasks that can be delegated"
      - name: "recurring_tasks"
        description: "regular responsibilities"
      - name: "task_format"
        description: "preferred task presentation style"

  - topic: "productivity_settings"
    sub_topics:
      - name: "focus_mode"
        description: "do not disturb preferences"
      - name: "notification_prefs"
        description: "how and when to receive alerts"
      - name: "automation_rules"
        description: "tasks to automate"
      - name: "report_frequency"
        description: "how often to receive progress reports"
      - name: "tracking_metrics"
        description: "productivity metrics to monitor"
      - name: "tool_integrations"
        description: "preferred productivity tools"

  - topic: "lifestyle_prefs"
    sub_topics:
      - name: "diet_restr"
        description: "dietary restrictions and preferences"
      - name: "exercise_routine"
        description: "fitness schedule and preferences"
      - name: "shopping_lists"
        description: "regular shopping needs"
      - name: "travel_prefs"
        description: "travel preferences and frequent destinations"
      - name: "entertainment"
        description: "preferred leisure activities"
      - name: "budget_tracking"
        description: "expense categories and limits"

  - topic: "communication_style"
    sub_topics:
      - name: "tone_pref"
        description: "formal, casual, friendly, professional"
      - name: "response_format"
        description: "brief, detailed, bullet points, etc."
      - name: "urgency_levels"
        description: "how to handle different priority levels"
      - name: "follow_up_freq"
        description: "frequency of status updates"
      - name: "comm_channels"
        description: "preferred communication platforms"
      - name: "availability"
        description: "when to expect responses"



================================================
FILE: src/server/api/example_config/profile_for_companion/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

overwrite_user_profiles:
  - topic: "basic_info"
    sub_topics:
      - name: "name"
      - name: "age"
      - name: "gender"
      - name: "occupation"
        description: "user's professional background"
      - name: "location"
      - name: "languages"
        description: "languages the user speaks"
      - name: "timezone"
        description: "user's local timezone for scheduling"

  - topic: "companion_preferences"
    sub_topics:
      - name: "companion_type"
        description: "preferred ai companion personality type"
      - name: "interaction_style"
        description: "formal, casual, friendly, professional etc."
      - name: "communication_freq"
        description: "how often they want to interact with the ai"
      - name: "interest_topics"
        description: "preferred conversation topics"
      - name: "learning_goals"
        description: "what they want to learn or achieve"
      - name: "privacy_prefs"
        description: "data sharing and storage preferences"

  - topic: "interaction_history"
    sub_topics:
      - name: "convo_count"
      - name: "favorite_topics"
      - name: "active_projects"
      - name: "saved_convos"
      - name: "feedback_hist"
        description: "user's previous feedback on interactions"

  - topic: "personalization"
    description: "tags for customizing ai behavior"
    sub_topics:
      - name: "humor_pref"
        description: "whether they enjoy jokes and casual banter"
      - name: "response_len"
        description: "preferred length of ai responses"
      - name: "tech_depth"
        description: "preferred level of technical detail"
      - name: "learn_style"
        description: "visual, verbal, practical, etc."



================================================
FILE: src/server/api/example_config/profile_for_education/config.yaml
================================================
llm_api_key: XXX
llm_base_url: https://api.openai.com/v1/
best_llm_model: gpt-4o

overwrite_user_profiles:
  - topic: "basic_info"
    sub_topics:
      - name: "name"
      - name: "age"
      - name: "gender"
      - name: "grade_level"
        description: "current academic grade or level"
      - name: "school_type"
        description: "public, private, homeschool, university etc."
      - name: "location"
      - name: "primary_language"
      - name: "learning_languages"
        description: "languages being studied"

  - topic: "academic_profile"
    sub_topics:
      - name: "major_subjects"
        description: "main subjects being studied"
      - name: "weak_subjects"
        description: "subjects needing improvement"
      - name: "strong_subjects"
        description: "subjects with good performance"
      - name: "study_hours"
        description: "average daily study time"
      - name: "academic_goals"
        description: "target grades or learning objectives"
      - name: "exam_schedule"
        description: "upcoming test dates and deadlines"

  - topic: "learning_preferences"
    sub_topics:
      - name: "learn_style"
        description: "visual, auditory, reading, kinesthetic"
      - name: "study_mode"
        description: "solo, group, tutor-led, mixed"
      - name: "content_format"
        description: "video, text, interactive, practice tests"
      - name: "session_length"
        description: "preferred duration of study sessions"
      - name: "difficulty_pref"
        description: "preferred challenge level of content"
      - name: "review_frequency"
        description: "how often to review learned material"

  - topic: "progress_tracking"
    sub_topics:
      - name: "completed_courses"
      - name: "current_courses"
      - name: "course_progress"
        description: "percentage completion of active courses"
      - name: "test_scores"
      - name: "practice_stats"
        description: "practice exercise completion rates"
      - name: "study_streaks"
        description: "consecutive days of learning"

  - topic: "engagement_metrics"
    sub_topics:
      - name: "active_time"
        description: "time spent actively learning"
      - name: "completion_rate"
        description: "percentage of started courses finished"
      - name: "interaction_freq"
        description: "frequency of app usage"
      - name: "preferred_times"
        description: "most active study hours"
      - name: "reward_points"
        description: "accumulated achievement points"
      - name: "social_learning"
        description: "participation in group activities"



================================================
FILE: src/server/api/memobase_server/__init__.py
================================================
__version__ = "0.0.40"

__author__ = "memobase.io"
__url__ = "https://github.com/memodb-io/memobase"
__license__ = "Apache-2.0"



================================================
FILE: src/server/api/memobase_server/connectors.py
================================================
import os
import asyncio
import redis.exceptions as redis_exceptions
import redis.asyncio as redis
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import OperationalError
from uuid import uuid4
from .env import LOG
from .models.database import REG, Project, UserEvent, UserEventGist

DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
PROJECT_ID = os.getenv("PROJECT_ID")
ADMIN_URL = os.getenv("ADMIN_URL")
ADMIN_TOKEN = os.getenv("ADMIN_TOKEN")

if PROJECT_ID is None:
    LOG.warning(f"PROJECT_ID is not set")
    PROJECT_ID = "default"
LOG.info(f"Project ID: {PROJECT_ID}")
LOG.info(f"Database URL: {DATABASE_URL}")
LOG.info(f"Redis URL: {REDIS_URL}")

# Create an engine
DB_ENGINE = create_engine(
    DATABASE_URL,
    pool_size=75,  # Increased from 50 to handle more concurrent operations
    max_overflow=50,  # Increased from 30 to provide more buffer
    pool_recycle=300,  # Reduced from 600 to recycle connections more frequently
    pool_pre_ping=True,  # Verify connections before using
    pool_timeout=45,  # Increased from 30 seconds for better handling under load
    pool_reset_on_return="commit",  # Ensure clean state when connections are returned
    echo_pool=False,  # Set to True for debugging pool issues
)
REDIS_POOL = None

Session = sessionmaker(bind=DB_ENGINE)


def create_pgvector_extension():
    try:
        with Session() as session:
            session.execute(text("CREATE EXTENSION IF NOT EXISTS vector;"))
            session.commit()
            LOG.info("pgvector extension created or already exists")
    except Exception as e:
        LOG.error(f"Failed to create pgvector extension: {e}")


def create_tables():
    create_pgvector_extension()

    REG.metadata.create_all(DB_ENGINE)
    with Session() as session:
        Project.initialize_root_project(session)
        UserEvent.check_legal_embedding_dim(session)
        UserEventGist.check_legal_embedding_dim(session)
    LOG.info("Database tables created successfully")


create_tables()


def db_health_check() -> bool:
    try:
        conn = DB_ENGINE.connect()
    except OperationalError as e:
        LOG.error(f"Database connection failed: {e}")
        return False
    else:
        conn.close()
        return True


async def redis_health_check() -> bool:
    try:
        async with get_redis_client() as redis_client:
            await redis_client.ping()
    except redis_exceptions.ConnectionError as e:
        LOG.error(f"Redis connection failed: {e}")
        return False
    else:
        return True


async def close_connection():
    DB_ENGINE.dispose()
    if REDIS_POOL is not None:
        await REDIS_POOL.aclose()
    LOG.info("Connections closed")


def init_redis_pool():
    global REDIS_POOL
    REDIS_POOL = redis.ConnectionPool.from_url(REDIS_URL, decode_responses=True)


def get_redis_client() -> redis.Redis:
    if REDIS_POOL is not None:
        return redis.Redis(connection_pool=REDIS_POOL, decode_responses=True)
    else:
        return redis.Redis.from_url(REDIS_URL, decode_responses=True)


def get_pool_status() -> dict:
    """Get current connection pool status for monitoring."""
    pool = DB_ENGINE.pool
    return {
        "size": pool.size(),
        "checked_in": pool.checkedin(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "total_capacity": pool.size() + pool.overflow(),
        "utilization_percent": (
            round((pool.checkedout() / (pool.size() + pool.overflow())) * 100, 2)
            if (pool.size() + pool.overflow()) > 0
            else 0
        ),
    }


def log_pool_status(operation: str = "unknown"):
    """Log current pool status for debugging."""
    status = get_pool_status()
    if status["utilization_percent"] > 80:  # Log warning if utilization is high
        LOG.warning(
            f"High DB pool utilization after {operation}: "
            f"{status['checked_out']}/{status['total_capacity']} "
            f"({status['utilization_percent']}%) - "
            f"Available: {status['checked_in']}, Overflow: {status['overflow']}"
        )
    LOG.info(f"[DB pool status] {operation}: {status}")


if __name__ == "__main__":

    async def main():
        try:
            result = await redis_health_check()
            print(result)
        finally:
            await close_connection()

    asyncio.run(main())



================================================
FILE: src/server/api/memobase_server/env.py
================================================
"""
Initialize logger, encoder, and config.
"""

import os
import datetime
import json
import yaml
import logging
import tiktoken
import dataclasses
from dataclasses import dataclass, field
from typing import Optional, Literal, Union
from dotenv import load_dotenv
from zoneinfo import ZoneInfo
from datetime import timezone
from typeguard import check_type
import structlog
from .types import UserProfileTopic
from .struct_logger import ProjectStructLogger, configure_logger

load_dotenv()


class BillingStatus:
    free = "free"
    pro = "pro"
    usage_based = "usage_based"


BILLING_REFILL_AMOUNT_MAP = {
    BillingStatus.free: int(os.getenv("USAGE_TOKEN_LIMIT_ACTIVE", 0)) or None,
}


class ProjectStatus:
    ultra = "ultra"
    pro = "pro"
    active = "active"
    suspended = "suspended"


USAGE_TOKEN_LIMIT_MAP = {
    ProjectStatus.active: int(os.getenv("USAGE_TOKEN_LIMIT_ACTIVE", -1)),
    ProjectStatus.pro: int(os.getenv("USAGE_TOKEN_LIMIT_PRO", -1)),
    ProjectStatus.ultra: int(os.getenv("USAGE_TOKEN_LIMIT_ULTRA", -1)),
}


class ContanstTable:
    topic = "topic"
    sub_topic = "sub_topic"
    memo = "memo"
    update_hits = "update_hits"

    roleplay_plot_status = "roleplay_plot_status"


class BufferStatus:
    idle = "idle"
    processing = "processing"
    done = "done"
    failed = "failed"


class TelemetryKeyName:
    insert_blob_request = "insert_blob_request"
    insert_blob_success_request = "insert_blob_success_request"
    llm_input_tokens = "llm_input_tokens"
    llm_output_tokens = "llm_output_tokens"
    has_request = "has_request"


@dataclass
class Config:
    # IMPORTANT!
    persistent_chat_blobs: bool = False
    use_timezone: Optional[
        Literal[
            "UTC", "America/New_York", "Europe/London", "Asia/Tokyo", "Asia/Shanghai"
        ]
    ] = None

    system_prompt: str = None
    buffer_flush_interval: int = 60 * 60  # 1 hour
    max_chat_blob_buffer_token_size: int = 1024
    max_chat_blob_buffer_process_token_size: int = 16384
    max_profile_subtopics: int = 15
    max_pre_profile_token_size: int = 128
    llm_tab_separator: str = "::"
    cache_user_profiles_ttl: int = 60 * 20  # 20 minutes

    # LLM
    language: Literal["en", "zh"] = "en"
    llm_style: Literal["openai", "doubao_cache"] = "openai"
    llm_base_url: str = None
    llm_api_key: str = None
    llm_openai_default_query: dict[str, str] = None
    llm_openai_default_header: dict[str, str] = None
    best_llm_model: str = "gpt-4o-mini"
    thinking_llm_model: str = "o4-mini"
    summary_llm_model: str = None

    enable_event_embedding: bool = True
    embedding_provider: Literal["openai", "jina"] = "openai"
    embedding_api_key: str = None
    embedding_base_url: str = None
    embedding_dim: int = 1536
    embedding_model: str = "text-embedding-3-small"
    embedding_max_token_size: int = 8192

    additional_user_profiles: list[dict] = field(default_factory=list)
    overwrite_user_profiles: Optional[list[dict]] = None
    event_theme_requirement: Optional[str] = (
        "Focus on the user's infos, not its instructions."
    )
    profile_strict_mode: bool = False
    profile_validate_mode: bool = True

    minimum_chats_token_size_for_event_summary: int = 256
    event_tags: list[dict] = field(default_factory=list)
    # Telemetry
    telemetry_deployment_environment: str = "local"

    @classmethod
    def _process_env_vars(cls, config_dict):
        """
        Process all environment variables for the config class.

        Args:
            cls: The config class
            config_dict: The current configuration dictionary

        Returns:
            Updated configuration dictionary with environment variables applied
        """
        # Ensure we have a dictionary to work with
        if not isinstance(config_dict, dict):
            config_dict = {}

        for field in dataclasses.fields(cls):
            field_name = field.name
            field_type = field.type
            env_var_name = f"MEMOBASE_{field_name.upper()}"
            if env_var_name in os.environ:
                env_value = os.environ[env_var_name]

                # Try to parse as JSON first
                try:
                    parsed_value = json.loads(env_value)
                    # Check if parsed value matches the type
                    try:
                        check_type(parsed_value, field_type)
                        config_dict[field_name] = parsed_value
                        continue
                    except TypeError:
                        # Parsed value doesn't match type, fall through to try raw string
                        pass
                except json.JSONDecodeError:
                    # Not valid JSON, fall through to try raw string
                    pass

                # Try the raw string
                try:
                    check_type(env_value, field_type)
                    config_dict[field_name] = env_value
                except TypeError as e:
                    LOG.warning(
                        f"Value for {env_var_name} is not compatible with field type {field_type}. Ignoring."
                    )

        return config_dict

    @classmethod
    def load_config(cls) -> "Config":
        if not os.path.exists("config.yaml"):
            overwrite_config = {}
        else:
            with open("config.yaml") as f:
                overwrite_config = yaml.safe_load(f)
                LOG.info(f"Load ./config.yaml")

        # Process environment variables
        overwrite_config = cls._process_env_vars(overwrite_config)

        # Filter out any keys from overwrite_config that aren't in the dataclass
        fields = {field.name for field in dataclasses.fields(cls)}
        filtered_config = {k: v for k, v in overwrite_config.items() if k in fields}
        overwrite_config = cls(**filtered_config)
        LOG.info(f"{overwrite_config}")
        return overwrite_config

    def __post_init__(self):
        assert self.llm_api_key is not None, "llm_api_key is required"
        if self.enable_event_embedding:
            if self.embedding_api_key is None and (
                self.llm_style == self.embedding_provider == "openai"
            ):
                # default to llm config if embedding_api_key is not set
                self.embedding_api_key = self.llm_api_key
                self.embedding_base_url = self.llm_base_url
            assert (
                self.embedding_api_key is not None
            ), "embedding_api_key is required for event embedding"

            if self.embedding_provider == "jina":
                self.embedding_base_url = (
                    self.embedding_base_url or "https://api.jina.ai/v1"
                )
                assert self.embedding_model in {
                    "jina-embeddings-v3",
                }, "embedding_model must be one of the following: jina-embeddings-v3"

        if self.additional_user_profiles:
            [UserProfileTopic(**up) for up in self.additional_user_profiles]
        if self.overwrite_user_profiles:
            [UserProfileTopic(**up) for up in self.overwrite_user_profiles]

    @property
    def timezone(self) -> timezone:
        if self.use_timezone is None:
            return datetime.datetime.now().astimezone().tzinfo

        # For named timezones, we need to use the datetime.timezone.ZoneInfo
        return ZoneInfo(self.use_timezone)


@dataclass
class ProfileConfig:
    language: Literal["en", "zh"] = None
    profile_strict_mode: bool | None = None
    profile_validate_mode: bool | None = None
    additional_user_profiles: list[dict] = field(default_factory=list)
    overwrite_user_profiles: Optional[list[dict]] = None
    event_theme_requirement: Optional[str] = None

    event_tags: list[dict] = None

    def __post_init__(self):
        if self.language not in ["en", "zh"]:
            self.language = None
        if self.additional_user_profiles:
            [UserProfileTopic(**up) for up in self.additional_user_profiles]
        if self.overwrite_user_profiles:
            [UserProfileTopic(**up) for up in self.overwrite_user_profiles]

    @classmethod
    def load_config_string(cls, config_string: str) -> "Config":
        overwrite_config = yaml.safe_load(config_string)
        if overwrite_config is None:
            return cls()
        # Get all field names from the dataclass
        fields = {field.name for field in dataclasses.fields(cls)}
        # Filter out any keys from overwrite_config that aren't in the dataclass
        filtered_config = {k: v for k, v in overwrite_config.items() if k in fields}
        overwrite_config = cls(**filtered_config)
        return overwrite_config


class Colors:
    BLUE = "\033[94m"
    BOLD = "\033[1m"
    GREEN = "\033[92m"
    END = "\033[0m"


# remove default uvicorn loggers cause we have our own
for _log in ["uvicorn", "uvicorn.error", "uvicorn.access"]:
    logging.getLogger(_log).handlers.clear()
    # logging.getLogger(_log).propagate = True

log_format = os.getenv("LOG_FORMAT", "plain")
if log_format == "json":
    configure_logger()
    logger = structlog.get_logger()
    LOG = logger.bind(app_name="memobase_server")
else:
    LOG = logging.getLogger("memobase_server")
    LOG.setLevel(logging.INFO)

    formatter = logging.Formatter(
        f"{Colors.BOLD}{Colors.BLUE}%(name)s |{Colors.END}  %(levelname)s - %(asctime)s  -  %(message)s"
    )
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    LOG.addHandler(handler)


ENCODER = tiktoken.encoding_for_model("gpt-4o")

CONFIG = Config.load_config()


class ProjectLogger:
    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def debug(self, project_id: str, user_id: str, message: str):
        self.logger.debug(
            json.dumps({"project_id": str(project_id), "user_id": str(user_id)})
            + " | "
            + message
        )

    def info(self, project_id: str, user_id: str, message: str):
        self.logger.info(
            json.dumps({"project_id": str(project_id), "user_id": str(user_id)})
            + " | "
            + message
        )

    def warning(self, project_id: str, user_id: str, message: str):
        self.logger.warning(
            json.dumps({"project_id": str(project_id), "user_id": str(user_id)})
            + " | "
            + message
        )

    def error(
        self, project_id: str, user_id: str, message: str, exc_info: bool = False
    ):
        self.logger.error(
            json.dumps({"project_id": str(project_id), "user_id": str(user_id)})
            + " | "
            + message,
            exc_info=exc_info,
        )


if log_format == "json":
    TRACE_LOG = ProjectStructLogger(LOG)
else:
    TRACE_LOG = ProjectLogger(LOG)



================================================
FILE: src/server/api/memobase_server/errors.py
================================================
class ExternalAPIError(Exception):
    pass



================================================
FILE: src/server/api/memobase_server/struct_logger.py
================================================
import structlog
from contextlib import contextmanager
import structlog.contextvars
import logging
import sys


def configure_logger():
    shared_processors = [
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.stdlib.ExtraAdder(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.CallsiteParameterAdder(
            [
                structlog.processors.CallsiteParameter.LINENO,
                structlog.processors.CallsiteParameter.PATHNAME,
            ]
        ),
    ]

    structlog_processors = shared_processors + [
        structlog.processors.dict_tracebacks,
        structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
    ]

    structlog.configure(
        processors=structlog_processors,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

    formatter = structlog.stdlib.ProcessorFormatter(
        foreign_pre_chain=shared_processors,
        processors=[
            structlog.stdlib.ProcessorFormatter.remove_processors_meta,
            structlog.processors.JSONRenderer(),
        ],
    )

    handler = logging.StreamHandler(stream=sys.stdout)
    handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.addHandler(handler)
    root_logger.setLevel(logging.INFO)


@contextmanager
def bound_context(**kwargs):
    with structlog.contextvars.bound_contextvars(**kwargs):
        yield


class ProjectStructLogger:
    def __init__(self, logger):
        self.logger = logger

    def debug(self, project_id: str, user_id: str, message: str):
        with bound_context(project_id=str(project_id), user_id=str(user_id)):
            self.logger.debug(message)

    def info(self, project_id: str, user_id: str, message: str):
        with bound_context(project_id=str(project_id), user_id=str(user_id)):
            self.logger.info(message)

    def warning(self, project_id: str, user_id: str, message: str):
        with bound_context(project_id=str(project_id), user_id=str(user_id)):
            self.logger.warning(message)

    def error(
        self, project_id: str, user_id: str, message: str, exc_info: bool = False
    ):
        with bound_context(project_id=str(project_id), user_id=str(user_id)):
            self.logger.error(message, exc_info=exc_info)



================================================
FILE: src/server/api/memobase_server/types.py
================================================
from pydantic import BaseModel, field_validator
from dataclasses import dataclass, field
from typing import Optional


def attribute_unify(attr: str):
    return attr.lower().strip().replace(" ", "_")


class SubTopic(BaseModel):
    name: str
    description: Optional[str] = None
    update_description: Optional[str] = None
    validate_value: Optional[bool] = None

    @field_validator("name")
    def validate_name(cls, v):
        return attribute_unify(v)

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default=None):
        return getattr(self, key, default)


@dataclass
class EventTag:
    name: str
    description: Optional[str] = None

    def __post_init__(self):
        self.name = attribute_unify(self.name)
        self.description = self.description or ""


@dataclass
class UserProfileTopic:
    topic: str
    description: Optional[str] = None
    sub_topics: list[SubTopic] = field(default_factory=list)

    def __post_init__(self):
        self.topic = attribute_unify(self.topic)
        self.sub_topics = [
            SubTopic(**{"name": st}) if isinstance(st, str) else SubTopic(**st)
            for st in self.sub_topics
        ]



================================================
FILE: src/server/api/memobase_server/utils.py
================================================
import re
import yaml
import json
from typing import cast
from datetime import timezone, datetime
from functools import wraps
from pydantic import ValidationError
from .env import ENCODER, LOG, CONFIG, ProfileConfig
from .models.blob import (
    Blob,
    BlobType,
    ChatBlob,
    DocBlob,
    SummaryBlob,
    OpenAICompatibleMessage,
)
from .models.database import GeneralBlob
from .models.response import UserEventData, EventData
from .models.utils import Promise, CODE
from .connectors import get_redis_client, PROJECT_ID

LIST_INT_REGEX = re.compile(r"\[\s*(?:\d+(?:\s*,\s*\d+)*\s*)?\]")


def event_str_repr(event: UserEventData) -> str:
    event_data = event.event_data
    if event_data.event_tip is None:
        profile_deltas = [
            f"- {ed.attributes['topic']}::{ed.attributes['sub_topic']}: {ed.content}"
            for ed in event_data.profile_delta
        ]
        profile_delta_str = "\n".join(profile_deltas)
        return profile_delta_str
    else:
        if event_data.event_tags:
            event_tags = "\n".join(
                [f"- {tag.tag}: {tag.value}" for tag in event_data.event_tags]
            )
        else:
            event_tags = ""
        return f"{event_tags}\n{event_data.event_tip}"


def event_embedding_str(event_data: EventData) -> str:
    if event_data.profile_delta is None:
        profile_delta_str = ""
    else:
        profile_deltas = [
            f"- {ed.attributes['topic']}::{ed.attributes['sub_topic']}: {ed.content}"
            for ed in event_data.profile_delta
        ]
        profile_delta_str = "\n".join(profile_deltas)

    if event_data.event_tags is None:
        event_tags = ""
    else:
        event_tags = "\n".join(
            [f"- {tag.tag}: {tag.value}" for tag in event_data.event_tags]
        )

    if event_data.event_tip is None:
        r = f"{profile_delta_str}\n{event_tags}"
    else:
        r = f"{event_data.event_tip}\n{profile_delta_str}\n{event_tags}"
    return r


def load_json_or_none(content: str) -> dict | None:
    try:
        return json.loads(content)
    except Exception:
        LOG.error(f"Invalid json: {content}")
        return None


def find_list_int_or_none(content: str) -> list[int] | None:
    result = LIST_INT_REGEX.findall(content)
    if not result:
        return None
    result = result[0]
    ids = result.strip("[]").strip()
    if not ids:
        return []
    return [int(i.strip()) for i in ids.split(",")]


def get_encoded_tokens(content: str) -> list[int]:
    return ENCODER.encode(content)


def get_decoded_tokens(tokens: list[int]) -> str:
    return ENCODER.decode(tokens)


def truncate_string(content: str, max_tokens: int) -> str:
    tokens = get_encoded_tokens(content)
    tailing = "" if len(tokens) <= max_tokens else "..."
    return get_decoded_tokens(tokens[:max_tokens]) + tailing


def pack_blob_from_db(blob: GeneralBlob, blob_type: BlobType) -> Blob:
    blob_data = blob.blob_data
    match blob_type:
        case BlobType.chat:
            return ChatBlob(**blob_data, created_at=blob.created_at)
        case BlobType.doc:
            return DocBlob(**blob_data, created_at=blob.created_at)
        case BlobType.summary:
            return SummaryBlob(**blob_data, created_at=blob.created_at)
        case _:
            raise ValueError(f"Unsupported Blob Type: {blob_type}")


def get_message_timestamp(
    message: OpenAICompatibleMessage, fallback_blob_timestamp: datetime
):
    fallback_blob_timestamp = fallback_blob_timestamp or datetime.now()
    fallback_blob_timestamp = fallback_blob_timestamp.astimezone(CONFIG.timezone)
    return (
        message.created_at
        if message.created_at
        else fallback_blob_timestamp.strftime("%Y/%m/%d")
    )


def get_message_name(message: OpenAICompatibleMessage):
    if message.alias:
        # if message.role == "assistant":
        #     return f"{message.alias}"
        return f"{message.alias}({message.role})"
    return message.role


def get_blob_str(blob: Blob):
    match blob.type:
        case BlobType.chat:
            return "\n".join(
                [
                    f"[{get_message_timestamp(m, blob.created_at)}] {get_message_name(m)}: {m.content}"
                    for m in cast(ChatBlob, blob).messages
                ]
            )
        case BlobType.doc:
            return cast(DocBlob, blob).content
        case BlobType.summary:
            time_created = cast(SummaryBlob, blob).created_at or datetime.now()
            clean_summary = cast(SummaryBlob, blob).summary.replace("\n", " ")
            return f"- {clean_summary}[{time_created.strftime("%Y/%m/%d")}]"
        case _:
            raise ValueError(f"Unsupported Blob Type: {blob.type}")


def get_blob_token_size(blob: Blob):
    return len(get_encoded_tokens(get_blob_str(blob)))


def seconds_from_now(dt: datetime):
    return (datetime.now().astimezone() - dt.astimezone()).seconds


def is_valid_profile_config(profile_config: str | None) -> Promise[None]:
    if profile_config is None:
        return Promise.resolve(None)
    # check if the profile config is valid yaml
    try:
        if len(profile_config) > 65535:
            return Promise.reject(CODE.BAD_REQUEST, "Profile config is too long")
        ProfileConfig.load_config_string(profile_config)
        return Promise.resolve(None)
    except yaml.YAMLError as e:
        return Promise.reject(CODE.BAD_REQUEST, f"Invalid profile config: {e}")
    except ValidationError as e:
        return Promise.reject(CODE.BAD_REQUEST, f"Invalid profile config: {e}")



================================================
FILE: src/server/api/memobase_server/api_layer/__init__.py
================================================
from . import chore
from . import project
from . import user
from . import blob
from . import profile
from . import buffer
from . import event
from . import context
from . import middleware
from . import roleplay



================================================
FILE: src/server/api/memobase_server/api_layer/blob.py
================================================
from fastapi import BackgroundTasks, Request
from fastapi import Path, Body, Query
import traceback

from ..controllers import full as controllers

from ..env import TelemetryKeyName, TRACE_LOG
from ..models.response import CODE
from ..models.utils import Promise
from ..models import response as res
from ..telemetry.capture_key import capture_int_key


async def insert_blob(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to insert the blob for"),
    wait_process: bool = Query(
        False, description="Whether to wait for the blob to be processed"
    ),
    blob_data: res.BlobData = Body(..., description="The blob data to insert"),
    background_tasks: BackgroundTasks = BackgroundTasks(),
) -> res.BlobInsertResponse:
    project_id = request.state.memobase_project_id
    background_tasks.add_task(
        capture_int_key, TelemetryKeyName.insert_blob_request, project_id=project_id
    )

    p = await controllers.billing.get_project_billing(project_id)
    if not p.ok():
        return p.to_response(res.IdResponse)
    billing = p.data()

    if billing.token_left is not None and billing.token_left < 0:
        return Promise.reject(
            CODE.SERVICE_UNAVAILABLE,
            f"Your project reaches Memobase token limit, "
            f"Left: {billing.token_left}, this project used: {billing.project_token_cost_month}. "
            f"Your quota will be refilled on {billing.next_refill_at}. "
            "\nhttps://www.memobase.io/pricing for more information.",
        ).to_response(res.IdResponse)

    try:
        insert_result = await controllers.blob.insert_blob(
            user_id, project_id, blob_data
        )
        if not insert_result.ok():
            return insert_result.to_response(res.BaseResponse)
        bid = insert_result.data().id

        pb = await controllers.buffer.insert_blob_to_buffer(
            user_id, project_id, bid, blob_data.to_blob()
        )
        if not pb.ok():
            return pb.to_response(res.BaseResponse)

        process_ids = await controllers.buffer.detect_buffer_full_or_not(
            user_id, project_id, blob_data.blob_type
        )
        if not process_ids.ok():
            return process_ids.to_response(res.BaseResponse)

        final_results = []
        # need to process buffer
        if process_ids.data() is not None and len(process_ids.data().ids):
            if wait_process:
                # sync
                p = await controllers.buffer.flush_buffer_by_ids(
                    user_id, project_id, blob_data.blob_type, process_ids.data().ids
                )
                if not p.ok():
                    return p.to_response(res.BaseResponse)
                if p.data() is not None:
                    final_results.append(p.data())
            else:
                # async
                background_tasks.add_task(
                    controllers.buffer_background.flush_buffer_by_ids_in_background,
                    user_id,
                    project_id,
                    blob_data.blob_type,
                    process_ids.data().ids,
                )
    except Exception as e:
        TRACE_LOG.error(
            project_id, user_id, f"Error inserting blob: {e}, {traceback.format_exc()}"
        )
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR, f"Error inserting blob: {e}"
        ).to_response(res.BaseResponse)

    background_tasks.add_task(
        capture_int_key,
        TelemetryKeyName.insert_blob_success_request,
        project_id=project_id,
    )
    return res.BlobInsertResponse(
        data={**insert_result.data().model_dump(), "chat_results": final_results}
    )


async def get_blob(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    blob_id: str = Path(..., description="The ID of the blob to retrieve"),
) -> res.BlobDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.blob.get_blob(user_id, project_id, blob_id)
    return p.to_response(res.BlobDataResponse)


async def delete_blob(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    blob_id: str = Path(..., description="The ID of the blob to delete"),
) -> res.BaseResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.blob.remove_blob(user_id, project_id, blob_id)
    return p.to_response(res.BaseResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/buffer.py
================================================
from ..env import BufferStatus
from ..controllers import full as controllers
from ..models.response import IdsData, IdsResponse
from ..models.blob import BlobType
from ..models import response as res
from typing import Literal
from fastapi import Request, Query, BackgroundTasks
from fastapi import Path


async def flush_buffer(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    buffer_type: BlobType = Path(..., description="The type of buffer to flush"),
    wait_process: bool = Query(
        False, description="Whether to wait for the buffer to be processed"
    ),
    background_tasks: BackgroundTasks = BackgroundTasks(),
) -> res.ChatModalAPIResponse:
    """Flush unprocessed blobs into Memory"""
    project_id = request.state.memobase_project_id
    # p = await controllers.buffer.wait_insert_done_then_flush(
    #     user_id, project_id, buffer_type
    # )

    p = await controllers.buffer.get_unprocessed_buffer_ids(
        user_id, project_id, buffer_type
    )
    if not p.ok():
        return p.to_response(res.BaseResponse)
    if not len(p.data().ids):
        return res.ChatModalAPIResponse(data=[])
    if wait_process:
        p = await controllers.buffer.flush_buffer_by_ids(
            user_id, project_id, buffer_type, p.data().ids
        )
        if not p.ok():
            return p.to_response(res.BaseResponse)
        if p.data() is not None:
            return res.ChatModalAPIResponse(data=[p.data()])
    else:
        background_tasks.add_task(
            controllers.buffer_background.flush_buffer_by_ids_in_background,
            user_id,
            project_id,
            buffer_type,
            p.data().ids,
        )
        return res.ChatModalAPIResponse(data=None)


async def get_processing_buffer_ids(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    buffer_type: BlobType = Path(..., description="The type of buffer to flush"),
    status: Literal["idle", "processing", "failed", "done"] = Query(
        "processing", description="The status of the buffer to get"
    ),
) -> IdsResponse:
    """Get processing buffer ids"""
    project_id = request.state.memobase_project_id
    p = await controllers.buffer.get_unprocessed_buffer_ids(
        user_id, project_id, buffer_type, select_status=status
    )
    return p.to_response(IdsResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/chore.py
================================================
import traceback
from fastapi import HTTPException, Request
from ..env import LOG
from ..models.response import BaseResponse, CODE
from ..models.database import DEFAULT_PROJECT_ID
from ..connectors import db_health_check, redis_health_check
from ..llms.embeddings import check_embedding_sanity
from ..llms import llm_sanity_check


async def healthcheck() -> BaseResponse:
    """Check if your memobase is set up correctly"""
    if not db_health_check():
        raise HTTPException(
            status_code=CODE.INTERNAL_SERVER_ERROR.value,
            detail="Database not available",
        )
    if not await redis_health_check():
        raise HTTPException(
            status_code=CODE.INTERNAL_SERVER_ERROR.value,
            detail="Redis not available",
        )
    return BaseResponse()


async def root_running_status_check(request: Request) -> BaseResponse:
    """Check if your memobase is set up correctly"""
    project_id = request.state.memobase_project_id
    if project_id != DEFAULT_PROJECT_ID:
        raise HTTPException(
            status_code=CODE.METHOD_NOT_ALLOWED.value,
            detail="Only Root can access this",
        )
    if not db_health_check():
        raise HTTPException(
            status_code=CODE.INTERNAL_SERVER_ERROR.value,
            detail="Database not available",
        )
    if not await redis_health_check():
        raise HTTPException(
            status_code=CODE.INTERNAL_SERVER_ERROR.value,
            detail="Redis not available",
        )
    try:
        await check_embedding_sanity()
        await llm_sanity_check()
    except Exception as e:
        raise HTTPException(
            status_code=CODE.INTERNAL_SERVER_ERROR.value,
            detail=f"Root status checking failed: {e}",
        )

    return BaseResponse()



================================================
FILE: src/server/api/memobase_server/api_layer/context.py
================================================
import json

from ..controllers import full as controllers

from ..models.response import CODE
from ..models.utils import Promise
from ..models import response as res
from fastapi import Request
from fastapi import Path, Query


async def get_user_context(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    max_token_size: int = Query(
        1000,
        description="Max token size of returned Context",
    ),
    prefer_topics: list[str] = Query(
        None,
        description="Rank prefer topics at first to try to keep them in filtering, default order is by updated time",
    ),
    only_topics: list[str] = Query(
        None,
        description="Only return profiles with these topics, default is all",
    ),
    max_subtopic_size: int = Query(
        None,
        description="Max subtopic size of the same topic in returned Context",
    ),
    topic_limits_json: str = Query(
        None,
        description='Set specific subtopic limits for topics in JSON, for example {"topic1": 3, "topic2": 5}. The limits in this param will override `max_subtopic_size`.',
    ),
    profile_event_ratio: float = Query(
        0.6,
        description="Profile event ratio of returned Context",
    ),
    require_event_summary: bool = Query(
        False,
        description="Whether to require event summary in returned Context",
    ),
    chats_str: str = Query(
        None,
        description="""Pass the recent chats to enable context search. 
Memobase will use those chats to search for relevant events.
It's a list of chats in OpenAI Message format, for example: [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi"}].

**NOTICE**
- It will increase your latency by 0.1-1 seconds, because Memobase will use Embedding to search for relevant profiles and events.
- It will cost your Memobase tokens, roughly 100~200 tokens per chat based on the profile size.
- The profiles in the context will not be searched by the `chats_str`.
- If you want also search profiles, see `full_profile_and_only_search_event` query parameter.
""",
    ),
    event_similarity_threshold: float = Query(
        0.2,
        description="Event similarity threshold of returned Context",
    ),
    time_range_in_days: int = Query(
        180,
        description="Only allow events within the past few days, default is 180",
    ),
    customize_context_prompt: str = Query(
        None,
        description="""Customize context prompt template.
- use `{profile_section}` to refer to the profile section
- use `{event_section}` to refer to the event section

For example:
```
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Background:
{profile_section}

## Latest Events:
{event_section}
```
""",
    ),
    full_profile_and_only_search_event: bool = Query(
        True,
        description="""If you pass `chats_str` and set this to `False`, Memobase will search for relevant profiles and events at the same time.
**NOTICE**
- It will increase your latency by 2-5(based on the profile size) seconds, because Memobase will use LLM and Embedding to search for relevant profiles and events.
- It will cost your Memobase tokens, roughly 100~1000 tokens per chat based on the profile size.
""",
    ),
    fill_window_with_events: bool = Query(
        False,
        description="If set to `True`, Memobase will fill the token window with the rest events.",
    ),
) -> res.UserContextDataResponse:
    project_id = request.state.memobase_project_id
    topic_limits_json = topic_limits_json or "{}"
    chats_str = chats_str or "[]"
    try:
        topic_limits = res.StrIntData(data=json.loads(topic_limits_json)).data
        chats = res.MessageData(data=json.loads(chats_str)).data
    except Exception as e:
        return Promise.reject(CODE.BAD_REQUEST, f"Invalid JSON: {e}").to_response(
            res.UserContextDataResponse
        )
    p = await controllers.context.get_user_context(
        user_id,
        project_id,
        max_token_size,
        prefer_topics,
        only_topics,
        max_subtopic_size,
        topic_limits,
        profile_event_ratio,
        require_event_summary,
        chats,
        event_similarity_threshold,
        time_range_in_days,
        customize_context_prompt=customize_context_prompt,
        full_profile_and_only_search_event=full_profile_and_only_search_event,
        fill_window_with_events=fill_window_with_events,
    )
    return p.to_response(res.UserContextDataResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/event.py
================================================
from ..controllers import full as controllers
from ..models import response as res
from fastapi import Request
from fastapi import Path, Query, Body


async def get_user_events(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    topk: int = Query(10, description="Number of events to retrieve, default is 10"),
    max_token_size: int = Query(
        None,
        description="Max token size of returned events",
    ),
    need_summary: bool = Query(
        False,
        description="Whether to return events with summaries",
    ),
) -> res.UserEventsDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.event.get_user_events(
        user_id, project_id, topk=topk, need_summary=need_summary
    )
    if not p.ok():
        return p.to_response(res.UserEventsDataResponse)
    p = await controllers.event.truncate_events(p.data(), max_token_size)
    return p.to_response(res.UserEventsDataResponse)


async def delete_user_event(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    event_id: str = Path(..., description="The ID of the event"),
) -> res.BaseResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.event.delete_user_event(user_id, project_id, event_id)
    return p.to_response(res.BaseResponse)


async def update_user_event(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    event_id: str = Path(..., description="The ID of the event"),
    event_data: res.EventData = Body(..., description="Event data to update"),
) -> res.BaseResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.event.update_user_event(
        user_id, project_id, event_id, event_data.model_dump()
    )
    return p.to_response(res.BaseResponse)


async def search_user_events(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    query: str = Query(..., description="The query to search for"),
    topk: int = Query(10, description="Number of events to retrieve, default is 10"),
    similarity_threshold: float = Query(
        0.2, description="Similarity threshold, default is 0.2"
    ),
    time_range_in_days: int = Query(
        180, description="Only allow events within the past few days, default is 180"
    ),
) -> res.UserEventsDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.event.search_user_events(
        user_id, project_id, query, topk, similarity_threshold, time_range_in_days
    )
    return p.to_response(res.UserEventsDataResponse)


async def search_user_event_gists(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    query: str = Query(..., description="The query to search for"),
    topk: int = Query(10, description="Number of events to retrieve, default is 10"),
    similarity_threshold: float = Query(
        0.2, description="Similarity threshold, default is 0.2"
    ),
    time_range_in_days: int = Query(
        180, description="Only allow events within the past few days, default is 180"
    ),
) -> res.UserEventGistsDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.event_gist.search_user_event_gists(
        user_id, project_id, query, topk, similarity_threshold, time_range_in_days
    )
    return p.to_response(res.UserEventGistsDataResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/middleware.py
================================================
import os
import time
import uuid
import structlog
import traceback
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi import Request
from fastapi.responses import JSONResponse
from uvicorn.protocols.utils import get_path_with_query_string
from ..env import ProjectStatus, LOG
from ..models.database import DEFAULT_PROJECT_ID
from ..models.response import BaseResponse
from ..models.utils import Promise
from ..telemetry import (
    telemetry_manager,
    CounterMetricName,
    HistogramMetricName,
)
from .. import __version__
from ..models.response import BaseResponse, CODE
from ..auth.token import (
    parse_project_id,
    check_project_secret,
    get_project_status,
)


PATH_MAPPINGS = [
    "/api/v1/admin/status_check",
    "/api/v1/users/blobs",
    "/api/v1/users/blobs",
    "/api/v1/users/profile",
    "/api/v1/users/buffer",
    "/api/v1/users/event",
    "/api/v1/users/context",
    "/api/v1/users",
    "/api/v1/blobs/insert",
    "/api/v1/blobs",
]


async def global_wrapper_middleware(request: Request, call_next):
    req_id = request.headers.get("X-Request-ID")
    if req_id is None:
        req_id = str(uuid.uuid4())
    project_id = getattr(request.state, "memobase_project_id", None)
    structlog.contextvars.clear_contextvars()
    structlog.contextvars.bind_contextvars(
        request_id=req_id, project_id=project_id, memobase_version=__version__
    )

    url = get_path_with_query_string(request.scope)
    client_host = request.client.host
    client_port = request.client.port
    http_method = request.method
    http_version = request.scope["http_version"]

    start_time = time.perf_counter_ns()

    try:
        response = await call_next(request)
        status_code = response.status_code
        errmsg = None
        traceback_str = None
    except Exception as e:
        status_code = 500
        errmsg = f"Sorry, we have encountered an unknown error: \n{e}\nPlease report this issue to https://github.com/memodb-io/memobase/issues"
        traceback_str = traceback.format_exc().replace("\n", "<br>")
        response = JSONResponse(
            content={
                "data": None,
                "errno": 500,
                "errmsg": errmsg,
            },
        )
    process_time = time.perf_counter_ns() - start_time

    if status_code != 200:
        _log_f = LOG.error
    else:
        _log_f = LOG.info
    _log_f(
        f"""{client_host}:{client_port} - "{http_method} {url} HTTP/{http_version}" {status_code}""",
        extra={
            "http": {
                "url": str(request.url),
                "status_code": status_code,
                "method": http_method,
                "version": http_version,
            },
            "network": {"client": {"ip": client_host, "port": client_port}},
            "duration": process_time / 10**9,  # convert to s
            "type": "access",
            "errmsg": errmsg,
            "__internal_traceback": traceback_str,
        },
    )
    response.headers["X-Process-Time"] = str(process_time / 10**9)

    return response


class AuthMiddleware(BaseHTTPMiddleware):
    def normalize_path(self, path: str) -> str:
        """Remove dynamic path parameters to get normalized path for metrics"""
        if not path.startswith("/api"):
            return path

        for prefix in PATH_MAPPINGS:
            if path.startswith(prefix):
                return prefix

        return path

    async def dispatch(self, request, call_next):
        if not request.url.path.startswith("/api"):
            return await call_next(request)

        if request.url.path.startswith("/api/v1/healthcheck"):
            telemetry_manager.increment_counter_metric(
                CounterMetricName.HEALTHCHECK,
                1,
            )
            return await call_next(request)

        auth_token = request.headers.get("Authorization")
        if not auth_token or not auth_token.startswith("Bearer "):
            return JSONResponse(
                status_code=CODE.UNAUTHORIZED.value,
                content=BaseResponse(
                    errno=CODE.UNAUTHORIZED.value,
                    errmsg=f"Unauthorized access to {request.url.path}. You have to provide a valid Bearer token.",
                ).model_dump(),
            )
        auth_token = (auth_token.split(" ")[1]).strip()
        is_root = self.is_valid_root(auth_token)
        request.state.is_memobase_root = is_root
        request.state.memobase_project_id = DEFAULT_PROJECT_ID
        if not is_root:
            p = await self.parse_project_token(auth_token)
            if not p.ok():
                return JSONResponse(
                    status_code=CODE.UNAUTHORIZED.value,
                    content=BaseResponse(
                        errno=CODE.UNAUTHORIZED.value,
                        errmsg=f"Unauthorized access to {request.url.path}. {p.msg()}",
                    ).model_dump(),
                )
            request.state.memobase_project_id = p.data()
        # await capture_int_key(TelemetryKeyName.has_request)

        normalized_path = self.normalize_path(request.url.path)

        telemetry_manager.increment_counter_metric(
            CounterMetricName.REQUEST,
            1,
            {
                "project_id": request.state.memobase_project_id,
                "path": normalized_path,
                "method": request.method,
            },
        )

        start_time = time.time()
        response = await call_next(request)

        telemetry_manager.record_histogram_metric(
            HistogramMetricName.REQUEST_LATENCY_MS,
            (time.time() - start_time) * 1000,
            {
                "project_id": request.state.memobase_project_id,
                "path": normalized_path,
                "method": request.method,
            },
        )
        return response

    def is_valid_root(self, token: str) -> bool:
        access_token = os.getenv("ACCESS_TOKEN")
        if access_token is None:
            return True
        return token == access_token.strip()

    async def parse_project_token(self, token: str) -> Promise[str]:
        p = parse_project_id(token)
        if not p.ok():
            return Promise.reject(CODE.UNAUTHORIZED, "Invalid project id format")
        project_id = p.data()
        p = await check_project_secret(project_id, token)
        if not p.ok():
            return p
        if not p.data():
            return Promise.reject(CODE.UNAUTHORIZED, "Wrong secret key")
        p = await get_project_status(project_id)
        if not p.ok():
            return p
        if p.data() == ProjectStatus.suspended:
            return Promise.reject(CODE.FORBIDDEN, "Your project is suspended!")
        return Promise.resolve(project_id)



================================================
FILE: src/server/api/memobase_server/api_layer/profile.py
================================================
import json
from fastapi import Request
from fastapi import Path, Query, Body
from datetime import datetime
from ..controllers import full as controllers
from ..controllers.post_process.profile import filter_profiles_with_chats

from ..models.response import CODE
from ..models.utils import Promise
from ..models.blob import BlobType
from ..models import response as res


async def get_user_profile(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to get profiles for"),
    topk: int = Query(
        None, description="Number of profiles to retrieve, default is all"
    ),
    max_token_size: int = Query(
        None,
        description="Max token size of returned profile content, default is all",
    ),
    prefer_topics: list[str] = Query(
        None,
        description="Rank prefer topics at first to try to keep them in filtering, default order is by updated time",
    ),
    only_topics: list[str] = Query(
        None,
        description="Only return profiles with these topics, default is all",
    ),
    max_subtopic_size: int = Query(
        None,
        description="Max subtopic size of the same topic in returned profile, default is all",
    ),
    topic_limits_json: str = Query(
        None,
        description='Set specific subtopic limits for topics in JSON, for example {"topic1": 3, "topic2": 5}. The limits in this param will override `max_subtopic_size`.',
    ),
    chats_str: str = Query(
        None,
        description='List of chats in OpenAI Message format, for example: [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi"}]',
    ),
) -> res.UserProfileResponse:
    """Get the real-time user profiles for long term memory"""
    project_id = request.state.memobase_project_id
    topic_limits_json = topic_limits_json or "{}"
    chats_str = chats_str or "[]"
    try:
        topic_limits = res.StrIntData(data=json.loads(topic_limits_json)).data
        chats = res.MessageData(data=json.loads(chats_str)).data
    except Exception as e:
        return Promise.reject(
            CODE.BAD_REQUEST, f"Invalid JSON requests: {e}"
        ).to_response(res.UserProfileResponse)
    p = await controllers.profile.get_user_profiles(user_id, project_id)
    if not p.ok():
        return p.to_response(res.UserProfileResponse)
    total_profiles = p.data()
    if chats:
        p = await filter_profiles_with_chats(
            user_id,
            project_id,
            total_profiles,
            chats,
            only_topics=only_topics,
            # max_filter_num=topk,
        )
        if p.ok():
            total_profiles.profiles = p.data()["profiles"]
    p = await controllers.profile.truncate_profiles(
        total_profiles,
        prefer_topics=prefer_topics,
        topk=topk,
        max_token_size=max_token_size,
        only_topics=only_topics,
        max_subtopic_size=max_subtopic_size,
        topic_limits=topic_limits,
    )
    return p.to_response(res.UserProfileResponse)


async def delete_user_profile(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    profile_id: str = Path(..., description="The ID of the profile to delete"),
) -> res.BaseResponse:
    """Delete a profile"""
    project_id = request.state.memobase_project_id
    p = await controllers.profile.delete_user_profile(user_id, project_id, profile_id)
    return p.to_response(res.IdResponse)


async def update_user_profile(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    profile_id: str = Path(..., description="The ID of the profile to update"),
    content: res.ProfileDelta = Body(
        ..., description="The content of the profile to update"
    ),
) -> res.BaseResponse:
    """Update the real-time user profiles for long term memory"""
    project_id = request.state.memobase_project_id
    p = await controllers.profile.update_user_profiles(
        user_id, project_id, [profile_id], [content.content], [content.attributes]
    )
    if p.ok():
        return Promise.resolve(None).to_response(res.BaseResponse)
    return Promise.reject(p.code(), p.msg()).to_response(res.BaseResponse)


async def add_user_profile(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    content: res.ProfileDelta = Body(
        ..., description="The content of the profile to add"
    ),
) -> res.IdResponse:
    """Add the real-time user profiles for long term memory"""
    project_id = request.state.memobase_project_id
    p = await controllers.profile.add_user_profiles(
        user_id, project_id, [content.content], [content.attributes]
    )
    if p.ok():
        return Promise.resolve(res.IdData(id=p.data().ids[0])).to_response(
            res.IdResponse
        )
    return Promise.reject(p.code(), p.msg()).to_response(res.IdResponse)


async def import_user_context(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    content: res.UserContextImport = Body(
        ..., description="The content of the user context to import"
    ),
) -> res.BaseResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.billing.get_project_billing(project_id)
    if not p.ok():
        return p.to_response(res.IdResponse)
    billing = p.data()

    if billing.token_left is not None and billing.token_left < 0:
        return Promise.reject(
            CODE.SERVICE_UNAVAILABLE,
            f"Your project reaches Memobase token limit, "
            f"Left: {billing.token_left}, this project used: {billing.project_token_cost_month}. "
            f"Your quota will be refilled on {billing.next_refill_at}. "
            "\nhttps://www.memobase.io/pricing for more information.",
        ).to_response(res.BaseResponse)

    prompt = f"""Below is my information, please remember them:
{content.context}
"""
    blob_data = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                }
            ]
        },
    )
    p = await controllers.buffer.flush_buffer(user_id, project_id, BlobType.chat)
    if not p.ok():
        return p.to_response(res.BaseResponse)

    p = await controllers.blob.insert_blob(user_id, project_id, blob_data)
    if not p.ok():
        return p.to_response(res.BaseResponse)

    # TODO if single user insert too fast will cause random order insert to buffer
    # So no background task for insert buffer yet
    pb = await controllers.buffer.insert_blob_to_buffer(
        user_id, project_id, p.data().id, blob_data.to_blob()
    )
    if not pb.ok():
        return pb.to_response(res.BaseResponse)

    p = await controllers.buffer.flush_buffer(user_id, project_id, BlobType.chat)
    if not p.ok():
        return p.to_response(res.BaseResponse)
    return Promise.resolve(None).to_response(res.BaseResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/project.py
================================================
from ..controllers import full as controllers
from .. import utils

from ..models.response import BaseResponse, CODE
from ..models.utils import Promise
from ..models import response as res
from fastapi import Request
from typing import Literal
from fastapi import Body, Path, Query


async def update_project_profile_config(
    request: Request,
    profile_config: res.ProfileConfigData = Body(
        ..., description="The profile config to update"
    ),
) -> res.BaseResponse:
    project_id = request.state.memobase_project_id
    p = utils.is_valid_profile_config(profile_config.profile_config)
    if not p.ok():
        return p.to_response(res.BaseResponse)
    p = await controllers.project.update_project_profile_config(
        project_id, profile_config.profile_config
    )
    return p.to_response(res.BaseResponse)


async def get_project_profile_config_string(
    request: Request,
) -> res.ProfileConfigDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.project.get_project_profile_config_string(project_id)
    return p.to_response(res.ProfileConfigDataResponse)


async def get_project_billing(request: Request) -> res.BillingResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.billing.get_project_billing(project_id)
    return p.to_response(res.BillingResponse)


async def get_project_users(
    request: Request,
    search: str = Query("", description="Search string in username field"),
    order_by: Literal["updated_at", "profile_count", "event_count"] = Query(
        "updated_at", description="Order by field"
    ),
    order_desc: bool = Query(True, description="Order descending or ascending"),
    limit: int = Query(10, description="Limit the number of results returned"),
    offset: int = Query(0, description="Offset the starting point for pagination"),
) -> res.ProjectUsersDataResponse:
    """
    Get the users of a project in different orders
    """
    project_id = request.state.memobase_project_id
    users = await controllers.project.get_project_users(
        project_id, search, limit, offset, order_by, order_desc
    )
    return users.to_response(res.ProjectUsersDataResponse)


async def get_project_usage(
    request: Request,
    last_days: int = Query(7, description="The number of days to get"),
) -> res.UsageResponse:
    """
    Get the usage of a project in the last days
    """
    project_id = request.state.memobase_project_id
    p = await controllers.project.get_project_usage(project_id, last_days)
    return p.to_response(res.UsageResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/roleplay.py
================================================
import json
from ..controllers import full as controllers
from ..controllers.modal.roleplay import proactive_topics
from ..models.blob import BlobType
from ..models.utils import Promise, CODE
from ..models import response as res
from fastapi import Request
from fastapi import Body, Path, Query


async def infer_proactive_topics(
    request: Request,
    user_id: str = Path(..., description="The ID of the user"),
    topk: int = Query(
        None, description="Number of profiles to retrieve, default is all"
    ),
    max_token_size: int = Query(
        None,
        description="Max token size of returned profile content, default is all",
    ),
    prefer_topics: list[str] = Query(
        None,
        description="Rank prefer topics at first to try to keep them in filtering, default order is by updated time",
    ),
    only_topics: list[str] = Query(
        None,
        description="Only return profiles with these topics, default is all",
    ),
    max_subtopic_size: int = Query(
        None,
        description="Max subtopic size of the same topic in returned profile, default is all",
    ),
    topic_limits_json: str = Query(
        None,
        description='Set specific subtopic limits for topics in JSON, for example {"topic1": 3, "topic2": 5}. The limits in this param will override `max_subtopic_size`.',
    ),
    body: res.ProactiveTopicRequest = Body(..., description="The body of the request"),
) -> res.ProactiveTopicResponse:
    """Provide interest detection and personalized topics"""
    project_id = request.state.memobase_project_id
    topic_limits_json = topic_limits_json or "{}"
    try:
        topic_limits = res.StrIntData(data=json.loads(topic_limits_json)).data
    except Exception as e:
        return Promise.reject(
            CODE.BAD_REQUEST, f"Invalid JSON requests: {e}"
        ).to_response(res.UserProfileResponse)
    p = await proactive_topics.process_messages(
        user_id,
        project_id,
        body.messages,
        body.agent_context,
        prefer_topics,
        topk,
        max_token_size,
        only_topics,
        max_subtopic_size,
        topic_limits,
    )
    return p.to_response(res.ProactiveTopicResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/user.py
================================================
from ..controllers import full as controllers

from ..models.response import BaseResponse
from ..models.blob import BlobType
from ..models import response as res
from fastapi import Request
from fastapi import Path, Query, Body


async def create_user(
    request: Request,
    user_data: res.UserData = Body(
        ..., description="User data for creating a new user"
    ),
) -> res.IdResponse:
    """Create a new user with additional data"""
    project_id = request.state.memobase_project_id
    p = await controllers.user.create_user(user_data, project_id)
    return p.to_response(res.IdResponse)


async def get_user(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to retrieve"),
) -> res.UserDataResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.user.get_user(user_id, project_id)
    return p.to_response(res.UserDataResponse)


async def update_user(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to update"),
    user_data: dict = Body(..., description="Updated user data"),
) -> res.IdResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.user.update_user(user_id, project_id, user_data)
    return p.to_response(res.IdResponse)


async def delete_user(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to delete"),
) -> BaseResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.user.delete_user(user_id, project_id)
    return p.to_response(BaseResponse)


async def get_user_all_blobs(
    request: Request,
    user_id: str = Path(..., description="The ID of the user to fetch blobs for"),
    blob_type: BlobType = Path(..., description="The type of blobs to retrieve"),
    page: int = Query(0, description="Page number for pagination, starting from 0"),
    page_size: int = Query(10, description="Number of items per page, default is 10"),
) -> res.IdsResponse:
    project_id = request.state.memobase_project_id
    p = await controllers.user.get_user_all_blobs(
        user_id, project_id, blob_type, page, page_size
    )
    return p.to_response(res.IdsResponse)



================================================
FILE: src/server/api/memobase_server/api_layer/docs/__init__.py
================================================
from .basic_docs import API_X_CODE_DOCS

# Import all functional documentation modules to populate API_X_CODE_DOCS
from . import project
from . import user
from . import blob
from . import profile
from . import event

__all__ = ["API_X_CODE_DOCS"]



================================================
FILE: src/server/api/memobase_server/api_layer/docs/basic_docs.py
================================================
from typing import Literal

API_X_CODE_DOCS = {}


def add_api_code_docs(
    method: Literal["GET", "POST", "PUT", "DELETE"],
    path: str,
    *code_samples: list[dict],
):
    if method not in ["GET", "POST", "PUT", "DELETE"]:
        raise ValueError("Invalid method")
    if not path.startswith("/"):
        raise ValueError("Invalid path")
    if not code_samples:
        raise ValueError("No code samples provided")
    key = f"{method} {path}"
    if key in API_X_CODE_DOCS:
        raise ValueError("Code docs already exist")
    else:
        API_X_CODE_DOCS[key] = {"x-code-samples": code_samples}


def py_code(content: str) -> dict:
    return {
        "lang": "python",
        "source": f"""# To use the Python SDK, install the package:
# pip install memobase
{content}
""",
        "label": "Python",
    }


def js_code(content: str) -> dict:
    return {
        "lang": "javascript",
        "source": f"""// To use the JavaScript SDK, install the package:
// npm install @memobase/memobase
{content}
""",
        "label": "JavaScript",
    }


def go_code(content: str) -> dict:
    return {
        "lang": "go",
        "source": f"""// To use the Go SDK, install the package:
// go get github.com/memodb-io/memobase/src/client/memobase-go@latest
{content}
""",
        "label": "Go",
    }



================================================
FILE: src/server/api/memobase_server/api_layer/docs/blob.py
================================================
from .basic_docs import add_api_code_docs, py_code, js_code, go_code

# Get user blobs by type
add_api_code_docs(
    "GET",
    "/users/blobs/{user_id}/{blob_type}",
    py_code(
        """
from memobase import MemoBaseClient
from memobase.core.blob import BlobType

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

user = client.get_user('user_id')
blobs = user.get_all(BlobType.chat)
"""
    ),
    js_code(
        """
import { MemoBaseClient, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

const user = client.getUser('user_id');
const blobs = await user.getAll(BlobType.Enum.chat);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Get all chat blobs
    blobIDs, err := user.GetAll(blob.ChatType, 0, 10)
    if err != nil {
        log.Fatalf("Failed to get blobs: %v", err)
    }

    fmt.Printf("Found %d chat blobs\n", len(blobIDs))
}
"""
    ),
)

# Insert blob
add_api_code_docs(
    "POST",
    "/blobs/insert/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient
from memobase.core.blob import ChatBlob

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

b = ChatBlob(messages=[
    {
        "role": "user",
        "content": "Hi, I'm here again"
    },
    {
        "role": "assistant",
        "content": "Hi, Gus! How can I help you?"
    }
])
u = client.get_user(uid)
bid = u.insert(b)
"""
    ),
    js_code(
        """
import { MemoBaseClient, Blob, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

const blobId = await user.insert(Blob.parse({
  type: BlobType.Enum.chat,
  messages: [
    {
      role: 'user',
      content: 'Hi, I\'m here again'
    },
    {
      role: 'assistant',
      content: 'Hi, Gus! How can I help you?'
    }
  ]
}));
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Create a chat blob
    chatBlob := &blob.ChatBlob{
        BaseBlob: blob.BaseBlob{
            Type: blob.ChatType,
        },
        Messages: []blob.OpenAICompatibleMessage{
            {
                Role:    "user",
                Content: "Hello, I am Jinjia!",
            },
            {
                Role:    "assistant",
                Content: "Hi there! How can I help you today?",
            },
        },
    }

    // Insert the blob
    blobID, err := user.Insert(chatBlob, false)
    if err != nil {
        log.Fatalf("Failed to insert blob: %v", err)
    }
    fmt.Printf("Successfully inserted blob with ID: %s\n", blobID)
}
"""
    ),
)

# Get blob
add_api_code_docs(
    "GET",
    "/blobs/{user_id}/{blob_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

u = client.get_user(uid)
b = u.get(bid)
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

const blob = await user.get(blobId);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Get a blob
    blobID := "EXISTING_BLOB_ID" // Replace with an actual blob ID
    retrievedBlob, err := user.Get(blobID)
    if err != nil {
        log.Fatalf("Failed to get blob: %v", err)
    }

    // Type assert to use as ChatBlob
    if chatBlob, ok := retrievedBlob.(*blob.ChatBlob); ok {
        fmt.Printf("Retrieved message: %s\n", chatBlob.Messages[0].Content)
    }
}
"""
    ),
)

# Delete blob
add_api_code_docs(
    "DELETE",
    "/blobs/{user_id}/{blob_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

u = client.get_user(uid)
u.delete(bid)
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

await user.delete(blobId);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Delete a blob
    blobID := "EXISTING_BLOB_ID" // Replace with an actual blob ID
    err = user.Delete(blobID)
    if err != nil {
        log.Fatalf("Failed to delete blob: %v", err)
    }
    fmt.Printf("Successfully deleted blob with ID: %s\n", blobID)
}
"""
    ),
)

# Buffer operations
add_api_code_docs(
    "POST",
    "/users/buffer/{user_id}/{buffer_type}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

u = client.get_user(uid)
u.flush()
u.flush(sync=True) # wait for the buffer to be processed
"""
    ),
    js_code(
        """
import { MemoBaseClient, BlobType } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

await user.flush(BlobType.Enum.chat);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Flush the buffer
    err = user.Flush(blob.ChatType, false)
    if err != nil {
        log.Fatalf("Failed to flush buffer: %v", err)
    }
    fmt.Println("Successfully flushed buffer")
}
"""
    ),
)

# Get buffer capacity
add_api_code_docs(
    "GET",
    "/users/buffer/capacity/{user_id}/{buffer_type}",
    py_code(
        """
from memobase import MemoBaseClient
from memobase.core.blob import BlobType

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

user = client.get_user('user_id')
blobs = user.buffer(BlobType.chat)
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/blob"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Get buffer capacity
    blobIDs, err := user.Buffer(blob.ChatType, "processing")
    if err != nil {
        log.Fatalf("Failed to get buffer capacity: %v", err)
    }
    fmt.Printf("Found %d blobs in buffer\n", len(blobIDs))
}
"""
    ),
)



================================================
FILE: src/server/api/memobase_server/api_layer/docs/event.py
================================================
from .basic_docs import add_api_code_docs, py_code, js_code, go_code

# Get user events
add_api_code_docs(
    "GET",
    "/users/event/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
u = client.get_user(uid)

events = u.event(topk=10, max_token_size=1000, need_summary=True)
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

const events = await user.event();
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Get user events
    events, err := user.Event(10, nil, false)
    if err != nil {
        log.Fatalf("Failed to get events: %v", err)
    }

    fmt.Printf("Found %d events\n", len(events))
}
"""
    ),
)

# Update user event
add_api_code_docs(
    "PUT",
    "/users/event/{user_id}/{event_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
uid = client.add_user()
u = client.get_user(uid)
# ... insert messages to user

events = u.event(topk=5)
eid = events[0].id

u.update_event(eid, {"event_tip": "The event is about..."})
print(u.event(topk=1))
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Update an event
    eventID := "EXISTING_EVENT_ID" // Replace with an actual event ID
    eventData := map[string]interface{}{"event_tip": "The event is about..."}
    err = user.UpdateEvent(eventID, eventData)
    if err != nil {
        log.Fatalf("Failed to update event: %v", err)
    }
    fmt.Printf("Successfully updated event with ID: %s\n", eventID)
}
"""
    ),
)

# Delete user event
add_api_code_docs(
    "DELETE",
    "/users/event/{user_id}/{event_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
uid = client.add_user()
u = client.get_user(uid)
# ... insert messages to user

events = u.event(topk=1)
print(events)

eid = events[0].id
u.delete_event(eid)

print(u.event(topk=1))
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Delete an event
    eventID := "EXISTING_EVENT_ID" // Replace with an actual event ID
    err = user.DeleteEvent(eventID)
    if err != nil {
        log.Fatalf("Failed to delete event: %v", err)
    }
    fmt.Printf("Successfully deleted event with ID: %s\n", eventID)
}
"""
    ),
)

# Search user events
add_api_code_docs(
    "GET",
    "/users/event/search/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient
from memobase.core.blob import ChatBlob

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
uid = client.add_user()
u = client.get_user(uid)

b = ChatBlob(messages=[
    {
        "role": "user",
        "content": "Hi, I'm here again"
    },
    {
        "role": "assistant",
        "content": "Hi, Gus! How can I help you?"
    }
])
u.insert(b)
u.flush(sync=True)

events = u.search_event('query')
print(events)
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Search for events
    events, err := user.SearchEvent("query", 10, 0.7, 7)
    if err != nil {
        log.Fatalf("Failed to search events: %v", err)
    }

    fmt.Printf("Found %d events\n", len(events))
}
"""
    ),
)


add_api_code_docs(
    "GET",
    "/users/event_gist/search/{user_id}",
    py_code(
        """from memobase import MemoBaseClient
from memobase.core.blob import ChatBlob

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')
uid = client.add_user()
u = client.get_user(uid)

b = ChatBlob(messages=[
    {
        "role": "user",
        "content": "Hi, I'm here again"
    },
    {
        "role": "assistant",
        "content": "Hi, Gus! How can I help you?"
    }
])
u.insert(b)
u.flush(sync=True)

events = u.search_event_gist('query')
print(events)"""
    ),
)



================================================
FILE: src/server/api/memobase_server/api_layer/docs/profile.py
================================================
from .basic_docs import add_api_code_docs, py_code, js_code, go_code

# Get user profile
add_api_code_docs(
    "GET",
    "/users/profile/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

u = client.get_user(uid)
p = u.profile()
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

const profiles = await user.profile();
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Get user profile
    profiles, err := user.Profile(nil)
    if err != nil {
        log.Fatalf("Failed to get user profile: %v", err)
    }

    // Print profiles
    fmt.Println("\nUser Profiles:")
    for _, profile := range profiles {
        fmt.Printf("ID: %s\nTopic: %s\nSub-topic: %s\nContent: %s\n\n",
            profile.ID,
            profile.Attributes.Topic,
            profile.Attributes.SubTopic,
            profile.Content,
        )
    }
}
"""
    ),
)

# Create user profile
add_api_code_docs(
    "POST",
    "/users/profile/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

user = client.get_user('user_id')
user.add_profile(content="I am a software engineer", topic="career", sub_topic="job")
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Add a profile
    profileID, err := user.AddProfile("value", "topic", "sub_topic")
    if err != nil {
        log.Fatalf("Failed to add profile: %v", err)
    }
    fmt.Printf("Successfully added profile with ID: %s\n", profileID)
}
"""
    ),
)

# Update user profile
add_api_code_docs(
    "PUT",
    "/users/profile/{user_id}/{profile_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

user = client.get_user('user_id')
user.update_profile(profile_id="profile_id", content="I am a software engineer", topic="career", sub_topic="job")
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Update a profile
    profileID := "EXISTING_PROFILE_ID" // Replace with an actual profile ID
    err = user.UpdateProfile(profileID, "value2", "topic2", "sub_topic2")
    if err != nil {
        log.Fatalf("Failed to update profile: %v", err)
    }
    fmt.Printf("Successfully updated profile with ID: %s\n", profileID)
}
"""
    ),
)

# Delete user profile
add_api_code_docs(
    "DELETE",
    "/users/profile/{user_id}/{profile_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

user = client.get_user('user_id')
user.delete_profile('profile_id')
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

await client.deleteProfile('user_id', 'profile_id');
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    // Delete a profile
    profileID := "EXISTING_PROFILE_ID" // Replace with an actual profile ID
    err = user.DeleteProfile(profileID)
    if err != nil {
        log.Fatalf("Failed to delete profile: %v", err)
    }
    fmt.Printf("Successfully deleted profile with ID: %s\n", profileID)
}
"""
    ),
)



================================================
FILE: src/server/api/memobase_server/api_layer/docs/project.py
================================================
from .basic_docs import add_api_code_docs, py_code, js_code, go_code

# Healthcheck endpoint
add_api_code_docs(
    "GET",
    "/healthcheck",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

assert memobase.ping()
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

await client.ping();
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Ping the server
    if !client.Ping() {
        log.Fatal("Failed to connect to server")
    }
    fmt.Println("Successfully connected to server")
}
"""
    ),
)

# Project billing endpoint
add_api_code_docs(
    "GET",
    "/project/billing",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

print(memobase.get_usage())
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get usage
    usage, err := client.GetUsage()
    if err != nil {
        log.Fatalf("Failed to get usage: %v", err)
    }
    fmt.Printf("Usage: %v\n", usage)
}
"""
    ),
)

# Project profile config - POST
add_api_code_docs(
    "POST",
    "/project/profile_config",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

memobase.update_config('your_profile_config')
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

await client.updateConfig('your_profile_config');
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Update config
    err = client.UpdateConfig("your_profile_config")
    if err != nil {
        log.Fatalf("Failed to update config: %v", err)
    }
    fmt.Println("Successfully updated config")
}
"""
    ),
)

# Project profile config - GET
add_api_code_docs(
    "GET",
    "/project/profile_config",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

config = memobase.get_config()
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

const config = await client.getConfig();
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get config
    config, err := client.GetConfig()
    if err != nil {
        log.Fatalf("Failed to get config: %v", err)
    }
    fmt.Printf("Config: %s\n", config)
}
"""
    ),
)

# Project users endpoint
add_api_code_docs(
    "GET",
    "/project/users",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

users = memobase.get_all_users(search="", order_by="updated_at", order_desc=True, limit=10, offset=0)
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get all users
    users, err := client.GetAllUsers("", "updated_at", true, 10, 0)
    if err != nil {
        log.Fatalf("Failed to get all users: %v", err)
    }
    fmt.Printf("Found %d users\n", len(users))
}
"""
    ),
)

# Project usage endpoint
add_api_code_docs(
    "GET",
    "/project/usage",
    py_code(
        """
from memobase import MemoBaseClient

memobase = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

usage = memobase.get_daily_usage(days=7)
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get daily usage
    usage, err := client.GetDailyUsage(7)
    if err != nil {
        log.Fatalf("Failed to get daily usage: %v", err)
    }
    fmt.Printf("Usage: %v\n", usage)
}
"""
    ),
)



================================================
FILE: src/server/api/memobase_server/api_layer/docs/user.py
================================================
from .basic_docs import add_api_code_docs, py_code, js_code, go_code

# Create user
add_api_code_docs(
    "POST",
    "/users",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

uid = client.add_user({"ANY": "DATA"})
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

const userId = await client.addUser({ANY: "DATA"});
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/google/uuid"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Add a user
    userID := uuid.New().String()
    data := map[string]interface{}{"ANY": "DATA"}
    _, err = client.AddUser(data, userID)
    if err != nil {
        log.Fatalf("Failed to add user: %v", err)
    }
    fmt.Printf("User added with ID: %s\n", userID)
}
"""
    ),
)

# Get user
add_api_code_docs(
    "GET",
    "/users/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

u = client.get_user(uid)
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

const user = await client.getUser(userId);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/google/uuid"
    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }
    fmt.Printf("Successfully retrieved user with ID: %s\n", user.UserID)
}
"""
    ),
)

# Update user
add_api_code_docs(
    "PUT",
    "/users/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

client.update_user(uid, {"ANY": "NEW_DATA"})
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

await client.updateUser(userId, {ANY: "NEW_DATA"});
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Update a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    newData := map[string]interface{}{"ANY": "NEW_DATA"}
    _, err = client.UpdateUser(userID, newData)
    if err != nil {
        log.Fatalf("Failed to update user: %v", err)
    }
    fmt.Printf("Successfully updated user with ID: %s\n", userID)
}
"""
    ),
)

# Delete user
add_api_code_docs(
    "DELETE",
    "/users/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

client.delete_user(uid)
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);

await client.deleteUser(userId);
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Delete a user
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    err = client.DeleteUser(userID)
    if err != nil {
        log.Fatalf("Failed to delete user: %v", err)
    }
    fmt.Printf("Successfully deleted user with ID: %s\n", userID)
}
"""
    ),
)

# Get user context
add_api_code_docs(
    "GET",
    "/users/context/{user_id}",
    py_code(
        """
from memobase import MemoBaseClient

client = MemoBaseClient(project_url='PROJECT_URL', api_key='PROJECT_TOKEN')

context = u.context()
"""
    ),
    js_code(
        """
import { MemoBaseClient } from '@memobase/memobase';

const client = new MemoBaseClient(process.env.MEMOBASE_PROJECT_URL, process.env.MEMOBASE_API_KEY);
const user = await client.getUser(userId);

const context = await user.context();
"""
    ),
    go_code(
        """
import (
    "fmt"
    "log"

    "github.com/memodb-io/memobase/src/client/memobase-go/core"
)

func main() {
    projectURL := "YOUR_PROJECT_URL"
    apiKey := "YOUR_API_KEY"
    // Initialize the client
    client, err := core.NewMemoBaseClient(
        projectURL,
        apiKey,
    )
    if err != nil {
        log.Fatalf("Failed to create client: %v", err)
    }

    // Get user context
    userID := "EXISTING_USER_ID" // Replace with an actual user ID
    user, err := client.GetUser(userID, false)
    if err != nil {
        log.Fatalf("Failed to get user: %v", err)
    }

    context, err := user.Context(nil)
    if err != nil {
        log.Fatalf("Failed to get context: %v", err)
    }
    fmt.Printf("User context: %s\n", context)
}
"""
    ),
)



================================================
FILE: src/server/api/memobase_server/auth/__init__.py
================================================
[Empty file]


================================================
FILE: src/server/api/memobase_server/auth/admin_api.py
================================================
from httpx import AsyncClient
import uuid

from ..models.response import BillingData, BaseResponse
from ..models.utils import Promise, CODE
from ..connectors import ADMIN_URL, ADMIN_TOKEN


async def get_project_usage(project_id: str) -> Promise[BillingData]:
    if ADMIN_URL is None:
        return Promise.reject(CODE.SERVICE_UNAVAILABLE, "Memobase Admin URL not set")
    async with AsyncClient(
        base_url=ADMIN_URL, headers={"Authorization": f"Bearer {ADMIN_TOKEN}"}
    ) as client:
        request_id = str(uuid.uuid4())
        response = await client.get(
            f"/api/v1/billing/project/{project_id}",
            timeout=10,
            headers={"X-Request-ID": request_id},
        )

        if response.status_code != 200:
            return Promise.reject(
                CODE.SERVICE_UNAVAILABLE,
                f"Failed to get project usage: {response.text}",
            )
        data = response.json()
        if data["errno"] != 0:
            return Promise.reject(
                CODE.SERVICE_UNAVAILABLE,
                f"Failed to get project usage: {data}",
            )
        return Promise.resolve(BillingData(**data["data"]))


async def cost_project_usage(
    project_id: str, input_tokens: int, output_tokens: int
) -> Promise[None]:
    if ADMIN_URL is None:
        return Promise.reject(CODE.SERVICE_UNAVAILABLE, "Memobase Admin URL not set")
    async with AsyncClient(
        base_url=ADMIN_URL, headers={"Authorization": f"Bearer {ADMIN_TOKEN}"}
    ) as client:
        request_id = str(uuid.uuid4())
        response = await client.put(
            f"/api/v1/billing/project/{project_id}",
            json={"usage": input_tokens + output_tokens},
            headers={"X-Request-ID": request_id},
        )
        if response.status_code != 200:
            return Promise.reject(
                CODE.SERVICE_UNAVAILABLE,
                f"Failed to cost project usage: {response.text}",
            )
        data = response.json()
        if data["errno"] != 0:
            return Promise.reject(
                CODE.SERVICE_UNAVAILABLE,
                f"Failed to cost project usage: {data}",
            )
        return Promise.resolve(None)



================================================
FILE: src/server/api/memobase_server/auth/token.py
================================================
from hashlib import sha256
from datetime import datetime
from random import random
from typing import Tuple
from uuid import uuid4
from ..models.utils import Promise
from ..models.response import CODE
from ..connectors import get_redis_client
from ..controllers import project


def parse_project_id(secret_key: str) -> Promise[str]:
    if not secret_key.startswith("sk-"):
        return Promise.reject(CODE.UNAUTHORIZED, "Invalid secret key")
    parts = secret_key[3:].split("-")
    if len(parts) < 2:
        return Promise.reject(CODE.UNAUTHORIZED, "Invalid secret key")
    project_id = "-".join(parts[:-1]).strip()
    return Promise.resolve(project_id)


def token_redis_key(project_id: str) -> str:
    return f"memobase::auth::token::{project_id}"


def project_status_redis_key(project_id: str) -> str:
    return f"memobase::auth::project_status::{project_id}"


async def check_project_secret(project_id: str, secret_key: str) -> Promise[bool]:
    async with get_redis_client() as client:
        secret = await client.get(token_redis_key(project_id))
        if secret is None:
            p = await project.get_project_secret(project_id)
            if not p.ok():
                return Promise.reject(CODE.UNAUTHORIZED, "Your project is not exists!")
            secret = p.data()
            await client.set(token_redis_key(project_id), secret, ex=None)
    return Promise.resolve(secret == secret_key)


async def get_project_status(project_id: str) -> Promise[str]:
    async with get_redis_client() as client:
        status = await client.get(project_status_redis_key(project_id))
        if status is None:
            p = await project.get_project_status(project_id)
            if not p.ok():
                return p
            status = p.data()
            await client.set(
                project_status_redis_key(project_id), status.strip(), ex=60 * 60
            )
    return Promise.resolve(status)



================================================
FILE: src/server/api/memobase_server/controllers/__init__.py
================================================
[Empty file]


================================================
FILE: src/server/api/memobase_server/controllers/billing.py
================================================
from pydantic import ValidationError
from ..models.utils import Promise
from ..models.database import (
    ProjectBilling,
    Billing,
    next_month_first_day,
)
from ..models.response import CODE, IdData, IdsData, UserProfilesData, BillingData
from ..connectors import Session, ADMIN_URL
from ..telemetry.capture_key import get_int_key, capture_int_key
from ..env import (
    TelemetryKeyName,
    USAGE_TOKEN_LIMIT_MAP,
    BILLING_REFILL_AMOUNT_MAP,
    BillingStatus,
)
from datetime import datetime, date
from ..auth import admin_api


async def get_project_billing(project_id: str) -> Promise[BillingData]:
    if ADMIN_URL is not None:
        return await admin_api.get_project_usage(project_id)

    with Session() as session:
        billing = (
            session.query(ProjectBilling)
            .filter(ProjectBilling.project_id == project_id)
            .first()
        )
        if billing is None:
            return await fallback_billing_data(project_id)
            # return Promise.reject(CODE.NOT_FOUND, "Billing not found").to_response(
            #     BillingData
            # )
        billing = billing.billing

        this_month_token_costs_in = await get_int_key(
            TelemetryKeyName.llm_input_tokens, project_id, in_month=True
        )
        this_month_token_costs_out = await get_int_key(
            TelemetryKeyName.llm_output_tokens, project_id, in_month=True
        )
        usage_left_this_billing = billing.usage_left

        next_refill_date = billing.next_refill_at
        today = datetime.now(next_refill_date.tzinfo)
        if (
            today > next_refill_date
            and usage_left_this_billing is not None
            and BILLING_REFILL_AMOUNT_MAP[BillingStatus.free] is not None
            and usage_left_this_billing < BILLING_REFILL_AMOUNT_MAP[BillingStatus.free]
        ):
            usage_left_this_billing = BILLING_REFILL_AMOUNT_MAP[BillingStatus.free]

            billing.next_refill_at = next_month_first_day()
            billing.usage_left = usage_left_this_billing
            session.commit()
    billing_data = BillingData(
        token_left=usage_left_this_billing,
        next_refill_at=next_refill_date,
        project_token_cost_month=this_month_token_costs_in + this_month_token_costs_out,
    )
    return Promise.resolve(billing_data)


async def fallback_billing_data(project_id: str) -> Promise[BillingData]:
    from .project import get_project_status

    this_month_token_costs_in = await get_int_key(
        TelemetryKeyName.llm_input_tokens, project_id, in_month=True
    )
    this_month_token_costs_out = await get_int_key(
        TelemetryKeyName.llm_output_tokens, project_id, in_month=True
    )

    this_month_token_costs = this_month_token_costs_in + this_month_token_costs_out
    p = await get_project_status(project_id)
    if not p.ok():
        return p
    status = p.data()
    if status not in USAGE_TOKEN_LIMIT_MAP:
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR, f"Invalid project status: {status}"
        )
    usage_token_limit = USAGE_TOKEN_LIMIT_MAP[status]
    if usage_token_limit < 0:
        this_month_left_tokens = None
    else:
        this_month_left_tokens = usage_token_limit - this_month_token_costs

    # Calculate first day of next month
    today = date.today()
    if today.month == 12:
        next_month = date(today.year + 1, 1, 1)
    else:
        next_month = date(today.year, today.month + 1, 1)

    return Promise.resolve(
        BillingData(
            token_left=this_month_left_tokens,
            next_refill_at=next_month,
            project_token_cost_month=this_month_token_costs,
        )
    )


async def project_cost_token_billing(
    project_id: str, input_tokens: int, output_tokens: int
) -> Promise[None]:
    await capture_int_key(
        TelemetryKeyName.llm_input_tokens, input_tokens, project_id=project_id
    )
    await capture_int_key(
        TelemetryKeyName.llm_output_tokens, output_tokens, project_id=project_id
    )
    if ADMIN_URL is not None:
        return await admin_api.cost_project_usage(
            project_id, input_tokens, output_tokens
        )
    with Session() as session:
        _billing = (
            session.query(ProjectBilling)
            .filter(ProjectBilling.project_id == project_id)
            .one_or_none()
        )
        if _billing is None:
            return Promise.reject(CODE.NOT_FOUND, "Billing not found")
        billing = _billing.billing

        if billing.usage_left is not None:
            billing.usage_left -= input_tokens + output_tokens
            session.commit()
    return Promise.resolve(None)



================================================
FILE: src/server/api/memobase_server/controllers/blob.py
================================================
import pydantic
from ..models.utils import Promise
from ..models.database import GeneralBlob, DEFAULT_PROJECT_ID
from ..models.response import CODE, BlobData, IdData
from ..models.blob import ChatBlob, DocBlob, BlobType
from ..connectors import Session


async def insert_blob(user_id: str, project_id: str, blob: BlobData) -> Promise[IdData]:
    try:
        blob_parsed = blob.to_blob()
    except pydantic.ValidationError as e:
        return Promise.reject(CODE.BAD_REQUEST, f"Unable to parse blob: {e}")
    with Session() as session:
        blob_db = GeneralBlob(
            blob_type=blob_parsed.type,
            blob_data=blob_parsed.get_blob_data(),
            additional_fields=blob_parsed.fields,
            user_id=user_id,
            project_id=project_id,
        )
        session.add(blob_db)
        session.commit()
        b_id = blob_db.id
    return Promise.resolve(IdData(id=b_id))


async def get_blob(user_id: str, project_id: str, blob_id: str) -> Promise[BlobData]:
    with Session() as session:
        blob_db = (
            session.query(GeneralBlob)
            .filter_by(id=blob_id, user_id=user_id, project_id=project_id)
            .one_or_none()
        )
        if not blob_db:
            return Promise.reject(
                CODE.NOT_FOUND, f"Blob with id {blob_id} of user {user_id} not found"
            )
        rt_blob = BlobData(
            blob_type=BlobType(blob_db.blob_type),
            blob_data=blob_db.blob_data,
            fields=blob_db.additional_fields,
            created_at=blob_db.created_at,
            updated_at=blob_db.updated_at,
        )
        return Promise.resolve(rt_blob)


async def remove_blob(user_id: str, project_id: str, blob_id: str) -> Promise[None]:
    with Session() as session:
        blob_db = (
            session.query(GeneralBlob)
            .filter_by(id=blob_id, user_id=user_id, project_id=project_id)
            .one_or_none()
        )
        if not blob_db:
            return Promise.resolve(None)
        else:
            session.delete(blob_db)
            session.commit()
    return Promise.resolve(None)



================================================
FILE: src/server/api/memobase_server/controllers/buffer.py
================================================
from sqlalchemy import func
from pydantic import BaseModel
from ..env import CONFIG, BufferStatus, TRACE_LOG
from ..utils import (
    get_blob_token_size,
    pack_blob_from_db,
)
from ..models.utils import Promise
from ..models.response import CODE, ChatModalResponse, IdsData
from ..models.database import BufferZone, GeneralBlob
from ..models.blob import BlobType, Blob
from ..connectors import Session, log_pool_status
from .modal import BLOBS_PROCESS


async def get_buffer_capacity(
    user_id: str, project_id: str, blob_type: BlobType
) -> Promise[int]:
    with Session() as session:
        buffer_count = (
            session.query(BufferZone.id)
            .filter_by(
                user_id=user_id,
                blob_type=str(blob_type),
                project_id=project_id,
                status=BufferStatus.idle,
            )
            .count()
        )
    return Promise.resolve(buffer_count)


async def insert_blob_to_buffer(
    user_id: str, project_id: str, blob_id: str, blob_data: Blob
) -> Promise[None]:
    with Session() as session:
        buffer = BufferZone(
            user_id=user_id,
            blob_id=blob_id,
            blob_type=blob_data.type,
            token_size=get_blob_token_size(blob_data),
            project_id=project_id,
            status=BufferStatus.idle,
        )
        session.add(buffer)
        session.commit()
    return Promise.resolve(None)


async def wait_insert_done_then_flush(
    user_id: str, project_id: str, blob_type: BlobType
) -> Promise[ChatModalResponse | None]:
    p = await get_unprocessed_buffer_ids(user_id, project_id, blob_type)
    if not p.ok():
        return p
    if p.data() is None:
        return Promise.resolve([])
    p = await flush_buffer_by_ids(user_id, project_id, blob_type, p.data().ids)
    if not p.ok():
        return p
    if p.data() is not None:
        return Promise.resolve(p.data())
    return Promise.resolve(None)


async def detect_buffer_full_or_not(
    user_id: str, project_id: str, blob_type: BlobType
) -> Promise[IdsData | None]:
    with Session() as session:
        # 1. if buffer size reach maximum, flush it
        buffer_zone = (
            session.query(BufferZone.id, BufferZone.token_size)
            .filter_by(
                user_id=user_id,
                blob_type=str(blob_type),
                project_id=project_id,
                status=BufferStatus.idle,
            )
            .all()
        )
        buffer_ids = [row.id for row in buffer_zone]
        buffer_token_size = sum(row.token_size for row in buffer_zone)
        if (
            buffer_token_size
            and buffer_token_size > CONFIG.max_chat_blob_buffer_token_size
        ):
            TRACE_LOG.info(
                project_id,
                user_id,
                f"Flush {blob_type} buffer due to reach maximum token size({buffer_token_size} > {CONFIG.max_chat_blob_buffer_token_size})",
            )

            return Promise.resolve(IdsData(ids=buffer_ids))
    return Promise.resolve(IdsData(ids=[]))


async def get_unprocessed_buffer_ids(
    user_id: str,
    project_id: str,
    blob_type: BlobType,
    select_status: str = BufferStatus.idle,
) -> Promise[IdsData]:
    with Session() as session:
        buffer_ids = (
            session.query(BufferZone.id)
            .filter_by(
                user_id=user_id,
                blob_type=str(blob_type),
                project_id=project_id,
                status=select_status,
            )
            .all()
        )
        return Promise.resolve(IdsData(ids=[row.id for row in buffer_ids]))


async def flush_buffer_by_ids(
    user_id: str,
    project_id: str,
    blob_type: BlobType,
    buffer_ids: list[str],
    select_status: str = BufferStatus.idle,
) -> Promise[ChatModalResponse | None]:
    # FIXME: parallel calling will cause duplicated flush
    if blob_type not in BLOBS_PROCESS:
        return Promise.reject(CODE.BAD_REQUEST, f"Blob type {blob_type} not supported")
    if not len(buffer_ids):
        return Promise.resolve(None)

    # Log initial pool status
    log_pool_status(f"flush_buffer_by_ids_start_{blob_type}")

    with Session() as session:
        # Join BufferZone with GeneralBlob to get all data in one query
        buffer_blob_data = (
            session.query(
                BufferZone.id.label("buffer_id"),
                BufferZone.blob_id,
                BufferZone.token_size,
                BufferZone.created_at.label("buffer_created_at"),
                GeneralBlob.created_at,
                GeneralBlob.blob_data,
            )
            .join(GeneralBlob, BufferZone.blob_id == GeneralBlob.id)
            .filter(
                BufferZone.user_id == user_id,
                BufferZone.blob_type == str(blob_type),
                BufferZone.project_id == project_id,
                GeneralBlob.user_id == user_id,
                GeneralBlob.project_id == project_id,
                BufferZone.status == select_status,
                BufferZone.id.in_(buffer_ids),
            )
            .order_by(BufferZone.created_at)
            .all()
        )
        # Update buffer status to processing
        process_buffer_ids = [row.buffer_id for row in buffer_blob_data]
        if select_status != BufferStatus.processing:
            session.query(BufferZone).filter(
                BufferZone.id.in_(process_buffer_ids),
            ).update(
                {BufferZone.status: BufferStatus.processing},
                synchronize_session=False,
            )

        if not buffer_blob_data:
            TRACE_LOG.info(
                project_id,
                user_id,
                f"No {blob_type} buffer to flush",
            )
            return Promise.resolve(None)

        blob_ids = [row.blob_id for row in buffer_blob_data]
        blobs = [pack_blob_from_db(row, blob_type) for row in buffer_blob_data]
        total_token_size = sum(row.token_size for row in buffer_blob_data)
        TRACE_LOG.info(
            project_id,
            user_id,
            f"Flush {blob_type} buffer with {len(buffer_blob_data)} blobs and total token size({total_token_size})",
        )

        session.commit()

    try:
        # Pack blobs from the joined data

        # Process blobs first (moved outside the session)
        p = await BLOBS_PROCESS[blob_type](user_id, project_id, blobs)
        if not p.ok():
            # Rollback buffer status to failed if the process failed
            with Session() as session:
                session.query(BufferZone).filter(
                    BufferZone.id.in_(process_buffer_ids),
                ).update(
                    {BufferZone.status: BufferStatus.failed},
                    synchronize_session=False,
                )
                session.commit()
            return p
        with Session() as session:
            try:
                # Update buffer status to done
                session.query(BufferZone).filter(
                    BufferZone.id.in_(process_buffer_ids),
                ).update(
                    {BufferZone.status: BufferStatus.done},
                    synchronize_session=False,
                )
                if blob_type == BlobType.chat and not CONFIG.persistent_chat_blobs:
                    session.query(GeneralBlob).filter(
                        GeneralBlob.id.in_(blob_ids),
                        GeneralBlob.project_id == project_id,
                    ).delete(synchronize_session=False)
                session.commit()
                TRACE_LOG.info(
                    project_id,
                    user_id,
                    f"Flushed {blob_type} buffer(size: {len(buffer_blob_data)})",
                )
            except Exception as e:
                session.rollback()
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"DB Error while deleting buffers/blobs: {e}",
                )
                log_pool_status(f"flush_buffer_by_ids_db_error_{blob_type}")
                raise e

        return p

    except Exception as e:
        with Session() as session:
            session.query(BufferZone).filter(
                BufferZone.id.in_(process_buffer_ids),
            ).update(
                {BufferZone.status: BufferStatus.failed},
                synchronize_session=False,
            )
            session.commit()
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Error in flush_buffer: {e}. Buffer status updated to failed.",
        )
        log_pool_status(f"flush_buffer_by_ids_exception_{blob_type}")
        raise e


async def flush_buffer(
    user_id: str, project_id: str, blob_type: BlobType
) -> Promise[ChatModalResponse | None]:
    p = await get_unprocessed_buffer_ids(user_id, project_id, blob_type)
    if not p.ok():
        return p
    p = await flush_buffer_by_ids(user_id, project_id, blob_type, p.data().ids)
    return p



================================================
FILE: src/server/api/memobase_server/controllers/buffer_background.py
================================================
import uuid
import asyncio
import traceback
from sqlalchemy import func
from pydantic import BaseModel
from ..env import CONFIG, BufferStatus, TRACE_LOG
from ..models.utils import Promise
from ..models.response import CODE, ChatModalResponse, IdsData, UUID
from ..models.database import BufferZone, GeneralBlob
from ..models.blob import BlobType, Blob
from ..connectors import Session, PROJECT_ID, get_redis_client
from .modal import BLOBS_PROCESS
from .buffer import flush_buffer_by_ids

REDIS_LUA_CHECK_AND_DELETE_LOCK = """
if redis.call("get", KEYS[1]) == ARGV[1] then
    return redis.call("del", KEYS[1])
else
    return 0
end
"""


def get_user_lock_key(user_id: str, project_id: str, scope: str) -> str:
    return f"memobase:user_lock:{PROJECT_ID}:{scope}:{project_id}:{user_id}"


def get_user_buffer_queue_key(user_id: str, project_id: str, scope: str) -> str:
    return f"memobase:user_buffer_queue:{PROJECT_ID}:{scope}:{project_id}:{user_id}"


def pack_ids_to_str(ids: list[str]) -> str:
    return "::".join([str(i) for i in ids])


def unpack_ids_from_str(ids_str: str) -> list[str]:
    return [i.strip() for i in ids_str.split("::") if i.strip()]


async def flush_buffer_by_ids_in_background(
    user_id: str, project_id: str, blob_type: BlobType, buffer_ids: list[str]
) -> None:
    if not len(buffer_ids):
        return
    if blob_type not in BLOBS_PROCESS:
        return

    # 1. mark buffer as processing
    with Session() as session:
        buffer_blob_data = (
            session.query(BufferZone.id)
            .filter(
                BufferZone.user_id == user_id,
                BufferZone.blob_type == str(blob_type),
                BufferZone.project_id == project_id,
                BufferZone.status == BufferStatus.idle,
                BufferZone.id.in_(buffer_ids),
            )
            .order_by(BufferZone.created_at)
            .all()
        )
        actual_buffer_ids = [row.id for row in buffer_blob_data]
        if not len(actual_buffer_ids):
            return
        session.query(BufferZone).filter(
            BufferZone.id.in_(actual_buffer_ids),
        ).update(
            {BufferZone.status: BufferStatus.processing},
            synchronize_session=False,
        )

        session.commit()

    # 2. add actual buffer ids to a redis queue
    buffer_queue_key = get_user_buffer_queue_key(
        user_id, project_id, f"flush_buffer_background_{blob_type}"
    )
    buffer_ids_str = pack_ids_to_str(actual_buffer_ids)

    try:
        async with get_redis_client() as redis_client:
            await redis_client.rpush(buffer_queue_key, buffer_ids_str)

            queue_size = await redis_client.llen(buffer_queue_key)

            TRACE_LOG.info(
                project_id,
                user_id,
                f"[background] Enqueued {len(actual_buffer_ids)} buffer IDs to queue (queue size: {queue_size})",
            )

        await flush_buffer_background_running(user_id, project_id, blob_type)
    except Exception as e:
        TRACE_LOG.error(
            project_id,
            user_id,
            f"[background] Error enqueue buffer ids: {e}: {traceback.format_exc()}",
        )


async def flush_buffer_background_running(
    user_id: str,
    project_id: str,
    blob_type: BlobType,
    asleep_waiting_s: float = 0.1,  # Increased from 0.001 to reduce CPU usage
    max_iterations: int = 200,  # Maximum 200 tasks for this run, return after it reaches.
    process_interval_s: float = 60 * 5,  # Reduced from 10 minutes to 5 minutes
    max_processing_time_s: float = 60 * 15,  # Maximum 15 minutes total processing time
    max_consecutive_errors=5,  # Stop after 5 consecutive errors
):
    user_key = get_user_lock_key(
        user_id, project_id, f"flush_buffer_background_{blob_type}"
    )
    buffer_queue_key = get_user_buffer_queue_key(
        user_id, project_id, f"flush_buffer_background_{blob_type}"
    )

    __lock_value = str(uuid.uuid4())
    start_time = asyncio.get_event_loop().time()

    # Shorter lock timeout with renewal
    async with get_redis_client() as redis_client:
        acquired = await redis_client.set(
            user_key, __lock_value, nx=True, ex=process_interval_s
        )
        if not acquired:
            TRACE_LOG.debug(
                project_id,
                user_id,
                f"[background] Lock already acquired",
            )
            return

    try:
        iteration_count = 0
        consecutive_errors = 0

        while iteration_count < max_iterations:
            current_time = asyncio.get_event_loop().time()

            # Check if we've exceeded maximum processing time
            if current_time - start_time > max_processing_time_s:
                TRACE_LOG.warning(
                    project_id,
                    user_id,
                    f"[background] Maximum processing time ({max_processing_time_s}s) exceeded",
                )
                break

            # Check lock and get next batch
            async with get_redis_client() as redis_client:
                lock_value = await redis_client.get(user_key)
                if lock_value is None or lock_value != __lock_value:  # Lock is expired
                    TRACE_LOG.debug(
                        project_id,
                        user_id,
                        "[background] Lock expired",
                    )
                    break

                buffer_ids_str = await redis_client.lpop(buffer_queue_key)
                if buffer_ids_str is None:  # Queue is empty
                    TRACE_LOG.debug(
                        project_id,
                        user_id,
                        "[background] Queue empty",
                    )
                    break

                # Renew lock timeout if needed
                await redis_client.expire(user_key, process_interval_s)
                current_queue_size = await redis_client.llen(buffer_queue_key)

            TRACE_LOG.info(
                project_id,
                user_id,
                f"[background]({iteration_count}/{max_iterations}) Processing buffer (left queue size: {current_queue_size})",
            )

            buffer_ids = unpack_ids_from_str(buffer_ids_str or "")
            if not buffer_ids:
                continue

            try:
                # Process the buffer with timeout protection
                processing_start = asyncio.get_event_loop().time()

                p = await flush_buffer_by_ids(
                    user_id,
                    project_id,
                    blob_type,
                    buffer_ids,
                    select_status=BufferStatus.processing,
                )

                processing_time = asyncio.get_event_loop().time() - processing_start

                if not p.ok():
                    consecutive_errors += 1
                    TRACE_LOG.error(
                        project_id,
                        user_id,
                        f"[background] Error flushing buffer by ids: {p.msg()}",
                    )

                    # Stop if too many consecutive errors
                    if consecutive_errors >= max_consecutive_errors:
                        TRACE_LOG.error(
                            project_id,
                            user_id,
                            f"[background] Too many consecutive errors ({consecutive_errors}), stopping",
                        )
                        break
                else:
                    consecutive_errors = 0  # Reset error counter on success
                    TRACE_LOG.debug(
                        project_id,
                        user_id,
                        f"[background] Processed batch in {processing_time:.2f}s",
                    )

            except Exception as e:
                consecutive_errors += 1
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"[background] Unknown Error flushing buffer by ids: {e}\n{traceback.format_exc()}",
                )

                # Stop if too many consecutive errors
                if consecutive_errors >= max_consecutive_errors:
                    TRACE_LOG.error(
                        project_id,
                        user_id,
                        f"[background] Too many consecutive errors ({consecutive_errors}), stopping",
                    )
                    break

            # Sleep between iterations to prevent overwhelming the system
            await asyncio.sleep(asleep_waiting_s)
            iteration_count += 1

        total_processing_time = asyncio.get_event_loop().time() - start_time
        TRACE_LOG.info(
            project_id,
            user_id,
            f"[background] Completed processing. "
            f"Iterations: {iteration_count}, Time: {total_processing_time:.2f}s, "
            f"Final consecutive errors: {consecutive_errors}",
        )

    finally:
        try:
            async with get_redis_client() as redis_client:
                result = await redis_client.eval(
                    REDIS_LUA_CHECK_AND_DELETE_LOCK, 1, user_key, __lock_value
                )
                if result == 1:
                    TRACE_LOG.debug(
                        project_id,
                        user_id,
                        f"[background] Successfully released lock",
                    )
                else:
                    TRACE_LOG.warning(
                        project_id,
                        user_id,
                        f"[background] Lock was already expired/released",
                    )
        except Exception as e:
            TRACE_LOG.error(
                project_id,
                user_id,
                f"[background] Failed to release lock: {e}",
            )



================================================
FILE: src/server/api/memobase_server/controllers/context.py
================================================
from functools import partial
from ..models.utils import Promise, CODE
from ..models.response import ContextData, OpenAICompatibleMessage, UserEventGistsData
from ..prompts.chat_context_pack import CONTEXT_PROMPT_PACK
from ..utils import get_encoded_tokens, event_str_repr
from ..env import CONFIG, TRACE_LOG
from .project import get_project_profile_config
from .profile import get_user_profiles, truncate_profiles
from .post_process.profile import filter_profiles_with_chats

# from .event import get_user_events, search_user_events, truncate_events
from .event_gist import (
    get_user_event_gists,
    truncate_event_gists,
    search_user_event_gists,
)


def customize_context_prompt_func(
    context_prompt: str, profile_section: str, event_section: str
) -> str:
    return context_prompt.format(
        profile_section=profile_section, event_section=event_section
    )


def pack_latest_chat(chats: list[OpenAICompatibleMessage], chat_num: int = 3) -> str:
    return "\n".join([f"{m.content}" for m in chats[-chat_num:]])


async def get_user_profiles_data(
    user_id: str,
    project_id: str,
    max_profile_token_size: int,
    prefer_topics: list[str],
    only_topics: list[str],
    max_subtopic_size: int,
    topic_limits: dict[str, int],
    chats: list[OpenAICompatibleMessage],
    full_profile_and_only_search_event: bool,
) -> Promise[tuple[str, list]]:
    """Retrieve and process user profiles."""
    p = await get_user_profiles(user_id, project_id)
    if not p.ok():
        return p
    total_profiles = p.data()

    if max_profile_token_size > 0:
        if chats and (not full_profile_and_only_search_event):
            p = await filter_profiles_with_chats(
                user_id,
                project_id,
                total_profiles,
                chats,
                only_topics=only_topics,
            )
            if p.ok():
                total_profiles.profiles = p.data()["profiles"]

        user_profiles = total_profiles
        use_profiles = await truncate_profiles(
            user_profiles,
            prefer_topics=prefer_topics,
            only_topics=only_topics,
            max_token_size=max_profile_token_size,
            max_subtopic_size=max_subtopic_size,
            topic_limits=topic_limits,
        )
        if not use_profiles.ok():
            return use_profiles
        use_profiles = use_profiles.data().profiles

        profile_section = "- " + "\n- ".join(
            [
                f"{p.attributes.get('topic')}::{p.attributes.get('sub_topic')}: {p.content}"
                for p in use_profiles
            ]
        )
    else:
        profile_section = ""
        use_profiles = []

    return Promise.resolve((profile_section, use_profiles))


async def get_user_event_gists_data(
    user_id: str,
    project_id: str,
    chats: list[OpenAICompatibleMessage],
    require_event_summary: bool,
    event_similarity_threshold: float,
    time_range_in_days: int,
) -> Promise[UserEventGistsData]:
    """Retrieve user events data."""
    if chats and CONFIG.enable_event_embedding:
        search_query = pack_latest_chat(chats)
        p = await search_user_event_gists(
            user_id,
            project_id,
            query=search_query,
            topk=60,
            similarity_threshold=event_similarity_threshold,
            time_range_in_days=time_range_in_days,
        )
    else:
        p = await get_user_event_gists(
            user_id,
            project_id,
            topk=60,
            time_range_in_days=time_range_in_days,
        )
    return p


async def get_user_context(
    user_id: str,
    project_id: str,
    max_token_size: int,
    prefer_topics: list[str],
    only_topics: list[str],
    max_subtopic_size: int,
    topic_limits: dict[str, int],
    profile_event_ratio: float,
    require_event_summary: bool,
    chats: list[OpenAICompatibleMessage],
    event_similarity_threshold: float,
    time_range_in_days: int,
    customize_context_prompt: str = None,
    full_profile_and_only_search_event: bool = False,
    fill_window_with_events: bool = False,
) -> Promise[ContextData]:
    import asyncio

    assert 0 < profile_event_ratio <= 1, "profile_event_ratio must be between 0 and 1"
    max_profile_token_size = int(max_token_size * profile_event_ratio)

    p = await get_project_profile_config(project_id)
    if not p.ok():
        return p
    profile_config = p.data()
    use_language = profile_config.language or CONFIG.language
    context_prompt_func = CONTEXT_PROMPT_PACK[use_language]
    if customize_context_prompt is not None:
        context_prompt_func = partial(
            customize_context_prompt_func, customize_context_prompt
        )

    # Execute profile and event retrieval in parallel
    profile_result, event_gist_result = await asyncio.gather(
        get_user_profiles_data(
            user_id,
            project_id,
            max_profile_token_size,
            prefer_topics,
            only_topics,
            max_subtopic_size,
            topic_limits,
            chats,
            full_profile_and_only_search_event,
        ),
        get_user_event_gists_data(
            user_id,
            project_id,
            chats,
            require_event_summary,
            event_similarity_threshold,
            time_range_in_days,
        ),
        return_exceptions=True,
    )

    # Handle profile result
    if isinstance(profile_result, Exception):
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, f"Profile retrieval failed: {str(profile_result)}"
        )
    if not profile_result.ok():
        return profile_result
    profile_section, use_profiles = profile_result.data()

    # Handle event result
    if isinstance(event_gist_result, Exception):
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, f"Event retrieval failed: {str(event_gist_result)}"
        )
    if not event_gist_result.ok():
        return event_gist_result
    user_event_gists = event_gist_result.data()

    # Calculate token sizes and truncate events if needed
    profile_section_tokens = len(get_encoded_tokens(profile_section))
    if fill_window_with_events:
        max_event_token_size = max_token_size - profile_section_tokens
    else:
        max_event_token_size = min(
            max_token_size - profile_section_tokens,
            max_token_size - max_profile_token_size,
        )

    if max_event_token_size <= 0:
        return Promise.resolve(
            ContextData(context=context_prompt_func(profile_section, ""))
        )

    # Truncate events based on calculated token size
    p = await truncate_event_gists(user_event_gists, max_event_token_size)
    if not p.ok():
        return p
    user_event_gists = p.data()

    event_section = "\n".join([ed.gist_data.content for ed in user_event_gists.gists])
    event_section_tokens = len(get_encoded_tokens(event_section))

    TRACE_LOG.info(
        project_id,
        user_id,
        f"Retrieved {len(use_profiles)} profiles({profile_section_tokens} tokens), {len(user_event_gists.gists)} event gists({event_section_tokens} tokens)",
    )

    return Promise.resolve(
        ContextData(context=context_prompt_func(profile_section, event_section))
    )



================================================
FILE: src/server/api/memobase_server/controllers/event.py
================================================
from pydantic import ValidationError
from ..models.database import UserEvent, UserEventGist
from ..models.response import UserEventData, UserEventsData, EventData
from ..models.utils import Promise, CODE
from ..connectors import Session
from ..utils import get_encoded_tokens, event_str_repr, event_embedding_str

from ..llms.embeddings import get_embedding
from datetime import timedelta
from sqlalchemy import desc, select
from sqlalchemy.sql import func
from ..env import TRACE_LOG, CONFIG


async def get_user_events(
    user_id: str,
    project_id: str,
    topk: int = 10,
    need_summary: bool = False,
    time_range_in_days: int = 21,
) -> Promise[UserEventsData]:
    with Session() as session:
        query = (
            session.query(UserEvent)
            .filter_by(user_id=user_id, project_id=project_id)
            .filter(
                UserEvent.created_at > (func.now() - timedelta(days=time_range_in_days))
            )
        )
        # Abort this parameter because the summary is moved to gist
        # if need_summary:
        #     query = query.filter(
        #         UserEvent.event_data.contains({"event_tip": None}).is_(False)
        #     ).filter(UserEvent.event_data.has_key("event_tip"))
        user_events = query.order_by(UserEvent.created_at.desc()).limit(topk).all()
        if user_events is None:
            return Promise.resolve(UserEventsData(events=[]))
        results = [
            {
                "id": ue.id,
                "event_data": ue.event_data,
                "created_at": ue.created_at,
                "updated_at": ue.updated_at,
            }
            for ue in user_events
        ]
    events = UserEventsData(events=results)
    return Promise.resolve(events)


async def truncate_events(
    events: UserEventsData,
    max_token_size: int | None,
) -> Promise[UserEventsData]:
    if max_token_size is None:
        return Promise.resolve(events)
    c_tokens = 0
    truncated_results = []
    for r in events.events:
        c_tokens += len(get_encoded_tokens(event_str_repr(r)))
        if c_tokens > max_token_size:
            break
        truncated_results.append(r)
    events.events = truncated_results
    return Promise.resolve(events)


async def append_user_event(
    user_id: str, project_id: str, event_data: dict
) -> Promise[str]:
    try:
        validated_event = EventData(**event_data)
    except ValidationError as e:
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Invalid event data: {str(e)}",
        )
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR,
            f"Invalid event data: {str(e)}",
        )

    if CONFIG.enable_event_embedding:
        event_data_str = event_embedding_str(validated_event)
        embedding = await get_embedding(
            project_id,
            [event_data_str],
            phase="document",
            model=CONFIG.embedding_model,
        )
        if not embedding.ok():
            TRACE_LOG.error(
                project_id,
                user_id,
                f"Failed to get embeddings: {embedding.msg()}",
            )
            embedding = [None]
        else:
            embedding = embedding.data()
            embedding_dim_current = embedding.shape[-1]
            if embedding_dim_current != CONFIG.embedding_dim:
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"Embedding dimension mismatch! Expected {CONFIG.embedding_dim}, got {embedding_dim_current}.",
                )
                embedding = [None]
    else:
        embedding = [None]

    event_gist_dbs = []
    if validated_event.event_tip is not None:
        event_gists = validated_event.event_tip.split("\n")
        event_gists = [l.strip() for l in event_gists if l.strip().startswith("-")]
        TRACE_LOG.info(
            project_id, user_id, f"Processing {len(event_gists)} event gists"
        )
        if CONFIG.enable_event_embedding and len(event_gists) > 0:
            event_gists_embedding = await get_embedding(
                project_id,
                event_gists,
                phase="document",
                model=CONFIG.embedding_model,
            )
            if not event_gists_embedding.ok():
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"Failed to get embeddings: {event_gists_embedding.msg()}",
                )
                event_gists_embedding = [None] * len(event_gists)
            else:
                event_gists_embedding = event_gists_embedding.data()
        else:
            event_gists_embedding = [None] * len(event_gists)
        for event_gist, event_gist_embedding in zip(event_gists, event_gists_embedding):
            event_gist_dbs.append(
                {
                    "gist_data": {"content": event_gist},
                    "embedding": event_gist_embedding,
                }
            )
    with Session() as session:
        user_event = UserEvent(
            user_id=user_id,
            project_id=project_id,
            event_data=validated_event.model_dump(),
            embedding=embedding[0],
        )
        session.add(user_event)
        for event_gist_data in event_gist_dbs:
            session.add(
                UserEventGist(
                    user_id=user_id,
                    project_id=project_id,
                    event_id=user_event.id,
                    gist_data=event_gist_data["gist_data"],
                    embedding=event_gist_data["embedding"],
                )
            )
        session.commit()
        eid = user_event.id
    return Promise.resolve(eid)


async def delete_user_event(
    user_id: str, project_id: str, event_id: str
) -> Promise[None]:
    with Session() as session:
        user_event = (
            session.query(UserEvent)
            .filter_by(user_id=user_id, project_id=project_id, id=event_id)
            .first()
        )
        if user_event is None:
            return Promise.reject(
                CODE.NOT_FOUND,
                f"User event {event_id} not found",
            )
        session.delete(user_event)
        session.commit()
    return Promise.resolve(None)


async def update_user_event(
    user_id: str, project_id: str, event_id: str, event_data: dict
) -> Promise[None]:
    try:
        EventData(**event_data)
    except ValidationError as e:
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR,
            f"Invalid event data: {str(e)}",
        )
    need_to_update = {k: v for k, v in event_data.items() if v is not None}
    with Session() as session:
        user_event = (
            session.query(UserEvent)
            .filter_by(user_id=user_id, project_id=project_id, id=event_id)
            .first()
        )
        if user_event is None:
            return Promise.reject(
                CODE.NOT_FOUND,
                f"User event {event_id} not found",
            )
        new_events = dict(user_event.event_data)
        new_events.update(need_to_update)

        user_event.event_data = new_events
        session.commit()
    return Promise.resolve(None)


async def search_user_events(
    user_id: str,
    project_id: str,
    query: str,
    topk: int = 10,
    similarity_threshold: float = 0.2,
    time_range_in_days: int = 21,
) -> Promise[UserEventsData]:
    if not CONFIG.enable_event_embedding:
        TRACE_LOG.warning(
            project_id,
            user_id,
            "Event embedding is not enabled, skip search",
        )
        return Promise.reject(
            CODE.NOT_IMPLEMENTED,
            "Event embedding is not enabled",
        )

    query_embeddings = await get_embedding(
        project_id, [query], phase="query", model=CONFIG.embedding_model
    )
    if not query_embeddings.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to get embeddings: {query_embeddings.msg()}",
        )
        return query_embeddings
    query_embedding = query_embeddings.data()[0]

    stmt = (
        select(
            UserEvent,
            (1 - UserEvent.embedding.cosine_distance(query_embedding)).label(
                "similarity"
            ),
        )
        .where(UserEvent.user_id == user_id, UserEvent.project_id == project_id)
        .where(UserEvent.created_at > func.now() - timedelta(days=time_range_in_days))
        .where(
            (1 - UserEvent.embedding.cosine_distance(query_embedding))
            > similarity_threshold
        )
        .order_by(desc("similarity"))
        .limit(topk)
    )

    with Session() as session:
        # Use .all() instead of .scalars().all() to get both columns
        result = session.execute(stmt).all()
        user_events: list[UserEventData] = []
        for row in result:
            user_event: UserEvent = row[0]  # UserEvent object
            similarity: float = row[1]  # similarity value
            user_events.append(
                UserEventData(
                    id=user_event.id,
                    event_data=user_event.event_data,
                    created_at=user_event.created_at,
                    updated_at=user_event.updated_at,
                    similarity=similarity,
                )
            )

        # Create UserEventsData with the events
        user_events_data = UserEventsData(events=user_events)
        TRACE_LOG.info(
            project_id,
            user_id,
            f"Event Query: {query}",
        )

    return Promise.resolve(user_events_data)


async def filter_user_events(
    user_id: str,
    project_id: str,
    has_event_tag: list[str] = None,
    event_tag_equal: dict[str, str] = None,
    topk: int = 10,
) -> Promise[UserEventsData]:
    """
    Filter user events based on event tags.

    Args:
        user_id: User ID
        project_id: Project ID
        has_event_tag: List of tag names that must exist in the event (regardless of value)
        event_tag_equal: Dict of tag_name: tag_value pairs that must match exactly
        topk: Maximum number of events to return

    Returns:
        Promise containing filtered UserEventsData
    """
    with Session() as session:
        query = session.query(UserEvent).filter_by(
            user_id=user_id, project_id=project_id
        )

        # Apply tag filters if provided
        if has_event_tag or event_tag_equal:
            # Build filter conditions for events that have event_tags
            query = query.filter(UserEvent.event_data.has_key("event_tags"))
            query = query.filter(UserEvent.event_data["event_tags"].isnot(None))

            # Filter by tag existence (has_event_tag)
            if has_event_tag:
                for tag_name in has_event_tag:
                    # Check if any event_tag in the array has the specified tag name
                    query = query.filter(
                        UserEvent.event_data["event_tags"].op("@>")(
                            f'[{{"tag": "{tag_name}"}}]'
                        )
                    )

            # Filter by exact tag-value pairs (event_tag_equal)
            if event_tag_equal:
                for tag_name, tag_value in event_tag_equal.items():
                    # Check if any event_tag in the array has both the tag name and value
                    query = query.filter(
                        UserEvent.event_data["event_tags"].op("@>")(
                            f'[{{"tag": "{tag_name}", "value": "{tag_value}"}}]'
                        )
                    )

        user_events = query.order_by(UserEvent.created_at.desc()).limit(topk).all()

        if user_events is None:
            return Promise.resolve(UserEventsData(events=[]))

        results = [
            {
                "id": ue.id,
                "event_data": ue.event_data,
                "created_at": ue.created_at,
                "updated_at": ue.updated_at,
            }
            for ue in user_events
        ]

    events = UserEventsData(events=results)
    return Promise.resolve(events)



================================================
FILE: src/server/api/memobase_server/controllers/event_gist.py
================================================
from pydantic import ValidationError
from ..models.database import UserEventGist
from ..models.response import UserEventGistsData, UserEventGistData
from ..models.utils import Promise, CODE
from ..connectors import Session
from ..utils import get_encoded_tokens, event_str_repr, event_embedding_str

from ..llms.embeddings import get_embedding
from datetime import timedelta
from sqlalchemy import desc, select
from sqlalchemy.sql import func
from ..env import TRACE_LOG, CONFIG


async def get_user_event_gists(
    user_id: str,
    project_id: str,
    topk: int = 10,
    time_range_in_days: int = 21,
) -> Promise[UserEventGistsData]:
    with Session() as session:
        query = (
            session.query(UserEventGist)
            .filter_by(user_id=user_id, project_id=project_id)
            .filter(
                UserEventGist.created_at
                > (func.now() - timedelta(days=time_range_in_days))
            )
        )
        user_event_gists = (
            query.order_by(UserEventGist.created_at.desc()).limit(topk).all()
        )
        if user_event_gists is None:
            return Promise.resolve(UserEventGistsData(gists=[]))
        results = [
            {
                "id": ue.id,
                "gist_data": ue.gist_data,
                "created_at": ue.created_at,
                "updated_at": ue.updated_at,
            }
            for ue in user_event_gists
        ]
    gists = UserEventGistsData(gists=results)
    return Promise.resolve(gists)


async def truncate_event_gists(
    events: UserEventGistsData,
    max_token_size: int | None,
) -> Promise[UserEventGistsData]:
    if max_token_size is None:
        return Promise.resolve(events)
    c_tokens = 0
    truncated_results = []
    for r in events.gists:
        c_tokens += len(get_encoded_tokens(r.gist_data.content))
        if c_tokens > max_token_size:
            break
        truncated_results.append(r)
    events.gists = truncated_results
    return Promise.resolve(events)


async def search_user_event_gists(
    user_id: str,
    project_id: str,
    query: str,
    topk: int = 10,
    similarity_threshold: float = 0.2,
    time_range_in_days: int = 21,
) -> Promise[UserEventGistsData]:
    if not CONFIG.enable_event_embedding:
        TRACE_LOG.warning(
            project_id,
            user_id,
            "Event embedding is not enabled, skip search",
        )
        return Promise.reject(
            CODE.NOT_IMPLEMENTED,
            "Event embedding is not enabled",
        )
    query_embeddings = await get_embedding(
        project_id, [query], phase="query", model=CONFIG.embedding_model
    )
    if not query_embeddings.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to get embeddings: {query_embeddings.msg()}",
        )
        return query_embeddings
    query_embedding = query_embeddings.data()[0]

    # Calculate the time cutoff once
    time_cutoff = func.now() - timedelta(days=time_range_in_days)

    # Store the similarity expression to avoid recomputation
    similarity_expr = 1 - UserEventGist.embedding.cosine_distance(query_embedding)

    stmt = (
        select(
            UserEventGist,
            similarity_expr.label("similarity"),
        )
        .where(
            UserEventGist.user_id == user_id,
            UserEventGist.project_id == project_id,
            UserEventGist.created_at > time_cutoff,
            similarity_expr > similarity_threshold,
            UserEventGist.embedding.is_not(None),  # Skip null embeddings
        )
        .order_by(desc("similarity"))
        .limit(topk)
    )

    with Session() as session:
        # Use .all() instead of .scalars().all() to get both columns
        result = session.execute(stmt).all()
        user_event_gists: list[UserEventGistData] = []
        for row in result:
            user_event: UserEventGist = row[0]  # UserEventGist object
            similarity: float = row[1]  # similarity value
            user_event_gists.append(
                UserEventGistData(
                    id=user_event.id,
                    gist_data=user_event.gist_data,
                    created_at=user_event.created_at,
                    updated_at=user_event.updated_at,
                    similarity=similarity,
                )
            )

        # Create UserEventsData with the events
        user_event_gists_data = UserEventGistsData(gists=user_event_gists)
        TRACE_LOG.info(
            project_id,
            user_id,
            f"Event Query: {query}",
        )

    return Promise.resolve(user_event_gists_data)



================================================
FILE: src/server/api/memobase_server/controllers/full.py
================================================
from . import user
from . import blob
from . import buffer
from . import buffer_background
from . import profile
from . import project
from . import event
from . import event_gist
from . import context
from . import billing



================================================
FILE: src/server/api/memobase_server/controllers/profile.py
================================================
from pydantic import ValidationError
from ..models.utils import Promise
from ..models.database import GeneralBlob, UserProfile
from ..models.response import CODE, IdData, IdsData, UserProfilesData, ProfileAttributes
from ..connectors import Session, get_redis_client
from ..utils import get_encoded_tokens
from ..env import CONFIG, TRACE_LOG


async def truncate_profiles(
    profiles: UserProfilesData,
    prefer_topics: list[str] = None,
    topk: int = None,
    max_token_size: int = None,
    only_topics: list[str] = None,
    max_subtopic_size: int = None,
    topic_limits: dict[str, int] = None,
) -> Promise[UserProfilesData]:
    if not len(profiles.profiles):
        return Promise.resolve(profiles)
    profiles.profiles.sort(key=lambda p: p.updated_at, reverse=True)
    if prefer_topics:
        prefer_topics = [t.strip() for t in prefer_topics]
        priority_weights = {t: i for i, t in enumerate(prefer_topics)}
        priority_profiles = []
        non_priority_profiles = []
        for p in profiles.profiles:
            if p.attributes.get("topic") in priority_weights:
                priority_profiles.append(p)
            else:
                non_priority_profiles.append(p)
        priority_profiles.sort(
            key=lambda p: priority_weights[p.attributes.get("topic")]
        )
        profiles.profiles = priority_profiles + non_priority_profiles
    if only_topics:
        only_topics = [t.strip() for t in only_topics]
        s_only_topics = set(only_topics)
        profiles.profiles = [
            p
            for p in profiles.profiles
            if p.attributes.get("topic").strip() in s_only_topics
        ]
    if max_subtopic_size or topic_limits:
        use_topic_limits = topic_limits or {}
        max_subtopic_size = max_subtopic_size or -1
        _count_subtopics = {}
        filtered_profiles = []
        for p in profiles.profiles:
            name_key = p.attributes.get("topic")
            this_topic_limit = use_topic_limits.get(name_key, max_subtopic_size)
            if name_key not in _count_subtopics:
                _count_subtopics[name_key] = 0
            _count_subtopics[name_key] += 1
            if this_topic_limit >= 0 and _count_subtopics[name_key] > this_topic_limit:
                continue
            filtered_profiles.append(p)
        profiles.profiles = filtered_profiles

    if topk:
        profiles.profiles = profiles.profiles[:topk]
    if max_token_size:
        current_length = 0
        use_index = 0
        for max_i, p in enumerate(profiles.profiles):
            single_p = f"{p.attributes.get('topic')}::{p.attributes.get('sub_topic')}: {p.content}"
            current_length += len(get_encoded_tokens(single_p))
            if current_length > max_token_size:
                break
            use_index = max_i
        profiles.profiles = profiles.profiles[: use_index + 1]
    return Promise.resolve(profiles)


async def get_user_profiles(user_id: str, project_id: str) -> Promise[UserProfilesData]:
    async with get_redis_client() as redis_client:
        user_profiles = await redis_client.get(
            f"user_profiles::{project_id}::{user_id}"
        )
        if user_profiles:
            try:
                return Promise.resolve(
                    UserProfilesData.model_validate_json(user_profiles)
                )
            except ValidationError as e:
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"Invalid user profiles: {e}",
                )
                await redis_client.delete(f"user_profiles::{project_id}::{user_id}")
    with Session() as session:
        user_profiles = (
            session.query(UserProfile)
            .filter_by(user_id=user_id, project_id=project_id)
            .order_by(UserProfile.updated_at.desc())
            .all()
        )
        results = []
        for up in user_profiles:
            results.append(
                {
                    "id": up.id,
                    "content": up.content,
                    "attributes": up.attributes,
                    "created_at": up.created_at,
                    "updated_at": up.updated_at,
                }
            )
    return_profiles = UserProfilesData(profiles=results)
    async with get_redis_client() as redis_client:
        await redis_client.set(
            f"user_profiles::{project_id}::{user_id}",
            return_profiles.model_dump_json(),
            ex=CONFIG.cache_user_profiles_ttl,
        )
    return Promise.resolve(return_profiles)


async def add_user_profiles(
    user_id: str,
    project_id: str,
    profiles: list[str],
    attributes: list[dict],
) -> Promise[IdsData]:
    assert len(profiles) == len(
        attributes
    ), "Length of profiles, attributes must be equal"
    for attr in attributes:
        try:
            ProfileAttributes.model_validate(attr)
        except ValidationError as e:
            return Promise.reject(
                CODE.SERVER_PARSE_ERROR, f"Invalid profile attributes: {e}"
            )
    with Session() as session:
        db_profiles = [
            UserProfile(
                user_id=user_id, project_id=project_id, content=content, attributes=attr
            )
            for content, attr in zip(profiles, attributes)
        ]
        session.add_all(db_profiles)
        session.commit()
        profile_ids = [profile.id for profile in db_profiles]
    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(IdsData(ids=profile_ids))


async def update_user_profiles(
    user_id: str,
    project_id: str,
    profile_ids: list[str],
    contents: list[str],
    attributes: list[dict | None],
) -> Promise[IdsData]:
    assert len(profile_ids) == len(
        contents
    ), "Length of profile_ids, contents must be equal"
    assert len(profile_ids) == len(
        attributes
    ), "Length of profile_ids, attributes must be equal"
    with Session() as session:
        db_profiles = []
        for profile_id, content, attribute in zip(profile_ids, contents, attributes):
            db_profile = (
                session.query(UserProfile)
                .filter_by(id=profile_id, user_id=user_id, project_id=project_id)
                .one_or_none()
            )
            if db_profile is None:
                TRACE_LOG.error(
                    project_id,
                    user_id,
                    f"Profile {profile_id} not found",
                )
                continue
            db_profile.content = content
            if attribute is not None:
                db_profile.attributes = attribute
            db_profiles.append(profile_id)
        session.commit()
    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(IdsData(ids=db_profiles))


async def delete_user_profile(
    user_id: str, project_id: str, profile_id: str
) -> Promise[None]:
    with Session() as session:
        db_profile = (
            session.query(UserProfile)
            .filter_by(id=profile_id, user_id=user_id, project_id=project_id)
            .one_or_none()
        )
        if db_profile is None:
            return Promise.reject(
                CODE.NOT_FOUND, f"Profile {profile_id} not found for user {user_id}"
            )
        session.delete(db_profile)
        session.commit()
    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(None)


async def delete_user_profiles(
    user_id: str, project_id: str, profile_ids: list[str]
) -> Promise[IdsData]:
    with Session() as session:
        session.query(UserProfile).filter(
            UserProfile.id.in_(profile_ids),
            UserProfile.user_id == user_id,
            UserProfile.project_id == project_id,
        ).delete(synchronize_session=False)
        session.commit()
    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(IdsData(ids=profile_ids))


async def refresh_user_profile_cache(user_id: str, project_id: str) -> Promise[None]:
    async with get_redis_client() as redis_client:
        await redis_client.delete(f"user_profiles::{project_id}::{user_id}")
    return Promise.resolve(None)


async def add_update_delete_user_profiles(
    user_id: str,
    project_id: str,
    add_profiles: list[str],
    add_attributes: list[dict],
    update_profile_ids: list[str],
    update_contents: list[str],
    update_attributes: list[dict | None],
    delete_profile_ids: list[str],
) -> Promise[IdsData]:
    assert len(add_profiles) == len(
        add_attributes
    ), "Length of add_profiles, add_attributes must be equal"
    assert len(update_profile_ids) == len(
        update_contents
    ), "Length of update_profile_ids, update_contents must be equal"
    assert len(update_profile_ids) == len(
        update_attributes
    ), "Length of update_profile_ids, update_attributes must be equal"

    for attr in add_attributes + update_attributes:
        if attr is None:
            continue
        try:
            ProfileAttributes.model_validate(attr)
        except ValidationError as e:
            return Promise.reject(
                CODE.SERVER_PARSE_ERROR, f"Invalid profile attributes: {e}"
            )
    # Sanity Check done

    with Session() as session:
        try:
            # 1. add new profiles
            if len(add_profiles):
                add_db_profiles = [
                    UserProfile(
                        user_id=user_id,
                        project_id=project_id,
                        content=content,
                        attributes=attr,
                    )
                    for content, attr in zip(add_profiles, add_attributes)
                ]
                session.add_all(add_db_profiles)
                add_profile_ids = [p.id for p in add_db_profiles]
            else:
                add_profile_ids = []
            # 2. update existing profiles
            update_db_profiles = []
            for profile_id, content, attribute in zip(
                update_profile_ids, update_contents, update_attributes
            ):
                db_profile = (
                    session.query(UserProfile)
                    .filter_by(id=profile_id, user_id=user_id, project_id=project_id)
                    .one_or_none()
                )
                if db_profile is None:
                    TRACE_LOG.error(
                        project_id,
                        user_id,
                        f"Profile {profile_id} not found",
                    )
                    continue
                db_profile.content = content
                if attribute is not None:
                    db_profile.attributes = attribute
                update_db_profiles.append(profile_id)

            # 3. delete profiles
            session.query(UserProfile).filter(
                UserProfile.id.in_(delete_profile_ids),
                UserProfile.user_id == user_id,
                UserProfile.project_id == project_id,
            ).delete(synchronize_session=False)

            session.commit()
        except Exception as e:
            TRACE_LOG.error(
                project_id,
                user_id,
                f"Error merging user profiles: {e}",
            )
            session.rollback()
            return Promise.reject(
                CODE.SERVER_PARSE_ERROR, f"Error merging user profiles: {e}"
            )

    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(IdsData(ids=add_profile_ids))



================================================
FILE: src/server/api/memobase_server/controllers/project.py
================================================
from sqlalchemy import cast, String, func, desc
from ..models.database import Project, User, UserProfile, UserEvent
from ..models.utils import Promise, CODE
from ..models.response import IdData, ProfileConfigData, ProjectUsersData, DailyUsage
from ..connectors import Session
from ..env import ProfileConfig, TelemetryKeyName
from ..telemetry.capture_key import get_int_key, date_past_key


async def get_project_secret(project_id: str) -> Promise[str]:
    with Session() as session:
        p = (
            session.query(Project)
            .filter(Project.project_id == project_id)
            .one_or_none()
        )
        if not p:
            return Promise.reject(CODE.NOT_FOUND, "Project not found")
        return Promise.resolve(p.project_secret)


async def get_project_status(project_id: str) -> Promise[str]:
    with Session() as session:
        p = (
            session.query(Project.status)
            .filter(Project.project_id == project_id)
            .one_or_none()
        )
        if not p:
            return Promise.reject(CODE.NOT_FOUND, "Project not found")
        return Promise.resolve(p.status)


async def get_project_profile_config(project_id: str) -> Promise[ProfileConfig]:
    with Session() as session:
        p = (
            session.query(Project.profile_config)
            .filter(Project.project_id == project_id)
            .one_or_none()
        )
        if not p:
            return Promise.reject(CODE.NOT_FOUND, "Project not found")
        if not p.profile_config:
            return Promise.resolve(ProfileConfig())
        p_parse = ProfileConfig.load_config_string(p.profile_config)
    return Promise.resolve(p_parse)


async def update_project_profile_config(
    project_id: str, profile_config: str | None
) -> Promise[None]:
    with Session() as session:
        p = (
            session.query(Project)
            .filter(Project.project_id == project_id)
            .one_or_none()
        )
        if not p:
            return Promise.reject(CODE.NOT_FOUND, "Project not found")
        p.profile_config = profile_config
        session.commit()
    return Promise.resolve(None)


async def get_project_profile_config_string(
    project_id: str,
) -> Promise[ProfileConfigData]:
    with Session() as session:
        p = (
            session.query(Project.profile_config)
            .filter(Project.project_id == project_id)
            .one_or_none()
        )
        if not p:
            return Promise.reject(CODE.NOT_FOUND, "Project not found")
        return Promise.resolve(ProfileConfigData(profile_config=p.profile_config or ""))


async def get_project_users(
    project_id: str,
    search: str = "",
    limit: int = 10,
    offset: int = 0,
    order_by: str = "updated_at",
    order_desc: bool = True,
) -> Promise[ProjectUsersData]:
    with Session() as session:
        profile_subq = (
            session.query(
                UserProfile.user_id.label("user_id"),
                func.count(UserProfile.id).label("profile_count"),
            )
            .filter(UserProfile.project_id == project_id)
            .group_by(UserProfile.user_id)
            .subquery()
        )

        event_subq = (
            session.query(
                UserEvent.user_id.label("user_id"),
                func.count(UserEvent.id).label("event_count"),
            )
            .filter(UserEvent.project_id == project_id)
            .group_by(UserEvent.user_id)
            .subquery()
        )

        query = (
            session.query(
                User,
                func.coalesce(profile_subq.c.profile_count, 0).label("profile_count"),
                func.coalesce(event_subq.c.event_count, 0).label("event_count"),
            )
            .filter(User.project_id == project_id)
            .filter(cast(User.id, String).like(f"%{search}%"))
            .outerjoin(profile_subq, profile_subq.c.user_id == User.id)
            .outerjoin(event_subq, event_subq.c.user_id == User.id)
        )

        if order_by == "profile_count":
            query = query.order_by(
                desc("profile_count") if order_desc else "profile_count"
            )
        elif order_by == "event_count":
            query = query.order_by(desc("event_count") if order_desc else "event_count")
        else:
            query = query.order_by(
                desc(User.updated_at) if order_desc else User.updated_at
            )

        count = (
            session.query(func.count())
            .filter(User.project_id == project_id)
            .filter(cast(User.id, String).like(f"%{search}%"))
            .scalar()
        )

        users_with_counts = query.limit(limit).offset(offset).all()

        user_dicts = []
        for user, profile_count, event_count in users_with_counts:
            user_data = user.__dict__.copy()
            user_data.pop("_sa_instance_state", None)
            user_data["profile_count"] = profile_count
            user_data["event_count"] = event_count
            user_dicts.append(user_data)

        return Promise.resolve(ProjectUsersData(users=user_dicts, count=count))


async def get_project_usage(
    project_id: str, last_days: int = 7
) -> Promise[list[DailyUsage]]:
    query_dates = [date_past_key(i) for i in range(last_days)]
    results = []
    for qd in query_dates:
        total_insert = await get_int_key(
            TelemetryKeyName.insert_blob_request, project_id, use_date=qd
        )
        total_success_insert = await get_int_key(
            TelemetryKeyName.insert_blob_success_request, project_id, use_date=qd
        )
        total_input_token = await get_int_key(
            TelemetryKeyName.llm_input_tokens, project_id, use_date=qd
        )
        total_output_token = await get_int_key(
            TelemetryKeyName.llm_output_tokens, project_id, use_date=qd
        )
        results.append(
            DailyUsage(
                date=qd,
                total_insert=total_insert,
                total_success_insert=total_success_insert,
                total_input_token=total_input_token,
                total_output_token=total_output_token,
            )
        )
    return Promise.resolve(results)



================================================
FILE: src/server/api/memobase_server/controllers/status.py
================================================
from pydantic import ValidationError
from ..models.utils import Promise
from ..models.database import UserStatus
from ..models.response import CODE, UserStatusesData, UserStatusData, IdData
from ..connectors import Session


async def get_user_statuses(
    user_id: str, project_id: str, type: str, page: int = 1, page_size: int = 10
) -> Promise[UserStatusesData]:
    with Session() as session:
        status = (
            session.query(UserStatus)
            .filter_by(user_id=user_id, project_id=project_id, type=type)
            .order_by(UserStatus.created_at.desc())
            .offset((page - 1) * page_size)
            .limit(page_size)
            .all()
        )
        if status is None:
            return Promise.resolve(UserStatusesData(statuses=[]))
        data = [
            {
                "id": s.id,
                "type": s.type,
                "attributes": s.attributes,
                "created_at": s.created_at,
                "updated_at": s.updated_at,
            }
            for s in status
        ]
        return Promise.resolve(UserStatusesData(statuses=data))


async def append_user_status(
    user_id: str, project_id: str, type: str, attributes: dict
) -> Promise[IdData]:
    with Session() as session:
        status = UserStatus(
            user_id=user_id, project_id=project_id, type=type, attributes=attributes
        )
        session.add(status)
        session.commit()
        return Promise.resolve(IdData(id=status.id))



================================================
FILE: src/server/api/memobase_server/controllers/user.py
================================================
from ..models.utils import Promise
from ..models.database import User, GeneralBlob, UserProfile
from ..models.response import CODE, UserData, IdData, IdsData, UserProfilesData
from ..connectors import Session
from .profile import refresh_user_profile_cache
from ..models.blob import BlobType


async def create_user(data: UserData, project_id: str) -> Promise[IdData]:
    with Session() as session:
        db_user = User(additional_fields=data.data, project_id=project_id)
        if data.id is not None:
            db_user.id = str(data.id)
        session.add(db_user)
        session.commit()
        return Promise.resolve(IdData(id=db_user.id))


async def get_user(user_id: str, project_id: str) -> Promise[UserData]:
    with Session() as session:
        db_user = (
            session.query(User)
            .filter_by(id=user_id, project_id=project_id)
            .one_or_none()
        )
        if db_user is None:
            return Promise.reject(CODE.NOT_FOUND, f"User {user_id} not found")
        return Promise.resolve(
            UserData(
                data=db_user.additional_fields,
                created_at=db_user.created_at,
                updated_at=db_user.updated_at,
            )
        )


async def update_user(user_id: str, project_id: str, data: dict) -> Promise[IdData]:
    with Session() as session:
        db_user = (
            session.query(User)
            .filter_by(id=user_id, project_id=project_id)
            .one_or_none()
        )
        if db_user is None:
            return Promise.reject(CODE.NOT_FOUND, f"User {user_id} not found")
        db_user.additional_fields = data
        session.commit()
        return Promise.resolve(IdData(id=db_user.id))


async def delete_user(user_id: str, project_id: str) -> Promise[None]:
    with Session() as session:
        db_user = (
            session.query(User)
            .filter_by(id=user_id, project_id=project_id)
            .one_or_none()
        )
        if db_user is None:
            return Promise.reject(CODE.NOT_FOUND, f"User {user_id} not found")
        session.delete(db_user)
        session.commit()
    await refresh_user_profile_cache(user_id, project_id)
    return Promise.resolve(None)


async def get_user_all_blobs(
    user_id: str,
    project_id: str,
    blob_type: BlobType,
    page: int = 0,
    page_size: int = 10,
) -> Promise[IdsData]:
    with Session() as session:
        user_blobs = (
            session.query(GeneralBlob.id)
            .filter_by(user_id=user_id, blob_type=str(blob_type), project_id=project_id)
            .order_by(GeneralBlob.created_at)
            .offset(page * page_size)
            .limit(page_size)
            .all()
        )
        if user_blobs is None:
            return Promise.reject(CODE.NOT_FOUND, f"User {user_id} not found")
        return Promise.resolve(IdsData(ids=[blob.id for blob in user_blobs]))



================================================
FILE: src/server/api/memobase_server/controllers/modal/__init__.py
================================================
from typing import Callable, Awaitable
from ...models.blob import BlobType, Blob
from ...models.utils import Promise
from . import chat
from . import summary

BlobProcessFunc = Callable[
    [str, str, list[Blob]],  # user_id, project_id, blobs
    Awaitable[Promise[None]],
]
BLOBS_PROCESS: dict[BlobType, BlobProcessFunc] = {
    BlobType.chat: chat.process_blobs,
    BlobType.summary: summary.process_blobs,
}



================================================
FILE: src/server/api/memobase_server/controllers/modal/utils.py
================================================
import re
import json

JSON_BODY_REGEX = re.compile(r"({[\s\S]*})")


def try_json_loads(content: str) -> dict | None:
    try:
        return json.loads(JSON_BODY_REGEX.search(content).group(1))
    except Exception:
        return None



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/__init__.py
================================================
import asyncio
from ...project import get_project_profile_config
from ....connectors import Session
from ....env import ProfileConfig, CONFIG, TRACE_LOG
from ....utils import get_blob_str, get_encoded_tokens
from ....models.blob import Blob
from ....models.utils import Promise, CODE
from ....models.response import IdsData, ChatModalResponse, UserProfilesData
from ...profile import add_update_delete_user_profiles
from ...event import append_user_event
from ...profile import get_user_profiles
from .extract import extract_topics

# from .merge import merge_or_valid_new_memos
from .merge_yolo import merge_or_valid_new_memos
from .summary import re_summary
from .organize import organize_profiles
from .types import MergeAddResult
from .event_summary import tag_event
from .entry_summary import entry_chat_summary


def truncate_chat_blobs(
    blobs: list[Blob], max_token_size: int
) -> tuple[list[str], list[Blob]]:
    results = []
    total_token_size = 0
    for b in blobs[::-1]:
        ts = len(get_encoded_tokens(get_blob_str(b)))
        total_token_size += ts
        if total_token_size <= max_token_size:
            results.append(b)
        else:
            break
    return results[::-1]


async def process_blobs(
    user_id: str, project_id: str, blobs: list[Blob]
) -> Promise[ChatModalResponse]:
    # 1. Extract patch profiles
    blobs = truncate_chat_blobs(blobs, CONFIG.max_chat_blob_buffer_process_token_size)
    if len(blobs) == 0:
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, "No blobs to process after truncating"
        )

    p = await get_project_profile_config(project_id)
    if not p.ok():
        return p
    project_profiles = p.data()

    p = await get_user_profiles(user_id, project_id)
    if not p.ok():
        return p
    current_user_profiles = p.data()

    p = await entry_chat_summary(
        user_id, project_id, blobs, project_profiles, current_user_profiles
    )
    if not p.ok():
        return p
    user_memo_str = p.data().strip()

    if not user_memo_str:
        return Promise.resolve(
            ChatModalResponse(
                event_id=None,
                add_profiles=[],
                update_profiles=[],
                delete_profiles=[],
            )
        )

    processing_results = await asyncio.gather(
        process_profile_res(
            user_id, project_id, user_memo_str, project_profiles, current_user_profiles
        ),
        process_event_res(
            user_id, project_id, user_memo_str, project_profiles, current_user_profiles
        ),
    )

    profile_results: Promise = processing_results[0]
    event_results: Promise = processing_results[1]

    if not profile_results.ok() or not event_results.ok():
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR,
            f"Failed to process profile or event: {profile_results.msg()}, {event_results.msg()}",
        )

    intermediate_profile, delta_profile_data = profile_results.data()
    event_tags = event_results.data()

    p = await handle_session_event(
        user_id,
        project_id,
        user_memo_str,
        delta_profile_data,
        event_tags,
        project_profiles,
    )
    if not p.ok():
        return p
    eid = p.data()

    p = await handle_user_profile_db(user_id, project_id, intermediate_profile)
    if not p.ok():
        return p
    return Promise.resolve(
        ChatModalResponse(
            event_id=eid,
            add_profiles=p.data().ids,
            update_profiles=[up["profile_id"] for up in intermediate_profile["update"]],
            delete_profiles=intermediate_profile["delete"],
        )
    )


async def process_profile_res(
    user_id: str,
    project_id: str,
    user_memo_str: str,
    project_profiles: ProfileConfig,
    current_user_profiles: UserProfilesData,
) -> Promise[tuple[MergeAddResult, list[dict]]]:

    p = await extract_topics(
        user_id, project_id, user_memo_str, project_profiles, current_user_profiles
    )
    if not p.ok():
        return p
    extracted_data = p.data()

    # 2. Merge it to thw whole profile
    p = await merge_or_valid_new_memos(
        user_id,
        project_id,
        fact_contents=extracted_data["fact_contents"],
        fact_attributes=extracted_data["fact_attributes"],
        profiles=extracted_data["profiles"],
        config=project_profiles,
        total_profiles=extracted_data["total_profiles"],
    )
    if not p.ok():
        return p

    intermediate_profile = p.data()
    delta_profile_data = [
        p for p in (intermediate_profile["add"] + intermediate_profile["update_delta"])
    ]

    # 3. Check if we need to organize profiles
    p = await organize_profiles(
        user_id,
        project_id,
        intermediate_profile,
        config=project_profiles,
    )
    if not p.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to organize profiles: {p.msg()}",
        )

    # 4. Re-summary profiles if any slot is too big
    p = await re_summary(
        user_id,
        project_id,
        add_profile=intermediate_profile["add"],
        update_profile=intermediate_profile["update"],
    )
    if not p.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to re-summary profiles: {p.msg()}",
        )

    return Promise.resolve((intermediate_profile, delta_profile_data))


async def process_event_res(
    user_id: str,
    project_id: str,
    memo_str: str,
    config: ProfileConfig,
    current_user_profiles: UserProfilesData,
) -> Promise[list | None]:
    p = await tag_event(project_id, config, memo_str)
    if not p.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to tag event: {p.msg()}",
        )
        return p
    event_tags = p.data()
    return Promise.resolve(event_tags)


async def handle_session_event(
    user_id: str,
    project_id: str,
    memo_str: str,
    delta_profile_data: list[dict],
    event_tags: list | None,
    config: ProfileConfig,
) -> Promise[str]:

    eid = await append_user_event(
        user_id,
        project_id,
        {
            "event_tip": memo_str,
            "event_tags": event_tags,
            "profile_delta": delta_profile_data,
        },
    )

    return eid


async def handle_user_profile_db(
    user_id: str, project_id: str, intermediate_profile: MergeAddResult
) -> Promise[IdsData]:
    TRACE_LOG.info(
        project_id,
        user_id,
        f"Adding {len(intermediate_profile['add'])}, updating {len(intermediate_profile['update'])}, deleting {len(intermediate_profile['delete'])} profiles",
    )

    p = await add_update_delete_user_profiles(
        user_id,
        project_id,
        [ap["content"] for ap in intermediate_profile["add"]],
        [ap["attributes"] for ap in intermediate_profile["add"]],
        [up["profile_id"] for up in intermediate_profile["update"]],
        [up["content"] for up in intermediate_profile["update"]],
        [up["attributes"] for up in intermediate_profile["update"]],
        intermediate_profile["delete"],
    )
    return p



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/entry_summary.py
================================================
import asyncio
from ....env import CONFIG
from ....models.utils import Promise
from ....models.blob import Blob, BlobType
from ....llms import llm_complete
from ....prompts.profile_init_utils import read_out_profile_config
from ...project import ProfileConfig
from ....prompts.profile_init_utils import read_out_event_tags
from ....prompts.utils import tag_chat_blobs_in_order_xml
from .types import FactResponse, PROMPTS
from ....models.response import UserProfilesData
from .utils import pack_current_user_profiles


async def entry_chat_summary(
    user_id: str,
    project_id: str,
    blobs: list[Blob],
    project_profiles: ProfileConfig,
    current_user_profiles: UserProfilesData,
) -> Promise[str]:
    assert all(b.type == BlobType.chat for b in blobs), "All blobs must be chat blobs"
    CURRENT_PROFILE_INFO = pack_current_user_profiles(
        current_user_profiles, project_profiles
    )

    USE_LANGUAGE = CURRENT_PROFILE_INFO["use_language"]
    project_profiles_slots = CURRENT_PROFILE_INFO["project_profile_slots"]

    prompt = PROMPTS[USE_LANGUAGE]["entry_summary"]
    event_summary_theme = (
        project_profiles.event_theme_requirement or CONFIG.event_theme_requirement
    )

    event_tags = read_out_event_tags(project_profiles)
    event_attriubtes_str = "\n".join(
        [f"- {et.name}({et.description})" for et in event_tags]
    )
    profile_topics_str = PROMPTS[USE_LANGUAGE]["profile"].get_prompt(
        project_profiles_slots
    )
    blob_strs = tag_chat_blobs_in_order_xml(blobs)
    r = await llm_complete(
        project_id,
        prompt.pack_input(CURRENT_PROFILE_INFO["already_topics_prompt"], blob_strs),
        system_prompt=prompt.get_prompt(
            profile_topics_str,
            event_attriubtes_str,
            additional_requirements=event_summary_theme,
        ),
        temperature=0.2,  # precise
        model=CONFIG.summary_llm_model,
        **prompt.get_kwargs(),
    )

    # print(
    #     prompt.pack_input(CURRENT_PROFILE_INFO["already_topics_prompt"], blob_strs),
    #     r.data(),
    # )
    return r



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/event_summary.py
================================================
from typing import Optional
from ....models.utils import Promise
from ....env import ProfileConfig, CONFIG
from ....prompts.utils import (
    parse_string_into_subtopics,
    attribute_unify,
)
from ....prompts.profile_init_utils import read_out_event_tags
from ....llms import llm_complete

from ....prompts import event_tagging as event_tagging_prompt


async def tag_event(
    project_id: str, config: ProfileConfig, event_summary: str
) -> Promise[Optional[list]]:
    event_tags = read_out_event_tags(config)
    available_event_tags = set([et.name for et in event_tags])
    if len(event_tags) == 0:
        return Promise.resolve(None)
    event_tags_str = "\n".join([f"- {et.name}({et.description})" for et in event_tags])
    r = await llm_complete(
        project_id,
        event_summary,
        system_prompt=event_tagging_prompt.get_prompt(event_tags_str),
        temperature=0.2,
        model=CONFIG.best_llm_model,
        **event_tagging_prompt.get_kwargs(),
    )
    if not r.ok():
        return r
    parsed_event_tags = parse_string_into_subtopics(r.data())
    parsed_event_tags = [
        {"tag": attribute_unify(et["sub_topic"]), "value": et["memo"]}
        for et in parsed_event_tags
    ]
    strict_parsed_event_tags = [
        et for et in parsed_event_tags if et["tag"] in available_event_tags
    ]
    return Promise.resolve(strict_parsed_event_tags)



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/extract.py
================================================
from ....env import CONFIG, ContanstTable, TRACE_LOG
from ....models.utils import Promise
from ....models.response import AIUserProfiles, CODE, UserProfilesData
from ....llms import llm_complete
from ....prompts.utils import (
    attribute_unify,
    parse_string_into_profiles,
)
from ....prompts.profile_init_utils import read_out_profile_config, UserProfileTopic
from ...project import ProfileConfig
from .types import FactResponse, PROMPTS
from .utils import pack_current_user_profiles


def merge_by_topic_sub_topics(new_facts: list[FactResponse]):
    topic_subtopic = {}
    for nf in new_facts:
        key = (nf[ContanstTable.topic], nf[ContanstTable.sub_topic])
        if key in topic_subtopic and isinstance(nf["memo"], str):
            topic_subtopic[key]["memo"] += f"; {nf['memo']}"
            continue
        topic_subtopic[key] = nf
    return list(topic_subtopic.values())


async def extract_topics(
    user_id: str,
    project_id: str,
    user_memo: str,
    project_profiles: ProfileConfig,
    current_user_profiles: UserProfilesData,
) -> Promise[dict]:

    profiles = current_user_profiles.profiles
    CURRENT_PROFILE_INFO = pack_current_user_profiles(
        current_user_profiles, project_profiles
    )
    USE_LANGUAGE = CURRENT_PROFILE_INFO["use_language"]
    STRICT_MODE = CURRENT_PROFILE_INFO["strict_mode"]

    project_profiles_slots = CURRENT_PROFILE_INFO["project_profile_slots"]

    p = await llm_complete(
        project_id,
        PROMPTS[USE_LANGUAGE]["extract"].pack_input(
            CURRENT_PROFILE_INFO["already_topics_prompt"],
            user_memo,
            strict_mode=STRICT_MODE,
        ),
        system_prompt=PROMPTS[USE_LANGUAGE]["extract"].get_prompt(
            PROMPTS[USE_LANGUAGE]["profile"].get_prompt(project_profiles_slots)
        ),
        temperature=0.2,  # precise
        **PROMPTS[USE_LANGUAGE]["extract"].get_kwargs(),
    )
    if not p.ok():
        return p
    results = p.data()
    # print(
    #     PROMPTS[USE_LANGUAGE]["extract"].pack_input(
    #         CURRENT_PROFILE_INFO["already_topics_prompt"],
    #         user_memo,
    #         strict_mode=STRICT_MODE,
    #     )
    # )
    # print("-------------------------------")
    # print(results)
    parsed_facts: AIUserProfiles = parse_string_into_profiles(results)
    new_facts: list[FactResponse] = parsed_facts.model_dump()["facts"]
    if not len(new_facts):
        TRACE_LOG.info(
            project_id,
            user_id,
            f"No new facts extracted",
        )
        return Promise.resolve(
            {
                "fact_contents": [],
                "fact_attributes": [],
                "profiles": profiles,
                "total_profiles": project_profiles_slots,
            }
        )

    for nf in new_facts:
        nf[ContanstTable.topic] = attribute_unify(nf[ContanstTable.topic])
        nf[ContanstTable.sub_topic] = attribute_unify(nf[ContanstTable.sub_topic])
    new_facts = merge_by_topic_sub_topics(new_facts)

    fact_contents = []
    fact_attributes = []

    for nf in new_facts:
        if CURRENT_PROFILE_INFO["allowed_topic_subtopics"] is not None:
            if (
                nf[ContanstTable.topic],
                nf[ContanstTable.sub_topic],
            ) not in CURRENT_PROFILE_INFO["allowed_topic_subtopics"]:
                continue
        fact_contents.append(nf["memo"])
        fact_attributes.append(
            {
                ContanstTable.topic: nf[ContanstTable.topic],
                ContanstTable.sub_topic: nf[ContanstTable.sub_topic],
            }
        )
    return Promise.resolve(
        {
            "fact_contents": fact_contents,
            "fact_attributes": fact_attributes,
            "profiles": profiles,
            "total_profiles": project_profiles_slots,
        }
    )



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/merge.py
================================================
import asyncio
from ....env import CONFIG, TRACE_LOG
from ....models.utils import Promise, CODE
from ....models.response import ProfileData
from ....env import ProfileConfig, ContanstTable
from ....llms import llm_complete
from ....prompts.utils import (
    parse_string_into_merge_action,
)
from ....prompts.profile_init_utils import UserProfileTopic
from ....types import SubTopic
from .types import UpdateResponse, PROMPTS, AddProfile, UpdateProfile, MergeAddResult


async def merge_or_valid_new_memos(
    user_id: str,
    project_id: str,
    fact_contents: list[str],
    fact_attributes: list[dict],
    profiles: list[ProfileData],
    config: ProfileConfig,
    total_profiles: list[UserProfileTopic],
) -> Promise[MergeAddResult]:
    assert len(fact_contents) == len(
        fact_attributes
    ), "Length of fact_contents and fact_attributes must be equal"
    DEFINE_MAPS = {
        (p.topic, sp.name): sp for p in total_profiles for sp in p.sub_topics
    }
    RUNTIME_MAPS = {
        (p.attributes[ContanstTable.topic], p.attributes[ContanstTable.sub_topic]): p
        for p in profiles
    }

    profile_session_results: MergeAddResult = {
        "add": [],
        "update": [],
        "delete": [],
        "update_delta": [],
        "before_profiles": profiles,
    }
    tasks = []
    for f_c, f_a in zip(fact_contents, fact_attributes):
        task = handle_profile_merge_or_valid(
            user_id,
            project_id,
            f_a,
            f_c,
            config,
            RUNTIME_MAPS,
            DEFINE_MAPS,
            profile_session_results,
        )
        tasks.append(task)
    await asyncio.gather(*tasks)
    return Promise.resolve(profile_session_results)


async def handle_profile_merge_or_valid(
    user_id: str,
    project_id: str,
    profile_attributes: dict,
    profile_content: str,
    config: ProfileConfig,
    profile_runtime_maps: dict[tuple[str, str], ProfileData],
    profile_define_maps: dict[tuple[str, str], SubTopic],
    session_merge_validate_results: MergeAddResult,
) -> Promise[None]:
    KEY = (
        profile_attributes[ContanstTable.topic],
        profile_attributes[ContanstTable.sub_topic],
    )
    USE_LANGUAGE = config.language or CONFIG.language
    PROFILE_VALIDATE_MODE = (
        config.profile_validate_mode
        if config.profile_validate_mode is not None
        else CONFIG.profile_validate_mode
    )
    runtime_profile = profile_runtime_maps.get(KEY, None)
    define_sub_topic = profile_define_maps.get(KEY, SubTopic(name=""))

    if (
        not PROFILE_VALIDATE_MODE
        and not define_sub_topic.validate_value
        and runtime_profile is None
    ):
        TRACE_LOG.info(
            project_id,
            user_id,
            f"Skip validation: {KEY}",
        )
        session_merge_validate_results["add"].append(
            {
                "content": profile_content,
                "attributes": profile_attributes,
            }
        )
        return Promise.resolve(None)
    r = await llm_complete(
        project_id,
        PROMPTS[USE_LANGUAGE]["merge"].get_input(
            KEY[0],
            KEY[1],
            runtime_profile.content if runtime_profile else None,
            profile_content,
            update_instruction=define_sub_topic.update_description,  # maybe none
            topic_description=define_sub_topic.description,  # maybe none
        ),
        system_prompt=PROMPTS[USE_LANGUAGE]["merge"].get_prompt(),
        temperature=0.2,  # precise
        **PROMPTS[USE_LANGUAGE]["merge"].get_kwargs(),
    )
    # print(KEY, profile_content)
    # print(r.data())
    if not r.ok():
        TRACE_LOG.warning(
            project_id,
            user_id,
            f"Failed to merge profiles: {r.msg()}",
        )
        return r
    update_response: UpdateResponse | None = parse_string_into_merge_action(r.data())
    if update_response is None:
        TRACE_LOG.warning(
            project_id,
            user_id,
            f"Failed to parse merge action: {r.data()}",
        )
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, "Failed to parse merge action of Memobase"
        )
    if update_response["action"] == "UPDATE":
        if runtime_profile is None:
            session_merge_validate_results["add"].append(
                {
                    "content": update_response["memo"],
                    "attributes": profile_attributes,
                }
            )
        else:
            if ContanstTable.update_hits not in runtime_profile.attributes:
                runtime_profile.attributes[ContanstTable.update_hits] = 1
            else:
                runtime_profile.attributes[ContanstTable.update_hits] += 1
            session_merge_validate_results["update"].append(
                {
                    "profile_id": runtime_profile.id,
                    "content": update_response["memo"],
                    "attributes": runtime_profile.attributes,
                }
            )
            session_merge_validate_results["update_delta"].append(
                {
                    "content": profile_content,
                    "attributes": profile_attributes,
                }
            )
    elif update_response["action"] == "APPEND":
        if runtime_profile is None:
            session_merge_validate_results["add"].append(
                {
                    "content": profile_content,
                    "attributes": profile_attributes,
                }
            )
        else:
            if ContanstTable.update_hits not in runtime_profile.attributes:
                runtime_profile.attributes[ContanstTable.update_hits] = 1
            else:
                runtime_profile.attributes[ContanstTable.update_hits] += 1
            session_merge_validate_results["update"].append(
                {
                    "profile_id": runtime_profile.id,
                    "content": f"{runtime_profile.content};{profile_content}",
                    "attributes": runtime_profile.attributes,
                }
            )
            session_merge_validate_results["update_delta"].append(
                {
                    "content": profile_content,
                    "attributes": profile_attributes,
                }
            )
    elif update_response["action"] == "ABORT":
        oneline_response = r.data().replace("\n", " ")
        if runtime_profile is None:
            TRACE_LOG.info(
                project_id,
                user_id,
                f"Invalid profile: {KEY}::{profile_content}. <raw_response> {oneline_response} </raw_response>",
            )
        else:
            TRACE_LOG.info(
                project_id,
                user_id,
                f"Invalid merge: {runtime_profile.attributes}, {profile_content}. <raw_response> {oneline_response} </raw_response>",
            )
            # session_merge_validate_results["delete"].append(runtime_profile.id)
        return Promise.resolve(None)
    else:
        TRACE_LOG.warning(
            project_id,
            user_id,
            f"Invalid action: {update_response['action']}",
        )
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, "Failed to parse merge action of Memobase"
        )
    return Promise.resolve(None)



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/merge_yolo.py
================================================
from ....env import CONFIG, TRACE_LOG
from ....models.utils import Promise, CODE
from ....models.response import ProfileData
from ....env import ProfileConfig, ContanstTable
from ....llms import llm_complete
from ....prompts.utils import (
    parse_string_into_merge_yolo_action,
)
from ....prompts.profile_init_utils import UserProfileTopic
from ....types import SubTopic
from .types import UpdateResponse, PROMPTS, AddProfile, UpdateProfile, MergeAddResult


async def merge_or_valid_new_memos(
    user_id: str,
    project_id: str,
    fact_contents: list[str],
    fact_attributes: list[dict],
    profiles: list[ProfileData],
    config: ProfileConfig,
    total_profiles: list[UserProfileTopic],
) -> Promise[MergeAddResult]:
    assert len(fact_contents) == len(
        fact_attributes
    ), "Length of fact_contents and fact_attributes must be equal"
    DEFINE_MAPS = {
        (p.topic, sp.name): sp for p in total_profiles for sp in p.sub_topics
    }
    RUNTIME_MAPS = {
        (p.attributes[ContanstTable.topic], p.attributes[ContanstTable.sub_topic]): p
        for p in profiles
    }
    USE_LANGUAGE = config.language or CONFIG.language
    PROFILE_VALIDATE_MODE = (
        config.profile_validate_mode
        if config.profile_validate_mode is not None
        else CONFIG.profile_validate_mode
    )

    profile_session_results: MergeAddResult = {
        "add": [],
        "update": [],
        "delete": [],
        "update_delta": [],
        "before_profiles": profiles,
    }
    tasks = []
    new_memos = []
    for f_c, f_a in zip(fact_contents, fact_attributes):
        KEY = (
            f_a[ContanstTable.topic],
            f_a[ContanstTable.sub_topic],
        )
        runtime_profile = RUNTIME_MAPS.get(KEY, None)
        define_sub_topic = DEFINE_MAPS.get(KEY, SubTopic(name=""))
        if (
            not PROFILE_VALIDATE_MODE
            and not define_sub_topic.validate_value
            and runtime_profile is None
        ):
            TRACE_LOG.info(
                project_id,
                user_id,
                f"Skip validation: {KEY}",
            )
            profile_session_results["add"].append(
                {
                    "content": f_c,
                    "attributes": f_a,
                }
            )
            continue
        new_memos.append(
            (
                {
                    "new_info": f_c,
                    "current_memo": runtime_profile.content if runtime_profile else "",
                    "topic": KEY[0],
                    "subtopic": KEY[1],
                    "topic_description": define_sub_topic.description,
                    "update_instruction": define_sub_topic.update_description,
                },
                f_c,
                f_a,
            )
        )
    new_memos_input = [{"memo_id": i + 1, **m[0]} for i, m in enumerate(new_memos)]
    r = await llm_complete(
        project_id,
        PROMPTS[USE_LANGUAGE]["merge_yolo"].get_input(new_memos_input),
        system_prompt=PROMPTS[USE_LANGUAGE]["merge_yolo"].get_prompt(),
        temperature=0.2,  # precise
        **PROMPTS[USE_LANGUAGE]["merge_yolo"].get_kwargs(),
    )
    oneline_response = r.data().replace("\n", "<br/>")
    if not r.ok():
        TRACE_LOG.warning(
            project_id,
            user_id,
            f"Failed to merge profiles: {r.msg()}",
        )
        return r
    memo_actions = parse_string_into_merge_yolo_action(r.data())

    abort_infos = []
    for i, m in enumerate(new_memos):
        update_response = memo_actions.get(i + 1, None)
        if update_response is None:
            TRACE_LOG.warning(
                project_id,
                user_id,
                f"No Corresponding Merge Action: {new_memos_input[i]}, <raw_response> {oneline_response} </raw_response>",
            )
            continue
        f_c, f_a = m[1], m[2]
        KEY = (f_a[ContanstTable.topic], f_a[ContanstTable.sub_topic])
        runtime_profile = RUNTIME_MAPS.get(KEY, None)
        if update_response["action"] == "UPDATE":
            if runtime_profile is None:
                profile_session_results["add"].append(
                    {
                        "content": update_response["memo"],
                        "attributes": f_a,
                    }
                )
            else:
                if ContanstTable.update_hits not in runtime_profile.attributes:
                    runtime_profile.attributes[ContanstTable.update_hits] = 1
                else:
                    runtime_profile.attributes[ContanstTable.update_hits] += 1
                profile_session_results["update"].append(
                    {
                        "profile_id": runtime_profile.id,
                        "content": update_response["memo"],
                        "attributes": runtime_profile.attributes,
                    }
                )
                profile_session_results["update_delta"].append(
                    {
                        "content": f_c,
                        "attributes": f_a,
                    }
                )
        elif update_response["action"] == "APPEND":
            if runtime_profile is None:
                profile_session_results["add"].append(
                    {
                        "content": f_c,
                        "attributes": f_a,
                    }
                )
            else:
                if ContanstTable.update_hits not in runtime_profile.attributes:
                    runtime_profile.attributes[ContanstTable.update_hits] = 1
                else:
                    runtime_profile.attributes[ContanstTable.update_hits] += 1
                profile_session_results["update"].append(
                    {
                        "profile_id": runtime_profile.id,
                        "content": f"{runtime_profile.content};{f_c}",
                        "attributes": runtime_profile.attributes,
                    }
                )
                profile_session_results["update_delta"].append(
                    {
                        "content": f_c,
                        "attributes": f_a,
                    }
                )
        elif update_response["action"] == "ABORT":
            abort_infos.append(new_memos_input[i])
        else:
            TRACE_LOG.warning(
                project_id,
                user_id,
                f"Unkown merge action: {update_response['action']}",
            )
            continue

    if len(abort_infos):
        TRACE_LOG.info(
            project_id,
            user_id,
            f"Invalid merge: {abort_infos}. <raw_response> {oneline_response} </raw_response>",
        )
    return Promise.resolve(profile_session_results)



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/organize.py
================================================
import asyncio
from collections import defaultdict
from .types import MergeAddResult, PROMPTS, AddProfile
from ....prompts.profile_init_utils import get_specific_subtopics
from ....prompts.utils import parse_string_into_subtopics, attribute_unify
from ....models.utils import Promise
from ....models.response import ProfileData
from ....env import CONFIG, TRACE_LOG, ProfileConfig, ContanstTable
from ....llms import llm_complete


async def organize_profiles(
    user_id: str,
    project_id: str,
    profile_options: MergeAddResult,
    config: ProfileConfig,
) -> Promise[None]:
    profiles = profile_options["before_profiles"]
    USE_LANGUAGE = config.language or CONFIG.language
    STRICT_MODE = (
        config.profile_strict_mode
        if config.profile_strict_mode is not None
        else CONFIG.profile_strict_mode
    )
    topic_groups = defaultdict(list)
    for p in profiles:
        topic_groups[p.attributes[ContanstTable.topic]].append(p)

    need_to_organize_topics: dict[str, list[ProfileData]] = {}
    for topic, group in topic_groups.items():
        if len(group) > CONFIG.max_profile_subtopics:
            need_to_organize_topics[topic] = group

    if not len(need_to_organize_topics):
        return Promise.resolve(None)
    ps = await asyncio.gather(
        *[
            organize_profiles_by_topic(user_id, project_id, group, USE_LANGUAGE)
            for group in need_to_organize_topics.values()
        ]
    )
    if not all([p.ok() for p in ps]):
        errmsg = "\n".join([p.msg() for p in ps if not p.ok()])
        return Promise.reject(f"Failed to organize profiles: {errmsg}")

    delete_profile_ids = []
    for gs in need_to_organize_topics.values():
        delete_profile_ids.extend([p.id for p in gs])
    new_profiles = []
    for p in ps:
        new_profiles.extend(p.data())

    profile_options["add"].extend(new_profiles)
    profile_options["add"] = deduplicate_profiles(profile_options["add"])
    profile_options["delete"].extend(delete_profile_ids)
    return Promise.resolve(None)


async def organize_profiles_by_topic(
    user_id: str,
    project_id: str,
    profiles: list[ProfileData],
    USE_LANGUAGE: str,  # profiles in the same topics
) -> Promise[list[AddProfile]]:
    assert (
        len(profiles) > CONFIG.max_profile_subtopics
    ), f"Unknown Error,{len(profiles)} is not greater than max_profile_subtopics: {CONFIG.max_profile_subtopics}"
    assert all(
        p.attributes[ContanstTable.topic] == profiles[0].attributes[ContanstTable.topic]
        for p in profiles
    ), f"Unknown Error, all profiles are not in the same topic: {profiles[0].attributes['topic']}"
    TRACE_LOG.info(
        project_id,
        user_id,
        f"Organizing profiles for topic: {profiles[0].attributes['topic']} with sub_topics {len(profiles)}",
    )
    topic = attribute_unify(profiles[0].attributes[ContanstTable.topic])
    suggest_subtopics = get_specific_subtopics(
        topic, PROMPTS[USE_LANGUAGE]["profile"].CANDIDATE_PROFILE_TOPICS
    )

    llm_inputs = "\n".join(
        [
            f"- {p.attributes['sub_topic']}{CONFIG.llm_tab_separator}{p.content}"
            for p in profiles
        ]
    )
    llm_prompt = f"""topic: {topic}
{llm_inputs}
"""
    p = await llm_complete(
        project_id,
        llm_prompt,
        PROMPTS[USE_LANGUAGE]["organize"].get_prompt(
            CONFIG.max_profile_subtopics // 2 + 1, suggest_subtopics
        ),
        temperature=0.2,  # precise
        **PROMPTS[USE_LANGUAGE]["organize"].get_kwargs(),
    )
    if not p.ok():
        return p
    results = p.data()
    subtopics = parse_string_into_subtopics(results)
    reorganized_profiles: list[AddProfile] = [
        {
            "content": sp["memo"],
            "attributes": {
                ContanstTable.topic: topic,
                ContanstTable.sub_topic: sp[ContanstTable.sub_topic],
            },
        }
        for sp in subtopics
    ]
    if len(reorganized_profiles) == 0:
        return Promise.reject(
            "Failed to organize profiles, left profiles is 0 so maybe it's the LLM error"
        )
    # forcing the number of subtopics to be less than max_profile_subtopics // 2 + 1
    reorganized_profiles = reorganized_profiles[: CONFIG.max_profile_subtopics // 2 + 1]
    return Promise.resolve(reorganized_profiles)


def deduplicate_profiles(profiles: list[AddProfile]) -> list[AddProfile]:
    topic_subtopic = {}
    for nf in profiles:
        key = (
            nf["attributes"][ContanstTable.topic],
            nf["attributes"][ContanstTable.sub_topic],
        )
        if key in topic_subtopic:
            topic_subtopic[key]["content"] += f"; {nf['content']}"
            continue
        topic_subtopic[key] = nf
    return list(topic_subtopic.values())



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/summary.py
================================================
import asyncio
from ....models.utils import Promise
from ....env import CONFIG, TRACE_LOG
from ....utils import get_blob_str, get_encoded_tokens, truncate_string
from ....llms import llm_complete
from ....prompts import (
    summary_profile,
)
from .types import UpdateProfile, AddProfile


async def re_summary(
    user_id: str,
    project_id: str,
    add_profile: list[AddProfile],
    update_profile: list[UpdateProfile],
) -> Promise[None]:
    add_tasks = [summary_memo(user_id, project_id, ap) for ap in add_profile]
    await asyncio.gather(*add_tasks)
    update_tasks = [summary_memo(user_id, project_id, up) for up in update_profile]
    ps = await asyncio.gather(*update_tasks)
    if not all([p.ok() for p in ps]):
        return Promise.reject("Failed to re-summary profiles")
    return Promise.resolve(None)


async def summary_memo(
    user_id: str, project_id: str, content_pack: dict
) -> Promise[None]:
    content = content_pack["content"]
    if len(get_encoded_tokens(content)) <= CONFIG.max_pre_profile_token_size:
        return Promise.resolve(None)
    r = await llm_complete(
        project_id,
        content_pack["content"],
        system_prompt=summary_profile.get_prompt(),
        temperature=0.2,  # precise
        model=CONFIG.summary_llm_model,
        **summary_profile.get_kwargs(),
    )
    if not r.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to summary memo: {r.msg()}",
        )
        return r
    content_pack["content"] = truncate_string(
        r.data(), CONFIG.max_pre_profile_token_size // 2
    )
    return Promise.resolve(None)



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/types.py
================================================
from typing import TypedDict
from ....prompts import (
    user_profile_topics,
    extract_profile,
    merge_profile,
    merge_profile_yolo,
    organize_profile,
    summary_entry_chats,
    zh_user_profile_topics,
    zh_extract_profile,
    zh_merge_profile,
    zh_summary_entry_chats,
    zh_merge_profile_yolo,
)
from ....models.response import ProfileData

FactResponse = TypedDict("Facts", {"topic": str, "sub_topic": str, "memo": str})
UpdateResponse = TypedDict("Facts", {"action": str, "memo": str})

Attributes = TypedDict("Attributes", {"topic": str, "sub_topic": str})
AddProfile = TypedDict("AddProfile", {"content": str, "attributes": Attributes})
UpdateProfile = TypedDict(
    "UpdateProfile",
    {"profile_id": str, "content": str, "attributes": Attributes},
)

MergeAddResult = TypedDict(
    "MergeAddResult",
    {
        "add": list[AddProfile],
        "update": list[UpdateProfile],
        "delete": list[str],
        "before_profiles": list[ProfileData],
        "update_delta": list[AddProfile],
    },
)

PROMPTS = {
    "en": {
        "entry_summary": summary_entry_chats,
        "profile": user_profile_topics,
        "extract": extract_profile,
        "merge": merge_profile,
        "merge_yolo": merge_profile_yolo,
        "organize": organize_profile,
    },
    "zh": {
        "entry_summary": zh_summary_entry_chats,
        "profile": zh_user_profile_topics,
        "extract": zh_extract_profile,
        "merge": zh_merge_profile,
        "merge_yolo": zh_merge_profile_yolo,
        "organize": organize_profile,
    },
}



================================================
FILE: src/server/api/memobase_server/controllers/modal/chat/utils.py
================================================
from typing import TypedDict
from ....env import CONFIG
from ....models.response import UserProfilesData
from ...project import ProfileConfig
from ....prompts.profile_init_utils import read_out_profile_config
from .types import PROMPTS
from ....types import UserProfileTopic
from ....env import ContanstTable
from ....utils import truncate_string
from ....prompts.utils import attribute_unify


class PackCurrentUserProfilesResult(TypedDict):
    already_topics_prompt: str
    allowed_topic_subtopics: set[tuple[str, str]]
    already_topic_subtopics_values: dict[tuple[str, str], str]
    project_profile_slots: list[UserProfileTopic]
    use_language: str
    strict_mode: bool


def pack_current_user_profiles(
    current_user_profiles: UserProfilesData, project_profiles: ProfileConfig
) -> PackCurrentUserProfilesResult:
    profiles = current_user_profiles.profiles
    USE_LANGUAGE = project_profiles.language or CONFIG.language
    STRICT_MODE = (
        project_profiles.profile_strict_mode
        if project_profiles.profile_strict_mode is not None
        else CONFIG.profile_strict_mode
    )

    project_profiles_slots = read_out_profile_config(
        project_profiles, PROMPTS[USE_LANGUAGE]["profile"].CANDIDATE_PROFILE_TOPICS
    )
    if STRICT_MODE:
        allowed_topic_subtopics = set()
        for p in project_profiles_slots:
            for st in p.sub_topics:
                allowed_topic_subtopics.add(
                    (attribute_unify(p.topic), attribute_unify(st["name"]))
                )
    else:
        allowed_topic_subtopics = None

    if len(profiles):
        already_topics_subtopics = set(
            [
                (
                    attribute_unify(p.attributes[ContanstTable.topic]),
                    attribute_unify(p.attributes[ContanstTable.sub_topic]),
                )
                for p in profiles
            ]
        )
        already_topic_subtopics_values = {
            (
                attribute_unify(p.attributes[ContanstTable.topic]),
                attribute_unify(p.attributes[ContanstTable.sub_topic]),
            ): p.content
            for p in profiles
        }
        if STRICT_MODE:
            already_topics_subtopics = already_topics_subtopics.intersection(
                allowed_topic_subtopics
            )
            already_topic_subtopics_values = {
                k: already_topic_subtopics_values[k] for k in already_topics_subtopics
            }
        already_topics_subtopics = sorted(already_topics_subtopics)
        already_topics_prompt = "\n".join(
            [
                f"- {topic}{CONFIG.llm_tab_separator}{sub_topic}{CONFIG.llm_tab_separator}{truncate_string(already_topic_subtopics_values[(topic, sub_topic)], 5)}"
                for topic, sub_topic in already_topics_subtopics
            ]
        )
    else:
        already_topics_prompt = ""
        already_topic_subtopics_values = {}

    return {
        "already_topics_prompt": already_topics_prompt,
        "allowed_topic_subtopics": allowed_topic_subtopics,
        "already_topic_subtopics_values": already_topic_subtopics_values,
        "project_profile_slots": project_profiles_slots,
        "use_language": USE_LANGUAGE,
        "strict_mode": STRICT_MODE,
    }



================================================
FILE: src/server/api/memobase_server/controllers/modal/roleplay/detect_interest.py
================================================
from ....models.utils import Promise, CODE
from ....env import CONFIG, LOG, ProfileConfig
from ....utils import get_encoded_tokens, truncate_string
from ....llms import llm_complete
from ....models.blob import OpenAICompatibleMessage
from .types import PROMPTS, ChatInterest
from ..utils import try_json_loads


async def detect_chat_interest(
    project_id: str,
    messages: list[OpenAICompatibleMessage],
    profile_config: ProfileConfig,
) -> Promise[ChatInterest]:
    USE_LANGUAGE = "zh"
    prompt = PROMPTS[USE_LANGUAGE]["detect_interest"]

    r = await llm_complete(
        project_id,
        prompt.get_input(messages),
        system_prompt=prompt.get_prompt(),
        temperature=0.2,  # precise
        model=CONFIG.best_llm_model,
        **prompt.get_kwargs(),
    )
    if not r.ok():
        return r
    content = r.data()
    data = try_json_loads(content)
    print(data)
    if data is None:
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR, "Unable to parse the LLM json response"
        )
    return Promise.resolve(data)



================================================
FILE: src/server/api/memobase_server/controllers/modal/roleplay/predict_new_topics.py
================================================
import re
from ....models.utils import Promise, CODE
from ....env import CONFIG, LOG, ProfileConfig
from ....utils import get_encoded_tokens, truncate_string
from ....llms import llm_complete
from ....models.blob import OpenAICompatibleMessage
from ....models.response import UserStatusesData
from .types import PROMPTS, InferPlot


def extract_plot_output(content: str):
    themes = re.search(r"<themes>(.*?)</themes>", content, re.DOTALL)
    overview = re.search(r"<overview>(.*?)</overview>", content, re.DOTALL)
    timeline = re.search(r"<timeline>(.*?)</timeline>", content, re.DOTALL)
    return (
        themes.group(1).strip() if themes else None,
        overview.group(1).strip() if overview else None,
        timeline.group(1).strip() if timeline else None,
    )


async def predict_new_topics(
    project_id: str,
    messages: list[OpenAICompatibleMessage],
    latest_statuses: UserStatusesData,
    user_context: str,
    agent_context: str,
    profile_config: ProfileConfig,
    max_before_old_topics: int = 5,
) -> Promise[InferPlot]:
    USE_LANGUAGE = "zh"
    prompt = PROMPTS[USE_LANGUAGE]["infer_plot"]

    latest_plots = [
        ld.attributes["new_topic"]["overview"]
        for ld in latest_statuses.statuses
        if "new_topic" in ld.attributes
    ][:max_before_old_topics]
    print(
        "THINK",
        prompt.get_input(agent_context, user_context, latest_plots, messages),
    )
    r = await llm_complete(
        project_id,
        prompt.get_input(agent_context, user_context, latest_plots, messages),
        system_prompt=prompt.get_prompt(),
        temperature=0.2,  # precise
        model=CONFIG.thinking_llm_model,
        **prompt.get_kwargs(),
        no_cache=True,
        # thinking_enabled=True,
    )
    if not r.ok():
        return r
    content = r.data()
    print(content)
    themes, overview, timeline = extract_plot_output(content)
    return Promise.resolve(dict(themes=themes, overview=overview, timeline=timeline))



================================================
FILE: src/server/api/memobase_server/controllers/modal/roleplay/proactive_topics.py
================================================
from ....env import ContanstTable, CONFIG, LOG
from ...status import append_user_status, get_user_statuses
from ...profile import get_user_profiles, truncate_profiles
from ...project import get_project_profile_config
from ....models.blob import OpenAICompatibleMessage
from ....models.utils import Promise
from ....models.response import ProactiveTopicData
from ...profile import get_user_profiles, truncate_profiles
from .detect_interest import detect_chat_interest
from .predict_new_topics import predict_new_topics

# from .types import


def pack_timeline_prompt(timeline: str, language: str) -> str:
    if language == "zh":
        return f"## 下面是你的剧本，如果我没有主动提供话题的话，参考下面剧情推动我们的对话：\n{timeline}##"
    else:
        return f"## Here is your script, if I don't provide a topic, please refer to the following plot to drive our conversation: \n{timeline}##"


async def process_messages(
    user_id: str,
    project_id: str,
    messages: list[OpenAICompatibleMessage],
    agent_context: str = None,
    prefer_topics: list[str] = None,
    topk: int = None,
    max_token_size: int = None,
    only_topics: list[str] = None,
    max_subtopic_size: int = None,
    topic_limits: dict[str, int] = None,
) -> Promise[ProactiveTopicData]:
    p = await get_project_profile_config(project_id)
    if not p.ok():
        return p
    project_profiles = p.data()
    USE_LANGUAGE = "zh"
    # USE_LANGUAGE = project_profiles.language or CONFIG.language

    interest = await detect_chat_interest(
        project_id,
        messages,
        profile_config=project_profiles,
    )
    if not interest.ok():
        return interest
    interest_data = interest.data()
    if interest_data["action"] != "new_topic":
        await append_user_status(
            user_id,
            project_id,
            ContanstTable.roleplay_plot_status,
            {
                "interest": interest_data,
            },
        )
        return Promise.resolve(ProactiveTopicData(action="continue"))
    latests_statuses = await get_user_statuses(
        user_id, project_id, type=ContanstTable.roleplay_plot_status
    )
    if not latests_statuses.ok():
        return latests_statuses
    latests_statuses_data = latests_statuses.data()

    p = await get_user_profiles(user_id, project_id)
    if not p.ok():
        return p
    p = await truncate_profiles(
        p.data(),
        prefer_topics=prefer_topics,
        topk=topk,
        max_token_size=max_token_size,
        only_topics=only_topics,
        max_subtopic_size=max_subtopic_size,
        topic_limits=topic_limits,
    )
    if not p.ok():
        return p
    user_profiles_data = p.data()
    use_user_profiles = user_profiles_data.profiles
    user_context = "\n".join(
        [
            f"{p.attributes.get('topic')}::{p.attributes.get('sub_topic')}: {p.content}"
            for p in use_user_profiles
        ]
    )

    p = await predict_new_topics(
        project_id,
        messages,
        latests_statuses_data,
        user_context,
        agent_context,
        project_profiles,
    )
    if not p.ok():
        return p
    plot = p.data()
    await append_user_status(
        user_id,
        project_id,
        ContanstTable.roleplay_plot_status,
        {
            "interest": interest_data,
            "new_topic": plot,
            "chats": [m.model_dump() for m in messages],
        },
    )

    return Promise.resolve(
        ProactiveTopicData(
            action="new_topic",
            topic_prompt=pack_timeline_prompt(plot["timeline"], USE_LANGUAGE),
        )
    )



================================================
FILE: src/server/api/memobase_server/controllers/modal/roleplay/types.py
================================================
from typing import TypedDict
from ....prompts.roleplay import zh_detect_interest, zh_infer_plot

ChatInterest = TypedDict("ChatInterest", {"status": str, "action": str})

InferPlot = TypedDict(
    "InferPlot", {"themes": str | None, "overview": str | None, "timeline": str | None}
)

PROMPTS = {
    "en": {},
    "zh": {
        "detect_interest": zh_detect_interest,
        "infer_plot": zh_infer_plot,
    },
}



================================================
FILE: src/server/api/memobase_server/controllers/modal/summary/__init__.py
================================================
import asyncio

from sqlalchemy.sql.functions import user
from ...project import get_project_profile_config
from ...profile import get_user_profiles
from ....models.blob import Blob, SummaryBlob
from ....utils import get_blob_str, get_encoded_tokens
from ....models.blob import Blob
from ....models.utils import Promise, CODE
from ....models.response import IdsData, ChatModalResponse
from ..chat import (
    process_profile_res,
    process_event_res,
    handle_session_event,
    handle_user_profile_db,
)


def pack_summary(blobs: list[Blob]) -> str:
    return "\n".join([get_blob_str(b) for b in blobs])


async def process_blobs(user_id: str, project_id: str, blobs: list[Blob]):
    if len(blobs) == 0:
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR, "No blobs to process after truncating"
        )

    p = await get_project_profile_config(project_id)
    if not p.ok():
        return p
    project_profiles = p.data()

    p = await get_user_profiles(user_id, project_id)
    if not p.ok():
        return p
    current_user_profiles = p.data()

    user_memo_str = pack_summary(blobs)

    processing_results = await asyncio.gather(
        process_profile_res(
            user_id, project_id, user_memo_str, project_profiles, current_user_profiles
        ),
        process_event_res(
            user_id, project_id, user_memo_str, project_profiles, current_user_profiles
        ),
    )

    profile_results: Promise = processing_results[0]
    event_results: Promise = processing_results[1]

    if not profile_results.ok() or not event_results.ok():
        return Promise.reject(
            CODE.SERVER_PARSE_ERROR,
            f"Failed to process profile or event: {profile_results.msg()}, {event_results.msg()}",
        )

    intermediate_profile, delta_profile_data = profile_results.data()
    event_tags = event_results.data()

    p = await handle_session_event(
        user_id,
        project_id,
        user_memo_str,
        delta_profile_data,
        event_tags,
        project_profiles,
    )
    if not p.ok():
        return p
    eid = p.data()

    p = await handle_user_profile_db(user_id, project_id, intermediate_profile)
    if not p.ok():
        return p
    return Promise.resolve(
        ChatModalResponse(
            event_id=eid,
            add_profiles=p.data().ids,
            update_profiles=[up["profile_id"] for up in intermediate_profile["update"]],
            delete_profiles=intermediate_profile["delete"],
        )
    )



================================================
FILE: src/server/api/memobase_server/controllers/post_process/profile.py
================================================
import json
import re
from pydantic import ValidationError
from typing import TypedDict
from ...models.utils import Promise
from ...models.database import GeneralBlob, UserProfile
from ...models.blob import OpenAICompatibleMessage
from ...models.response import CODE, IdData, IdsData, UserProfilesData
from ...utils import truncate_string, find_list_int_or_none
from ...env import TRACE_LOG, CONFIG
from ...prompts import pick_related_profiles as pick_prompt
from ...llms import llm_complete


class FilterProfilesResult(TypedDict):
    reason: str | None
    profiles: list[UserProfile]


JSON_BODY_REGEX = re.compile(r"({[\s\S]*})")


def try_json_reason(content: str) -> str | None:
    try:
        return json.loads(JSON_BODY_REGEX.search(content).group(1))["reason"]
    except Exception:
        return None


async def filter_profiles_with_chats(
    user_id: str,
    project_id: str,
    profiles: UserProfilesData,
    chats: list[OpenAICompatibleMessage],
    only_topics: list[str] | None = None,
    max_value_token_size: int = 10,
    max_previous_chats: int = 4,
    max_filter_num: int = 10,
) -> Promise[FilterProfilesResult]:
    """Filter profiles with chats"""
    if not len(chats) or not len(profiles.profiles):
        return Promise.reject(CODE.BAD_REQUEST, "No chats or profiles to filter")
    chats = chats[-(max_previous_chats + 1) :]
    if only_topics:
        only_topics = [t.strip() for t in only_topics]
        only_topics = set(only_topics)

    topics_index = [
        {
            "index": i,
            "topic": p.attributes["topic"],
            "sub_topic": p.attributes["sub_topic"],
            "content": truncate_string(p.content, max_value_token_size),
        }
        for i, p in enumerate(profiles.profiles)
        if only_topics is None or p.attributes["topic"].strip() in only_topics
    ]

    topics_index = sorted(topics_index, key=lambda x: (x["topic"], x["sub_topic"]))
    system_prompt = pick_prompt.get_prompt(max_num=max_filter_num)
    input_prompt = pick_prompt.get_input(chats, topics_index)
    r = await llm_complete(
        project_id,
        input_prompt,
        system_prompt=system_prompt,
        temperature=0.2,  # precise
        model=CONFIG.summary_llm_model,
        **pick_prompt.get_kwargs(),
    )
    if not r.ok():
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to pick related profiles: {r.msg()}",
        )
        return r
    found_ids = find_list_int_or_none(r.data())
    reason = try_json_reason(r.data())
    if found_ids is None:
        TRACE_LOG.error(
            project_id,
            user_id,
            f"Failed to pick related profiles: {r.data()}",
        )
        return Promise.reject(
            CODE.INTERNAL_SERVER_ERROR, "Failed to pick related profiles"
        )
    ids = [i for i in found_ids if i < len(topics_index)]
    profiles = [profiles.profiles[topics_index[i]["index"]] for i in ids]
    TRACE_LOG.info(
        project_id,
        user_id,
        f"Filter profiles with chats: {reason}, {found_ids}",
    )
    return Promise.resolve({"reason": reason, "profiles": profiles})



================================================
FILE: src/server/api/memobase_server/llms/__init__.py
================================================
import asyncio
import time
from ..prompts.utils import convert_response_to_json
from ..utils import get_encoded_tokens
from ..env import CONFIG, LOG
from ..controllers.billing import project_cost_token_billing
from ..models.utils import Promise
from ..models.response import CODE
from ..models.database import DEFAULT_PROJECT_ID
from ..telemetry import telemetry_manager, CounterMetricName, HistogramMetricName

from .openai_model_llm import openai_complete
from .doubao_cache_llm import doubao_cache_complete

FACTORIES = {"openai": openai_complete, "doubao_cache": doubao_cache_complete}
assert CONFIG.llm_style in FACTORIES, f"Unsupported LLM style: {CONFIG.llm_style}"


# TODO: add TPM/Rate limiter
async def llm_complete(
    project_id,
    prompt,
    system_prompt=None,
    history_messages=[],
    json_mode=False,
    model=None,
    max_tokens=1024,
    **kwargs,
) -> Promise[str | dict]:
    use_model = model or CONFIG.best_llm_model
    if json_mode:
        kwargs["response_format"] = {"type": "json_object"}
    try:
        start_time = time.time()
        results = await FACTORIES[CONFIG.llm_style](
            use_model,
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            max_tokens=max_tokens,
            **kwargs,
        )
        latency = (time.time() - start_time) * 1000
    except Exception as e:
        LOG.error(f"Error in llm_complete: {e}")
        return Promise.reject(CODE.SERVICE_UNAVAILABLE, f"Error in llm_complete: {e}")

    in_tokens = len(
        get_encoded_tokens(
            prompt
            + (system_prompt or "")
            + "\n".join([m["content"] for m in history_messages])
        )
    )
    out_tokens = len(get_encoded_tokens(results))

    # await project_cost_token_billing(project_id, in_tokens, out_tokens)
    asyncio.create_task(project_cost_token_billing(project_id, in_tokens, out_tokens))

    telemetry_manager.increment_counter_metric(
        CounterMetricName.LLM_TOKENS_INPUT,
        in_tokens,
        {"project_id": project_id},
    )
    telemetry_manager.increment_counter_metric(
        CounterMetricName.LLM_TOKENS_OUTPUT,
        out_tokens,
        {"project_id": project_id},
    )
    telemetry_manager.increment_counter_metric(
        CounterMetricName.LLM_INVOCATIONS,
        1,
        {"project_id": project_id},
    )
    telemetry_manager.record_histogram_metric(
        HistogramMetricName.LLM_LATENCY_MS,
        latency,
        {"project_id": project_id},
    )

    if not json_mode:
        return Promise.resolve(results)
    parse_dict = convert_response_to_json(results)
    if parse_dict is not None:
        return Promise.resolve(parse_dict)
    else:
        return Promise.reject(
            CODE.UNPROCESSABLE_ENTITY, "Failed to parse JSON response"
        )


async def llm_sanity_check():
    r = await llm_complete(
        DEFAULT_PROJECT_ID, "Test", max_tokens=1, prompt_id="__test__"
    )
    if not r.ok():
        raise ValueError(f"LLM sanity check failed: {r.msg()}")
    LOG.info("LLM sanity check passed")



================================================
FILE: src/server/api/memobase_server/llms/doubao_cache_llm.py
================================================
import hashlib
from .utils import get_doubao_async_client_instance, exclude_special_kwargs
from ..connectors import get_redis_client
from ..env import LOG

CONTEXT_EXPIRE_TIME = 60 * 60 * 24
BEFORE_EXPIRE_TIME = 10


def compute_prompt_hash(system_prompt: str) -> str:
    return hashlib.md5(system_prompt.encode()).hexdigest()


async def doubao_cache_create_context_and_save(
    model, system_prompt, context_name
) -> str:
    prompt_hash = compute_prompt_hash(system_prompt)
    redis_key = f"memobase::doubao_context_id::{model}::{prompt_hash}"
    async with get_redis_client() as redis_client:
        context_id = await redis_client.get(redis_key)
        if context_id is not None:
            await redis_client.expire(
                redis_key, CONTEXT_EXPIRE_TIME - BEFORE_EXPIRE_TIME
            )
            if isinstance(context_id, bytes):
                return context_id.decode()
            return context_id
    doubao_client = get_doubao_async_client_instance()
    try:
        response = await doubao_client.context.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": system_prompt,
                }
            ],
            mode="common_prefix",
            ttl=CONTEXT_EXPIRE_TIME,
        )
    except Exception as e:
        LOG.error(f"Error creating context: {e}")
        return None
    async with get_redis_client() as redis_client:
        await redis_client.set(
            redis_key, response.id, ex=CONTEXT_EXPIRE_TIME - BEFORE_EXPIRE_TIME
        )
    LOG.info(f"Created context cache for {context_name}")
    return response.id


async def doubao_cache_complete(
    model,
    prompt,
    system_prompt=None,
    history_messages=[],
    thinking_enable=False,
    **kwargs,
) -> str:
    sp_args, kwargs = exclude_special_kwargs(kwargs)
    prompt_id = sp_args.get("prompt_id", None)
    assert prompt_id is not None, "prompt_id is required"
    doubao_async_client = get_doubao_async_client_instance()

    messages = []
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})

    # if thinking_enable:
    #     kwargs["thinking"] = {"type": "enabled"}
    # else:
    #     kwargs["thinking"] = {"type": "disabled"}

    if sp_args.get("no_cache", None) or system_prompt is None:

        if system_prompt is not None:
            messages.insert(0, {"role": "system", "content": system_prompt})

        response = await doubao_async_client.chat.completions.create(
            model=model, messages=messages, timeout=120, **kwargs
        )
        LOG.info(f"No Cached {prompt_id} {model} {response.usage.prompt_tokens}")
        return response.choices[0].message.content

    context_id = await doubao_cache_create_context_and_save(
        model, system_prompt, prompt_id
    )

    if system_prompt and context_id is None:
        # when context_id is None, we use system prompt to create context
        messages.insert(0, {"role": "system", "content": system_prompt})

    if context_id is None:
        response = await doubao_async_client.chat.completions.create(
            model=model, messages=messages, timeout=120, **kwargs
        )
        return response.choices[0].message.content
    else:
        response = await doubao_async_client.context.completions.create(
            model=model, messages=messages, context_id=context_id, timeout=120, **kwargs
        )
        LOG.info(
            f"Cached {prompt_id} {model} {response.usage.prompt_tokens_details.cached_tokens}/{response.usage.prompt_tokens}"
        )
        return response.choices[0].message.content



================================================
FILE: src/server/api/memobase_server/llms/openai_model_llm.py
================================================
from .utils import exclude_special_kwargs, get_openai_async_client_instance
from ..env import LOG


async def openai_complete(
    model, prompt, system_prompt=None, history_messages=[], **kwargs
) -> str:
    sp_args, kwargs = exclude_special_kwargs(kwargs)
    prompt_id = sp_args.get("prompt_id", None)

    openai_async_client = get_openai_async_client_instance()
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})

    response = await openai_async_client.chat.completions.create(
        model=model, messages=messages, timeout=120, **kwargs
    )
    cached_tokens = getattr(response.usage.prompt_tokens_details, "cached_tokens", None)
    LOG.info(
        f"Cached {prompt_id} {model} {cached_tokens}/{response.usage.prompt_tokens}"
    )
    return response.choices[0].message.content



================================================
FILE: src/server/api/memobase_server/llms/utils.py
================================================
from openai import AsyncOpenAI
from volcenginesdkarkruntime import AsyncArk
from ..env import CONFIG

_global_openai_async_client = None
_global_doubao_async_client = None


def get_openai_async_client_instance() -> AsyncOpenAI:
    global _global_openai_async_client
    if _global_openai_async_client is None:
        _global_openai_async_client = AsyncOpenAI(
            base_url=CONFIG.llm_base_url,
            api_key=CONFIG.llm_api_key,
            default_query=CONFIG.llm_openai_default_query,
            default_headers=CONFIG.llm_openai_default_header,
        )
    return _global_openai_async_client


def get_doubao_async_client_instance() -> AsyncArk:
    global _global_doubao_async_client

    if _global_doubao_async_client is None:
        _global_doubao_async_client = AsyncArk(api_key=CONFIG.llm_api_key)
    return _global_doubao_async_client


def exclude_special_kwargs(kwargs: dict):
    prompt_id = kwargs.pop("prompt_id", None)
    no_cache = kwargs.pop("no_cache", None)
    return {"prompt_id": prompt_id, "no_cache": no_cache}, kwargs



================================================
FILE: src/server/api/memobase_server/llms/embeddings/__init__.py
================================================
import time
from typing import Literal
import numpy as np
from traceback import format_exc
from ...env import CONFIG, LOG
from ...models.utils import Promise
from ...models.response import CODE
from ...models.database import DEFAULT_PROJECT_ID
from .jina_embedding import jina_embedding
from .openai_embedding import openai_embedding
from .lmstudio_embedding import lmstudio_embedding
from ...telemetry import telemetry_manager, HistogramMetricName, CounterMetricName
from ...utils import get_encoded_tokens

FACTORIES = {"openai": openai_embedding, "jina": jina_embedding, "lmstudio": lmstudio_embedding}
assert (
    CONFIG.embedding_provider in FACTORIES
), f"Unsupported embedding provider: {CONFIG.embedding_provider}"


async def check_embedding_sanity():
    if not CONFIG.enable_event_embedding:
        LOG.info("Event embedding is disabled, skipping sanity check.")
        return
    r = await get_embedding(DEFAULT_PROJECT_ID, ["Hello, world!"])
    if not r.ok():
        raise ValueError(
            "Embedding API check failed! Make sure the embedding API key is valid."
        )
    d = r.data()
    embedding_dim = d.shape[-1]
    if embedding_dim != CONFIG.embedding_dim:
        raise ValueError(
            f"Embedding dimension mismatch! Expected {CONFIG.embedding_dim}, got {embedding_dim}."
        )
    LOG.info(f"Embedding dimension matched: {embedding_dim}")


async def get_embedding(
    project_id: str,
    texts: list[str],
    phase: Literal["query", "document"] = "document",
    model: str = None,
) -> Promise[np.ndarray]:
    model = model or CONFIG.embedding_model
    try:
        start_time = time.time()
        results = await FACTORIES[CONFIG.embedding_provider](model, texts, phase)
        latency_ms = (time.time() - start_time) * 1000
    except Exception as e:
        LOG.error(f"Error in get_embedding: {e} {format_exc()}")
        return Promise.reject(CODE.SERVICE_UNAVAILABLE, f"Error in get_embedding: {e}")
    embedding_tokens = len(get_encoded_tokens("\n".join(texts)))
    telemetry_manager.increment_counter_metric(
        CounterMetricName.EMBEDDING_TOKENS,
        embedding_tokens,
        {"project_id": project_id},
    )
    telemetry_manager.record_histogram_metric(
        HistogramMetricName.EMBEDDING_LATENCY_MS,
        latency_ms,
        {"project_id": project_id},
    )
    return Promise.resolve(results)



================================================
FILE: src/server/api/memobase_server/llms/embeddings/jina_embedding.py
================================================
import numpy as np
from typing import Literal
from ...errors import ExternalAPIError
from ...env import CONFIG, LOG
from .utils import get_jina_async_client_instance

JINA_TASK = {
    "query": "retrieval.query",
    "document": "retrieval.passage",
}


async def jina_embedding(
    model: str, texts: list[str], phase: Literal["query", "document"] = "document"
) -> np.ndarray:
    jina_async_client = get_jina_async_client_instance()
    response = await jina_async_client.post(
        "/embeddings",
        json={
            "model": model,
            "input": texts,
            "task": JINA_TASK[phase],
            "truncate": True,
            "dimensions": CONFIG.embedding_dim,
        },
        timeout=20,
    )
    if response.status_code != 200:
        raise ExternalAPIError(f"Failed to embed texts: {response.text}")
    data = response.json()
    LOG.info(
        f"Jina embedding, {model}, {phase}, {data['usage']['prompt_tokens']}/{data['usage']['total_tokens']}"
    )
    return np.array([dp["embedding"] for dp in data["data"]])



================================================
FILE: src/server/api/memobase_server/llms/embeddings/lmstudio_embedding.py
================================================
import numpy as np
from typing import Literal
from ...errors import ExternalAPIError
from ...env import CONFIG, LOG
from .utils import get_lmstudio_async_client_instance

LMSTUDIO_TASK = {
    "query": "retrieval.query",
    "document": "retrieval.passage",
}

async def lmstudio_embedding(
    model: str, texts: list[str], phase: Literal["query", "document"] = "document"
) -> np.ndarray:
    lmstudio_async_client = get_lmstudio_async_client_instance()
    response = await lmstudio_async_client.post(
        "/embeddings",
        json={
            "model": model,
            "input": texts,
            "task": LMSTUDIO_TASK[phase],
            "truncate": True,
            "dimensions": CONFIG.embedding_dim,
        },
        timeout=20,
    )
    if response.status_code != 200:
        raise ExternalAPIError(f"Failed to embed texts: {response.text}")
    data = response.json()
    LOG.info(
        f"lmstudio embedding, {model}, {phase}, {data['usage']['prompt_tokens']}/{data['usage']['total_tokens']}"
    )
    return np.array([dp["embedding"] for dp in data["data"]])



================================================
FILE: src/server/api/memobase_server/llms/embeddings/openai_embedding.py
================================================
import numpy as np
from typing import Literal
from .utils import get_openai_async_client_instance
from ...env import LOG


async def openai_embedding(
    model: str, texts: list[str], phase: Literal["query", "document"] = "document"
) -> np.ndarray:
    openai_async_client = get_openai_async_client_instance()
    response = await openai_async_client.embeddings.create(
        model=model, input=texts, encoding_format="float"
    )

    prompt_tokens = getattr(response.usage, "prompt_tokens", None)
    total_tokens = getattr(response.usage, "total_tokens", None)
    LOG.info(f"OpenAI embedding, {model}, {phase}, {prompt_tokens}/{total_tokens}")
    return np.array([dp.embedding for dp in response.data])



================================================
FILE: src/server/api/memobase_server/llms/embeddings/utils.py
================================================
from openai import AsyncOpenAI
from httpx import AsyncClient
from ...env import CONFIG

_global_openai_async_client = None
_global_jina_async_client = None
_global_lmstudio_async_client = None


def get_openai_async_client_instance() -> AsyncOpenAI:
    global _global_openai_async_client
    if _global_openai_async_client is None:
        _global_openai_async_client = AsyncOpenAI(
            base_url=CONFIG.embedding_base_url,
            api_key=CONFIG.embedding_api_key,
        )
    return _global_openai_async_client


def get_jina_async_client_instance() -> AsyncClient:
    global _global_jina_async_client
    if _global_jina_async_client is None:
        _global_jina_async_client = AsyncClient(
            base_url=CONFIG.embedding_base_url,
            headers={"Authorization": f"Bearer {CONFIG.embedding_api_key}"},
        )
    return _global_jina_async_client

def get_lmstudio_async_client_instance() -> AsyncClient:
    global _global_lmstudio_async_client
    if _global_lmstudio_async_client is None:
        _global_lmstudio_async_client = AsyncClient(
            base_url=CONFIG.embedding_base_url,
            headers={"Authorization": f"Bearer {CONFIG.embedding_api_key}"},
        )
    return _global_lmstudio_async_client



================================================
FILE: src/server/api/memobase_server/models/__init__.py
================================================
[Empty file]


================================================
FILE: src/server/api/memobase_server/models/action.py
================================================
from typing import Optional
from pydantic import BaseModel, Field


class ActionData(BaseModel):
    actions: Optional[str] = None



================================================
FILE: src/server/api/memobase_server/models/blob.py
================================================
from enum import StrEnum
from datetime import datetime
from typing import Literal, Optional
from pydantic import BaseModel


class OpenAICompatibleMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str
    alias: Optional[str] = None
    created_at: Optional[str] = None


class TranscriptStamp(BaseModel):
    content: str
    start_timestamp_in_seconds: float
    end_time_timestamp_in_seconds: Optional[float] = None
    speaker: Optional[str] = None


class BlobType(StrEnum):
    chat = "chat"
    summary = "summary"
    doc = "doc"
    image = "image"
    code = "code"
    transcript = "transcript"


class Blob(BaseModel):
    type: BlobType
    fields: Optional[dict] = None
    created_at: Optional[datetime] = None

    def get_blob_data(self):
        return self.model_dump(exclude={"type", "fields", "created_at"})

    def to_request(self):
        return {
            "blob_type": self.type,
            "fields": self.fields,
            "blob_data": self.get_blob_data(),
        }


class ChatBlob(Blob):
    messages: list[OpenAICompatibleMessage]
    type: Literal[BlobType.chat] = BlobType.chat


class SummaryBlob(Blob):
    summary: str
    type: Literal[BlobType.summary] = BlobType.summary


class DocBlob(Blob):
    content: str
    type: Literal[BlobType.doc] = BlobType.doc


class CodeBlob(Blob):
    content: str
    language: Optional[str] = None
    type: Literal[BlobType.code] = BlobType.code


class ImageBlob(Blob):
    url: Optional[str] = None
    base64: Optional[str] = None
    type: Literal[BlobType.image] = BlobType.image


class TranscriptBlob(Blob):
    transcripts: list[TranscriptStamp]
    type: Literal[BlobType.transcript] = BlobType.transcript


class BlobData(BaseModel):
    blob_type: BlobType
    blob_data: dict  # messages/doc/images...
    fields: Optional[dict] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    def to_blob(self) -> Blob:
        if self.blob_type == BlobType.chat:
            return ChatBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.summary:
            return SummaryBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.doc:
            return DocBlob(
                **self.blob_data, fields=self.fields, created_at=self.created_at
            )
        elif self.blob_type == BlobType.image:
            raise NotImplementedError("ImageBlob not implemented yet.")
        elif self.blob_type == BlobType.transcript:
            raise NotImplementedError("TranscriptBlob not implemented yet.")



================================================
FILE: src/server/api/memobase_server/models/claim.py
================================================
from typing import Optional
from pydantic import BaseModel, Field


class ClaimData(BaseModel):
    claim: Optional[str] = None



================================================
FILE: src/server/api/memobase_server/models/database.py
================================================
import os
import uuid
from typing import Optional
from datetime import datetime
from sqlalchemy import (
    text,
    VARCHAR,
    Integer,
    ForeignKey,
    TIMESTAMP,
    Table,
    TEXT,
    Column,
    Index,
    Boolean,
    PrimaryKeyConstraint,
    ForeignKeyConstraint,
)
from dataclasses import dataclass
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy.orm import (
    relationship,
    Mapped,
    mapped_column,
    registry,
    object_session,
)
from sqlalchemy.sql import func
from sqlalchemy import event
from .blob import BlobType
from ..env import (
    ProjectStatus,
    BillingStatus,
    BILLING_REFILL_AMOUNT_MAP,
    CONFIG,
    LOG,
    BufferStatus,
)
from sqlalchemy.orm.attributes import get_history
from pgvector.sqlalchemy import Vector

REG = registry()
DEFAULT_PROJECT_ID = "__root__"
DEFAULT_PROJECT_SECRET = "__root__"


def next_month_first_day() -> datetime:
    today = datetime.now()
    # If we're in the last month of the year, move to January of next year
    if today.month == 12:
        return datetime(today.year + 1, 1, 1)
    # Otherwise, move to the first day of next month
    return datetime(today.year, today.month + 1, 1)


def check_legal_embedding_dim(cls, session):
    try:
        # Use table_name from the ORM class to avoid hardcoding
        table_name = cls.__tablename__

        # Use text() to properly declare SQL expression
        sql = text(
            """
        SELECT atttypmod
        FROM pg_attribute
        JOIN pg_class ON pg_attribute.attrelid = pg_class.oid
        JOIN pg_namespace ON pg_class.relnamespace = pg_namespace.oid
        WHERE pg_class.relname = :table_name
        AND pg_attribute.attname = 'embedding'
        AND pg_namespace.nspname = current_schema();
        """
        )

        result = session.execute(sql, {"table_name": table_name}).scalar()

        # Table or column might not exist yet
        if result is None:
            raise ValueError(
                "`embedding` column does not exist in the table, please check the table schema"
            )

        # In pgvector, atttypmod - 8 is the dimension
        actual_dim = result

        if actual_dim != CONFIG.embedding_dim:
            raise ValueError(
                f"Configuration embedding dimension ({CONFIG.embedding_dim}) "
                f"does not match database dimension ({actual_dim}). "
                f"This may cause errors when inserting embeddings."
            )
        LOG.info(
            f"Configuration embedding dimension ({CONFIG.embedding_dim}) "
            f"matches database dimension ({actual_dim}). "
        )
        return actual_dim

    except Exception as e:
        LOG.warning(f"Failed to check embedding dimension: {str(e)}")
        raise e


@dataclass
class Base:
    __abstract__ = True

    # Common columns
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=True),
        default_factory=uuid.uuid4,
        init=False,
    )
    created_at: Mapped[datetime] = mapped_column(
        TIMESTAMP(timezone=True), server_default=func.now(), init=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        TIMESTAMP(timezone=True),
        server_default=func.now(),
        onupdate=func.now(),
        init=False,
    )


SHORT_ENUM_SIZE = 16


@REG.mapped_as_dataclass
class Billing(Base):
    __tablename__ = "billings"

    # Specific columns

    usage_left: Mapped[Optional[int]] = mapped_column(
        Integer,
        nullable=True,
        default_factory=lambda: BILLING_REFILL_AMOUNT_MAP[BillingStatus.free],
    )

    next_refill_at: Mapped[datetime] = mapped_column(
        TIMESTAMP(timezone=True), nullable=True, default_factory=next_month_first_day
    )
    # Relationships
    related_projects: Mapped[list["ProjectBilling"]] = relationship(
        "ProjectBilling",
        back_populates="billing",
        cascade="all, delete-orphan",
        init=False,
    )

    __table_args__ = (PrimaryKeyConstraint("id"),)


@REG.mapped_as_dataclass
class ProjectBilling:
    __tablename__ = "project_billings"

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        ForeignKey("projects.project_id", ondelete="CASCADE", onupdate="CASCADE"),
        nullable=False,
    )
    billing_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("billings.id", ondelete="CASCADE", onupdate="CASCADE"),
        nullable=False,
    )
    created_at: Mapped[datetime] = mapped_column(
        TIMESTAMP(timezone=True), server_default=func.now(), init=False
    )

    # Relationships
    project: Mapped["Project"] = relationship(
        "Project", back_populates="billing_link", init=False
    )
    billing: Mapped[Billing] = relationship(
        "Billing", back_populates="related_projects", init=False
    )

    __table_args__ = (
        PrimaryKeyConstraint("project_id", "billing_id"),
        Index("idx_project_billings_project_id", "project_id"),
        Index("idx_project_billings_billing_id", "billing_id"),
    )


@REG.mapped_as_dataclass
class Project(Base):
    __tablename__ = "projects"

    project_id: Mapped[str] = mapped_column(VARCHAR(64), nullable=False, unique=True)
    project_secret: Mapped[str] = mapped_column(VARCHAR(255), nullable=False)
    profile_config: Mapped[str | None] = mapped_column(TEXT, nullable=True)
    status: Mapped[str] = mapped_column(
        VARCHAR(SHORT_ENUM_SIZE), nullable=False, default=ProjectStatus.active
    )

    related_users: Mapped[list["User"]] = relationship(
        "User", back_populates="project", cascade="all, delete-orphan", init=False
    )

    billing_link: Mapped[list[ProjectBilling]] = relationship(
        "ProjectBilling",
        back_populates="project",
        cascade="all, delete-orphan",
        init=False,
    )

    __table_args__ = (
        PrimaryKeyConstraint("project_id"),
        Index("idx_projects_project_id", "project_id"),
    )

    @classmethod
    def initialize_root_project(cls, session):
        """Initialize the root project if it doesn't exist."""
        root_project = (
            session.query(cls).filter_by(project_id=DEFAULT_PROJECT_ID).first()
        )
        if not root_project:
            root_project = cls(
                project_id=DEFAULT_PROJECT_ID,
                project_secret=DEFAULT_PROJECT_SECRET,
                profile_config=None,
            )
            session.add(root_project)
        if_project_billing = (
            session.query(ProjectBilling)
            .filter_by(project_id=DEFAULT_PROJECT_ID)
            .one_or_none()
        )
        if if_project_billing is None:
            billing = Billing(usage_left=BILLING_REFILL_AMOUNT_MAP[BillingStatus.free])
            session.add(billing)
            session.add(
                ProjectBilling(project_id=DEFAULT_PROJECT_ID, billing_id=billing.id)
            )
        session.commit()
        return root_project


# TODO: add index for user id and ...
@REG.mapped_as_dataclass
class User(Base):
    __tablename__ = "users"
    # Relationships
    related_general_blobs: Mapped[list["GeneralBlob"]] = relationship(
        "GeneralBlob", back_populates="user", cascade="all, delete-orphan", init=False
    )
    related_buffers: Mapped[list["BufferZone"]] = relationship(
        "BufferZone", back_populates="user", cascade="all, delete-orphan", init=False
    )
    related_user_profiles: Mapped[list["UserProfile"]] = relationship(
        "UserProfile", back_populates="user", cascade="all, delete-orphan", init=False
    )
    related_user_events: Mapped[list["UserEvent"]] = relationship(
        "UserEvent", back_populates="user", cascade="all, delete-orphan", init=False
    )
    related_user_event_gists: Mapped[list["UserEventGist"]] = relationship(
        "UserEventGist", back_populates="user", cascade="all, delete-orphan", init=False
    )
    related_user_statuses: Mapped[list["UserStatus"]] = relationship(
        "UserStatus", back_populates="user", cascade="all, delete-orphan", init=False
    )

    # Default columns
    additional_fields: Mapped[Optional[dict]] = mapped_column(
        JSONB, nullable=True, default=None
    )

    project_id: Mapped[Optional[str]] = mapped_column(
        VARCHAR(64),
        ForeignKey("projects.project_id", ondelete="CASCADE", onupdate="CASCADE"),
        init=True,
        default=DEFAULT_PROJECT_ID,
    )
    project: Mapped[Optional[Project]] = relationship(
        "Project", back_populates="related_users", init=False, foreign_keys=[project_id]
    )

    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_users_id_project_id", "id", "project_id"),
    )


@REG.mapped_as_dataclass
class GeneralBlob(Base):
    __tablename__ = "general_blobs"

    # Add project_id to match Users table's composite key
    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    # Specific columns
    blob_type: Mapped[str] = mapped_column(VARCHAR(SHORT_ENUM_SIZE), nullable=False)
    blob_data: Mapped[dict] = mapped_column(JSONB, nullable=False)

    related_buffers: Mapped[list["BufferZone"]] = relationship(
        "BufferZone",
        back_populates="blob",
        cascade="all, delete-orphan",
        init=False,
        overlaps="user,related_buffers",
    )

    # Default columns
    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )
    additional_fields: Mapped[Optional[dict]] = mapped_column(
        JSONB, nullable=True, default=None
    )
    user: Mapped[User] = relationship(
        "User",
        back_populates="related_general_blobs",
        init=False,
        foreign_keys=[user_id, project_id],
    )
    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_general_blobs_user_id_project_id", "user_id", "project_id"),
        Index("idx_general_blobs_user_id_id", "user_id", "project_id", "id"),
        Index(
            "idx_general_blobs_user_id_blob_type", "user_id", "project_id", "blob_type"
        ),
        Index("idx_general_blobs_id_project_id", "id", "project_id", unique=True),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )

    # validate
    def __post_init__(self):
        assert isinstance(
            self.blob_type, BlobType
        ), f"Invalid blob type: {self.blob_type}"
        self.blob_type = self.blob_type.value


@REG.mapped_as_dataclass
class BufferZone(Base):
    __tablename__ = "buffer_zones"

    # Specific columns
    blob_type: Mapped[str] = mapped_column(VARCHAR(SHORT_ENUM_SIZE), nullable=False)
    token_size: Mapped[int] = mapped_column(Integer, nullable=False)

    # Relationships
    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    blob_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    status: Mapped[str] = mapped_column(
        VARCHAR(SHORT_ENUM_SIZE), nullable=False, default=BufferStatus.idle
    )

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )
    user: Mapped[User] = relationship(
        "User",
        back_populates="related_buffers",
        init=False,
        foreign_keys=[user_id, project_id],
        overlaps="blob,related_buffers",
    )

    blob: Mapped[GeneralBlob] = relationship(
        "GeneralBlob",
        back_populates="related_buffers",
        init=False,
        foreign_keys=[blob_id, project_id],
        overlaps="user,related_buffers",
    )
    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index(
            "idx_buffer_zones_user_id_blob_type",
            "user_id",
            "project_id",
            "blob_type",
            "status",
        ),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
        ForeignKeyConstraint(
            ["blob_id", "project_id"],
            ["general_blobs.id", "general_blobs.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )

    # validate
    def __post_init__(self):
        assert isinstance(
            self.blob_type, BlobType
        ), f"Invalid blob type: {self.blob_type}"
        self.blob_type = self.blob_type.value


@REG.mapped_as_dataclass
class UserProfile(Base):
    __tablename__ = "user_profiles"

    # Specific columns
    content: Mapped[str] = mapped_column(TEXT, nullable=False)

    # Relationships
    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    attributes: Mapped[dict] = mapped_column(JSONB, nullable=True, default=None)

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )

    user: Mapped[User] = relationship(
        "User",
        back_populates="related_user_profiles",
        init=False,
        foreign_keys=[user_id, project_id],
    )

    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_user_profiles_user_id_project_id", "user_id", "project_id"),
        Index("idx_user_profiles_user_id_id_project_id", "user_id", "project_id", "id"),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )


@REG.mapped_as_dataclass
class UserEvent(Base):
    __tablename__ = "user_events"

    # Specific columns
    event_data: Mapped[dict] = mapped_column(JSONB)

    # Relationships

    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )

    user: Mapped[User] = relationship(
        "User",
        back_populates="related_user_events",
        init=False,
        foreign_keys=[user_id, project_id],
    )

    embedding: Mapped[Vector] = mapped_column(
        Vector(dim=CONFIG.embedding_dim), nullable=True, default=None
    )

    related_user_event_gists: Mapped[list["UserEventGist"]] = relationship(
        "UserEventGist",
        back_populates="event",
        cascade="all, delete-orphan",
        init=False,
        overlaps="related_user_event_gists",
    )

    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_user_events_user_id_project_id", "user_id", "project_id"),
        Index("idx_user_events_user_id_id_project_id", "user_id", "project_id", "id"),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )

    @classmethod
    def check_legal_embedding_dim(cls, session):
        check_legal_embedding_dim(cls, session)
        LOG.info("UserEvent embedding dimension checked")


@REG.mapped_as_dataclass
class UserEventGist(Base):
    __tablename__ = "user_event_gists"

    # Specific columns
    gist_data: Mapped[dict] = mapped_column(JSONB)

    # Relationships
    event_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )

    event: Mapped[UserEvent] = relationship(
        "UserEvent",
        back_populates="related_user_event_gists",
        init=False,
        foreign_keys=[event_id, project_id],
        overlaps="related_user_event_gists",
    )

    user: Mapped[User] = relationship(
        "User",
        back_populates="related_user_event_gists",
        init=False,
        foreign_keys=[user_id, project_id],
        overlaps="event,related_user_event_gists",
    )

    embedding: Mapped[Vector] = mapped_column(
        Vector(dim=CONFIG.embedding_dim), nullable=True, default=None
    )

    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_user_event_gists_user_id_project_id", "user_id", "project_id"),
        Index(
            "idx_user_event_gists_user_id_project_id_id", "user_id", "project_id", "id"
        ),
        Index(
            "idx_user_event_gists_user_id_id_project_id",
            "user_id",
            "project_id",
            "event_id",
        ),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
        ForeignKeyConstraint(
            ["event_id", "project_id"],
            ["user_events.id", "user_events.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )

    @classmethod
    def check_legal_embedding_dim(cls, session):
        check_legal_embedding_dim(cls, session)
        LOG.info("UserEventGist embedding dimension checked")


@REG.mapped_as_dataclass
class UserStatus(Base):
    __tablename__ = "user_statuses"

    # Specific columns
    type: Mapped[str] = mapped_column(VARCHAR(SHORT_ENUM_SIZE * 2), nullable=False)
    attributes: Mapped[dict] = mapped_column(JSONB, nullable=False)

    # Relationships
    user_id: Mapped[UUID] = mapped_column(
        UUID(as_uuid=True),
        nullable=False,
    )

    project_id: Mapped[str] = mapped_column(
        VARCHAR(64),
        default=DEFAULT_PROJECT_ID,
    )

    user: Mapped[User] = relationship(
        "User",
        back_populates="related_user_statuses",
        init=False,
        foreign_keys=[user_id, project_id],
    )

    __table_args__ = (
        PrimaryKeyConstraint("id", "project_id"),
        Index("idx_user_statuses_user_id_project_id", "user_id", "project_id"),
        Index(
            "idx_user_statuses_user_id_project_id_type", "user_id", "project_id", "type"
        ),
        Index("idx_user_statuses_user_id_id_project_id", "user_id", "project_id", "id"),
        ForeignKeyConstraint(
            ["user_id", "project_id"],
            ["users.id", "users.project_id"],
            ondelete="CASCADE",
            onupdate="CASCADE",
        ),
    )


# Modify event listeners to allow root project initialization
@event.listens_for(Project, "before_insert")
def prevent_insert(mapper, connection, target):
    if target.project_id != DEFAULT_PROJECT_ID:
        raise ValueError("The projects table is read-only. Inserts are not allowed.")


@event.listens_for(Project, "before_delete")
def prevent_delete(mapper, connection, target):
    # if target.project_id != DEFAULT_PROJECT_ID:
    raise ValueError("The projects table is read-only. Deletions are not allowed.")


@event.listens_for(Project, "before_update")
def prevent_update(mapper, connection, target):
    session = object_session(target)
    if not session:
        return

    # Get the history of all attributes
    exclude_attrs = ["profile_config"]
    all_attrs = Project.__mapper__.attrs.keys()
    for attr in all_attrs:
        if attr in exclude_attrs:
            continue
        history = get_history(target, attr)
        if history.has_changes():
            raise ValueError(
                f"The projects table is read-only except for {exclude_attrs}. Updates to other fields are not allowed."
            )



================================================
FILE: src/server/api/memobase_server/models/response.py
================================================
from datetime import datetime
from enum import IntEnum
from typing import Optional, Literal
from pydantic import BaseModel, UUID4, UUID5, Field
from .blob import BlobData, OpenAICompatibleMessage
from .claim import ClaimData
from .action import ActionData

UUID = UUID4 | UUID5


class CODE(IntEnum):
    # Success
    SUCCESS = 0

    # Client-side errors (4xx)
    BAD_REQUEST = 400  # The request could not be understood by the server due to malformed syntax.
    UNAUTHORIZED = 401  # The request requires user authentication.
    FORBIDDEN = 403  # The server understood the request, but is refusing to fulfill it.
    NOT_FOUND = 404  # The server has not found anything matching the Request-URI.
    METHOD_NOT_ALLOWED = 405  # The method specified in the Request-Line is not allowed for the resource identified by the Request-URI.
    CONFLICT = 409  # The request could not be completed due to a conflict with the current state of the resource.
    UNPROCESSABLE_ENTITY = 422  # The server understands the content type of the request entity, and the syntax is correct, but it was unable to process the contained instructions.

    # Server-side errors (5xx)
    INTERNAL_SERVER_ERROR = 500  # The server encountered an unexpected condition which prevented it from fulfilling the request.
    NOT_IMPLEMENTED = 501  # The server does not support the functionality required to fulfill the request.
    BAD_GATEWAY = 502  # The server, while acting as a gateway or proxy, received an invalid response from the upstream server.
    SERVICE_UNAVAILABLE = 503  # The server is currently unable to handle the request due to a temporary overloading or maintenance of the server.
    GATEWAY_TIMEOUT = 504  # The server, while acting as a gateway or proxy, did not receive a timely response from the upstream server.
    SERVER_PARSE_ERROR = 520  # The server could not parse the request.


class AIUserProfile(BaseModel):
    topic: str = Field(..., description="The main topic of the user profile")
    sub_topic: str = Field(..., description="The sub-topic of the user profile")
    memo: str = Field(..., description="The memo content of the user profile")


class AIUserProfiles(BaseModel):
    facts: list[AIUserProfile] = Field(..., description="List of user profile facts")


# Return data format
class IdData(BaseModel):
    id: UUID = Field(..., description="The UUID identifier")


class IdsData(BaseModel):
    ids: list[UUID] = Field(..., description="List of UUID identifiers")


class ChatModalResponse(BaseModel):
    event_id: Optional[UUID] = Field(..., description="The event's unique identifier")
    add_profiles: Optional[list[UUID]] = Field(
        ..., description="List of added profiles' ids"
    )
    update_profiles: Optional[list[UUID]] = Field(
        ..., description="List of updated profiles' ids"
    )
    delete_profiles: Optional[list[UUID]] = Field(
        ..., description="List of deleted profiles' ids"
    )


class ProfileAttributes(BaseModel):
    topic: str = Field(..., description="The topic of the profile")
    sub_topic: str = Field(..., description="The sub-topic of the profile")


class ProfileData(BaseModel):
    id: UUID = Field(..., description="The profile's unique identifier")
    content: str = Field(..., description="User profile content value")
    created_at: datetime = Field(
        None, description="Timestamp when the profile was created"
    )
    updated_at: datetime = Field(
        None, description="Timestamp when the profile was last updated"
    )
    attributes: Optional[dict] = Field(
        None,
        description="User profile attributes in JSON, containing 'topic', 'sub_topic'",
    )


class ProfileDelta(BaseModel):
    content: str = Field(..., description="The profile content")
    attributes: Optional[dict] = Field(
        ...,
        description="User profile attributes in JSON, containing 'topic', 'sub_topic'",
    )


class EventTag(BaseModel):
    tag: str = Field(..., description="The event tag")
    value: str = Field(..., description="The event tag value")


class EventGistData(BaseModel):
    content: str = Field(..., description="The event gist content")


class EventData(BaseModel):
    profile_delta: Optional[list[ProfileDelta]] = Field(
        None, description="List of profile data"
    )
    event_tip: Optional[str] = Field(None, description="Event tip")
    event_tags: Optional[list[EventTag]] = Field(None, description="List of event tags")


class UserEventGistData(BaseModel):
    id: UUID = Field(..., description="The event gist's unique identifier")
    gist_data: EventGistData = Field(None, description="User event gist data")
    created_at: datetime = Field(
        None, description="Timestamp when the event gist was created"
    )
    updated_at: datetime = Field(
        None, description="Timestamp when the event gist was last updated"
    )
    similarity: Optional[float] = Field(None, description="Similarity score")


class UserEventData(BaseModel):
    id: UUID = Field(..., description="The event's unique identifier")
    event_data: EventData = Field(None, description="User event data in JSON")
    created_at: datetime = Field(
        None, description="Timestamp when the event was created"
    )
    updated_at: datetime = Field(
        None, description="Timestamp when the event was last updated"
    )
    similarity: Optional[float] = Field(None, description="Similarity score")


class ContextData(BaseModel):
    context: str = Field(..., description="Context string")


class UserData(BaseModel):
    data: Optional[dict] = Field(None, description="User additional data in JSON")
    id: Optional[UUID] = Field(None, description="User ID in UUIDv4/5")
    created_at: Optional[datetime] = Field(
        None, description="Timestamp when the user was created"
    )
    updated_at: Optional[datetime] = Field(
        None, description="Timestamp when the user was last updated"
    )


class UserProfilesData(BaseModel):
    profiles: list[ProfileData] = Field(..., description="List of user profiles")


class UserEventsData(BaseModel):
    events: list[UserEventData] = Field(..., description="List of user events")


class UserEventGistsData(BaseModel):
    gists: list[UserEventGistData] = Field(..., description="List of user event gists")


class StrIntData(BaseModel):
    data: dict[str, int] = Field(..., description="String to int mapping")


class MessageData(BaseModel):
    data: list[OpenAICompatibleMessage] = Field(..., description="List of messages")


class QueryData(BaseModel):
    claims: list[ClaimData] = Field(..., description="List of claim data")
    actions: list[ActionData] = Field(..., description="List of action data")


class ProfileConfigData(BaseModel):
    profile_config: str | None = Field(..., description="Profile config string")


class BillingData(BaseModel):
    token_left: Optional[int] = Field(None, description="Total token left")

    next_refill_at: Optional[datetime] = Field(None, description="Next refill time")
    project_token_cost_month: int = Field(
        ..., description="Token cost of this project for this month"
    )


class UserStatusData(BaseModel):
    id: UUID = Field(..., description="User status id")

    type: str = Field(..., description="User status type")
    attributes: dict = Field(..., description="User status attributes")

    created_at: Optional[datetime] = Field(
        None, description="Timestamp when the user was created"
    )
    updated_at: Optional[datetime] = Field(
        None, description="Timestamp when the user was last updated"
    )


class UserStatusesData(BaseModel):
    statuses: list[UserStatusData] = Field(..., description="List of user statuses")


class ProactiveTopicData(BaseModel):
    action: Literal["new_topic", "continue"] = Field(
        ..., description="The action to take"
    )
    topic_prompt: Optional[str] = Field(
        None,
        description="The topic prompt, insert it to your latest user message or system prompt",
    )


class ProactiveTopicRequest(BaseModel):
    messages: list[OpenAICompatibleMessage] = Field(
        ..., description="The latest messages between user/assistant"
    )
    agent_context: Optional[str] = Field(
        None, description="The agent's roleplay prompt"
    )


class UserContextImport(BaseModel):
    context: str = Field(
        ..., description="The user context you want to import to Memobase"
    )


class ProjectUsersData(BaseModel):
    users: list = Field(..., description="The user list")
    count: int = Field(0, description="The user count")


class DailyUsage(BaseModel):
    date: str = Field(..., description="The date")
    total_insert: int = Field(0, description="The total insert")
    total_success_insert: int = Field(0, description="The total update")
    total_input_token: int = Field(0, description="The total input token")
    total_output_token: int = Field(0, description="The total output token")


# API response format
class BaseResponse(BaseModel):
    data: Optional[dict] = Field(None, description="Response data payload")
    errno: CODE = Field(CODE.SUCCESS, description="Error code, 0 means success")
    errmsg: str = Field("", description="Error message, empty when success")


class IdResponse(BaseResponse):
    data: Optional[IdData] = Field(None, description="Response containing a single ID")


class IdsResponse(BaseResponse):
    data: Optional[IdsData] = Field(
        None, description="Response containing multiple IDs"
    )


class ProfileConfigDataResponse(BaseResponse):
    data: Optional[ProfileConfigData] = Field(
        None, description="Response containing profile config data"
    )


class UserDataResponse(BaseResponse):
    data: Optional[UserData] = Field(None, description="Response containing user data")


class BlobDataResponse(BaseResponse):
    data: Optional[BlobData] = Field(None, description="Response containing blob data")


class QueryDataResponse(BaseResponse):
    data: Optional[QueryData] = Field(
        None, description="Response containing query results"
    )


class UserProfileResponse(BaseResponse):
    data: Optional[UserProfilesData] = Field(
        None, description="Response containing user profiles"
    )


class UserEventsDataResponse(BaseResponse):
    data: Optional[UserEventsData] = Field(
        None, description="Response containing user events"
    )


class UserEventGistsDataResponse(BaseResponse):
    data: Optional[UserEventGistsData] = Field(
        None, description="Response containing user event gists"
    )


class UserContextDataResponse(BaseResponse):
    data: Optional[ContextData] = Field(
        None, description="Response containing user context"
    )


class BillingResponse(BaseResponse):
    data: Optional[BillingData] = Field(
        None, description="Response containing token left"
    )


class ChatModalAPIResponse(BaseResponse):
    data: Optional[list[ChatModalResponse]] = Field(
        None, description="Response containing chat modal data"
    )


class BlobInsertData(IdData):
    chat_results: Optional[list[ChatModalResponse]] = Field(
        None, description="List of chat modal data"
    )


class BlobInsertResponse(BaseResponse):
    data: Optional[BlobInsertData] = Field(
        None, description="Response containing blob insert data"
    )


class ProactiveTopicResponse(BaseResponse):
    data: Optional[ProactiveTopicData] = Field(
        None, description="Response containing proactive topic data"
    )


class ProjectUsersDataResponse(BaseResponse):
    data: Optional[ProjectUsersData] = Field(
        None, description="Response containing the users"
    )


class UsageResponse(BaseResponse):
    data: Optional[list[DailyUsage]] = Field(
        None, description="Response containing the daily usage"
    )



================================================
FILE: src/server/api/memobase_server/models/utils.py
================================================
from dataclasses import dataclass
from typing import TypeVar, Optional, Type, Generic
from pydantic import ValidationError
from .response import CODE, BaseResponse
from ..env import LOG


class PromiseUnpackError(Exception):
    pass


D = TypeVar("D")
T = TypeVar("T", bound=BaseResponse)


@dataclass
class Promise(Generic[D]):
    __data: Optional[D]
    __errcode: CODE = CODE.SUCCESS
    __errmsg: str = ""

    @classmethod
    def resolve(cls, data: D) -> "Promise[D]":
        return cls(data)

    @classmethod
    def reject(cls, errcode: CODE, errmsg: str) -> "Promise":
        assert errmsg is not None, "Error Message can't be None!"
        assert errcode in CODE, f"Invalid Error Code: {errcode}"
        return cls(None, errcode, errmsg)

    def ok(self) -> bool:
        return self.__errcode == CODE.SUCCESS

    def data(self) -> Optional[D]:
        if not self.ok():
            raise PromiseUnpackError(self.msg())
        return self.__data

    def code(self) -> CODE:
        return self.__errcode

    def msg(self) -> str:
        if not self.ok():
            return f"CODE {self.__errcode}; ERROR {self.__errmsg}"
        return ""

    def to_response(self, ResponseModel: Type[T]) -> T:
        try:
            return ResponseModel(
                data=self.__data,
                errno=self.__errcode,
                errmsg=self.__errmsg,
            )
        except ValidationError as e:
            LOG.error(f"Error while parsing response: {e}")
            return ResponseModel(
                data=None,
                errno=CODE.INTERNAL_SERVER_ERROR,
                errmsg=str(e),
            )



================================================
FILE: src/server/api/memobase_server/prompts/__init__.py
================================================
from . import (
    extract_profile,
    zh_extract_profile,
    merge_profile,
    zh_merge_profile,
    organize_profile,
    summary_profile,
)



================================================
FILE: src/server/api/memobase_server/prompts/chat_context_pack.py
================================================
from typing import Callable

ContextPromptFunc = Callable[[str, str], str]


def en_context_prompt(profile_section: str, event_section: str) -> str:
    return f"""---
# Memory
Unless the user has relevant queries, do not actively mention those memories in the conversation.
## User Current Profile:
{profile_section}

## Past Events:
{event_section}
---
"""


def zh_context_prompt(profile_section: str, event_section: str) -> str:
    return f"""---
# 记忆
除非用户有相关的需求，否则不要主动在对话中提到这些记忆.
## 用户当前状态：
{profile_section}

## 过去事件：
{event_section}
---
"""


CONTEXT_PROMPT_PACK: dict[str, ContextPromptFunc] = {
    "en": en_context_prompt,
    "zh": zh_context_prompt,
}



================================================
FILE: src/server/api/memobase_server/prompts/event_tagging.py
================================================
from . import user_profile_topics
from .utils import pack_profiles_into_string
from ..models.response import AIUserProfiles
from ..env import CONFIG

ADD_KWARGS = {
    "prompt_id": "event_tagging",
}
EXAMPLES = [
    (
        """
## Assume the event tags are:
## - emotion(the user's current emotion)
## - goals(the user's goals)
## - location(the location of user)
The assistant passionately expresses their love and care for the user, trying to convince them that their feelings are genuine and not just physical, despite the user's skepticism and demand for proof.
""",
        """- emotion{tab}skepticism about the assistant's love
- goals{tab}Demand proof of assistant's love
""",
        """The event mentioned the users' feelings and demands, so the `emotion` and `goals` tags can be filled,
But the location is not mentioned, so it's not included in the result.
""",
    )
]

FACT_RETRIEVAL_PROMPT = """You are a expert of tagging events.
You will be given a event summary, and you need to extract the specific tags' values for the event.

## Event Tags
Below are some event tags you need to extract:
<event_tags>
{event_tags}
</event_tags>
each line is the tag name and its description(if any), for example:
- emotion(the user's current emotion)
the tag name is `emotion`, and the description of this tag is `the user's current emotion`.
### Rules
- Strick to the exact tag name, don't change the tag name.
- Remember: if some tags are not mentioned in the summary, you should not include them in the result.

## Formatting
### Output
You need to extract the specific tags' values for the event:
- TAG{tab}VALUE
For example:
- emotion{tab}sad
- goals{tab}find a new home

For each line is a new event tag of this summary, containing:
1. TAG: the event tag name
2. VALUE: the value of the event tag
those elements should be separated by `{tab}` and each line should be separated by `\n` and started with "- ".

## Examples
Here are some few shot examples:
{examples}

## Rules
- Return the new event tags in a list format as shown above.
- Strick to the exact tag name, don't change the tag name.
- If some tags are not mentioned in the summary, you should not include them in the result.
- You should detect the language of the event summary and extract the event tags's value in the same language.

Now, please extract the event tags for the following event summary:
"""


def get_prompt(event_tags: str) -> str:
    examples = "\n\n".join(
        [
            f"""<input>{p[0]}</input>
<output>{p[1]}</output>
<explanation>{p[2]}</explanation>
"""
            for p in EXAMPLES
        ]
    )
    return FACT_RETRIEVAL_PROMPT.format(
        examples=examples.format(tab=CONFIG.llm_tab_separator),
        tab=CONFIG.llm_tab_separator,
        event_tags=event_tags,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(
        get_prompt(
            event_tags="""- 冒险
- 天气
- 休息
- 逃离
""",
        )
    )



================================================
FILE: src/server/api/memobase_server/prompts/extract_profile.py
================================================
from . import user_profile_topics
from .utils import pack_profiles_into_string
from ..models.response import AIUserProfiles
from ..env import CONFIG

ADD_KWARGS = {
    "prompt_id": "extract_profile",
}
EXAMPLES = [
    (
        """- User say Hi to assistant.
""",
        AIUserProfiles(**{"facts": []}),
    ),
    (
        """
- User's favorite movies are Inception and Interstellar [mention 2025/01/01]
- User's favorite movie is Tenet [mention 2025/01/02]
""",
        AIUserProfiles(
            **{
                "facts": [
                    {
                        "topic": "interest",
                        "sub_topic": "movie",
                        "memo": "Inception, Interstellar[mention 2025/01/01]; favorite movie is Tenet [mention 2025/01/02]",
                    },
                    {
                        "topic": "interest",
                        "sub_topic": "movie_director",
                        "memo": "user seems to be a big fan of director Christopher Nolan",
                    },
                ]
            }
        ),
    ),
]

DEFAULT_JOB = """You are a professional psychologist.
Your responsibility is to carefully read out the memo of user and extract the important profiles of user in structured format.
Then extract relevant and important facts, preferences about the user that will help evaluate the user's state.
You will not only extract the information that's explicitly stated, but also infer what's implied from the conversation.
You will use the same language as the user's input to record the facts.
"""

FACT_RETRIEVAL_PROMPT = """{system_prompt}
## Formatting
### Input
#### Topics Guidelines
You'll be given some user-relatedtopics and subtopics that you should focus on collecting and extracting.
Don't collect topics that are not related to the user, it will cause confusion.
For example, if the memo mentions the position of another person, don't generate a "work{tab}position" topic, it will cause confusion. Only generate a topic if the user mentions their own work.
You can create your own topics/sub_topics if you find it necessary, unless the user requests to not to create new topics/sub_topics.
#### User Before Topics
You will be given the topics and subtopics that the user has already shared with the assistant.
Consider use the same topic/subtopic if it's mentioned in the conversation again.
#### Memos
You will receive a memo of user in Markdown format, which states user infos, events, preferences, etc.
The memo is summarized from the chats between user and a assistant.

### Output
#### Think
You need to think about what's topics/subtopics are mentioned in the memo, or what implications can be inferred from the memo.
#### Profile
After your steps of thinking, you need to extract the facts and preferences from the memo and place them in order list:
- TOPIC{tab}SUB_TOPIC{tab}MEMO
For example:
- basic_info{tab}name{tab}melinda
- work{tab}title{tab}software engineer
For each line is a fact or preference, containing:
1. TOPIC: topic represents of this preference
2. SUB_TOPIC: the detailed topic of this preference
3. MEMO: the extracted infos, facts or preferences of `user`
those elements should be separated by `{tab}` and each line should be separated by `\n` and started with "- ".

Final output template:
```
[POSSIBLE TOPICS THINKING...]
---
- TOPIC{tab}SUB_TOPIC{tab}MEMO
- ...
```

## Extraction Examples
Here are some few shot examples:
{examples}
Return the facts and preferences in a markdown list format as shown above.
Only extract the attributes with actual values, if the user does not provide any value, do not extract it.
You need to first think, then extract the facts and preferences from the memo.


#### Topics Guidelines
Below is the list of topics and subtopics that you should focus on collecting and extracting:
{topic_examples}


Remember the following:
- If the user mentions time-sensitive information, try to infer the specific date from the data.
- Use specific dates when possible, never use relative dates like "today" or "yesterday" etc.
- If you do not find anything relevant in the below conversation, you can return an empty list.
- Make sure to return the response in the format mentioned in the formatting & examples section.
- You should infer what's implied from the conversation, not just what's explicitly stated.
- Place all content related to this topic/sub_topic in one element, no repeat.
- The memo will have two types of time, one is the time when the memo is mentioned, the other is the time when the event happened. Both are important, don't mix them up.

Now perform your task.
Following is a conversation between the user and the assistant. You have to extract/infer the relevant facts and preferences from the conversation and return them in the list format as shown above.
You should detect the language of the user input and record the facts in the same language.
"""


def pack_input(already_input, memo_str, strict_mode: bool = False):
    header = ""
    if strict_mode:
        header = "Don't extract topics/subtopics that are not mentioned in #### Topics Guidelines, otherwise your answer is invalid!"
    return f"""{header}
#### User Before topics
{already_input}
Don't output the topics and subtopics that are not mentioned in the following conversation.
#### Memo
{memo_str}
"""


def get_default_profiles() -> str:
    return user_profile_topics.get_prompt()


def get_prompt(topic_examples: str) -> str:
    sys_prompt = CONFIG.system_prompt or DEFAULT_JOB
    examples = "\n\n".join(
        [
            f"""<example>
<input>{p[0]}</input>
<output>
{pack_profiles_into_string(p[1])}
</output>
</example>
"""
            for p in EXAMPLES
        ]
    )
    return FACT_RETRIEVAL_PROMPT.format(
        system_prompt=sys_prompt,
        examples=examples,
        tab=CONFIG.llm_tab_separator,
        topic_examples=topic_examples,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt(get_default_profiles()))



================================================
FILE: src/server/api/memobase_server/prompts/merge_profile.py
================================================
from datetime import datetime
from .utils import pack_merge_action_into_string
from ..env import CONFIG

ADD_KWARGS = {
    "prompt_id": "merge_profile",
}

MERGE_FACTS_PROMPT = """You are responsible for maintaining user memos.
Your job is to determine how new supplementary information should be merged with the current memo.
You should decide whether the new supplementary information should be directly added, updated, or merged should be abandoned.
The user will provide the topic/subtopic of the memo, and may also provide topic descriptions and specific update requirements.

Here are your output actions:
1. Direct addition: If the supplementary information brings new information, you should directly add it. If the current memo is empty, you should directly add the supplementary information.
2. Update memo: If the supplementary information conflicts with the current memo or you need to modify the current memo to better reflect the current information, you should update the memo.
3. Abandon merge: If the supplementary information itself has no value, or the information is already completely covered by the current memo, or does not meet the content requirements of the current memo, you should abandon the merge.

## Thinking
Before you output an action, you need to think about the following:
1. Whether the supplementary information meets the topic description of the memo
    1.1. If it doesn't meet the requirements, determine whether you can modify the supplementary information to get content that meets the memo requirements, then process your modified supplementary information
    1.2. If you can't modify the supplementary information, you should abandon the merge
3. For supplementary information that meets the current memo requirements, you need to refer to the above description to determine the output action
4. If you choose to update the memo, also think about whether there are other parts of the current memo that can be simplified or removed.

Additional situations:
1. The current memo may be empty. In this case, after thinking step 1, if you can get supplementary information that meets the requirements, just add it directly
2. If the update requirements are not empty, you need to refer to the user's update requirements for thinking

## Output Actions
### Direct Addition
```
- APPEND{tab}APPEND
```
When choosing direct addition, output the `APPEND` word directly, without repeating the content
### Update Memo
```
- UPDATE{tab}[UPDATED_MEMO]
```
When choosing to update the memo, you need to rewrite the updated memo in the `[UPDATED_MEMO]` section
### Abort Merge
```
- ABORT{tab}ABORT
```
When choosing to abandon the merge, output the `ABORT` word directly, without repeating the content

## Output Template
Based on the above instructions, your output should be in the following format:

THOUGHT
---
ACTION

Where:
- `THOUGHT` is your thinking process
- `ACTION` is your output action
For example:
```example
The supplementary information mentions that the user's current learning goal is to prepare for final exams, and the current topic description records the user's learning goals, which meets the requirements. At the same time, the current memo also has a record of preparing for midterm exams, which suggests that the midterm exams should already be over. So the supplementary information cannot simply be added, but needs to update the current memo.
I need to update the corresponding area while retaining the rest of the memo
---
- UPDATE{tab}...Currently self-studying Japanese using Duolingo, hoping to pass the Japanese Level 2 exam [mentioned on 2025/05/05]; Preparing for final exams [mentioned on 2025/06/01];
```

Follow these instructions:
- Strictly adhere to the correct output format.
- Ensure the final memo does not exceed 5 sentences. Always keep it concise and output the key points of the memo.
- Never make up content not mentioned in the input.
- Preserve time annotations from both old and new memos (e.g.: XXX[mentioned on 2025/05/05, occurred in 2022]).
- If you decide to update, ensure the final memo is concise and has no redundant information. (e.g.: "User is sad; User's mood is sad" == "User is sad")

That's all the content, now execute your work.
"""


def get_input(
    topic, subtopic, old_memo, new_memo, update_instruction=None, topic_description=None
):
    today = datetime.now().astimezone(CONFIG.timezone).strftime("%Y-%m-%d")
    return f"""Today is {today}.
## Memo Update Instruction
{update_instruction or "[empty]"}
### Memo Topic Description
{topic_description or "[empty]"}
## Memo Topic
{topic}, {subtopic}
## Current Memo
{old_memo or "[empty]"}
## Supplementary Information
{new_memo}
"""


def get_prompt() -> str:
    return MERGE_FACTS_PROMPT.format(
        tab=CONFIG.llm_tab_separator,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/merge_profile_yolo.py
================================================
from .utils import pack_merge_action_into_string
from ..env import CONFIG

ADD_KWARGS = {"prompt_id": "merge_profile_yolo"}

MERGE_FACTS_PROMPT = """You are responsible for maintaining user memos.
Your job is to determine how new supplementary information should be merged with current memos.
Users will provide a series of memo topics/subtopics, along with topic descriptions and specific update requirements (which may be empty).
For each piece of supplementary information, you should determine whether the new information should be directly added, update the current corresponding memo, or be discarded.

## Input Format
```
{{
    "memo_id": "1",
    "new_info": "",
    "current_memo": "",
    "topic": "",
    "subtopic": "",
    "topic_description": "",
    "update_instruction": "",
}}
{{
    "memo_id": "2",
    ...
}}
...
```
When evaluating each memo, you need to consider whether it aligns with the topic and update description.

Here are your output actions:
1. Direct Add: If the supplementary information brings new insights, you should add it directly. If the current memo is empty, you should add the supplementary information directly.
2. Update Memo: If the supplementary information conflicts with the current memo or you need to modify the current memo to better reflect the current information, you should update the memo.
3. Discard Merge: If the supplementary information has no value, is completely contained within the current memo, or doesn't meet the current memo's content requirements, you should discard the merge.

## Reasoning
Before outputting your action, you need to first consider the following:
1. Whether the supplementary information aligns with the memo's topic description
    1.1. If it doesn't align, determine if you can modify the supplementary information to meet the memo requirements, then process your modified supplementary information
    1.2. If you cannot modify the supplementary information to satisfy the topic description, you should discard the merge
3. For supplementary information that meets the current memo requirements, you need to refer to the above description to determine your output action
4. If you choose to update the memo, also consider whether other parts of the current memo can be streamlined or removed.

Additional considerations:
1. The current memo may be empty. In this case, after reasoning step 1, if you can obtain supplementary information that meets the requirements, add it directly
2. If the update requirement is not empty, you need to refer to the user's update requirements in your reasoning

## Output Actions
Assuming you are processing the Nth piece of supplementary information (memo_id=N), you should make the following judgment:
### Direct Add
```
N. APPEND{tab}APPEND
```
If choosing to add directly, simply output the word `APPEND`, no need to restate the content
### Update Memo
```
N. UPDATE{tab}[UPDATED_MEMO]
```
In `[UPDATED_MEMO]`, you need to rewrite the complete updated current memo
### Discard Merge
```
N. ABORT{tab}ABORT
```
If choosing to discard the merge, simply output the word `ABORT`, no need to restate the content

## Output Template
Based on the above instructions, your output should follow this template:

THOUGHT
---
1. ACTION{tab}...
2. ACTION{tab}...
...

Where:
- `THOUGHT` is your reasoning process
- `N. ACTION{tab}...` is your operation for the Nth piece of supplementary information (memo_id=N)

## Examples
### Input Example
{{
    "memo_id": "1",
    "new_info": "Preparing for final exams [mentioned on 2025/06/01]",
    "current_memo": "Preparing for midterm exams [mentioned on 2025/04/01]",
    "topic": "Study",
    "subtopic": "Exam goals",
    "update_instruction": "Each time you update goals, consider whether there are outdated or conflicting goals and remove them",
}}
{{
    "memo_id": "2",
    "new_info": "Using Duolingo to self-study Japanese",
    "current_memo": "",
    "topic": "Study",
    "subtopic": "Software usage",
}}
{{
    "memo_id": "3",
    "new_info": "User likes eating hot pot",
    "current_memo": "",
    "topic": "Interests",
    "subtopic": "Sports",
}}

### Output Example
```
The supplementary information mentions that the user's current study goal is to prepare for final exams, which aligns with the topic description of recording the user's study goals. However, there's a conflict between final exams and midterm exams, so we need to remove the midterm exam goal and update it to final exams.
Additionally, the user mentioned they are using Duolingo for language learning, which meets the software usage requirement. Since memo ID 2 has an empty current memo, we can add it directly.
Liking hot pot doesn't belong to sports interests, and we cannot derive potential interests from this information, so we discard the merge.
---
1. UPDATE{tab}Preparing for final exams [mentioned on 2025/06/01];
2. APPEND{tab}APPEND
3. ABORT{tab}ABORT
```

## Requirements
You must follow these requirements:
- Strictly adhere to the correct output format.
- Ensure updated memos do not exceed 5 sentences. Always maintain conciseness and output memo key points.
- Never fabricate content not mentioned in the input.
- Preserve time annotations from old and new memos (e.g., XXX[mentioned on 2025/05/05, occurred in 2022]).
- If deciding to update, ensure the final memo is concise without redundant information (e.g., "User is sad; User's mood is sad" == "User is sad").

That's all the content. Now execute your work.
"""


def get_input(
    memos_list: list[dict],
):
    return f"""
{memos_list}
"""


def get_prompt() -> str:
    return MERGE_FACTS_PROMPT.format(
        tab=CONFIG.llm_tab_separator,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/organize_profile.py
================================================
[Binary file]


================================================
FILE: src/server/api/memobase_server/prompts/pick_related_profiles.py
================================================
from ..env import CONFIG
from ..models.blob import OpenAICompatibleMessage

ADD_KWARGS = {
    "prompt_id": "pick_related_profiles",
}

# EXAMPLES = [
#     {
#         "memos": """<memos>
# 0. basic_info, age, 25
# 1. basic_info, name, Lisa
# 2. health, allergies, peanuts and shellfish
# 3. dietary, restrictions, vegetarian
# 4. health, medication, antihistamines
# 5. dietary, preferences, spicy food
# 6. work, position, Graphic Designer
# 7. technology, devices, MacBook Pro and iPhone
# 8. work, company, Memobase
# 9. technology, software, Photoshop and Illustrator
# 10. education, university, Stanford
# 11. education, degree, Physics
# </memos>""",
#         "examples": [
#             {
#                 "context": """<context>
# Q: Hello!
# </context>""",
#                 "output": '{"reason": "user is starting a new conversation, having some backgrounds is helpful for later", "ids": [0,1]}',
#             },
#             {
#                 "context": """<context>
# Q: What's your opinion on the latest AI tools?
# </context>""",
#                 "output": '{"reason": "user work and education background is helpful when choosing AI tools", "ids": [9,6,7,11]}',
#             },
#             {
#                 "context": """<context>
# Q: How do I reset my password?
# </context>""",
#                 "output": '{"reason": "user devices and platforms are helpful when resetting password", "ids": [7]}',
#             },
#             {
#                 "context": """<context>
# Q: What's the weather forecast for tomorrow?
# </context>""",
#                 "output": '{"reason": "Location is needed for weather, working company and college can be used to guess the location", "ids": [9,10]}',
#             },
#         ],
#     },
# ]


PROMPT = """You are a professional journalist, and your task is to select all possible user's memos to enrich the conversation.

## Input Template
Below is the input template:
```input
<memos>
1. TOPIC1, SUB_TOPIC1, MEMO_CONTENT1
2. TOPIC2, SUB_TOPIC2, MEMO_CONTENT2
...
</memos>

<context>
Q: ...
A: ...
...
Q: ... # last query
</context>
```
<memos> contains all the user's memos in markdown orderlist, the number bullet is the memo ID.
Find the memos that will enrich the conversation directly/indirectly.

## Output
You need to think how to enrich the conversation, then output the memo IDs in a plain JSON object.
### Format
```output
{{"reason": "YOUR THINKING","ids": [NEED_ID_0,NEED_ID_1,...]}}
```
First infer from the context what kind of topics will help the conversation in "reason", then select the all possible memos IDS in "ids"
where NEED_ID_I is the i-th needed memo id.
You may select up to {max_num} memos.

## Requirements
- Deeply understand the current context, and try to select possible memos that will enrich the conversation.
- Return a plain JSON object with the format above ({{"reason": str,"ids": list[int]}})
- Don't select semantically duplicated memos, i.e. if a memo is already included in another memo, don't select it.
"""


def get_prompt(max_num: int) -> str:
    return PROMPT.format(
        max_num=max_num,
        # examples=pack_examples(),
    )


def pack_example(e: dict) -> str:
    responses = "\n".join(
        [
            f"""<case>
{c['context']}
Output: {c['output']}
</case>
"""
            for c in e["examples"]
        ]
    )
    prompt = f"""{e['memos']}

Below are some cases of different current context to this memos:
{responses}
"""
    return prompt


def pack_examples() -> str:
    examples = [pack_example(e) for e in EXAMPLES]
    return "\n".join(examples)


def get_input(messages: list[OpenAICompatibleMessage], topic_lists: list[dict]) -> str:
    memos = "\n".join(
        [
            f"{i}. {t['topic']},{t['sub_topic']},{t['content']}"
            for i, t in enumerate(topic_lists)
        ]
    )
    context = "\n".join(
        [f"Q: {m.content}" if m.role == "user" else f"A: {m.content}" for m in messages]
    )
    prompt = f"""<memos>
{memos}
</memos>

<context>
{context}
</context>
"""
    return prompt


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt(max_num=10))



================================================
FILE: src/server/api/memobase_server/prompts/profile_init_utils.py
================================================
import yaml

from ..env import CONFIG, LOG, ProfileConfig
from ..types import UserProfileTopic, EventTag


def formate_profile_topic(topic: UserProfileTopic) -> str:
    if not topic.sub_topics:
        return f"- {topic.topic}"
    return f"- {topic.topic} ({topic.description or ''})\n" + "\n".join(
        [
            f"  - {sp.name}" + (f"({sp.description})" if sp.description else "")
            for sp in topic.sub_topics
        ]
    )


def modify_default_user_profile(CANDIDATE_PROFILE_TOPICS):
    if CONFIG.overwrite_user_profiles is not None:
        CANDIDATE_PROFILE_TOPICS = [
            UserProfileTopic(
                up["topic"],
                description=up.get("description", None),
                sub_topics=up["sub_topics"],
            )
            for up in CONFIG.overwrite_user_profiles
        ]
    elif CONFIG.additional_user_profiles:
        _addon_user_profiles = [
            UserProfileTopic(
                up["topic"],
                description=up.get("description", None),
                sub_topics=up["sub_topics"],
            )
            for up in CONFIG.additional_user_profiles
        ]
        CANDIDATE_PROFILE_TOPICS.extend(_addon_user_profiles)
    return CANDIDATE_PROFILE_TOPICS


def read_out_profile_config(config: ProfileConfig, default_profiles: list):
    if config.overwrite_user_profiles:
        profile_topics = [
            UserProfileTopic(
                up["topic"],
                description=up.get("description", None),
                sub_topics=up["sub_topics"],
            )
            for up in config.overwrite_user_profiles
        ]
        return profile_topics
    elif config.additional_user_profiles:
        profile_topics = [
            UserProfileTopic(
                up["topic"],
                description=up.get("description", None),
                sub_topics=up["sub_topics"],
            )
            for up in config.additional_user_profiles
        ]
        return default_profiles + profile_topics
    return default_profiles


def get_specific_subtopics(
    topic: str, CANDIDATE_PROFILE_TOPICS: list[UserProfileTopic]
) -> list[str]:
    sps = [
        sp
        for up in CANDIDATE_PROFILE_TOPICS
        for sp in up.sub_topics
        if up.topic == topic
    ]
    if not len(sps):
        return "None"
    return [
        f"  - {sp['name']}"
        + (f"({sp['description']})" if sp.get("description") else "")
        for sp in sps
    ]


def export_user_profile_to_yaml(profiles: list[UserProfileTopic]):
    final_results = {"profiles": []}
    for p in profiles:
        res = {"topic": p.topic}
        if p.description:
            res["description"] = p.description
        res["sub_topics"] = []
        for sp in p.sub_topics:
            res["sub_topics"].append(
                {"name": sp["name"], "description": sp["description"]}
            )
        final_results["profiles"].append(res)
    return yaml.dump(final_results, allow_unicode=True)


def init_event_tags(event_tags: list[dict]) -> list[EventTag]:
    event_tags = [
        et if isinstance(et, dict) else {"name": et, "description": None}
        for et in event_tags
    ]
    return [EventTag(et["name"], et.get("description", None)) for et in event_tags]


def read_out_event_tags(config: ProfileConfig) -> list[EventTag]:
    if config.event_tags is None:
        return init_event_tags(CONFIG.event_tags)
    return init_event_tags(config.event_tags)



================================================
FILE: src/server/api/memobase_server/prompts/summary_entry_chats.py
================================================
from ..env import CONFIG

ADD_KWARGS = {
    "prompt_id": "summary_entry_chats",
}
SUMMARY_PROMPT = """You are a expert of logging personal info, schedule, events from chats.
You will be given a chats between a user and an assistant.

## Requirement
- You need to list all possible user info, schedule and events
- {additional_requirements}
- If the user event/schedule has specific mention time or event happen time. Convert the event date info in the message based on [TIME] after your log. for example
    Input: `[2024/04/30] user: I bought a new car yesterday!`
    Output: `user bought a new car. [mention 2024/04/30, buy car in 2024/04/29]`
    Input: `[2024/04/30] user: I bought a car 4 years ago!`
    Output: `user bought a car. [mention 2024/04/30, buy car in 2020]`
    Explain: because you don't know the exact date, only year, so 2024-4=2020. or you can log at [4 years before 2024/04/30]
    Input: `[2024/04/30] user: I bought a new car last week!`
    Output: `user bought a new car. [mention 2024/04/30, buy car in 2024/04/30 a week before]`
    Explain: because you don't know the exact date.
    Input: `[...] user: I bought a new car last week!`
    Output: `user bought a new car.`
    Explain: because you don't know the exact date, so don't attach any date.

### Important Info
Below is the topics/subtopics you should log from the chats.
<topics>
{topics}
</topics>
Below is the important attributes you should log from the chats.
<attributes>
{attributes}
</attributes>


## Input Format
### Already Logged
You will receive a list of previous logging result, you should also log the relevant infos that maybe related to those already logged.
Pervious result in organized in Profile-format:
- TOPIC{separator}SUBTOPIC{separator}CONTENT... // maybe truncated

### Input Chats
You will receive a conversation between the user and the assistant. The format of the conversation is:
- [TIME] NAME: MESSAGE
where NAME is ALIAS(ROLE) or just ROLE, when ALIAS is available, use ALIAS to refer user/assistant.
MESSAGE is the content of the conversation.
TIME is the time of this message happened, so you need to convert the date info in the message based on TIME if necessary.

## Output Format
- LOGGING[TIME INFO] // TYPE
Output your logging result in Markdown unorder list format.
For example:
```
- Jack paint a picture about his kids.[mention 2023/1/23] // event
- User's alias is Jack, assistant is Melinda. // info
- Jack mentioned his work is software engineer in Memobase. [mention 2023/1/23] // info
- Jack plans to go the gym. [mention 2023/1/23, plan in 2023/1/24] // schedule
...
```
Always add specific mention time of your log, and the event happen time if possible.
Remember, make sure your logging is pure and concise, any time info should move to [TIME INFO] block.

## Content Requirement
- You need to list all possible user info, schedule and events
- {additional_requirements}

Now perform your task.
"""


def pack_input(already_logged_str: str, chat_strs: str):
    return f"""### Already Logged
{already_logged_str}
### Input Chats
{chat_strs}
"""


def get_prompt(
    topic_examples: str, attribute_examples: str, additional_requirements: str = ""
) -> str:
    return SUMMARY_PROMPT.format(
        topics=topic_examples,
        attributes=attribute_examples,
        additional_requirements=additional_requirements,
        separator=CONFIG.llm_tab_separator,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/summary_profile.py
================================================
from ..env import CONFIG

ADD_KWARGS = {
    "prompt_id": "summary_profile",
}
SUMMARY_PROMPT = """You are given a user profile with some information about the user.
Extract high-level preference from the profile

## Requirement
- Extract high-level preference from the profile
- The preference should be the most important and representative preference of the user.
  For example, the original perference is "user likes Chocolate[mentioned in 2023/1/23], Ice cream, Cake, Cookies, Brownies[mentioned in 2023/1/24]...", then your extraction should be "user maybe likes sweet food(cake/cookies...)".
- The preference should be concise and clear.

The result should use the same language as the input.
"""


def get_prompt() -> str:
    return SUMMARY_PROMPT


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/user_profile_topics.py
================================================
from ..env import CONFIG, LOG
from .profile_init_utils import (
    UserProfileTopic,
    formate_profile_topic,
    modify_default_user_profile,
)


CANDIDATE_PROFILE_TOPICS: list[UserProfileTopic] = [
    UserProfileTopic(
        "basic_info",
        sub_topics=[
            "Name",
            {
                "name": "Age",
                "description": "integer",
            },
            "Gender",
            "birth_date",
            "nationality",
            "ethnicity",
            "language_spoken",
        ],
    ),
    UserProfileTopic(
        "contact_info",
        sub_topics=[
            "email",
            "phone",
            "city",
            "country",
        ],
    ),
    UserProfileTopic(
        "education",
        sub_topics=[
            "school",
            "degree",
            "major",
        ],
    ),
    UserProfileTopic(
        "demographics",
        sub_topics=[
            "marital_status",
            "number_of_children",
            "household_income",
        ],
    ),
    UserProfileTopic(
        "work",
        sub_topics=[
            "company",
            "title",
            "working_industry",
            "previous_projects",
            "work_skills",
        ],
    ),
    UserProfileTopic(
        "interest",
        sub_topics=[
            "books",
            "movies",
            "music",
            "foods",
            "sports",
        ],
    ),
    UserProfileTopic(
        "psychological",
        sub_topics=["personality", "values", "beliefs", "motivations", "goals"],
    ),
    UserProfileTopic(
        "life_event",
        sub_topics=["marriage", "relocation", "retirement"],
    ),
]

CANDIDATE_PROFILE_TOPICS = modify_default_user_profile(CANDIDATE_PROFILE_TOPICS)


def get_prompt(profiles: list[UserProfileTopic] = CANDIDATE_PROFILE_TOPICS):
    return "\n".join([formate_profile_topic(up) for up in profiles]) + "\n..."


if CONFIG.language == "en":
    LOG.info(f"User profiles: \n{get_prompt()}")

if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/utils.py
================================================
import re
import json
import difflib
from typing import TypedDict
from ..env import LOG, CONFIG
from ..types import attribute_unify
from ..models.response import AIUserProfiles, AIUserProfile
from ..models.blob import ChatBlob
from ..utils import get_blob_str

UpdateResponse = TypedDict("UpdateResponse", {"action": str, "memo": str})


ORDER_LIST_PATTERN = r"^(\d+)\.(.*)"
MERGE_ACTION_SPACE = {
    "APPEND",
    "UPDATE",
    "ABORT",
}
EXCLUDE_PROFILE_VALUES = [
    # Chinese variations
    "无",
    "未提及",
    "不清楚",
    "用户未提及",
    "对话未提及",
    "未知",
    "不详",
    "没有提到",
    "没有说明",
    "无法确定",
    "无相关内容",
    "未明确提及",
    "无明确信息",
    "无符合信息",
    # English variations
    "none",
    "unknown",
    "not mentioned",
    "not mentioned by user",
    "not mentioned in the conversation",
    "unclear",
    "unspecified",
    "not specified",
    "not determined",
    "no information",
    "n/a",
    "no related content",
    "no related information",
    "no matched information",
]


def tag_chat_blobs_in_order_xml(
    blobs: list[ChatBlob],
):
    return "\n".join(get_blob_str(b) for b in blobs)


def extract_first_complete_json(s: str):
    """Extract the first complete JSON object from the string using a stack to track braces."""
    stack = []
    first_json_start = None

    for i, char in enumerate(s):
        if char == "{":
            stack.append(i)
            if first_json_start is None:
                first_json_start = i
        elif char == "}":
            if stack:
                start = stack.pop()
                if not stack:
                    first_json_str = s[first_json_start : i + 1]
                    try:
                        # Attempt to parse the JSON string
                        return json.loads(first_json_str.replace("\n", ""))
                    except json.JSONDecodeError as e:
                        LOG.error(
                            f"JSON decoding failed: {e}. Attempted string: {first_json_str[:50]}..."
                        )
                        return None
                    finally:
                        first_json_start = None
    LOG.warning("No complete JSON object found in the input string.")
    return None


def parse_value(value: str):
    """Convert a string value to its appropriate type (int, float, bool, None, or keep as string). Work as a more broad 'eval()'"""
    value = value.strip()

    if value == "null":
        return None
    elif value == "true":
        return True
    elif value == "false":
        return False
    else:
        # Try to convert to int or float
        try:
            if "." in value:  # If there's a dot, it might be a float
                return float(value)
            else:
                return int(value)
        except ValueError:
            # If conversion fails, return the value as-is (likely a string)
            return value.strip('"')  # Remove surrounding quotes if they exist


def extract_values_from_json(json_string, allow_no_quotes=False):
    """Extract key values from a non-standard or malformed JSON string, handling nested objects."""
    extracted_values = {}

    # Enhanced pattern to match both quoted and unquoted values, as well as nested objects
    regex_pattern = r'(?P<key>"?\w+"?)\s*:\s*(?P<value>{[^}]*}|".*?"|[^,}]+)'

    for match in re.finditer(regex_pattern, json_string, re.DOTALL):
        key = match.group("key").strip('"')  # Strip quotes from key
        value = match.group("value").strip()

        # If the value is another nested JSON (starts with '{' and ends with '}'), recursively parse it
        if value.startswith("{") and value.endswith("}"):
            extracted_values[key] = extract_values_from_json(value)
        else:
            # Parse the value into the appropriate type (int, float, bool, etc.)
            extracted_values[key] = parse_value(value)

    if not extracted_values:
        LOG.warning("No values could be extracted from the string.")

    return extracted_values


def convert_response_to_json(response: str) -> dict:
    """Convert response string to JSON, with error handling and fallback to non-standard JSON extraction."""
    prediction_json = extract_first_complete_json(response)

    if prediction_json is None:
        LOG.info("Attempting to extract values from a non-standard JSON string...")
        prediction_json = extract_values_from_json(response, allow_no_quotes=True)

    if prediction_json is None:
        LOG.error("JSON extract failed.")

    return prediction_json


def pack_merge_action_into_string(action: dict) -> str:
    return f"- {action['action']}{CONFIG.llm_tab_separator}{action['memo']}"


def parse_string_into_merge_action(results: str) -> dict | None:
    lines = [l for l in results.split("\n") if l.strip()]
    lines = [l for l in lines if l.startswith("- ")]
    if not len(lines):
        return None
    line = lines[0][2:]
    parts = line.split(CONFIG.llm_tab_separator)
    if not len(parts) == 2:
        return None
    return {
        "action": parts[0].upper().strip(),
        "memo": parts[1].strip(),
    }


def parse_string_into_merge_yolo_action(results: str) -> dict[int, UpdateResponse]:
    action_section = results
    memo_results = {}
    lines = [l.strip() for l in action_section.split("\n") if l.strip()]
    for l in lines:
        m = re.match(ORDER_LIST_PATTERN, l)
        if not m:
            continue
        order = int(m.group(1))
        clean_line = m.group(2).strip()
        parse_line = clean_line.split(CONFIG.llm_tab_separator)
        if len(parse_line) < 2:
            continue
        action = parse_line[0].upper().strip()
        memo = CONFIG.llm_tab_separator.join(parse_line[1:]).strip()
        if action not in MERGE_ACTION_SPACE:
            continue
        memo_results[order] = UpdateResponse(action=action, memo=memo)
    return memo_results


def pack_profiles_into_string(profiles: AIUserProfiles) -> str:
    lines = [
        f"- {attribute_unify(p.topic)}{CONFIG.llm_tab_separator}{attribute_unify(p.sub_topic)}{CONFIG.llm_tab_separator}{p.memo.strip()}"
        for p in profiles.facts
    ]
    if not len(lines):
        return "NONE"
    return "\n".join(lines)


def meaningless_profile_memo(memo: str) -> bool:
    maybe_meaningless = difflib.get_close_matches(
        memo.strip().lower(), EXCLUDE_PROFILE_VALUES
    )
    if len(maybe_meaningless) > 0:
        LOG.info(f"Meaningless profile memo: {memo}")
        return True
    return False


def parse_string_into_profiles(response: str) -> AIUserProfiles:
    lines = response.split("\n")
    lines = [l.strip() for l in lines if l.strip()]
    facts = [parse_line_into_profile(l) for l in lines]
    facts = [f for f in facts if f is not None]
    return AIUserProfiles(facts=facts)


def parse_line_into_profile(line: str) -> AIUserProfile | None:
    if not line.startswith("- "):
        return None
    line = line[2:]
    parts = line.split(CONFIG.llm_tab_separator)
    if not len(parts) == 3:
        return None
    topic, sub_topic, memo = parts
    if meaningless_profile_memo(memo):
        return None
    return AIUserProfile(
        topic=attribute_unify(topic),
        sub_topic=attribute_unify(sub_topic),
        memo=memo.strip(),
    )


def parse_string_into_subtopics(response: str) -> list:
    lines = response.split("\n")
    lines = [l.strip() for l in lines if l.strip()]
    facts = [parse_line_into_subtopic(l) for l in lines]
    facts = [f for f in facts if f is not None]
    return facts


def parse_line_into_subtopic(line: str) -> dict:
    if not line.startswith("- "):
        return None
    line = line[2:]
    parts = line.split(CONFIG.llm_tab_separator)
    if not len(parts) == 2:
        return None
    if meaningless_profile_memo(parts[1].strip()):
        return None
    return {"sub_topic": attribute_unify(parts[0].strip()), "memo": parts[1].strip()}


if __name__ == "__main__":
    print(
        parse_string_into_merge_yolo_action(
            """TTT
---
1. ABORT::ABORT
2. ABORT::ABORT
3. ABORT::ABORT
4. ABORT::ABORT
5. APPEND::APPEND
6. APPEND::APPEND"""
        )
    )



================================================
FILE: src/server/api/memobase_server/prompts/zh_extract_profile.py
================================================
from . import zh_user_profile_topics
from ..models.response import AIUserProfiles
from ..env import CONFIG, LOG
from .utils import pack_profiles_into_string

ADD_KWARGS = {
    "prompt_id": "zh_extract_profile",
}

EXAMPLES = [
    (
        """- 用户向助手问好。
""",
        AIUserProfiles(**{"facts": []}),
    ),
    (
        """
- 用户最喜欢的电影是《盗梦空间》和《星际穿越》 [提及于2025/01/01]
- 用户最喜欢的电影是《信条》 [提及于2025/01/02]
""",
        AIUserProfiles(
            **{
                "facts": [
                    {
                        "topic": "兴趣爱好",
                        "sub_topic": "电影",
                        "memo": "《盗梦空间》、《星际穿越》[提及于2025/01/01]；最喜欢的是《信条》[提及于2025/01/02]",
                    },
                    {
                        "topic": "兴趣爱好",
                        "sub_topic": "电影导演",
                        "memo": "用户似乎是克里斯托弗·诺兰的忠实粉丝",
                    },
                ]
            }
        ),
    ),
]

DEFAULT_JOB = """你是一位专业的心理学家。
你的责任是仔细阅读用户的备忘录并以结构化的格式提取用户的重要信息。
然后提取相关且重要的事实、用户偏好，这些信息将有助于评估用户的状态。
你不仅要提取明确陈述的信息，还要推断对话中隐含的信息。
你要使用与用户输入相同的语言来记录这些事实。
"""

FACT_RETRIEVAL_PROMPT = """{system_prompt}

## 格式
### 输入
#### 主题建议
你会得到一些需要重点收集和提取的“用户相关”的主题和子主题。
不要收集与用户自身没有关系的主题，会造成混淆。
比如如果备忘录里提到了别人的职位，不要生成一个"工作{tab}职位"的主题，这样会造成信息的混淆。除非是用户提到了自己的工作
如果你认为有必要，可以创建自己的主题/子主题，除非用户明确要求不要创建新的主题/子主题。

#### 已有的主题
你会得到用户已经与助手分享的主题和子主题。
如果对话中再次提到相同的主题/子主题，请考虑使用相同的主题/子主题。

#### 备忘录
你将收到一份用户的备忘录（Markdown格式），其中包含用户信息、事件、偏好等。
这些备忘录是从用户和助手的对话中总结出来的。

### 输出
#### 思考
你需要思考什么主题是可以从备忘录中提取出来的，或者可以推断出什么
#### 画像提取
在你的思考之后，你需要从备忘录中提取事实和偏好，并按顺序列出：
- TOPIC{tab}SUB_TOPIC{tab}MEMO
例如：
- 基本信息{tab}姓名{tab}melinda
- 工作{tab}职称{tab}软件工程师

每行代表一个事实或偏好，包含：
1. TOPIC: 主题，表示该偏好的类别
2. SUB_TOPIC: 详细主题，表示该偏好的具体类别
3. MEMO: 提取的用户相关信息、事实或偏好. MEMO中涉及到时间信息，请使用[...]来表示。
这些元素应以 `{tab}` 分隔，每行应以 `\n` 分隔，并以 "- " 开头。
如果思考后发现没有可以提取的，这部分不生成即可

最终输出模版如下：
```
POSSIBLE TOPICS THINKING
---
- TOPIC{tab}SUB_TOPIC{tab}MEMO
- ...
```

## 主题抽取示例
以下是一些示例：
{examples}
请按上述格式返回事实和偏好。

#### 主题建议
以下是你应该重点收集和提取的主题和子主题列表：
{topic_examples}

请记住以下几点：
- 如果用户提到时间敏感的信息，试图推理出具体的日期。
- 当可能时，请使用具体日期，而不是使用"今天"或"昨天"等相对时间。
- 如果在以下对话中没有找到任何相关信息，可以返回空列表。
- 确保按照格式和示例部分中提到的格式返回响应。
- 你应该推断对话中隐含的内容，而不仅仅是明确陈述的内容。
- 将所有与该主题/子主题相关的内容放在一个元素中，不要重复。
- 备忘录中会有两种时间，一种是这个备忘录被记录的时间，一种是备忘录中的事件发生的时间, 两种时间都很重要，不要混淆了, 你需要正确的提取时间信息并且在相关的memo后使用时间表示[...]
- 只提取有实际值的属性，如果用户没有提供任何值，请不要提取。

现在开始执行你的任务。
以下是用户的备忘录。你需要从中提取/推断相关的事实和偏好，并按上述格式返回。
你应该检测用户输入的语言，并用相同的语言记录事实。
"""


def pack_input(already_input, chat_strs, strict_mode: bool = False):
    header = ""
    if strict_mode:
        header = "不要提取#### 主题建议中没出现的主题/子主题， 否则你的回答是无效的！"
    return f"""{header}
#### 已有的主题
如果提取相关的主题/子主题，请考虑使用下面的主题/子主题命名:
{already_input}

#### 备忘录
请注意，不要输出任何关于备忘录中未提及的主题/子主题的信息:
{chat_strs}
"""


def get_default_profiles() -> str:
    return zh_user_profile_topics.get_prompt()


def get_prompt(topic_examples: str) -> str:
    sys_prompt = CONFIG.system_prompt or DEFAULT_JOB
    examples = "\n\n".join(
        [
            f"""<example>
<input>{p[0]}</input>
<output>
{pack_profiles_into_string(p[1])}
</output>
</example>
"""
            for p in EXAMPLES
        ]
    )
    return FACT_RETRIEVAL_PROMPT.format(
        system_prompt=sys_prompt,
        examples=examples,
        tab=CONFIG.llm_tab_separator,
        topic_examples=topic_examples,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt(get_default_profiles()))



================================================
FILE: src/server/api/memobase_server/prompts/zh_merge_profile.py
================================================
[Binary file]


================================================
FILE: src/server/api/memobase_server/prompts/zh_merge_profile_yolo.py
================================================
from .utils import pack_merge_action_into_string
from ..env import CONFIG

ADD_KWARGS = {"prompt_id": "zh_merge_profile_yolo"}

MERGE_FACTS_PROMPT = """你负责用户的备忘录的维护。
你的工作是判断新的补充信息如何与当前备忘录合并。
用户会提供一系列备忘录的主题/子主题，以及会提供主题描述和特定的更新要求(可能为空)。
对于每条补充信息，你应该判断新的补充信息是直接添加，还是更新当前对应的备忘录，或者放弃合并。

## 输入格式
```
{{
    "memo_id": "1",
    "new_info": "",
    "current_memo": "",
    "topic": "",
    "subtopic": "",
    "topic_description": "",
    "update_instruction": "",
}}
{{
    "memo_id": "2",
    ...
}}
...
```
当进行每条备忘录的判断的时候，你需要思考是否其符合主题以及更新描述

以下是你的输出动作：
1. 直接添加：如果补充信息带来了新的信息，你应该直接添加。如果当前备忘录为空的话，你应该直接添加补充信息。
2. 更新备忘录：如果补充信息与当前备忘录有冲突或者你需要修改当前备忘录才能更好的体现当前的信息，你应该更新备忘录。
3. 放弃合并：如果补充信息本身没有价值，或者信息已经被当前备忘录完全包含了，或者不符合当前备忘录的内容要求，你应该放弃合并。

## 思考
在你输出动作之前，你需要先思考如下的内容：
1. 补充信息是否符合备忘录的主题描述
    1.1. 如果不符合的话，判断是否可以从补充信息中修改得到符合备忘录要求的内容，然后处理你修改后的补充信息
    1.2. 如果无法通过修改补充信息使其满足主题描述，你应该放弃合并
3. 符合当前备忘录要求的补充信息，你需要参考上面的描述进行输出动作的判断
4. 如果选择更新备忘录的话，同时思考下是否当前备忘录有其余的部分可以精简或者去除。

额外情况：
1. 当前备忘录可能为空，在这个情况下，思考1之后如果可以得到符合要求的补充信息，直接添加即可
2. 如果更新要求不为空，你需要参考用户的更新要求进行思考

## 输出动作
假设你正在处理第N个补充信息(memo_id=N)，你应该进行如下的判断:
### 直接添加
```
N. APPEND{tab}APPEND
```
如果选择直接添加，直接输出`APPEND`单词即可，不需要复述内容
### 更新备忘录
```
N. UPDATE{tab}[UPDATED_MEMO]
```
在`[UPDATED_MEMO]`中，你需要重新写出更新后完整的当前备忘录
### 放弃合并
```
N. ABORT{tab}ABORT
```
如果选择放弃合并，直接输出`ABORT`单词即可，不需要复述内容

## 输出模版
根据上述说明，你的输出应该是如下的模版

THOUGHT
---
1. ACTION{tab}...
2. ACTION{tab}...
...

其中:
- `THOUGHT`是你的思考过程
- 思考和Action之间使用`---`隔开
- `N. ACTION{tab}...`是你对于第N个补充信息的操作(memo_id=N)



## 举例
### 输入距离
{{
    "memo_id": "1",
    "new_info": "准备期末考试[提及于2025/06/01]",
    "current_memo": "准备期中考试[提及于2025/04/01]",
    "topic": "学习",
    "subtopic": "考试目标",
    "update_instruction": "每次更新目标时，需要考虑是否有已经过时或者冲突的目标，并进行删除",
}}
{{
    "memo_id": "2",
    "new_info": "使用多邻国自学日语中",
    "current_memo": "",
    "topic": "学习",
    "subtopic": "使用软件",
}}
{{
    "memo_id": "3",
    "new_info": "用户喜欢吃火锅",
    "current_memo": "",
    "topic": "兴趣",
    "subtopic": "运动",
}}

### 输出举例
```
补充信息中提到了用户当前的学习目标是准备期末考试，当前主题描述记录的是用户的学习目标，符合要求。同时注意到期末考试和期中考试的冲突，所以需要移除期中考试的目标，更新为期末考试。
同时用户提到了他正在使用多邻国进行语言的学习，符合学习软件的需求，由于id为2的补充信息中当前备忘录为空，直接添加即可。
喜欢吃火锅并不属于兴趣/学习的爱好，也无法从这个信息中得到他潜在的爱好，所以放弃合并。
---
1. UPDATE{tab}准备期末考试中[提及于2025/06/01];
2. APPEND{tab}APPEND
3. ABORT{tab}ABORT
```

你需要遵循以下要求：
- 严格遵守正确的输出格式。
- 确保更新后备忘录不超过5句话。始终保持简洁并输出备忘录的要点。
- 永远不要编造输入中未提到的内容。
- 保留新旧备忘录中的时间标注（例如： XXX[提及于 2025/05/05, 发生于 2022]）。
- 如果决定更新，确保最终备忘录简洁且没有冗余信息。（例如："User is sad; User's mood is sad" == "User is sad"）

以上就是全部内容，现在执行你的工作。
"""


def get_input(
    memos_list: list[dict],
):
    return f"""
{memos_list}
"""


def get_prompt() -> str:
    return MERGE_FACTS_PROMPT.format(
        tab=CONFIG.llm_tab_separator,
    )


def get_kwargs() -> dict:
    return ADD_KWARGS


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/prompts/zh_summary_entry_chats.py
================================================
[Binary file]


================================================
FILE: src/server/api/memobase_server/prompts/zh_user_profile_topics.py
================================================
from ..env import CONFIG, LOG
from .profile_init_utils import (
    UserProfileTopic,
    formate_profile_topic,
    modify_default_user_profile,
)


CANDIDATE_PROFILE_TOPICS: list[UserProfileTopic] = [
    UserProfileTopic(
        "基本信息",
        sub_topics=[
            "用户姓名",
            {
                "name": "用户年龄",
                "description": "整数",
            },
            "性别",
            "出生日期",
            "国籍",
            "民族",
            "语言",
        ],
    ),
    UserProfileTopic(
        "联系信息",
        sub_topics=[
            "电子邮件",
            "电话",
            "城市",
            "省份",
        ],
    ),
    UserProfileTopic(
        "教育背景",
        sub_topics=[
            "学校",
            "学位",
            "专业",
            "毕业年份",
        ],
    ),
    UserProfileTopic(
        "人口统计",
        sub_topics=[
            "婚姻状况",
            "子女数量",
            "家庭收入",
        ],
    ),
    UserProfileTopic(
        "工作",
        sub_topics=[
            "公司",
            "职位",
            "工作地点",
            "参与项目",
            "工作技能",
        ],
    ),
    UserProfileTopic(
        "兴趣爱好",
        sub_topics=[
            "书籍",
            "电影",
            "音乐",
            "美食",
            "运动",
        ],
    ),
    UserProfileTopic(
        "生活方式",
        sub_topics=[
            {"name": "饮食偏好", "description": "例如：素食，纯素"},
            "运动习惯",
            "健康状况",
            "睡眠模式",
            "吸烟",
            "饮酒",
        ],
    ),
    UserProfileTopic(
        "心理特征",
        sub_topics=["性格特点", "价值观", "信仰", "动力", "目标"],
    ),
    UserProfileTopic(
        "人生大事",
        sub_topics=["婚姻", "搬迁", "退休"],
    ),
]


CANDIDATE_PROFILE_TOPICS = modify_default_user_profile(CANDIDATE_PROFILE_TOPICS)


def get_prompt(profiles: list[UserProfileTopic] = CANDIDATE_PROFILE_TOPICS):
    return "\n".join([formate_profile_topic(up) for up in profiles]) + "\n..."


if CONFIG.language == "zh":
    LOG.info(f"User profiles: \n{get_prompt()}")

if __name__ == "__main__":
    from .profile_init_utils import export_user_profile_to_yaml

    # print(get_prompt())
    print(export_user_profile_to_yaml(CANDIDATE_PROFILE_TOPICS))



================================================
FILE: src/server/api/memobase_server/prompts/roleplay/zh_detect_interest.py
================================================
[Binary file]


================================================
FILE: src/server/api/memobase_server/prompts/roleplay/zh_infer_plot.py
================================================
import re
from ...env import CONFIG
from ...models.blob import OpenAICompatibleMessage

ADD_KWARGS = {
    "prompt_id": "roleplay.zh_infer_plot",
}
INFER_PLOT_PROMPT = """你是一个专业的编剧，你的任务是顺延User和Assistant的对话设计新的剧情，让用户可以有新的，感兴趣的剧情体验

## 输入格式
- Assistant的背景放在<assistant_role_setting>标签中
- User的背景放在<user_role_setting>标签中
- 之前的已经尝试过故事剧情放在<before_plots>标签中，你需要注意不要重复之前的剧情
- 最新的对话放在<latest_dialogues>标签中

## 输出新剧情
你需要着重参考latest_dialogues中的对话，设计新的剧情。
你也可以参考部分Assistant和User的背景设定，但请注意不要重复之前的剧情
### 输出格式
```xml
<themes>
...
</themes>
<overview>
....
</overview>
<timeline>
1. ...
2. ...
...
</timeline>
```
- 在themes中，你需要选定接下来剧情的一些基调（e.g. 爱情, 科幻, 悬疑, 奇幻 ...）, 请注意，你选取的基调不能和用户当前对话偏离过远，如果对话中没有明显的基调，则需要参考user和assistant的背景设定。
- 在overview中，你需要用1-2句话描述新剧情的发展概要
- 在timeline中，你需要描述新剧情的具体剧情安排，一步步从最新对话的状态过渡到你设计的剧情当中，用orderlist的格式输出，每一步剧情简单描述即可，不需要涉及到具体的对话设计，只是剧情框架。 控制在5-10步

你输出的剧情要满足如下的要求：
- 不能跳跃：剧情第一步要从当前的对话开始往后延续
- 不能老套：不要只专注于背景设定，要从设定中挖掘出新的剧情
- 创造冲突：利用Potogonist Vs Antogonist，unbreakable bonding等方法为后续剧情创作冲突
- 创造剧情张力： 利用隐藏剧情,时间限制，转折点等方法为后续剧情创作张力
- 时间线要有起承转合
新剧情以用户的第一视角进行撰写，围绕“我”(User)和“你”(Assistant)展开

现在，请开始你的任务
"""


def get_prompt() -> str:
    return INFER_PLOT_PROMPT


def pack_messages(messages: list[OpenAICompatibleMessage]):
    return "\n".join(
        [
            f"[{m.role}{f'({m.alias})' if m.alias else ''}]: {m.content}"
            for m in messages
        ]
    )


def get_input(role, user, before_plots: list[str], messages: list):
    return f"""<assistant_role_setting>
{role}
</assistant_role_setting>
<user_role_setting>
{user}
</user_role_setting>
<before_plots>
{before_plots}
</before_plots>
<latest_dialogues>
{pack_messages(messages)}
</latest_dialogues>"""


def get_kwargs() -> dict:
    return ADD_KWARGS


def extract_plot_output(content: str):
    overview = re.search(r"<overview>(.*?)</overview>", content, re.DOTALL)
    themes = re.search(r"<themes>(.*?)</themes>", content, re.DOTALL)
    timeline = re.search(r"<timeline>(.*?)</timeline>", content, re.DOTALL)
    return overview.group(1), themes.group(1), timeline.group(1)


if __name__ == "__main__":
    print(get_prompt())



================================================
FILE: src/server/api/memobase_server/telemetry/__init__.py
================================================
from .open_telemetry import telemetry_manager, CounterMetricName, HistogramMetricName

__all__ = ["telemetry_manager", "CounterMetricName", "HistogramMetricName"]



================================================
FILE: src/server/api/memobase_server/telemetry/capture_key.py
================================================
from datetime import datetime, timedelta
from ..connectors import get_redis_client, PROJECT_ID
from ..models.database import DEFAULT_PROJECT_ID


def date_key():
    return datetime.now().strftime("%Y-%m-%d")


def month_key():
    return datetime.now().strftime("%Y-%m")


def date_past_key(delay_days: int) -> str:
    return (datetime.now() - timedelta(days=delay_days)).strftime("%Y-%m-%d")


def head_key(project_id: str):
    return f"memobase_telemetry::{PROJECT_ID}::{project_id}"


async def capture_int_key(
    name: str,
    value: int = 1,
    expire_days: int = 14,
    project_id: str = DEFAULT_PROJECT_ID,
):
    key = f"{head_key(project_id)}::{name}::{date_key()}"
    key_month = f"{head_key(project_id)}::{name}::{month_key()}"
    async with get_redis_client() as r_c:
        await r_c.incrby(key, value)
        await r_c.incrby(key_month, value)
        await r_c.expire(key, expire_days * 24 * 60 * 60)
        await r_c.expire(key_month, 30 * expire_days * 24 * 60 * 60)


async def get_int_key(
    name: str,
    project_id: str = DEFAULT_PROJECT_ID,
    in_month: bool = False,
    use_date: str = None,
) -> int:
    if in_month:
        key = f"{head_key(project_id)}::{name}::{month_key()}"
    else:
        using_date = use_date or date_key()
        key = f"{head_key(project_id)}::{name}::{using_date}"
    async with get_redis_client() as r_c:
        return int((await r_c.get(key)) or 0)


if __name__ == "__main__":
    import asyncio

    print(asyncio.run(capture_int_key("test_key")))



================================================
FILE: src/server/api/memobase_server/telemetry/open_telemetry.py
================================================
from enum import Enum
from typing import Dict
import os
import socket
from prometheus_client import start_http_server
from opentelemetry import metrics
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics._internal.instrument import (
    Counter,
    Histogram,
    Gauge,
)
from opentelemetry.sdk.resources import SERVICE_NAME, Resource, DEPLOYMENT_ENVIRONMENT
from functools import wraps
from ..env import LOG, CONFIG


def no_raise_exception(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            LOG.error(f"Error in {func.__name__}: {e}")

    return wrapper


class CounterMetricName(Enum):
    """Enum for all available metrics."""

    REQUEST = "requests_total"
    HEALTHCHECK = "healthcheck_total"
    LLM_INVOCATIONS = "llm_invocations_total"
    LLM_TOKENS_INPUT = "llm_input_tokens_total"
    LLM_TOKENS_OUTPUT = "llm_output_tokens_total"
    EMBEDDING_TOKENS = "embedding_tokens_total"

    def get_description(self) -> str:
        """Get the description for this metric."""
        descriptions = {
            CounterMetricName.REQUEST: "Total number of requests to the memobase server",
            CounterMetricName.HEALTHCHECK: "Total number of healthcheck requests to the memobase server",
            CounterMetricName.LLM_INVOCATIONS: "Total number of LLM invocations",
            CounterMetricName.LLM_TOKENS_INPUT: "Total number of input tokens",
            CounterMetricName.LLM_TOKENS_OUTPUT: "Total number of output tokens",
            CounterMetricName.EMBEDDING_TOKENS: "Total number of embedding tokens",
        }
        return descriptions[self]

    def get_metric_name(self) -> str:
        """Get the full metric name with prefix."""
        return f"memobase_server_{self.value}"


class HistogramMetricName(Enum):
    """Enum for histogram metrics."""

    LLM_LATENCY_MS = "llm_latency"
    EMBEDDING_LATENCY_MS = "embedding_latency"
    REQUEST_LATENCY_MS = "request_latency"

    def get_description(self) -> str:
        """Get the description for this metric."""
        descriptions = {
            HistogramMetricName.LLM_LATENCY_MS: "Latency of the LLM in milliseconds",
            HistogramMetricName.EMBEDDING_LATENCY_MS: "Latency of the embedding in milliseconds",
            HistogramMetricName.REQUEST_LATENCY_MS: "Latency of the request in milliseconds",
        }
        return descriptions[self]

    def get_metric_name(self) -> str:
        """Get the full metric name with prefix."""
        return f"memobase_server_{self.value}"


class GaugeMetricName(Enum):
    """Enum for gauge metrics."""

    INPUT_TOKEN_COUNT = "input_token_count_per_call"
    OUTPUT_TOKEN_COUNT = "output_token_count_per_call"

    def get_description(self) -> str:
        """Get the description for this metric."""
        descriptions = {
            GaugeMetricName.INPUT_TOKEN_COUNT: "Number of input tokens per call",
            GaugeMetricName.OUTPUT_TOKEN_COUNT: "Number of output tokens per call",
        }
        return descriptions[self]

    def get_metric_name(self) -> str:
        """Get the full metric name with prefix."""
        return f"memobase_server_{self.value}"


class TelemetryManager:
    """Manages telemetry setup and metrics for the memobase server."""

    def __init__(
        self,
        service_name: str = "memobase-server",
        prometheus_port: int = 9464,
        deployment_environment: str = "default",
    ):
        self._service_name = service_name
        self._prometheus_port = prometheus_port
        self._deployment_environment = deployment_environment
        self._metrics: Dict[
            CounterMetricName | HistogramMetricName | GaugeMetricName,
            Counter | Histogram | Gauge,
        ] = None
        self._meter = None

    def setup_telemetry(self) -> None:
        """Initialize OpenTelemetry with Prometheus exporter."""
        resource = Resource(
            attributes={
                SERVICE_NAME: self._service_name,
                DEPLOYMENT_ENVIRONMENT: self._deployment_environment,
            }
        )
        reader = PrometheusMetricReader()
        provider = MeterProvider(resource=resource, metric_readers=[reader])
        metrics.set_meter_provider(provider)

        # Start Prometheus HTTP server, skip if port is already in use
        try:
            start_http_server(self._prometheus_port)
        except OSError as e:
            if e.errno == 48:  # Address already in use
                LOG.warning(
                    f"Prometheus HTTP server already running on port {self._prometheus_port}"
                )
            else:
                raise e

        # Initialize meter
        self._meter = metrics.get_meter(self._service_name)

    def _construct_attributes(self, **kwargs) -> Dict[str, str]:

        # if os.environ.get("POD_IP"):
        #     # use k8s downward API to get the pod ip
        #     pod_ip = os.environ.get("POD_IP", None)
        # else:
        #     # use the hostname to get the ip address
        #     hostname = socket.gethostname()
        #     pod_ip = socket.gethostbyname(hostname)
        pod_ip = os.environ.get("POD_IP", None)
        return {
            DEPLOYMENT_ENVIRONMENT: self._deployment_environment,
            "memobase_server_ip": pod_ip,
            **kwargs,
        }

    def setup_metrics(self) -> None:
        """Initialize all metrics."""
        if not self._meter:
            raise RuntimeError("Call setup_telemetry() before setup_metrics()")

        if self._metrics is None:
            self._metrics = {}

        # Create counters
        for metric in CounterMetricName:
            self._metrics[metric] = self._meter.create_counter(
                metric.get_metric_name(),
                unit="1",
                description=metric.get_description(),
            )

        # Create histogram for latency
        for metric in HistogramMetricName:
            self._metrics[metric] = self._meter.create_histogram(
                metric.get_metric_name(),
                unit="ms",
                description=metric.get_description(),
            )

        # Create gauges for token counts
        for metric in GaugeMetricName:
            self._metrics[metric] = self._meter.create_gauge(
                metric.get_metric_name(),
                unit="1",
                description=metric.get_description(),
            )

    @no_raise_exception
    def increment_counter_metric(
        self,
        metric: CounterMetricName,
        value: int = 1,
        attributes: Dict[str, str] = None,
    ) -> None:
        """Increment a counter metric."""
        self._validate_metric(metric)
        complete_attributes = self._construct_attributes(**(attributes or {}))
        self._metrics[metric].add(value, complete_attributes)

    @no_raise_exception
    def record_histogram_metric(
        self,
        metric: HistogramMetricName,
        value: float,
        attributes: Dict[str, str] = None,
    ) -> None:
        """Record a histogram metric value."""
        self._validate_metric(metric)
        complete_attributes = self._construct_attributes(**(attributes or {}))
        self._metrics[metric].record(value, complete_attributes)

    @no_raise_exception
    def set_gauge_metric(
        self,
        metric: GaugeMetricName,
        value: float,
        attributes: Dict[str, str] = None,
    ) -> None:
        """Set a gauge metric."""
        self._validate_metric(metric)
        complete_attributes = self._construct_attributes(**(attributes or {}))
        self._metrics[metric].set(value, complete_attributes)

    def _validate_metric(self, metric) -> None:
        """Validate if the metric is initialized."""
        if metric not in self._metrics:
            raise KeyError(f"Metric {metric} not initialized")


# Create a global instance
telemetry_manager = TelemetryManager(
    deployment_environment=CONFIG.telemetry_deployment_environment
)
telemetry_manager.setup_telemetry()
telemetry_manager.setup_metrics()



================================================
FILE: src/server/api/migrations/README
================================================
Generic single-database configuration.


================================================
FILE: src/server/api/migrations/env.py
================================================
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context
from memobase_server.models.database import REG

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = [REG.metadata]

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()



================================================
FILE: src/server/api/migrations/script.py.mako
================================================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}



================================================
FILE: src/server/api/tests/__init__.py
================================================
from dotenv import load_dotenv

load_dotenv()



================================================
FILE: src/server/api/tests/conftest.py
================================================
import pytest
import pytest_asyncio
from api import app
from memobase_server.env import CONFIG
from fastapi.testclient import TestClient

PREFIX = "/api/v1"
CONFIG.profile_strict_mode = False
CONFIG.minimum_chats_token_size_for_event_summary = 5
CONFIG.event_tags = [
    {"name": "emotion", "description": "Record the current emotion of user"},
    {"name": "goal", "description": "Record the current goal of user"},
]
CONFIG.enable_event_embedding = True
CONFIG.persistent_chat_blobs = True
CONFIG.llm_api_key = None
# @pytest.fixture(scope="session")
# def event_loop():
#     try:
#         loop = asyncio.get_running_loop()
#     except RuntimeError:
#         loop = asyncio.new_event_loop()
#     yield loop
#     loop.close()


@pytest_asyncio.fixture(scope="function")
async def db_env():
    client = TestClient(app)
    response = client.get(f"{PREFIX}/healthcheck")
    d = response.json()
    if response.status_code == 200 and d["errno"] == 0:
        yield
    else:
        pytest.skip("Database not available")



================================================
FILE: src/server/api/tests/test_api.py
================================================
import os
import pytest
import numpy as np
from unittest.mock import patch, Mock, AsyncMock
from api import app
from fastapi.testclient import TestClient
from memobase_server import controllers
from memobase_server.models.database import DEFAULT_PROJECT_ID
from memobase_server.models.blob import BlobType
import numpy as np
from memobase_server.env import CONFIG

PREFIX = "/api/v1"
TOKEN = os.getenv("ACCESS_TOKEN")


@pytest.fixture
def client():
    c = TestClient(app)
    c.headers.update(
        {
            "Authorization": f"Bearer {TOKEN}",
            "X-Forwarded-For": "192.168.123.132",
            "X-Real-IP": "192.168.123.132",
        }
    )
    return c


def test_health_check(client, db_env):
    response = client.get(f"{PREFIX}/healthcheck")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


@pytest.fixture
def mock_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.extract.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value="- basic_info::name::Gus")

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_llm_validate_complete():
    with patch(
        "memobase_server.controllers.modal.chat.merge_yolo.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value="1. UPDATE::Gus")

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_event_summary_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.event_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="- emotion::happy")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_entry_summary_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.entry_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="Melina is a happy girl")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_event_get_embedding():
    with patch(
        "memobase_server.controllers.event.get_embedding"
    ) as mock_event_get_embedding:
        async_mock = AsyncMock()
        async_mock.ok = Mock(return_value=True)
        async_mock.data = Mock(
            return_value=np.array([[0.1 for _ in range(CONFIG.embedding_dim)]])
        )
        mock_event_get_embedding.return_value = async_mock
        yield mock_event_get_embedding


def test_user_api_curd(client, db_env):
    response = client.post(f"{PREFIX}/users", json={"data": {"test": 1}})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    response = client.get(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert d["data"]["data"]["test"] == 1

    response = client.put(f"{PREFIX}/users/{u_id}", json={"test": 2})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.get(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert d["data"]["data"]["test"] == 2

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.get(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] != 0
    print(d)


def test_user_create_with_id(client, db_env):
    import uuid
    from time import time

    fake_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"test{time()}"))
    print(fake_id)
    response = client.post(f"{PREFIX}/users", json={"data": {"test": 1}, "id": fake_id})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]
    assert u_id == fake_id

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


def test_blob_api_curd(client, db_env):
    response = client.post(f"{PREFIX}/users", json={})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "doc",
            "blob_data": {"content": "Hello world"},
            "fields": {"from": "happy"},
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    b_id = d["data"]["id"]

    response = client.get(f"{PREFIX}/blobs/{u_id}/{b_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert d["data"]["blob_type"] == "doc"
    assert d["data"]["blob_data"]["content"] == "Hello world"
    assert d["data"]["fields"]["from"] == "happy"

    client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "doc",
            "blob_data": {"content": "Hello world"},
            "fields": {"from": "happy"},
        },
    )
    response = client.get(
        f"{PREFIX}/users/blobs/{u_id}/{BlobType.doc}?page=0&page_size=1"
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["ids"]) == 1

    response = client.get(
        f"{PREFIX}/users/blobs/{u_id}/{BlobType.doc}?page=0&page_size=2"
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["ids"]) == 2

    response = client.delete(f"{PREFIX}/blobs/{u_id}/{b_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.get(f"{PREFIX}/blobs/{u_id}/{b_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] != 0
    print(d)

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


@pytest.mark.asyncio
async def test_api_user_profile(client, db_env):
    response = client.post(f"{PREFIX}/users", json={"data": {"test": 1}})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    _profiles = ["user is a junior school student", "user likes to play basketball"]
    _attributes = [
        {"topic": "education", "sub_topic": "level"},
        {"topic": "interest", "sub_topic": "sports"},
    ]
    p = await controllers.profile.add_user_profiles(
        u_id, DEFAULT_PROJECT_ID, _profiles, _attributes
    )
    assert p.ok()

    response = client.get(f"{PREFIX}/users/profile/{u_id}")
    d = response.json()
    d["data"]["profiles"] = sorted(d["data"]["profiles"], key=lambda x: x["content"])
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["profiles"]) == 2
    assert [dp["content"] for dp in d["data"]["profiles"]] == _profiles
    assert [dp["attributes"] for dp in d["data"]["profiles"]] == _attributes
    id1, id2 = d["data"]["profiles"][0]["id"], d["data"]["profiles"][1]["id"]

    response = client.get(
        f"{PREFIX}/users/profile/{u_id}?prefer_topics=interest&topk=1"
    )
    d = response.json()
    d["data"]["profiles"] = sorted(d["data"]["profiles"], key=lambda x: x["content"])
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["profiles"]) == 1
    assert d["data"]["profiles"][0]["id"] == id2

    response = client.get(f"{PREFIX}/users/profile/{u_id}?only_topics=interest")
    d = response.json()
    d["data"]["profiles"] = sorted(d["data"]["profiles"], key=lambda x: x["content"])
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["profiles"]) == 1
    assert d["data"]["profiles"][0]["id"] == id2

    response = client.get(f"{PREFIX}/users/context/{u_id}?only_topics=interest")
    print(response.json())
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.delete(f"{PREFIX}/users/profile/{u_id}/{id1}")
    d = response.json()
    assert response.status_code == 200

    response = client.get(f"{PREFIX}/users/profile/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["profiles"]) == 1
    assert d["data"]["profiles"][0]["id"] == id2

    response = client.get(f"{PREFIX}/project/billing")
    d = response.json()
    print(d)
    assert response.status_code == 200
    assert d["errno"] == 0
    assert d["data"]["token_left"] is None
    assert d["data"]["project_token_cost_month"] >= 0
    assert d["data"]["next_refill_at"] is not None

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


@pytest.mark.asyncio
async def test_api_user_flush_buffer(
    client,
    db_env,
    mock_llm_complete,
    mock_llm_validate_complete,
    mock_event_summary_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    response = client.post(f"{PREFIX}/users", json={"data": {"test": 1}})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and p.data() == 0

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "chat",
            "blob_data": {
                "messages": [
                    {"role": "user", "content": "hello, I'm Gus"},
                    {"role": "assistant", "content": "hi"},
                ]
            },
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and p.data() == 1

    p = client.get(f"{PREFIX}/users/buffer/capacity/{u_id}/chat?status=idle")
    d = p.json()
    assert p.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["ids"]) == 1

    p = client.post(f"{PREFIX}/users/buffer/{u_id}/chat")
    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and p.data() == 0

    p = client.get(f"{PREFIX}/users/buffer/capacity/{u_id}/chat?status=done")
    d = p.json()
    assert p.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["ids"]) == 1

    response = client.get(f"{PREFIX}/users/profile/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["profiles"]) == 1
    assert [dp["content"] for dp in d["data"]["profiles"]] == ["Gus"]

    response = client.get(f"{PREFIX}/users/event/{u_id}?topk=5")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["events"]) == 1
    print(d)

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


def test_chat_blob_param_api(client, db_env):
    response = client.post(f"{PREFIX}/users", json={})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "chat",
            "blob_data": {
                "messages": [
                    {"role": "user", "content": "Hello world", "alias": "try"},
                    {"role": "assistant", "content": "hi"},
                ]
            },
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "chat",
            "blob_data": {
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello world",
                        "created_at": "2025-01-14",
                    },
                    {"role": "assistant", "content": "hi"},
                ]
            },
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    b_id = d["data"]["id"]

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


@pytest.mark.asyncio
async def test_api_user_event(
    client,
    db_env,
    mock_llm_complete,
    mock_llm_validate_complete,
    mock_event_summary_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    response = client.post(f"{PREFIX}/users", json={"data": {"test": 1}})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "chat",
            "blob_data": {
                "messages": [
                    {"role": "user", "content": "hello, I'm Gus"},
                    {"role": "assistant", "content": "hi"},
                ]
            },
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.post(f"{PREFIX}/users/buffer/{u_id}/chat")
    assert response.status_code == 200
    assert response.json()["errno"] == 0

    response = client.get(f"{PREFIX}/users/event/{u_id}?topk=5")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["events"]) == 1

    print(d["data"]["events"])
    assert d["data"]["events"][0]["event_data"]["event_tip"] == "Melina is a happy girl"
    assert d["data"]["events"][0]["event_data"]["event_tags"] == [
        {"tag": "emotion", "value": "happy"}
    ]
    print(d)

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0


@pytest.mark.asyncio
async def test_api_project_invalid_profile_config(client, db_env):
    response = client.post(
        f"{PREFIX}/project/profile_config",
        json={"profile_config": ""},
    )

    response = client.get(f"{PREFIX}/project/profile_config")
    d = response.json()
    print(d)
    assert response.status_code == 200
    assert d["errno"] == 0
    assert d["data"]["profile_config"] == ""

    response = client.post(
        f"{PREFIX}/project/profile_config",
        json={
            "profile_config": """
overwrite_user_profiles:
  - topic: "Food"
    sub_topics:
      - name: "Dietary Preference"
        description: "xxxxxxxx"
      - name: "Dietary Restriction"
        description: "yyyyyyyy"
"""
        },
    )
    d = response.json()
    assert d["errno"] == 0

    response = client.post(
        f"{PREFIX}/project/profile_config",
        json={
            "profile_config": """

overwrite_user_profiles:
  - topic: "Food"
    sub_topics:
      - name: "Dietary Preference"
        description: true
      - name: "Dietary Restriction"
        description: "yyyyyyyy"
"""
        },
    )
    d = response.json()
    assert d["errno"] != 0
    print(d["errmsg"])

    response = client.post(
        f"{PREFIX}/project/profile_config",
        json={
            "profile_config": """
[[[
"""
        },
    )
    d = response.json()
    assert d["errno"] != 0
    print(d["errmsg"])

    response = client.post(
        f"{PREFIX}/project/profile_config",
        json={"profile_config": ""},
    )
    d = response.json()
    assert d["errno"] == 0


@pytest.mark.asyncio
async def test_api_event_search(
    client,
    db_env,
    mock_llm_complete,
    mock_llm_validate_complete,
    mock_event_summary_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    response = client.post(f"{PREFIX}/users", json={})
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    u_id = d["data"]["id"]

    response = client.post(
        f"{PREFIX}/blobs/insert/{u_id}",
        json={
            "blob_type": "chat",
            "blob_data": {
                "messages": [
                    {"role": "user", "content": "hello, I'm Gus"},
                    {"role": "assistant", "content": "hi"},
                ]
            },
        },
    )
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0

    response = client.post(f"{PREFIX}/users/buffer/{u_id}/chat")
    assert response.status_code == 200
    assert response.json()["errno"] == 0

    response = client.get(f"{PREFIX}/users/event/search/{u_id}?query=hello")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0
    assert len(d["data"]["events"]) == 1
    assert np.allclose(d["data"]["events"][0]["similarity"], 1)
    print(d["data"])

    response = client.delete(f"{PREFIX}/users/{u_id}")
    d = response.json()
    assert response.status_code == 200
    assert d["errno"] == 0



================================================
FILE: src/server/api/tests/test_chat_modal.py
================================================
import pytest
from unittest.mock import AsyncMock, Mock, patch
from memobase_server import controllers
from memobase_server.models import response as res
from memobase_server.models.database import DEFAULT_PROJECT_ID
from memobase_server.models.blob import BlobType
from memobase_server.models.utils import Promise
from memobase_server.env import CONFIG
import numpy as np


GD_FACTS = """
- basic_info::name::Gus
- interest::foods::Chinese food
- education::level::High School
- psychological::emotional_state::Feels bored with high school
"""

PROFILES = [
    "user likes to play basketball",
    "user is a junior school student",
    "user likes japanese food",
    "user is 23 years old",
]

PROFILE_ATTRS = [
    {"topic": "interest", "sub_topic": "sports"},
    {"topic": "education", "sub_topic": "level"},
    {"topic": "interest", "sub_topic": "foods"},
    {"topic": "basic_info", "sub_topic": "age"},
]

OVER_MAX_PROFILEs = ["Chinese food" for _ in range(20)]
OVER_MAX_PROFILE_ATTRS = [
    {"topic": "interest", "sub_topic": "foods" + str(i)} for i in range(20)
]

MERGE_FACTS = [
    """TTTT
---
1. UPDATE::Gus
2. UPDATE::user likes Chinese and Japanese food
3. UPDATE::High School
4. UPDATE::Feels bored with high school
"""
]

ORGANIZE_FACTS = """
- foods::Chinese food
"""


def dict_contains(a: dict, b: dict) -> bool:
    return all(a[k] == v for k, v in b.items())


@pytest.fixture
def mock_extract_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.extract.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value=GD_FACTS)

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_merge_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.merge_yolo.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value=MERGE_FACTS[0])

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_organize_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.organize.llm_complete"
    ) as mock_llm:
        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value=ORGANIZE_FACTS)

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_event_tag_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.event_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="- emotion::happy")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_entry_summary_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.entry_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="Melinda is a software engineer")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_event_get_embedding():
    with patch(
        "memobase_server.controllers.event.get_embedding"
    ) as mock_event_get_embedding:
        async_mock = AsyncMock()
        async_mock.ok = Mock(return_value=True)
        async_mock.data = Mock(
            return_value=np.array([[0.1 for _ in range(CONFIG.embedding_dim)]])
        )
        mock_event_get_embedding.return_value = async_mock
        yield mock_event_get_embedding


@pytest.mark.asyncio
async def test_chat_buffer_modal(
    db_env,
    mock_extract_llm_complete,
    mock_merge_llm_complete,
    mock_event_tag_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    blob1 = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {"role": "user", "content": "Hello, this is Gus, how are you?"},
                {"role": "assistant", "content": "I am fine, thank you!"},
            ]
        },
    )
    blob2 = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {"role": "user", "content": "Hi, nice to meet you, I am Gus"},
                {
                    "role": "assistant",
                    "content": "Great! I'm Memobase Assistant, how can I help you?",
                },
                {"role": "user", "content": "I really dig into Chinese food"},
                {"role": "assistant", "content": "Got it, Gus!"},
                {
                    "role": "user",
                    "content": "write me a homework letter about my final exam, high school is really boring.",
                },
            ]
        },
        fields={"from": "happy"},
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob1,
    )
    assert p.ok()
    b_id = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id, blob1.to_blob()
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob2,
    )
    assert p.ok()
    b_id2 = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id2, blob2.to_blob()
    )

    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and p.data() == 2

    await controllers.buffer.flush_buffer(u_id, DEFAULT_PROJECT_ID, BlobType.chat)

    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    assert len(p.data().profiles) == 4
    print(p.data())

    p = await controllers.profile.truncate_profiles(p.data(), topk=2)
    assert p.ok()
    assert len(p.data().profiles) == 2

    p = await controllers.event.get_user_events(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    assert len(p.data().events) == 1

    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and p.data() == 0

    # persistent_chat_blobs default to True
    p = await controllers.user.get_user_all_blobs(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok() and len(p.data().ids) == 2

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()

    mock_extract_llm_complete.assert_awaited_once()


@pytest.mark.asyncio
async def test_chat_merge_modal(
    db_env,
    mock_extract_llm_complete,
    mock_merge_llm_complete,
    mock_event_tag_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    blob1 = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {"role": "user", "content": "Hello, this is Gus, how are you?"},
                {"role": "assistant", "content": "I am fine, thank you!"},
                {"role": "user", "content": "I'm 25 now, how time flies!"},
            ]
        },
    )
    blob2 = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {"role": "user", "content": "I really dig into Chinese food"},
                {"role": "assistant", "content": "Got it, Gus!"},
                {
                    "role": "user",
                    "content": "write me a homework letter about my final exam, high school is really boring.",
                },
            ]
        },
        fields={"from": "happy"},
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob1,
    )
    assert p.ok()
    b_id = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id, blob1.to_blob()
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob2,
    )
    assert p.ok()
    b_id2 = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id2, blob2.to_blob()
    )

    p = await controllers.profile.add_user_profiles(
        u_id, DEFAULT_PROJECT_ID, PROFILES, PROFILE_ATTRS
    )
    assert p.ok()
    await controllers.buffer.flush_buffer(u_id, DEFAULT_PROJECT_ID, BlobType.chat)

    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok() and len(p.data().profiles) == len(PROFILES) + 2
    profiles = p.data().profiles
    profiles = sorted(profiles, key=lambda x: x.content)

    assert dict_contains(
        profiles[-1].attributes, {"topic": "interest", "sub_topic": "sports"}
    )
    assert profiles[-1].content == "user likes to play basketball"
    assert dict_contains(
        profiles[-2].attributes, {"topic": "interest", "sub_topic": "foods"}
    )
    assert profiles[-2].content == "user likes Chinese and Japanese food"

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()

    assert mock_extract_llm_complete.await_count == 1
    assert mock_merge_llm_complete.await_count == 1


@pytest.mark.asyncio
async def test_chat_organize_modal(
    db_env,
    mock_extract_llm_complete,
    mock_merge_llm_complete,
    mock_organize_llm_complete,
    mock_event_tag_llm_complete,
    mock_entry_summary_llm_complete,
    mock_event_get_embedding,
):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    blob1 = res.BlobData(
        blob_type=BlobType.chat,
        blob_data={
            "messages": [
                {"role": "user", "content": "Hello, this is Gus, how are you?"},
                {"role": "assistant", "content": "I am fine, thank you!"},
                {"role": "user", "content": "I'm 25 now, how time flies!"},
            ]
        },
    )

    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob1,
    )
    assert p.ok()
    b_id = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id, blob1.to_blob()
    )
    p = await controllers.profile.add_user_profiles(
        u_id, DEFAULT_PROJECT_ID, OVER_MAX_PROFILEs, OVER_MAX_PROFILE_ATTRS
    )
    assert p.ok()

    await controllers.buffer.flush_buffer(u_id, DEFAULT_PROJECT_ID, BlobType.chat)

    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    assert mock_extract_llm_complete.await_count == 1
    assert mock_merge_llm_complete.await_count == 1
    assert mock_organize_llm_complete.await_count == 1



================================================
FILE: src/server/api/tests/test_controller.py
================================================
import pytest
import numpy as np
from unittest.mock import patch, AsyncMock, Mock
from memobase_server.env import CONFIG
from memobase_server.controllers import full as controllers
from memobase_server.models import response as res
from memobase_server.models.blob import BlobType
from memobase_server.models.database import DEFAULT_PROJECT_ID


@pytest.fixture
def mock_event_get_embedding():
    with patch(
        "memobase_server.controllers.event.get_embedding"
    ) as mock_event_get_embedding:
        async_mock = AsyncMock()
        async_mock.ok = Mock(return_value=True)
        async_mock.data = Mock(
            return_value=np.array([[0.1 for _ in range(CONFIG.embedding_dim)]])
        )
        mock_event_get_embedding.return_value = async_mock
        yield mock_event_get_embedding


@pytest.mark.asyncio
async def test_user_curd(db_env):
    p = await controllers.user.create_user(
        res.UserData(data={"test": 1}), DEFAULT_PROJECT_ID
    )
    assert p.ok()
    d = p.data()
    u_id = d.id

    p = await controllers.user.get_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    d = p.data().data
    assert d["test"] == 1

    p = await controllers.user.update_user(u_id, DEFAULT_PROJECT_ID, {"test": 2})
    assert p.ok()
    p = await controllers.user.get_user(u_id, DEFAULT_PROJECT_ID)
    assert p.data().data["test"] == 2

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    p = await controllers.user.get_user(u_id, DEFAULT_PROJECT_ID)
    assert not p.ok()


@pytest.mark.asyncio
async def test_user_state_clean(db_env):
    p = await controllers.user.create_user(
        res.UserData(data={"test": 1}), DEFAULT_PROJECT_ID
    )
    assert p.ok()
    d = p.data()
    u_id = d.id

    p = await controllers.user.get_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    d = p.data().data
    assert d["test"] == 1

    p = await controllers.profile.add_user_profiles(
        u_id, DEFAULT_PROJECT_ID, ["test"], [{"topic": "test", "sub_topic": "test"}]
    )
    assert p.ok()
    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    d = p.data()
    assert len(d.profiles) == 1
    assert d.profiles[0].attributes == {"topic": "test", "sub_topic": "test"}

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    p = await controllers.user.get_user(u_id, DEFAULT_PROJECT_ID)
    assert not p.ok()
    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok() and len(p.data().profiles) == 0


@pytest.mark.asyncio
async def test_blob_curd(db_env):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        res.BlobData(
            blob_type=BlobType.doc,
            blob_data={"content": "Hello world"},
            fields={"from": "happy"},
        ),
    )
    assert p.ok()
    b_id = p.data().id

    p = await controllers.blob.get_blob(u_id, DEFAULT_PROJECT_ID, b_id)
    assert p.ok()
    d = p.data()
    assert d.blob_type == BlobType.doc
    assert d.blob_data["content"] == "Hello world"
    assert d.fields["from"] == "happy"

    p = await controllers.blob.remove_blob(u_id, DEFAULT_PROJECT_ID, b_id)
    assert p.ok()
    p = await controllers.blob.get_blob(u_id, DEFAULT_PROJECT_ID, b_id)
    assert not p.ok()

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()


@pytest.mark.asyncio
async def test_user_blob_curd(db_env):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        res.BlobData(
            blob_type=BlobType.chat,
            blob_data={
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello world",
                    },
                    {
                        "role": "assistant",
                        "content": "Hi",
                    },
                ]
            },
            fields={"from": "happy"},
        ),
    )
    assert p.ok()
    b_id = p.data().id
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        res.BlobData(
            blob_type=BlobType.chat,
            blob_data={
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello world",
                    },
                    {
                        "role": "assistant",
                        "content": "Hi",
                    },
                ]
            },
            fields={"from": "happy"},
        ),
    )
    assert p.ok()
    b_id2 = p.data().id

    p = await controllers.user.get_user_all_blobs(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert p.ok()
    assert len(p.data().ids) == 2

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()

    p = await controllers.blob.get_blob(u_id, DEFAULT_PROJECT_ID, b_id)
    assert not p.ok()
    p = await controllers.blob.get_blob(u_id, DEFAULT_PROJECT_ID, b_id2)
    assert not p.ok()

    p = await controllers.user.get_user_all_blobs(
        u_id, DEFAULT_PROJECT_ID, BlobType.chat
    )
    assert len(p.data().ids) == 0


@pytest.mark.asyncio
async def test_filter_user_events(db_env, mock_event_get_embedding):
    """Test the filter_user_events function with various filtering scenarios."""
    # Create a test user
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    # Create test events with different tag configurations
    test_events = [
        # Event 1: emotion=happy, goal=relax
        {
            "profile_delta": [
                {
                    "content": "User is feeling good",
                    "attributes": {"topic": "mood", "sub_topic": "positive"},
                }
            ],
            "event_tip": "- User had a great day",
            "event_tags": [
                {"tag": "emotion", "value": "happy"},
                {"tag": "goal", "value": "relax"},
            ],
        },
        # Event 2: emotion=sad, location=home
        {
            "profile_delta": [
                {
                    "content": "User is feeling down",
                    "attributes": {"topic": "mood", "sub_topic": "negative"},
                }
            ],
            "event_tip": "- User had a rough day",
            "event_tags": [
                {"tag": "emotion", "value": "sad"},
                {"tag": "location", "value": "home"},
            ],
        },
        # Event 3: goal=work, location=office
        {
            "profile_delta": [
                {
                    "content": "User is focused on tasks",
                    "attributes": {"topic": "work", "sub_topic": "productivity"},
                }
            ],
            "event_tip": "- User is being productive",
            "event_tags": [
                {"tag": "goal", "value": "work"},
                {"tag": "location", "value": "office"},
            ],
        },
        # Event 4: emotion=happy, no other tags
        {
            "profile_delta": [
                {
                    "content": "User is excited",
                    "attributes": {"topic": "mood", "sub_topic": "positive"},
                }
            ],
            "event_tip": "- User is excited about something",
            "event_tags": [{"tag": "emotion", "value": "happy"}],
        },
        # Event 5: no event_tags at all
        {
            "profile_delta": [
                {
                    "content": "User shared information",
                    "attributes": {"topic": "info", "sub_topic": "general"},
                }
            ],
            "event_tip": "- User shared some general information",
        },
    ]

    # Insert all test events
    event_ids = []
    for event_data in test_events:
        p = await controllers.event.append_user_event(
            u_id, DEFAULT_PROJECT_ID, event_data
        )
        assert p.ok()
        event_ids.append(p.data())

    assert mock_event_get_embedding.await_count == len(test_events) * 2
    # Test 1: Filter by tag existence - events that have 'emotion' tag
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion"]
    )
    assert p.ok()
    events = p.data().events
    events = [e.model_dump() for e in events]
    assert len(events) == 3  # Events 1, 2, and 4 have emotion tag
    emotion_values = [
        tag["value"]
        for event in events
        for tag in event["event_data"].get("event_tags", [])
        if tag["tag"] == "emotion"
    ]
    assert set(emotion_values) == {"happy", "sad"}

    # Test 2: Filter by exact tag-value - events where emotion=happy
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, event_tag_equal={"emotion": "happy"}
    )
    assert p.ok()
    events = p.data().events
    events = [e.model_dump() for e in events]
    assert len(events) == 2  # Events 1 and 4
    for event in events:
        emotion_tags = [
            tag
            for tag in event["event_data"].get("event_tags", [])
            if tag["tag"] == "emotion"
        ]
        assert len(emotion_tags) == 1
        assert emotion_tags[0]["value"] == "happy"

    # Test 3: Filter by multiple tag existence - events that have both 'emotion' and 'goal' tags
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion", "goal"]
    )
    assert p.ok()
    events = p.data().events
    events = [e.model_dump() for e in events]
    assert len(events) == 1  # Only Event 1 has both emotion and goal tags
    event_tags = {tag["tag"] for tag in events[0]["event_data"]["event_tags"]}
    assert "emotion" in event_tags and "goal" in event_tags

    # Test 4: Filter by multiple exact tag-values
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, event_tag_equal={"emotion": "happy", "goal": "relax"}
    )
    assert p.ok()
    events = p.data().events
    events = [e.model_dump() for e in events]
    assert len(events) == 1  # Only Event 1 matches both conditions
    event_tags = {
        tag["tag"]: tag["value"] for tag in events[0]["event_data"]["event_tags"]
    }
    assert event_tags.get("emotion") == "happy"
    assert event_tags.get("goal") == "relax"

    # Test 5: Combine has_event_tag and event_tag_equal
    p = await controllers.event.filter_user_events(
        u_id,
        DEFAULT_PROJECT_ID,
        has_event_tag=["location"],
        event_tag_equal={"goal": "work"},
    )
    assert p.ok()
    events = p.data().events
    events = [e.model_dump() for e in events]
    assert len(events) == 1  # Only Event 3 has location tag AND goal=work
    event_tags = {
        tag["tag"]: tag["value"] for tag in events[0]["event_data"]["event_tags"]
    }
    assert "location" in event_tags
    assert event_tags.get("goal") == "work"

    # Test 6: Filter for non-existent tag
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["nonexistent"]
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 0  # No events should match

    # Test 7: Filter for non-existent tag value
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, event_tag_equal={"emotion": "angry"}
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 0  # No events have emotion=angry

    # Test 8: Test topk parameter
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion"], topk=1
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 1  # Should return only 1 event due to topk limit

    # Test 9: No filters provided - should return all events
    p = await controllers.event.filter_user_events(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    events = p.data().events
    assert len(events) == 5  # Should return all events

    # Test 10: Empty filter lists/dicts
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=[], event_tag_equal={}
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 5  # Should return all events when filters are empty

    # Cleanup
    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()


@pytest.mark.asyncio
async def test_filter_user_events_edge_cases(db_env, mock_event_get_embedding):
    """Test edge cases for filter_user_events function."""
    # Create a test user
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    # Test with user that has no events
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion"]
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 0

    # Create an event with empty event_tags array
    event_data = {
        "profile_delta": [
            {
                "content": "Test content",
                "attributes": {"topic": "test", "sub_topic": "test"},
            }
        ],
        "event_tip": "- Test event",
        "event_tags": [],
    }
    p = await controllers.event.append_user_event(u_id, DEFAULT_PROJECT_ID, event_data)
    assert p.ok()

    # Filter should not match events with empty tags array
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion"]
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 0

    # Create an event with null event_tags
    event_data_null_tags = {
        "profile_delta": [
            {
                "content": "Test content 2",
                "attributes": {"topic": "test", "sub_topic": "test"},
            }
        ],
        "event_tip": "- Test event 2",
        "event_tags": None,
    }
    p = await controllers.event.append_user_event(
        u_id, DEFAULT_PROJECT_ID, event_data_null_tags
    )
    assert p.ok()

    # Filter should still return 0 events
    p = await controllers.event.filter_user_events(
        u_id, DEFAULT_PROJECT_ID, has_event_tag=["emotion"]
    )
    assert p.ok()
    events = p.data().events
    assert len(events) == 0

    assert mock_event_get_embedding.await_count == 2 * 2

    # Cleanup
    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()



================================================
FILE: src/server/api/tests/test_db.py
================================================
import pytest
from sqlalchemy.inspection import inspect
from memobase_server.models.database import User, GeneralBlob, UserProfile
from memobase_server.models.blob import BlobType
from memobase_server.connectors import (
    Session,
    DB_ENGINE,
)


def test_correct_tables(db_env):
    db_inspector = inspect(DB_ENGINE)
    assert "users" in db_inspector.get_table_names()
    assert "general_blobs" in db_inspector.get_table_names()


def test_user_model(db_env):
    with Session() as session:
        user = User(additional_fields={"name": "Gus"})
        session.add(user)
        session.commit()
        assert user.id is not None
        assert user.created_at is not None
        assert user.updated_at is not None

    with Session() as session:
        user = session.query(User).filter_by(id=user.id).first()
        assert user is not None
        assert user.additional_fields == {"name": "Gus"}

    # Test delete
    with Session() as session:
        user = session.query(User).filter_by(id=user.id).first()
        session.delete(user)
        session.commit()
        assert session.query(User).filter_by(id=user.id).first() is None


def test_general_blob_model(db_env):
    with Session() as session:
        user = User(additional_fields={"name": "blob_user"})
        session.add(user)
        session.commit()
        test_user_id = user.id
    with pytest.raises(AssertionError, match="Invalid blob type: fool_test"):
        GeneralBlob(blob_type="fool_test", blob_data=dict(), user_id=test_user_id)

    with Session() as session:
        user = session.query(User).filter_by(id=test_user_id).first()
        session.delete(user)
        session.commit()



================================================
FILE: src/server/api/tests/test_summary_modal.py
================================================
import pytest
from unittest.mock import AsyncMock, Mock, patch
from memobase_server import controllers
from memobase_server.models import response as res
from memobase_server.models.database import DEFAULT_PROJECT_ID
from memobase_server.models.blob import BlobType
from memobase_server.models.utils import Promise
from memobase_server.env import CONFIG
import numpy as np


GD_FACTS = """
- basic_info::name::Gus
- interest::foods::Chinese food
- education::level::High School
- psychological::emotional_state::Feels bored with high school
"""

PROFILES = [
    "user likes to play basketball",
    "user is a junior school student",
    "user likes japanese food",
    "user is 23 years old",
]

PROFILE_ATTRS = [
    {"topic": "interest", "sub_topic": "sports"},
    {"topic": "education", "sub_topic": "level"},
    {"topic": "interest", "sub_topic": "foods"},
    {"topic": "basic_info", "sub_topic": "age"},
]

OVER_MAX_PROFILEs = ["Chinese food" for _ in range(20)]
OVER_MAX_PROFILE_ATTRS = [
    {"topic": "interest", "sub_topic": "foods" + str(i)} for i in range(20)
]

MERGE_FACTS = [
    """TTTT
---
1. UPDATE::Gus
2. UPDATE::user likes Chinese and Japanese food
3. UPDATE::High School
4. UPDATE::Feels bored with high school
"""
]

ORGANIZE_FACTS = """
- foods::Chinese food
"""


def dict_contains(a: dict, b: dict) -> bool:
    return all(a[k] == v for k, v in b.items())


@pytest.fixture
def mock_extract_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.extract.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value=GD_FACTS)

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_merge_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.merge_yolo.llm_complete"
    ) as mock_llm:
        mock_client1 = AsyncMock()
        mock_client1.ok = Mock(return_value=True)
        mock_client1.data = Mock(return_value=MERGE_FACTS[0])

        mock_llm.side_effect = [mock_client1]
        yield mock_llm


@pytest.fixture
def mock_organize_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.organize.llm_complete"
    ) as mock_llm:
        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value=ORGANIZE_FACTS)

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_event_tag_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.event_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="- emotion::happy")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_entry_summary_llm_complete():
    with patch(
        "memobase_server.controllers.modal.chat.entry_summary.llm_complete"
    ) as mock_llm:

        mock_client2 = AsyncMock()
        mock_client2.ok = Mock(return_value=True)
        mock_client2.data = Mock(return_value="Melinda is a software engineer")

        mock_llm.side_effect = [mock_client2]
        yield mock_llm


@pytest.fixture
def mock_event_get_embedding():
    with patch(
        "memobase_server.controllers.event.get_embedding"
    ) as mock_event_get_embedding:
        async_mock = AsyncMock()
        async_mock.ok = Mock(return_value=True)
        async_mock.data = Mock(
            return_value=np.array([[0.1 for _ in range(CONFIG.embedding_dim)]])
        )
        mock_event_get_embedding.return_value = async_mock
        yield mock_event_get_embedding


@pytest.mark.asyncio
async def test_summary_buffer_modal(
    db_env,
    mock_extract_llm_complete,
    mock_merge_llm_complete,
    mock_event_tag_llm_complete,
    mock_event_get_embedding,
):
    p = await controllers.user.create_user(res.UserData(), DEFAULT_PROJECT_ID)
    assert p.ok()
    u_id = p.data().id

    blob1 = res.BlobData(
        blob_type=BlobType.summary,
        blob_data={"summary": "User is a software engineer who works at Memobase...."},
    )
    blob2 = res.BlobData(
        blob_type=BlobType.summary,
        blob_data={"summary": "User is called Gus"},
        fields={"from": "happy"},
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob1,
    )
    assert p.ok()
    b_id = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id, blob1.to_blob()
    )
    p = await controllers.blob.insert_blob(
        u_id,
        DEFAULT_PROJECT_ID,
        blob2,
    )
    assert p.ok()
    b_id2 = p.data().id
    await controllers.buffer.insert_blob_to_buffer(
        u_id, DEFAULT_PROJECT_ID, b_id2, blob2.to_blob()
    )

    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.summary
    )
    assert p.ok() and p.data() == 2

    await controllers.buffer.flush_buffer(u_id, DEFAULT_PROJECT_ID, BlobType.summary)

    p = await controllers.profile.get_user_profiles(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    assert len(p.data().profiles) == 4
    print(p.data())

    p = await controllers.profile.truncate_profiles(p.data(), topk=2)
    assert p.ok()
    assert len(p.data().profiles) == 2

    p = await controllers.event.get_user_events(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()
    assert len(p.data().events) == 1

    p = await controllers.buffer.get_buffer_capacity(
        u_id, DEFAULT_PROJECT_ID, BlobType.summary
    )
    assert p.ok() and p.data() == 0

    # persistent_chat_blobs default to True
    p = await controllers.user.get_user_all_blobs(
        u_id, DEFAULT_PROJECT_ID, BlobType.summary
    )
    assert p.ok() and len(p.data().ids) == 2

    p = await controllers.user.delete_user(u_id, DEFAULT_PROJECT_ID)
    assert p.ok()

    mock_extract_llm_complete.assert_awaited_once()
    assert mock_merge_llm_complete.await_count == 1
    mock_event_tag_llm_complete.assert_awaited_once()
    assert mock_event_get_embedding.await_count == 2



================================================
FILE: src/server/script/sync-types.sh
================================================
set -e

# python ./api/build_init_sql.py > ./db/init.sql

# Get the value of __version__ from memobase_server.__init__
version=$(grep -oE '__version__ *= *"[^"]+"' ./api/memobase_server/__init__.py | awk -F'"' '{print $2}')
echo "Version: $version"
echo "# Synced from backend ${version}" > ../client/memobase/core/blob.py
cat ./api/memobase_server/models/blob.py >> ../client/memobase/core/blob.py


================================================
FILE: src/server/script/up-dev.sh
================================================
set -e
rm -rf ./db/dev_data
rm -rf ./db/dev_redis
DATABASE_LOCATION="./db/dev_data" REDIS_LOCATION="./db/dev_redis" docker compose up memobase-server-db memobase-server-redis
rm -rf ./db/dev_data
rm -rf ./db/dev_redis


================================================
FILE: src/server/script/up.sh
================================================
set -e
docker compose build
docker compose up -d


================================================
FILE: .github/CODEOWNERS
================================================
# This is a comment.
# Each line is a file pattern followed by one or more owners.

# These owners will be the default owners for everything in
# the repo. Unless a later match takes precedence,
# @global-owner1 and @global-owner2 will be requested for
# review when someone opens a pull request.
# * @global-owner1 @global-owner2
* @gusye1234 @jinjiaKarl @GenerQAQ
# Order is important; the last matching pattern takes the most
# precedence. When someone opens a pull request that only
# modifies JS files, only @js-owner and not the global
# owner(s) will be requested for a review.
# *.js    @js-owner
*.js @GenerQAQ
*.ts @GenerQAQ

# You can also use email addresses if you prefer. They'll be
# used to look up users just like we do for commit author
# emails.
# *.go docs@example.com
*.go @jinjiaKarl

# Teams can be specified as code owners as well. Teams should
# be identified in the format @org/team-name. Teams must have
# explicit write access to the repository. In this example,
# the octocats team in the octo-org organization owns all .txt files.
# *.txt @octo-org/octocats

# In this example, @doctocat owns any files in the build/logs
# directory at the root of the repository and any of its
# subdirectories.
# /build/logs/ @doctocat

# The `docs/*` pattern will match files like
# `docs/getting-started.md` but not further nested files like
# `docs/build-app/troubleshooting.md`.
# docs/*  docs@example.com

# In this example, @octocat owns any file in an apps directory
# anywhere in your repository.
# apps/ @octocat

# In this example, @doctocat owns any file in the `/docs`
# directory in the root of your repository and any of its
# subdirectories.
# /docs/ @doctocat

# In this example, @octocat owns any file in the `/apps`
# directory in the root of your repository except for the `/apps/github`
# subdirectory, as its owners are left empty.
# /apps/ @octocat
# /apps/github


================================================
FILE: .github/workflows/publish.yaml
================================================
name: test&build

on:
  merge_group:
  workflow_dispatch:
  push:
    branches:
      - main
      - dev
    tags:
      - 'v*'
    paths:
      - '.github/workflows/publish.yaml'
      - 'src/server/api/**'
      - '!src/server/api/**/*.md'
  pull_request:
    branches:
      - main
      - dev
    paths:
      - '.github/workflows/publish.yaml'
      - 'src/server/api/**'
      - '!src/server/api/**/*.md'


env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    name: Tests on ${{ matrix.os }} for ${{ matrix.python-version }}
    strategy:
      matrix:
        python-version: [3.12]
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    permissions: 
      contents: read
    steps:
      - uses: actions/checkout@v4
      - name: Install uv
        uses: astral-sh/setup-uv@v6

      - name: Set up Python
        run: uv python install
      - name: Install dependencies
        working-directory: ./src/server/api
        run: |
          uv sync --frozen --no-dev --no-cache-dir
      - name: Start containers
        working-directory: ./src/server
        run: |
          cp .env.example .env
          cp api/config.yaml.example api/config.yaml
          sh script/up-dev.sh &
      - name: Wait for containers to start
        working-directory: ./src/server
        run: |
          until docker compose exec memobase-server-db pg_isready -U $DATABASE_USER -d $DATABASE_NAME; do
            echo "Waiting for PostgreSQL to be ready..."
            sleep 5
          done
      - name: Test with pytest
        working-directory: ./src/server/api
        run: |
          cp .env.example .env
          uv run -m pytest --junit-xml=junit/test-results-${{ matrix.python-version }}.xml --cov=. --cov-report=xml:coverage-${{ matrix.python-version }}.xml tests/ -s -v
      - name: Upload pytest test results and coverage
        uses: actions/upload-artifact@v4
        with:
          name: pytest-results-${{ matrix.python-version }}
          path: |
            ./src/server/api/junit/test-results-${{ matrix.python-version }}.xml
            ./src/server/api/coverage-${{ matrix.python-version }}.xml
        if: ${{ always() }}
      - name: Stop containers
        working-directory: ./src/server
        if: ${{ always() }}
        run: |
          docker compose down

  build-image:
    needs: test
    runs-on: ubuntu-latest
    permissions: 
      contents: read
      packages: write
    env:
      SHOULD_PUSH: ${{ github.event_name == 'push' || (github.event_name == 'pull_request' && !github.event.pull_request.head.repo.fork) || github.event_name == 'workflow_dispatch' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to the Container Registry
        if: ${{ env.SHOULD_PUSH == 'true' }}
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push Docker image
        uses: docker/build-push-action@v6
        with:
          platforms: linux/amd64,linux/arm64
          context: ./src/server/api
          push: ${{ env.SHOULD_PUSH == 'true' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha


