Directory structure:
â””â”€â”€ blaizzy-mlx-audio/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ MANIFEST.in
    â”œâ”€â”€ Package.resolved
    â”œâ”€â”€ Package.swift
    â”œâ”€â”€ pytest.ini
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ examples/
    â”‚   â””â”€â”€ bible-audiobook/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ package.json
    â”‚       â”œâ”€â”€ tsconfig.json
    â”‚       â”œâ”€â”€ bibles/
    â”‚       â”‚   â””â”€â”€ bible-akjv.txt
    â”‚       â””â”€â”€ src/
    â”‚           â”œâ”€â”€ convert-to-mp3.ts
    â”‚           â”œâ”€â”€ index.ts
    â”‚           â””â”€â”€ mp3-checker.ts
    â”œâ”€â”€ mlx_audio/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ server.py
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â”œâ”€â”€ version.py
    â”‚   â”œâ”€â”€ codec/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ bigvgan/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ activation.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ amp.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ bigvgan.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ conv.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ resample.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ descript/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dac.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ nn/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ layers.py
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ quantize.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ encodec/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ encodec.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ mimi/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mimi.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ modules/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ conv.py
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ kv_cache.py
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ quantization.py
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ seanet.py
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ transformer.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ s3/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model_v2.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ snac/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ attention.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ layers.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ snac.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vq.py
    â”‚   â”‚   â”‚   â””â”€â”€ vocos/
    â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ mel.py
    â”‚   â”‚   â”‚       â””â”€â”€ vocos.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ test_bigvgan.py
    â”‚   â”‚       â”œâ”€â”€ test_descript.py
    â”‚   â”‚       â”œâ”€â”€ test_encodec.py
    â”‚   â”‚       â”œâ”€â”€ test_mimi.py
    â”‚   â”‚       â”œâ”€â”€ test_s3.py
    â”‚   â”‚       â”œâ”€â”€ test_snac.py
    â”‚   â”‚       â””â”€â”€ test_vocos.py
    â”‚   â”œâ”€â”€ sts/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ voice_pipeline.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_voice_pipeline.py
    â”‚   â”œâ”€â”€ stt/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ generate.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ parakeet/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ alignment.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ attention.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ audio.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ conformer.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ctc.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ parakeet.py
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rnnt.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tokenizer.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ wav2vec/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ wav2vec.py
    â”‚   â”‚   â”‚   â””â”€â”€ whisper/
    â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ audio.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ decoding.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ timing.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ tokenizer.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ whisper.py
    â”‚   â”‚   â”‚       â””â”€â”€ writers.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_models.py
    â”‚   â””â”€â”€ tts/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ audio_player.html
    â”‚       â”œâ”€â”€ audio_player.py
    â”‚       â”œâ”€â”€ convert.py
    â”‚       â”œâ”€â”€ generate.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â”œâ”€â”€ models/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ base.py
    â”‚       â”‚   â”œâ”€â”€ interpolate.py
    â”‚       â”‚   â”œâ”€â”€ bark/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ bark.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ isftnet.py
    â”‚       â”‚   â”‚   â””â”€â”€ pipeline.py
    â”‚       â”‚   â”œâ”€â”€ dia/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ audio.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ config.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ dia.py
    â”‚       â”‚   â”‚   â””â”€â”€ layers.py
    â”‚       â”‚   â”œâ”€â”€ indextts/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ attention.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ bigvgan.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ conformer.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ gpt2.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ indextts.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ mel.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ normalize.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ perceiver.py
    â”‚       â”‚   â”‚   â””â”€â”€ ecapa_tdnn/
    â”‚       â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚       â”œâ”€â”€ asp.py
    â”‚       â”‚   â”‚       â”œâ”€â”€ ecapa_tdnn.py
    â”‚       â”‚   â”‚       â”œâ”€â”€ se_res2net.py
    â”‚       â”‚   â”‚       â””â”€â”€ tdnn.py
    â”‚       â”‚   â”œâ”€â”€ kokoro/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ istftnet.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ kokoro.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ modules.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ pipeline.py
    â”‚       â”‚   â”‚   â””â”€â”€ voice.py
    â”‚       â”‚   â”œâ”€â”€ llama/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â””â”€â”€ llama.py
    â”‚       â”‚   â”œâ”€â”€ outetts/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ audio_processor.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ dac_interface.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ default_speaker.json
    â”‚       â”‚   â”‚   â”œâ”€â”€ outetts.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ prompt_processor.py
    â”‚       â”‚   â”‚   â””â”€â”€ tokens.py
    â”‚       â”‚   â”œâ”€â”€ sesame/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ attention.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ sesame.py
    â”‚       â”‚   â”‚   â””â”€â”€ watermarking.py
    â”‚       â”‚   â””â”€â”€ spark/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”œâ”€â”€ audio_tokenizer.py
    â”‚       â”‚       â”œâ”€â”€ bicodec.py
    â”‚       â”‚       â”œâ”€â”€ spark.py
    â”‚       â”‚       â”œâ”€â”€ modules/
    â”‚       â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”‚   â”œâ”€â”€ finite_scalar_quantization.py
    â”‚       â”‚       â”‚   â”œâ”€â”€ residual.py
    â”‚       â”‚       â”‚   â”œâ”€â”€ residual_fsq.py
    â”‚       â”‚       â”‚   â”œâ”€â”€ blocks/
    â”‚       â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”‚   â”‚   â””â”€â”€ sampler.py
    â”‚       â”‚       â”‚   â”œâ”€â”€ encoder_decoder/
    â”‚       â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”‚   â”‚   â”œâ”€â”€ feat_decoder.py
    â”‚       â”‚       â”‚   â”‚   â”œâ”€â”€ feat_encoder.py
    â”‚       â”‚       â”‚   â”‚   â””â”€â”€ wave_generator.py
    â”‚       â”‚       â”‚   â””â”€â”€ speaker/
    â”‚       â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”‚       â”œâ”€â”€ ecapa_tdnn.py
    â”‚       â”‚       â”‚       â”œâ”€â”€ perceiver_encoder.py
    â”‚       â”‚       â”‚       â”œâ”€â”€ pooling_layers.py
    â”‚       â”‚       â”‚       â””â”€â”€ speaker_encoder.py
    â”‚       â”‚       â””â”€â”€ utils/
    â”‚       â”‚           â”œâ”€â”€ audio.py
    â”‚       â”‚           â”œâ”€â”€ file.py
    â”‚       â”‚           â””â”€â”€ token_parser.py
    â”‚       â””â”€â”€ tests/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ test_base.py
    â”‚           â”œâ”€â”€ test_convert.py
    â”‚           â”œâ”€â”€ test_interpolate.py
    â”‚           â””â”€â”€ test_models.py
    â””â”€â”€ .github/
        â”œâ”€â”€ FUNDING.yml
        â”œâ”€â”€ pull_request_template.md
        â””â”€â”€ workflows/
            â”œâ”€â”€ python-publish.yml
            â”œâ”€â”€ swift.yml
            â””â”€â”€ tests.yml

================================================
FILE: README.md
================================================
# MLX-Audio

A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple's MLX framework, providing efficient speech synthesis on Apple Silicon.

## Features

- Fast inference on Apple Silicon (M series chips)
- Multiple language support
- Voice customization options
- Adjustable speech speed control (0.5x to 2.0x)
- Interactive web interface with 3D audio visualization
- REST API for TTS generation
- Quantization support for optimized performance
- Direct access to output files via Finder/Explorer integration

## Installation

```bash
# Install the package
pip install mlx-audio

# For web interface and API dependencies
pip install -r requirements.txt
```

### Quick Start

To generate audio with an LLM use:

```bash
# Basic usage
mlx_audio.tts.generate --text "Hello, world"

# Specify prefix for output file
mlx_audio.tts.generate --text "Hello, world" --file_prefix hello

# Adjust speaking speed (0.5-2.0)
mlx_audio.tts.generate --text "Hello, world" --speed 1.4
```

### How to call from python

To generate audio with an LLM use:

```python
from mlx_audio.tts.generate import generate_audio

# Example: Generate an audiobook chapter as mp3 audio
generate_audio(
    text=("In the beginning, the universe was created...\n"
        "...or the simulation was booted up."),
    model_path="prince-canuma/Kokoro-82M",
    voice="af_heart",
    speed=1.2,
    lang_code="a", # Kokoro: (a)f_heart, or comment out for auto
    file_prefix="audiobook_chapter1",
    audio_format="wav",
    sample_rate=24000,
    join_audio=True,
    verbose=True  # Set to False to disable print messages
)

print("Audiobook chapter successfully generated!")

```

### Web Interface & API Server

MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:

1. Generate TTS with different voices and speed settings
2. Upload and play your own audio files
3. Visualize audio with an interactive 3D orb
4. Automatically saves generated audio files to the outputs directory in the current working folder
5. Open the output folder directly from the interface (when running locally)

#### Features

- **Multiple Voice Options**: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)
- **Adjustable Speech Speed**: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)
- **Real-time 3D Visualization**: A responsive 3D orb that reacts to audio frequencies
- **Audio Upload**: Play and visualize your own audio files
- **Auto-play Option**: Automatically play generated audio
- **Output Folder Access**: Convenient button to open the output folder in your system's file explorer

To start the web interface and API server:

```bash
# Using the command-line interface
mlx_audio.server

# With custom host and port
mlx_audio.server --host 0.0.0.0 --port 9000

# With verbose logging
mlx_audio.server --verbose
```

Available command line arguments:
- `--host`: Host address to bind the server to (default: 127.0.0.1)
- `--port`: Port to bind the server to (default: 8000)

Then open your browser and navigate to:
```
http://127.0.0.1:8000
```

#### API Endpoints

The server provides the following REST API endpoints:

- `POST /tts`: Generate TTS audio
  - Parameters (form data):
    - `text`: The text to convert to speech (required)
    - `voice`: Voice to use (default: "af_heart")
    - `speed`: Speech speed from 0.5 to 2.0 (default: 1.0)
  - Returns: JSON with filename of generated audio

- `GET /audio/{filename}`: Retrieve generated audio file

- `POST /play`: Play audio directly from the server
  - Parameters (form data):
    - `filename`: The filename of the audio to play (required)
  - Returns: JSON with status and filename

- `POST /stop`: Stop any currently playing audio
  - Returns: JSON with status

- `POST /open_output_folder`: Open the output folder in the system's file explorer
  - Returns: JSON with status and path
  - Note: This feature only works when running the server locally

> Note: Generated audio files are stored in `~/.mlx_audio/outputs` by default, or in a fallback directory if that location is not writable.

## Models

### Kokoro

Kokoro is a multilingual TTS model that supports various languages and voice styles.

#### Example Usage

```python
from mlx_audio.tts.models.kokoro import KokoroPipeline
from mlx_audio.tts.utils import load_model
from IPython.display import Audio
import soundfile as sf

# Initialize the model
model_id = 'prince-canuma/Kokoro-82M'
model = load_model(model_id)

# Create a pipeline with American English
pipeline = KokoroPipeline(lang_code='a', model=model, repo_id=model_id)

# Generate audio
text = "The MLX King lives. Let him cook!"
for _, _, audio in pipeline(text, voice='af_heart', speed=1, split_pattern=r'\n+'):
    # Display audio in notebook (if applicable)
    display(Audio(data=audio, rate=24000, autoplay=0))

    # Save audio to file
    sf.write('audio.wav', audio[0], 24000)
```

#### Language Options

- ðŸ‡ºðŸ‡¸ `'a'` - American English
- ðŸ‡¬ðŸ‡§ `'b'` - British English
- ðŸ‡¯ðŸ‡µ `'j'` - Japanese (requires `pip install misaki[ja]`)
- ðŸ‡¨ðŸ‡³ `'z'` - Mandarin Chinese (requires `pip install misaki[zh]`)

### CSM (Conversational Speech Model)

CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.

#### Example Usage

```bash
# Generate speech using CSM-1B model with reference audio
python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text "Hello from Sesame." --play --ref_audio ./conversational_a.wav
```

You can pass any audio to clone the voice from or download sample audio file from [here](https://huggingface.co/mlx-community/csm-1b/tree/main/prompts).

## Advanced Features

### Quantization

You can quantize models for improved performance:

```python
from mlx_audio.tts.utils import quantize_model, load_model
import json
import mlx.core as mx

model = load_model(repo_id='prince-canuma/Kokoro-82M')
config = model.config

# Quantize to 8-bit
group_size = 64
bits = 8
weights, config = quantize_model(model, config, group_size, bits)

# Save quantized model
with open('./8bit/config.json', 'w') as f:
    json.dump(config, f)

mx.save_safetensors("./8bit/kokoro-v1_0.safetensors", weights, metadata={"format": "mlx"})
```

## Requirements

- MLX
- Python 3.8+
- Apple Silicon Mac (for optimal performance)
- For the web interface and API:
  - FastAPI
  - Uvicorn
  
## License

[MIT License](LICENSE)

## Acknowledgements

- Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.
- This project uses the Kokoro model architecture for text-to-speech synthesis.
- The 3D visualization uses Three.js for rendering.



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Prince Canuma

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
include ./requirements.txt
recursive-include mlx_audio/ *.py
recursive-include mlx_audio/tts *.html 


================================================
FILE: Package.resolved
================================================
{
  "pins" : [
    {
      "identity" : "mlx-swift",
      "kind" : "remoteSourceControl",
      "location" : "https://github.com/ml-explore/mlx-swift",
      "state" : {
        "revision" : "d3f89b1f520ea9e52fd9de68c1dca708f040406a",
        "version" : "0.25.3"
      }
    },
    {
      "identity" : "swift-numerics",
      "kind" : "remoteSourceControl",
      "location" : "https://github.com/apple/swift-numerics",
      "state" : {
        "revision" : "e0ec0f5f3af6f3e4d5e7a19d2af26b481acb6ba8",
        "version" : "1.0.3"
      }
    }
  ],
  "version" : 2
}



================================================
FILE: Package.swift
================================================
// swift-tools-version:5.9
import PackageDescription

let package = Package(
    name: "Swift-TTS",
    platforms: [.macOS(.v14), .iOS(.v16)],
    products: [
        .library(
            name: "mlx-swift-audio",
            targets: ["Swift-TTS","ESpeakNG"]),
    ],
    dependencies: [
         .package(url: "https://github.com/ml-explore/mlx-swift", from: "0.25.2")
    ],
    targets: [
        .binaryTarget(
            name: "ESpeakNG",
            path: "mlx_audio_swift/tts/Swift-TTS/Kokoro/Frameworks/ESpeakNG.xcframework"
        ),
        .target(
            name: "Swift-TTS",
            dependencies: [
                .product(name: "MLX", package: "mlx-swift"),
                .product(name: "MLXFFT", package: "mlx-swift"),
                .product(name: "MLXNN", package: "mlx-swift"),
                "ESpeakNG"
            ],
            path: "mlx_audio_swift/tts/Swift-TTS",
            exclude: ["Preview Content", "Assets.xcassets", "Swift_TTSApp.swift", "Swift_TTS.entitlements"],
            resources: [.process("Kokoro/Resources")] // Access the voices from Kokoro
        ),
        .testTarget(
            name: "Swift-TTS-Tests",
            dependencies: ["Swift-TTS"],
            path: "mlx_audio_swift/tts/Tests"
        ),
    ]
)



================================================
FILE: pytest.ini
================================================
[pytest]
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function 


================================================
FILE: requirements.txt
================================================
misaki[en]>=0.8.2
loguru>=0.7.3
num2words>=0.5.14
spacy>=3.8.4
phonemizer-fork>=3.3.2
espeakng-loader>=0.2.4
mlx>=0.25.2
mlx-vlm>=0.1.27
numpy>=1.26.4
transformers>=4.49.0
sentencepiece>=0.2.0
huggingface_hub>=0.27.0
sounddevice>=0.5.1
soundfile>=0.13.1
fastapi>=0.95.0
uvicorn>=0.22.0
einops>=0.8.1
tiktoken>=0.9.0
tqdm>=4.67.1
pyloudnorm>=0.1.1
omegaconf==2.3.0
einops==0.8.1
einx==0.3.0
fastrtc[vad, stt]
webrtcvad>=2.0.10
dacite>=1.9.2
pytest-asyncio>=1.0.0



================================================
FILE: setup.py
================================================
import sys
from pathlib import Path

from setuptools import find_packages, setup

# Get the project root directory
root_dir = Path(__file__).parent

# Add the package directory to the Python path
package_dir = root_dir / "mlx_audio"
sys.path.append(str(package_dir))

# Read the requirements from the requirements.txt file
requirements_path = root_dir / "requirements.txt"
with open(requirements_path) as fid:
    requirements = [l.strip() for l in fid.readlines()]

# Import the version from the package
from mlx_audio.version import __version__

# Setup configuration
setup(
    name="mlx-audio",
    version=__version__,
    description="MLX-Audio is a package for inference of text-to-speech (TTS) and speech-to-speech (STS) models locally on your Mac using MLX",
    long_description=open(root_dir / "README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    author_email="prince.gdt@gmail.com",
    author="Prince Canuma",
    url="https://github.com/Blaizzy/mlx-audio",
    license="MIT",
    install_requires=requirements,
    packages=find_packages(where=root_dir),
    include_package_data=True,
    package_data={
        "mlx_audio": [
            "tts/*.html",
            "tts/*.js",
            "tts/*.css",
            "tts/**/*.json",
            "tts/static/**/*",
            "stt/**/*.tiktoken",
        ],
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.8",
    extras_require={
        "py38": ["importlib_resources"],
    },
    entry_points={
        "console_scripts": [
            "mlx_audio.tts.generate = mlx_audio.tts.generate:main",
            "mlx_audio.server = mlx_audio.server:main",
        ]
    },
)



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
-   repo: https://github.com/psf/black-pre-commit-mirror
    rev: 24.2.0
    hooks:
    -   id: black
-   repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
    -   id: isort
        args:
            - --profile=black


================================================
FILE: examples/bible-audiobook/README.md
================================================
# bible-audiobook

## Why? Because I can, that's why

I grabbed a copy of the American King James Bible at [openbible.com](https://openbible.com/texts.htm)  
I wrote me a script to call the mx_audio.server and convert, verse by verse.
It took me **28 hours and 05 minutes** to convert all **31,102** verses, resulting in **86h32m36s**.
I ran it on a Mac Mini M4pro, 12 cores, 24GB Ram;

![Screenshot 2025-03-18 at 14 52 35](https://github.com/user-attachments/assets/4a213b39-a24c-4e93-b0dc-04b385ea20ed)


Some example results, are located at `./audios/bible_akjv`

## Want to try it yourself?

#### just do it!

#### Notes:

- I am a javascript developer so all the code is in javascript / typescript
- I work specifically with bun, and there is code here that won't work in nodejs
- for the code to work flawlessly (fault tolerant) and as is, you need to also have a nodejs installation so you can run the mlx_audio.server running through [pm2](https://pm2.keymetrics.io/)
  - you can do it without node and pm2 but there's a close to 100% chance the mlx_audio.server will crash a few times during the hard work so i built a mechanism that depends on pm2 so it doesn't need my manual intervention to keep going
- before running any of the scripts, edit the file to change your output directories and what not
- if the process does crash, or if you need to stop / pause the process, change the value on _line 37_ of `./src/index.ts` so you can pick up where it left off

### Instructions

install [Bun](https://bun.sh) if you don't have it

```bash
curl -fsSL https://bun.sh/install | bash
```

- Install local dependencies:

```bash
bun install
```

to synthesize the whole American King James Bible, run:

```bash
bun run src/index.ts
```

to convert all the .wav files to .mp3 (needs ffmpeg on your machine) run

```bash
bun run src/convert-to-mp3.ts
```

for statistics and to check the integrity of the **mp3** files (needs ffmpeg on your machine), run

```bash
bun run src/mp3-checker.ts
```

This project was created using `bun init` in bun v1.2.0. [Bun](https://bun.sh) is a fast all-in-one JavaScript runtime.



================================================
FILE: examples/bible-audiobook/package.json
================================================
{
  "name": "bible-audiobook",
  "module": "src/index.ts",
  "type": "module",
  "dependencies": {
    "@types/fluent-ffmpeg": "^2.1.27",
    "D": "^1.0.0",
    "fluent-ffmpeg": "^2.1.3",
    "mkdirp": "^3.0.1"
  },
  "devDependencies": {
    "@types/bun": "latest",
    "@types/node": "^22.13.10"
  },
  "peerDependencies": {
    "typescript": "^5.0.0"
  }
}



================================================
FILE: examples/bible-audiobook/tsconfig.json
================================================
{
  "compilerOptions": {
    // Enable latest features
    "lib": ["ESNext", "DOM"],
    "target": "ESNext",
    "module": "ESNext",
    "moduleDetection": "force",
    "jsx": "react-jsx",
    "allowJs": true,

    // Bundler mode
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "noEmit": true,

    // Best practices
    "strict": true,
    "skipLibCheck": true,
    "noFallthroughCasesInSwitch": true,

    // Some stricter flags (disabled by default)
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "noPropertyAccessFromIndexSignature": false
  }
}



================================================
FILE: examples/bible-audiobook/bibles/bible-akjv.txt
================================================
Genesis 1:1	In the beginning God created the heaven and the earth.
Genesis 1:2	And the earth was without form, and void; and darkness was on the face of the deep. And the Spirit of God moved on the face of the waters.
Genesis 1:3	And God said, Let there be light: and there was light.
Genesis 1:4	And God saw the light, that it was good: and God divided the light from the darkness.
Genesis 1:5	And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.
Genesis 1:6	And God said, Let there be a firmament in the middle of the waters, and let it divide the waters from the waters.
Genesis 1:7	And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.
Genesis 1:8	And God called the firmament Heaven. And the evening and the morning were the second day.
Genesis 1:9	And God said, Let the waters under the heaven be gathered together to one place, and let the dry land appear: and it was so.
Genesis 1:10	And God called the dry land Earth; and the gathering together of the waters called he Seas: and God saw that it was good.
Genesis 1:11	And God said, Let the earth bring forth grass, the herb yielding seed, and the fruit tree yielding fruit after his kind, whose seed is in itself, on the earth: and it was so.
Genesis 1:12	And the earth brought forth grass, and herb yielding seed after his kind, and the tree yielding fruit, whose seed was in itself, after his kind: and God saw that it was good.
Genesis 1:13	And the evening and the morning were the third day.
Genesis 1:14	And God said, Let there be lights in the firmament of the heaven to divide the day from the night; and let them be for signs, and for seasons, and for days, and years:
Genesis 1:15	And let them be for lights in the firmament of the heaven to give light on the earth: and it was so.
Genesis 1:16	And God made two great lights; the greater light to rule the day, and the lesser light to rule the night: he made the stars also.
Genesis 1:17	And God set them in the firmament of the heaven to give light on the earth,
Genesis 1:18	And to rule over the day and over the night, and to divide the light from the darkness: and God saw that it was good.
Genesis 1:19	And the evening and the morning were the fourth day.
Genesis 1:20	And God said, Let the waters bring forth abundantly the moving creature that has life, and fowl that may fly above the earth in the open firmament of heaven.
Genesis 1:21	And God created great whales, and every living creature that moves, which the waters brought forth abundantly, after their kind, and every winged fowl after his kind: and God saw that it was good.
Genesis 1:22	And God blessed them, saying, Be fruitful, and multiply, and fill the waters in the seas, and let fowl multiply in the earth.
Genesis 1:23	And the evening and the morning were the fifth day.
Genesis 1:24	And God said, Let the earth bring forth the living creature after his kind, cattle, and creeping thing, and beast of the earth after his kind: and it was so.
Genesis 1:25	And God made the beast of the earth after his kind, and cattle after their kind, and every thing that creeps on the earth after his kind: and God saw that it was good.
Genesis 1:26	And God said, Let us make man in our image, after our likeness: and let them have dominion over the fish of the sea, and over the fowl of the air, and over the cattle, and over all the earth, and over every creeping thing that creeps on the earth.
Genesis 1:27	So God created man in his own image, in the image of God created he him; male and female created he them.
Genesis 1:28	And God blessed them, and God said to them, Be fruitful, and multiply, and replenish the earth, and subdue it: and have dominion over the fish of the sea, and over the fowl of the air, and over every living thing that moves on the earth.
Genesis 1:29	And God said, Behold, I have given you every herb bearing seed, which is on the face of all the earth, and every tree, in the which is the fruit of a tree yielding seed; to you it shall be for meat.
Genesis 1:30	And to every beast of the earth, and to every fowl of the air, and to every thing that creeps on the earth, wherein there is life, I have given every green herb for meat: and it was so.
Genesis 1:31	And God saw every thing that he had made, and, behold, it was very good. And the evening and the morning were the sixth day.



================================================
FILE: examples/bible-audiobook/src/convert-to-mp3.ts
================================================
import fs from 'node:fs/promises'
import path from 'node:path'
import { mkdirp } from 'mkdirp'
import ffmpeg from 'fluent-ffmpeg'

const origin = 'audios/bible-akjv/am_michael';
const destination = 'audios/bible-akjv/destination';

const regex = /(\d{8})-(.+)(-)(\d+)\:(\d+)(\.wav)/

const allFiles = (await fs.readdir(origin)).toSorted();

for (let file of allFiles) {
  // console.log(file)
  if (file.startsWith('.')) continue;
  const filePath = path.join(origin, file);
  const fileStat = await fs.stat(filePath);
  if (fileStat.isFile()) {
    const match = file.match(regex);
    //console.log(match);
    const [, index, book, , chapter, verse, extension] = match!;
    const idx = parseInt(index!, 10);

    // if (idx < 17850) continue;

    const destinationPath = path.join(destination, book!, chapter!);
    await mkdirp(destinationPath);
    const newFileName = `${index}_${book}_${chapter}_${verse}.mp3`;
    // console.log({ newFileName });
    const newFilePath = path.join(destinationPath, newFileName);
    console.log(idx, newFilePath)
    await new Promise((resolve, reject) => {
      ffmpeg(filePath)
        .toFormat('mp3')
        .on('end', resolve)
        .on('error', reject)
        .save(newFilePath);
    });
  } else {
    throw new Error('Not a file');
  }
  await new Promise(resolve => setTimeout(resolve, 500));
}


================================================
FILE: examples/bible-audiobook/src/index.ts
================================================
import { createReadStream } from "node:fs";
import path from 'node:path'
import readline from "node:readline";
import { rename } from "node:fs/promises";
import { mkdirp } from "mkdirp"
import { $ } from 'bun';
const regex = /(.*)\s(\d+\:\d+)\t(.*)/

const outputPath = '/path/to/output/dir';

const main = async (bibleName: string, voice: string, withTitle: boolean = false) => {
  const file = `./bibles/${bibleName}.txt`;
  await mkdirp(`./audios/${bibleName}`);

  const targetPath = path.resolve(__dirname, '..', 'audios', bibleName, voice);
  await mkdirp(targetPath);

  let lineCount = 0;
  // for (let voice of voices) {
  const rs = createReadStream(file);
  const rl = readline.createInterface({ input: rs, crlfDelay: Infinity });
  await mkdirp(`./audios/${bibleName}/${voice}`);
  for await (const line of rl) {
    const match = line.match(regex);
    if (!match) continue;
    const [book, chapter, text] = match.slice(1);
    console.log(`${lineCount}: ${book} | ${chapter} | ${text}`);
    const fileName = `${leftpad((++lineCount).toString(), 8)}-${book}-${chapter!.replace('/', ':')}.wav`;
    const title = `${book} ${chapter}`;

    /**
     * in case we have to stop the process and resume later,
     * set the lineCount to the last index you processed
     * or better yet, a few indexes before, just to make sure
     */

    if (lineCount <= 0) continue;

    console.log(title, fileName)
    const remoteFileName = await transcribe(title, text!, voice);
    await rename(`${outputPath}/${remoteFileName}`, path.resolve(targetPath, fileName));

  }
};

main("bible-akjv", 'am_michael', false);

const transcribe = async (title: string, text: string, voice: string) => {
  try {
    const result = await fetch("http://localhost:3333/tts", {
      "headers": {
        "accept": "*/*",
        "accept-language": "en-US,en;q=0.8",
        // "content-type": "multipart/form-data; boundary=----WebKitFormBoundaryL6e2mv0qkI5aWBAs",
        "sec-ch-ua": "\"Not(A:Brand\";v=\"99\", \"Brave\";v=\"133\", \"Chromium\";v=\"133\"",
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": "\"macOS\"",
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "sec-gpc": "1",
        "cookie": "NEXT_LOCALE=en; token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjIwZTdhMDFmLWIwMjItNDZjZi1iZjdhLWQ4ZmYwMzU1YWI4MSJ9.CblzSRvfQlUxVZGd08aVHpqbD7bRAJLON8XXpbV5Py0",
        "Referer": "http://localhost:3333/",
        "Referrer-Policy": "strict-origin-when-cross-origin"
      },
      "body": getBody(title, text, voice),
      "method": "POST"
    });
    const data: any = await result.json()
    return data.filename;
  } catch (ex) {
    // await fetch('https://ntfy.andrepadez.com/mlx-audio', {
    //   method: 'POST', // PUT works too
    //   body: 'Macmini bible synth process has crashed'
    // });

    /**
     * for this safeguard to work, we need to be running mlx-audio.server
     * through pm2, so it can restart itself when it crashes
     * and continue seemlessly from where it left off
     */

    console.log('ERRORED!', 'restarting mlx-audio server in 5 seconds');
    await new Promise((resolve) => setTimeout(resolve, 5000));
    await $`/opt/homebrew/bin/pm2 restart "MLX-AUDIO"`.text();
    await new Promise((resolve) => setTimeout(resolve, 5000));
    return transcribe(title, text, voice);
  }
}

const getBody = (title: string, text: string, voice: string) => {
  const formData = new FormData();
  formData.append("text", `\n\n${text}`);
  formData.append("voice", voice);
  formData.append("model", "mlx-community/Kokoro-82M-bf16");
  formData.append("speed", "1");
  return formData;
}




const voices = [
  "af_heart",
  "af_nova",
  "af_bella",
  "af_nicole",
  "af_sarah",
  "af_sky",
  "am_adam",
  "am_michael",
  "bf_emma",
  "bf_isabella",
  "bm_george",
  "bm_lewis",
]

const leftpad = (str: string, len: number = 2, ch: string = "0") => {
  const length = len - str.length;
  return ch.repeat(length) + str;
}


================================================
FILE: examples/bible-audiobook/src/mp3-checker.ts
================================================
const ffmpeg = require('fluent-ffmpeg');
const fs = require('fs').promises;
const path = require('path');

// Directory containing your MP3 files (root folder)
const rootDirectoryPath = '/path/to/output/dir'; // Replace with your root directory path

// Recursive function to get all MP3 files in a directory and its subdirectories
async function getAllMp3Files(dirPath: string) {
  let mp3Files: any = [];
  const files = await fs.readdir(dirPath, { withFileTypes: true });

  for (const file of files) {
    const fullPath = path.join(dirPath, file.name);
    if (file.isDirectory()) {
      // Recursively get MP3s from subdirectories
      const subDirFiles = await getAllMp3Files(fullPath);
      mp3Files = mp3Files.concat(subDirFiles);
    } else if (file.name.endsWith('.mp3') && !file.name.startsWith('.')) {
      // Add MP3 file with its full relative path
      mp3Files.push(fullPath);
    }
  }

  return mp3Files;
}

let counter: number = 1;

async function getMp3DurationsWithValidation() {
  try {
    // Get all MP3 files recursively
    const mp3Files = await getAllMp3Files(rootDirectoryPath);

    if (mp3Files.length === 0) {
      console.log('No MP3 files found in the directory or its subdirectories.');
      return;
    }

    let totalDuration = 0;
    let corruptFiles = 0;
    const corruptFileList = [];

    // Process each MP3 file
    for (const filePath of mp3Files) {
      // Use relative path from root for cleaner output
      const relativeFilePath = path.relative(rootDirectoryPath, filePath);

      try {
        const duration: number = await new Promise((resolve, reject) => {
          ffmpeg.ffprobe(filePath, (err: any, metadata: any) => {
            if (err) {
              return reject(new Error(`ffprobe error: ${err.message}`));
            }

            const durationInSeconds = metadata.format.duration;
            if (typeof durationInSeconds !== 'number' || isNaN(durationInSeconds) || durationInSeconds <= 0) {
              return reject(new Error('Invalid or missing duration in metadata'));
            }

            resolve(durationInSeconds);
          });
        });

        console.log(counter, `${relativeFilePath}: ${duration.toFixed(2)} seconds (Valid)`);
        totalDuration += duration;

      } catch (error: any) {
        console.log(counter, `${relativeFilePath}: CORRUPT or UNPARSEABLE - ${error.message}`);
        corruptFiles++;
        corruptFileList.push(`${relativeFilePath} - ${error.message}`);
      } finally {
        counter++;
      }
    }

    // Summary
    console.log(`\nProcessed ${mp3Files.length} MP3 files:`);
    console.log(`- Valid files: ${mp3Files.length - corruptFiles}`);
    console.log(`- Corrupt or unparseable files: ${corruptFiles}`);

    if (corruptFiles > 0) {
      console.log('\nList of corrupt or unparseable files:');
      corruptFileList.forEach(file => console.log(`- ${file}`));
    } else {
      console.log('\nNo corrupt or unparseable files found.');
    }

    if (totalDuration > 0) {
      // Convert total duration to human-readable format
      const hours = Math.floor(totalDuration / 3600);
      const minutes = Math.floor((totalDuration % 3600) / 60);
      const seconds = Math.floor(totalDuration % 60);

      console.log(`\nTotal duration of valid files: ${hours}h ${minutes}m ${seconds}s`);
      console.log(`Total duration in seconds: ${totalDuration.toFixed(2)}`);
    } else {
      console.log('\nNo valid files to calculate total duration.');
    }

  } catch (error) {
    console.error('An unexpected error occurred:', error);
  }
}

// Run the function
getMp3DurationsWithValidation();


================================================
FILE: mlx_audio/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/server.py
================================================
import argparse
import importlib.util
import logging
import os
import sys
import tempfile
import uuid

import numpy as np
import requests
import soundfile as sf
import uvicorn
from fastapi import FastAPI, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastrtc import ReplyOnPause, Stream, get_stt_model
from numpy.typing import NDArray
from pydantic import BaseModel


# Configure logging
def setup_logging(verbose: bool = False):
    level = logging.DEBUG if verbose else logging.INFO
    format_str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    if verbose:
        format_str = "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"

    logging.basicConfig(level=level, format=format_str)
    return logging.getLogger("mlx_audio_server")


logger = setup_logging()  # Will be updated with verbose setting in main()

from mlx_audio.tts.generate import main as generate_main

# Import from mlx_audio package
from mlx_audio.tts.utils import load_model

from .tts.audio_player import AudioPlayer

app = FastAPI()

# Add CORS middleware to allow requests from the same origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins, will be restricted by host binding
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load the model once on server startup.
# You can change the model path or pass arguments as needed.
# For performance, load once globally:
tts_model = None  # Will be loaded when the server starts
audio_player = None  # Will be initialized when the server starts
stt_model = get_stt_model()
# Make sure the output folder for generated TTS files exists
# Use an absolute path that's guaranteed to be writable
OUTPUT_FOLDER = os.path.join(os.path.expanduser("~"), ".mlx_audio", "outputs")
os.makedirs(OUTPUT_FOLDER, exist_ok=True)
logger.debug(f"Using output folder: {OUTPUT_FOLDER}")


def speech_to_speech_handler(
    audio: tuple[int, NDArray[np.int16]], voice: str, speed: float, model: str
):
    text = stt_model.stt(audio)
    for segment in tts_model.generate(
        text=text,
        voice=voice,
        speed=speed,
        lang_code=voice[0],
        verbose=False,
    ):
        yield (24_000, np.array(segment.audio, copy=False))
        yield (24_000, np.zeros(2_400, dtype=np.float32))


stream = Stream(
    ReplyOnPause(speech_to_speech_handler, output_sample_rate=24_000),
    mode="send-receive",
    modality="audio",
)
stream.mount(app)


class SpeechToSpeechArgs(BaseModel):
    voice: str
    speed: float
    model: str
    webrtc_id: str


@app.post("/speech_to_speech_input")
def speech_to_speech_endpoint(args: SpeechToSpeechArgs):
    stream.set_input(args.webrtc_id, args.voice, args.speed, args.model)
    return {"status": "success"}


@app.post("/tts")
def tts_endpoint(
    text: str = Form(...),
    voice: str = Form("af_heart"),
    speed: float = Form(1.0),
    model: str = Form("mlx-community/Kokoro-82M-4bit"),
):
    """
    POST an x-www-form-urlencoded form with 'text' (and optional 'voice', 'speed', and 'model').
    We run TTS on the text, save the audio in a unique file,
    and return JSON with the filename so the client can retrieve it.
    """
    global tts_model

    if not text.strip():
        return JSONResponse({"error": "Text is empty"}, status_code=400)

    # Validate speed parameter
    try:
        speed_float = float(speed)
        if speed_float < 0.5 or speed_float > 2.0:
            return JSONResponse(
                {"error": "Speed must be between 0.5 and 2.0"}, status_code=400
            )
    except ValueError:
        return JSONResponse({"error": "Invalid speed value"}, status_code=400)

    # Validate model parameter
    valid_models = [
        "mlx-community/Kokoro-82M-4bit",
        "mlx-community/Kokoro-82M-6bit",
        "mlx-community/Kokoro-82M-8bit",
        "mlx-community/Kokoro-82M-bf16",
    ]
    if model not in valid_models:
        return JSONResponse(
            {"error": f"Invalid model. Must be one of: {', '.join(valid_models)}"},
            status_code=400,
        )

    # Store current model repo_id for comparison
    current_model_repo_id = (
        getattr(tts_model, "repo_id", None) if tts_model is not None else None
    )

    # Load the model if it's not loaded or if a different model is requested
    if tts_model is None or current_model_repo_id != model:
        try:
            logger.debug(f"Loading TTS model from {model}")
            tts_model = load_model(model)
            logger.debug("TTS model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading TTS model: {str(e)}")
            return JSONResponse(
                {"error": f"Failed to load model: {str(e)}"}, status_code=500
            )

    # We'll do something like the code in model.generate() from the TTS library:
    # Generate the unique filename
    unique_id = str(uuid.uuid4())
    filename = f"tts_{unique_id}.wav"
    output_path = os.path.join(OUTPUT_FOLDER, filename)

    logger.debug(
        f"Generating TTS for text: '{text[:50]}...' with voice: {voice}, speed: {speed_float}, model: {model}"
    )
    logger.debug(f"Output file will be: {output_path}")

    # We'll use the high-level "model.generate" method:
    results = tts_model.generate(
        text=text,
        voice=voice,
        speed=speed_float,
        lang_code=voice[0],
        verbose=False,
    )

    # We'll just gather all segments (if any) into a single wav
    # It's typical for multi-segment text to produce multiple wave segments:
    audio_arrays = []
    for segment in results:
        audio_arrays.append(segment.audio)

    # If no segments, return error
    if not audio_arrays:
        logger.error("No audio segments generated")
        return JSONResponse({"error": "No audio generated"}, status_code=500)

    # Concatenate all segments
    cat_audio = np.concatenate(audio_arrays, axis=0)

    # Write the audio as a WAV
    try:
        sf.write(output_path, cat_audio, 24000)
        logger.debug(f"Successfully wrote audio file to {output_path}")

        # Verify the file exists
        if not os.path.exists(output_path):
            logger.error(f"File was not created at {output_path}")
            return JSONResponse(
                {"error": "Failed to create audio file"}, status_code=500
            )

        # Check file size
        file_size = os.path.getsize(output_path)
        logger.debug(f"File size: {file_size} bytes")

        if file_size == 0:
            logger.error("File was created but is empty")
            return JSONResponse(
                {"error": "Generated audio file is empty"}, status_code=500
            )

    except Exception as e:
        logger.error(f"Error writing audio file: {str(e)}")
        return JSONResponse(
            {"error": f"Failed to save audio: {str(e)}"}, status_code=500
        )

    return {"filename": filename}


@app.get("/audio/{filename}")
def get_audio_file(filename: str):
    """
    Return an audio file from the outputs folder.
    The user can GET /audio/<filename> to fetch the WAV file.
    """
    file_path = os.path.join(OUTPUT_FOLDER, filename)
    logger.debug(f"Requested audio file: {file_path}")

    if not os.path.exists(file_path):
        logger.error(f"File not found: {file_path}")
        # List files in the directory to help debug
        try:
            files = os.listdir(OUTPUT_FOLDER)
            logger.debug(f"Files in output directory: {files}")
        except Exception as e:
            logger.error(f"Error listing output directory: {str(e)}")

        return JSONResponse({"error": "File not found"}, status_code=404)

    logger.debug(f"Serving audio file: {file_path}")
    return FileResponse(file_path, media_type="audio/wav")


@app.get("/")
def root():
    """
    Serve the audio_player.html page or a fallback HTML if not found
    """
    try:
        # Try to find the audio_player.html file in the package
        static_dir = find_static_dir()
        audio_player_path = os.path.join(static_dir, "audio_player.html")
        return FileResponse(audio_player_path)
    except Exception as e:
        # If there's an error, return a simple HTML page with error information
        return HTMLResponse(
            content=f"""
            <html>
                <head><title>MLX-Audio TTS Server</title></head>
                <body>
                    <h1>MLX-Audio TTS Server</h1>
                    <p>The server is running, but the web interface could not be loaded.</p>
                    <p>Error: {str(e)}</p>
                    <h2>API Endpoints</h2>
                    <ul>
                        <li><code>POST /tts</code> - Generate TTS audio</li>
                        <li><code>GET /audio/{{filename}}</code> - Retrieve generated audio file</li>
                    </ul>
                </body>
            </html>
            """,
            status_code=200,
        )


def find_static_dir():
    """Find the static directory containing HTML files."""
    # Try different methods to find the static directory

    # Method 1: Use importlib.resources (Python 3.9+)
    try:
        import importlib.resources as pkg_resources

        static_dir = pkg_resources.files("mlx_audio").joinpath("tts")
        static_dir_str = str(static_dir)
        if os.path.exists(static_dir_str):
            return static_dir_str
    except (ImportError, AttributeError):
        pass

    # Method 2: Use importlib_resources (Python 3.8)
    try:
        import importlib_resources

        static_dir = importlib_resources.files("mlx_audio").joinpath("tts")
        static_dir_str = str(static_dir)
        if os.path.exists(static_dir_str):
            return static_dir_str
    except ImportError:
        pass

    # Method 3: Use pkg_resources
    try:
        static_dir_str = pkg_resources.resource_filename("mlx_audio", "tts")
        if os.path.exists(static_dir_str):
            return static_dir_str
    except (ImportError, pkg_resources.DistributionNotFound):
        pass

    # Method 4: Try to find the module path directly
    try:
        module_spec = importlib.util.find_spec("mlx_audio")
        if module_spec and module_spec.origin:
            package_dir = os.path.dirname(module_spec.origin)
            static_dir_str = os.path.join(package_dir, "tts")
            if os.path.exists(static_dir_str):
                return static_dir_str
    except (ImportError, AttributeError):
        pass

    # Method 5: Look in sys.modules
    try:
        if "mlx_audio" in sys.modules:
            module = sys.modules["mlx_audio"]
            if hasattr(module, "__file__"):
                package_dir = os.path.dirname(module.__file__)
                static_dir_str = os.path.join(package_dir, "tts")
                if os.path.exists(static_dir_str):
                    return static_dir_str
    except Exception:
        pass

    # If all methods fail, raise an error
    raise RuntimeError("Could not find static directory")


@app.post("/play")
def play_audio(filename: str = Form(...)):
    """
    Play audio directly from the server using the AudioPlayer.
    Expects a filename that exists in the OUTPUT_FOLDER.
    """
    global audio_player

    if audio_player is None:
        return JSONResponse({"error": "Audio player not initialized"}, status_code=500)

    file_path = os.path.join(OUTPUT_FOLDER, filename)
    if not os.path.exists(file_path):
        return JSONResponse({"error": "File not found"}, status_code=404)

    try:
        # Load the audio file
        audio_data, sample_rate = sf.read(file_path)

        # If audio is stereo, convert to mono
        if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:
            audio_data = audio_data.mean(axis=1)

        # Queue the audio for playback
        audio_player.queue_audio(audio_data)

        return {"status": "playing", "filename": filename}
    except Exception as e:
        return JSONResponse(
            {"error": f"Failed to play audio: {str(e)}"}, status_code=500
        )


@app.post("/stop")
def stop_audio():
    """
    Stop any currently playing audio.
    """
    global audio_player

    if audio_player is None:
        return JSONResponse({"error": "Audio player not initialized"}, status_code=500)

    try:
        audio_player.stop()
        return {"status": "stopped"}
    except Exception as e:
        return JSONResponse(
            {"error": f"Failed to stop audio: {str(e)}"}, status_code=500
        )


@app.post("/open_output_folder")
def open_output_folder():
    """
    Open the output folder in the system file explorer (Finder on macOS).
    This only works when running on localhost for security reasons.
    """
    global OUTPUT_FOLDER

    # Check if the request is coming from localhost
    # Note: In a production environment, you would want to check the request IP

    try:
        # For macOS (Finder)
        if sys.platform == "darwin":
            os.system(f"open {OUTPUT_FOLDER}")
        # For Windows (Explorer)
        elif sys.platform == "win32":
            os.system(f"explorer {OUTPUT_FOLDER}")
        # For Linux (various file managers)
        elif sys.platform == "linux":
            os.system(f"xdg-open {OUTPUT_FOLDER}")
        else:
            return JSONResponse(
                {"error": f"Unsupported platform: {sys.platform}"}, status_code=500
            )

        logger.debug(f"Opened output folder: {OUTPUT_FOLDER}")
        return {"status": "opened", "path": OUTPUT_FOLDER}
    except Exception as e:
        logger.error(f"Error opening output folder: {str(e)}")
        return JSONResponse(
            {"error": f"Failed to open output folder: {str(e)}"}, status_code=500
        )


def setup_server():
    """Setup the server by loading the model and creating the output directory."""
    global tts_model, audio_player, OUTPUT_FOLDER

    # Make sure the output folder for generated TTS files exists
    try:
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        # Test write permissions by creating a test file
        test_file = os.path.join(OUTPUT_FOLDER, "test_write.txt")
        with open(test_file, "w") as f:
            f.write("Test write permissions")
        os.remove(test_file)
        logger.debug(f"Output directory {OUTPUT_FOLDER} is writable")
    except Exception as e:
        logger.error(f"Error with output directory {OUTPUT_FOLDER}: {str(e)}")
        # Try to use a fallback directory in /tmp
        fallback_dir = os.path.join("/tmp", "mlx_audio_outputs")
        logger.debug(f"Trying fallback directory: {fallback_dir}")
        try:
            os.makedirs(fallback_dir, exist_ok=True)
            OUTPUT_FOLDER = fallback_dir
            logger.debug(f"Using fallback output directory: {OUTPUT_FOLDER}")
        except Exception as fallback_error:
            logger.error(f"Error with fallback directory: {str(fallback_error)}")

    # Load the model if not already loaded
    if tts_model is None:
        try:
            default_model = (
                "mlx-community/Kokoro-82M-4bit"  # Same default as in tts_endpoint
            )
            logger.debug(f"Loading TTS model from {default_model}")
            tts_model = load_model(default_model)
            logger.debug("TTS model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading TTS model: {str(e)}")
            raise

    # Initialize the audio player if not already initialized
    if audio_player is None:
        try:
            logger.debug("Initializing audio player")
            audio_player = AudioPlayer()
            logger.debug("Audio player initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing audio player: {str(e)}")

    # Try to mount the static files directory
    try:
        static_dir = find_static_dir()
        logger.debug(f"Found static directory: {static_dir}")
        app.mount("/static", StaticFiles(directory=static_dir), name="static")
        logger.debug("Static files mounted successfully")
    except Exception as e:
        logger.error(f"Could not mount static files directory: {e}")
        logger.warning(
            "The server will still function, but the web interface may be limited."
        )


def main(host="127.0.0.1", port=8000, verbose=False):
    """Parse command line arguments for the server and start it."""
    parser = argparse.ArgumentParser(description="Start the MLX-Audio TTS server")
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="Host address to bind the server to (default: 127.0.0.1)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port to bind the server to (default: 8000)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging with detailed debug information",
    )
    args = parser.parse_args()

    # Update logger with verbose setting
    global logger
    logger = setup_logging(args.verbose)

    # Start the server with the parsed arguments
    setup_server()
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="debug" if args.verbose else "info",
    )


if __name__ == "__main__":
    main()



================================================
FILE: mlx_audio/utils.py
================================================
import math
from functools import lru_cache
from typing import Optional

import mlx.core as mx

# Common window functions


@lru_cache(maxsize=None)
def hanning(size):
    return mx.array(
        [0.5 * (1 - math.cos(2 * math.pi * n / (size - 1))) for n in range(size)]
    )


@lru_cache(maxsize=None)
def hamming(size):
    return mx.array(
        [0.54 - 0.46 * math.cos(2 * math.pi * n / (size - 1)) for n in range(size)]
    )


@lru_cache(maxsize=None)
def blackman(size):
    return mx.array(
        [
            0.42
            - 0.5 * math.cos(2 * math.pi * n / (size - 1))
            + 0.08 * math.cos(4 * math.pi * n / (size - 1))
            for n in range(size)
        ]
    )


@lru_cache(maxsize=None)
def bartlett(size):
    return mx.array([1 - 2 * abs(n - (size - 1) / 2) / (size - 1) for n in range(size)])


STR_TO_WINDOW_FN = {
    "hann": hanning,
    "hanning": hanning,
    "hamming": hamming,
    "blackman": blackman,
    "bartlett": bartlett,
}

# STFT and ISTFT


def stft(
    x,
    n_fft=800,
    hop_length=None,
    win_length=None,
    window: mx.array | str = "hann",
    center=True,
    pad_mode="reflect",
):
    if hop_length is None:
        hop_length = n_fft // 4
    if win_length is None:
        win_length = n_fft

    if isinstance(window, str):
        window_fn = STR_TO_WINDOW_FN.get(window.lower())
        if window_fn is None:
            raise ValueError(f"Unknown window function: {window}")
        w = window_fn(win_length)
    else:
        w = window

    if w.shape[0] < n_fft:
        pad_size = n_fft - w.shape[0]
        w = mx.concatenate([w, mx.zeros((pad_size,))], axis=0)

    def _pad(x, padding, pad_mode="reflect"):
        if pad_mode == "constant":
            return mx.pad(x, [(padding, padding)])
        elif pad_mode == "reflect":
            prefix = x[1 : padding + 1][::-1]
            suffix = x[-(padding + 1) : -1][::-1]
            return mx.concatenate([prefix, x, suffix])
        else:
            raise ValueError(f"Invalid pad_mode {pad_mode}")

    if center:
        x = _pad(x, n_fft // 2, pad_mode)

    num_frames = 1 + (x.shape[0] - n_fft) // hop_length
    if num_frames <= 0:
        raise ValueError(
            f"Input is too short (length={x.shape[0]}) for n_fft={n_fft} with "
            f"hop_length={hop_length} and center={center}."
        )

    shape = (num_frames, n_fft)
    strides = (hop_length, 1)
    frames = mx.as_strided(x, shape=shape, strides=strides)
    return mx.fft.rfft(frames * w)


def istft(
    x,
    hop_length=None,
    win_length=None,
    window="hann",
    center=True,
    length=None,
):
    if win_length is None:
        win_length = (x.shape[1] - 1) * 2
    if hop_length is None:
        hop_length = win_length // 4

    if isinstance(window, str):
        window_fn = STR_TO_WINDOW_FN.get(window.lower())
        if window_fn is None:
            raise ValueError(f"Unknown window function: {window}")
        w = window_fn(win_length + 1)[:-1]
    else:
        w = window

    if w.shape[0] < win_length:
        w = mx.concatenate([w, mx.zeros((win_length - w.shape[0],))], axis=0)

    num_frames = x.shape[1]
    t = (num_frames - 1) * hop_length + win_length

    reconstructed = mx.zeros(t)
    window_sum = mx.zeros(t)

    # inverse FFT of each frame
    frames_time = mx.fft.irfft(x, axis=0).transpose(1, 0)

    # get the position in the time-domain signal to add the frame
    frame_offsets = mx.arange(num_frames) * hop_length
    indices = frame_offsets[:, None] + mx.arange(win_length)
    indices_flat = indices.flatten()

    updates_reconstructed = (frames_time * w).flatten()
    updates_window = mx.tile(w, (num_frames,)).flatten()

    # overlap-add the inverse transformed frame, scaled by the window
    reconstructed = reconstructed.at[indices_flat].add(updates_reconstructed)
    window_sum = window_sum.at[indices_flat].add(updates_window)

    # normalize by the sum of the window values
    reconstructed = mx.where(window_sum != 0, reconstructed / window_sum, reconstructed)

    if center and length is None:
        reconstructed = reconstructed[win_length // 2 : -win_length // 2]

    if length is not None:
        reconstructed = reconstructed[:length]

    return reconstructed


# Mel filterbank


@lru_cache(maxsize=None)
def mel_filters(
    sample_rate: int,
    n_fft: int,
    n_mels: int,
    f_min: float = 0,
    f_max: Optional[float] = None,
    norm: Optional[str] = None,
    mel_scale: str = "htk",
) -> mx.array:
    def hz_to_mel(freq, mel_scale="htk"):
        if mel_scale == "htk":
            return 2595.0 * math.log10(1.0 + freq / 700.0)

        # slaney scale
        f_min, f_sp = 0.0, 200.0 / 3
        mels = (freq - f_min) / f_sp
        min_log_hz = 1000.0
        min_log_mel = (min_log_hz - f_min) / f_sp
        logstep = math.log(6.4) / 27.0
        if freq >= min_log_hz:
            mels = min_log_mel + math.log(freq / min_log_hz) / logstep
        return mels

    def mel_to_hz(mels, mel_scale="htk"):
        if mel_scale == "htk":
            return 700.0 * (10.0 ** (mels / 2595.0) - 1.0)

        # slaney scale
        f_min, f_sp = 0.0, 200.0 / 3
        freqs = f_min + f_sp * mels
        min_log_hz = 1000.0
        min_log_mel = (min_log_hz - f_min) / f_sp
        logstep = math.log(6.4) / 27.0
        freqs = mx.where(
            mels >= min_log_mel,
            min_log_hz * mx.exp(logstep * (mels - min_log_mel)),
            freqs,
        )
        return freqs

    f_max = f_max or sample_rate / 2

    # generate frequency points

    n_freqs = n_fft // 2 + 1
    all_freqs = mx.linspace(0, sample_rate // 2, n_freqs)

    # convert frequencies to mel and back to hz

    m_min = hz_to_mel(f_min, mel_scale)
    m_max = hz_to_mel(f_max, mel_scale)
    m_pts = mx.linspace(m_min, m_max, n_mels + 2)
    f_pts = mel_to_hz(m_pts, mel_scale)

    # compute slopes for filterbank

    f_diff = f_pts[1:] - f_pts[:-1]
    slopes = mx.expand_dims(f_pts, 0) - mx.expand_dims(all_freqs, 1)

    # calculate overlapping triangular filters

    down_slopes = (-slopes[:, :-2]) / f_diff[:-1]
    up_slopes = slopes[:, 2:] / f_diff[1:]
    filterbank = mx.maximum(
        mx.zeros_like(down_slopes), mx.minimum(down_slopes, up_slopes)
    )

    if norm == "slaney":
        enorm = 2.0 / (f_pts[2 : n_mels + 2] - f_pts[:n_mels])
        filterbank *= mx.expand_dims(enorm, 0)

    filterbank = filterbank.moveaxis(0, 1)
    return filterbank



================================================
FILE: mlx_audio/version.py
================================================
__version__ = "0.2.3"



================================================
FILE: mlx_audio/codec/__init__.py
================================================
from .models import DAC, Encodec, Mimi, Vocos



================================================
FILE: mlx_audio/codec/models/__init__.py
================================================
from .descript import DAC
from .encodec import Encodec
from .mimi import Mimi
from .snac import SNAC
from .vocos import Vocos



================================================
FILE: mlx_audio/codec/models/bigvgan/__init__.py
================================================
from .bigvgan import BigVGAN, BigVGANConfig



================================================
FILE: mlx_audio/codec/models/bigvgan/activation.py
================================================
import mlx.core as mx
import mlx.nn as nn


class Snake(nn.Module):
    def __init__(
        self, in_features: int, alpha: float = 1.0, alpha_logscale: bool = False
    ):
        super().__init__()

        self.alpha_logscale = alpha_logscale

        self.alpha = (
            mx.zeros(in_features) if alpha_logscale else mx.ones(in_features)
        ) * alpha

    def __call__(self, x: mx.array):
        alpha = self.alpha[None, :, None]
        if self.alpha_logscale:
            alpha = mx.exp(alpha)

        x += (1.0 / (alpha + 1e-9)) * mx.power(mx.sin(x * alpha), 2)

        return x


class SnakeBeta(nn.Module):
    def __init__(
        self, in_features: int, alpha: float = 1.0, alpha_logscale: bool = False
    ):
        super().__init__()

        self.alpha_logscale = alpha_logscale

        self.alpha = (
            mx.zeros(in_features) if alpha_logscale else mx.ones(in_features)
        ) * alpha
        self.beta = (
            mx.zeros(in_features) if alpha_logscale else mx.ones(in_features)
        ) * alpha

    def __call__(self, x: mx.array):
        alpha = self.alpha[None, None, :]
        beta = self.beta[None, None, :]
        if self.alpha_logscale:
            alpha = mx.exp(alpha)
            beta = mx.exp(beta)

        x += (1.0 / (beta + 1e-9)) * mx.power(mx.sin(x * alpha), 2)

        return x



================================================
FILE: mlx_audio/codec/models/bigvgan/amp.py
================================================
import mlx.core as mx
import mlx.nn as nn
from typing_extensions import Literal

from mlx_audio.codec.models.bigvgan.activation import Snake, SnakeBeta
from mlx_audio.codec.models.bigvgan.conv import WNConv1d
from mlx_audio.codec.models.bigvgan.resample import Activation1d


class AMPBlock1(nn.Module):
    def __init__(
        self,
        channels: int,
        snake_logscale: bool,
        activation: Literal["snake", "snakebeta"],
        kernel_size=3,
        dilation: list[int] = [1, 3, 5],
    ):
        super().__init__()

        self.convs1 = [
            WNConv1d(
                channels,
                channels,
                kernel_size,
                stride=1,
                dilation=d,
                padding=((kernel_size - 1) * d) // 2,
            )
            for d in dilation
        ]
        self.convs2 = [
            WNConv1d(
                channels,
                channels,
                kernel_size,
                stride=1,
                dilation=1,
                padding=(kernel_size - 1) // 2,
            )
            for _ in dilation
        ]
        self.activations = [
            Activation1d(
                Snake(channels, alpha_logscale=snake_logscale)
                if activation == "snake"
                else SnakeBeta(channels, alpha_logscale=snake_logscale)
            )
            for _ in range(len(dilation) * 2)
        ]

    def __call__(self, x: mx.array):
        for conv1, conv2, activation1, activation2 in zip(
            self.convs1, self.convs2, self.activations[::2], self.activations[1::2]
        ):
            x = x + conv2(activation2(conv1(activation1(x))))

        return x


class AMPBlock2(nn.Module):
    def __init__(
        self,
        channels: int,
        snake_logscale: bool,
        activation: Literal["snake", "snakebeta"],
        kernel_size=3,
        dilation: list[int] = [1, 3, 5],
    ):
        super().__init__()

        self.convs = [
            WNConv1d(
                channels,
                channels,
                kernel_size,
                stride=1,
                dilation=d,
                padding=((kernel_size - 1) * d) // 2,
            )
            for d in dilation
        ]
        self.activations = [
            Activation1d(
                Snake(channels, alpha_logscale=snake_logscale)
                if activation == "snake"
                else SnakeBeta(channels, alpha_logscale=snake_logscale)
            )
            for _ in dilation
        ]

    def __call__(self, x: mx.array):
        for conv, activation in zip(self.convs, self.activations):
            x = x + conv(activation(x))

        return x



================================================
FILE: mlx_audio/codec/models/bigvgan/bigvgan.py
================================================
from dataclasses import dataclass
from typing import Literal

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten

from mlx_audio.codec.models.bigvgan.activation import Snake, SnakeBeta
from mlx_audio.codec.models.bigvgan.amp import AMPBlock1, AMPBlock2
from mlx_audio.codec.models.bigvgan.conv import WNConv1d, WNConvTranspose1d
from mlx_audio.codec.models.bigvgan.resample import Activation1d


@dataclass
class BigVGANConfig:
    num_mels: int
    upsample_rates: list[int]
    upsample_kernel_sizes: list[int]
    upsample_initial_channel: int
    resblock: Literal["1", "2"]
    resblock_kernel_sizes: list[int]
    resblock_dilation_sizes: list[list[int]]
    activation: Literal["snakebeta", "snake"]
    snake_logscale: bool
    use_bias_at_final: bool = True  # compatability
    use_tanh_at_final: bool = True  # compatability


class BigVGAN(nn.Module):
    def __init__(self, config: BigVGANConfig):
        super().__init__()

        self.num_kernels = len(config.resblock_kernel_sizes)
        self.num_upsamples = len(config.upsample_rates)
        self.use_tanh_at_final = config.use_tanh_at_final

        self.conv_pre = WNConv1d(
            config.num_mels, config.upsample_initial_channel, 7, 1, 3
        )
        self.ups = [
            [
                WNConvTranspose1d(
                    config.upsample_initial_channel // (2**i),
                    config.upsample_initial_channel // (2 ** (i + 1)),
                    k,
                    u,
                    padding=(k - u) // 2,
                )
            ]
            for i, (u, k) in enumerate(
                zip(config.upsample_rates, config.upsample_kernel_sizes)
            )
        ]
        self.resblocks = [
            (
                AMPBlock1(
                    config.upsample_initial_channel // (2 ** (i + 1)),
                    config.snake_logscale,
                    config.activation,
                    k,
                    d,
                )
                if config.resblock == "1"
                else AMPBlock2(
                    config.upsample_initial_channel // (2 ** (i + 1)),
                    config.snake_logscale,
                    config.activation,
                    k,
                    d,
                )
            )
            for i in range(len(self.ups))
            for j, (k, d) in enumerate(
                zip(config.resblock_kernel_sizes, config.resblock_dilation_sizes)
            )
        ]
        self.activation_post = Activation1d(
            Snake(
                config.upsample_initial_channel // (2 ** len(self.ups)),
                alpha_logscale=config.snake_logscale,
            )
            if config.activation == "snake"
            else SnakeBeta(
                config.upsample_initial_channel // (2 ** len(self.ups)),
                alpha_logscale=config.snake_logscale,
            )
        )
        self.conv_post = WNConv1d(
            config.upsample_initial_channel // (2 ** len(self.ups)),
            1,
            7,
            1,
            padding=3,
            bias=config.use_bias_at_final,
        )

    def __call__(
        self, x: mx.array, *args, **kwargs
    ) -> mx.array:  # (batch, num_mels, seq)
        x = x.transpose(0, 2, 1)

        x = self.conv_pre(x)

        for step in range(self.num_upsamples):
            for idx in range(len(self.ups[step])):
                x = self.ups[step][idx](x)

            xs = self.resblocks[step * self.num_kernels](x)
            for idx in range(1, self.num_kernels):
                xs += self.resblocks[step * self.num_kernels + idx](x)

            x = xs / self.num_kernels

        x = self.activation_post(x)
        x = self.conv_post(x)

        if self.use_tanh_at_final:
            x = mx.tanh(x)
        else:
            x = mx.clip(x, -1.0, 1.0)

        return x.transpose(0, 2, 1)

    def sanitize(self, weights: dict[str, mx.array]):
        new_weights = {}
        curr_weights = dict(tree_flatten(self.parameters()))

        for key, value in weights.items():
            if "num_batches_tracked" in key:
                continue

            if "conv" in key or "lowpass.filter" in key or "upsample.filter" in key:
                if value.ndim == 3:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(0, 2, 1)
                elif value.ndim == 4:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(0, 2, 3, 1)

            if "ups." in key:
                if value.ndim == 3:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(1, 2, 0)

            new_weights[key] = value

        del curr_weights

        return new_weights



================================================
FILE: mlx_audio/codec/models/bigvgan/conv.py
================================================
import math

import mlx.core as mx
import mlx.nn as nn


def normalize_weight(x, except_dim=0):
    if x.ndim != 3:
        raise ValueError("Input tensor must have 3 dimensions")

    axes = tuple(i for i in range(x.ndim) if i != except_dim)
    return mx.sqrt(mx.sum(mx.power(x, 2), axis=axes, keepdims=True))


class WNConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        if bias:
            self.bias = mx.zeros((out_channels,))

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels),
        )
        self.weight_g = normalize_weight(weight_init)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"in_channels={self.weight_v.shape[2]}, out_channels={self.weight_v.shape[0]}, "
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = self.weight_g * self.weight_v / normalize_weight(self.weight_v)
        y = mx.conv1d(x, weight, self.stride, self.padding, self.dilation, self.groups)
        if "bias" in self:
            y = y + self.bias
        return y


class WNConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        output_padding: int = 0,
        bias: bool = True,
    ):
        super().__init__()

        self.bias = mx.zeros((out_channels,)) if bias else None

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.output_padding = output_padding

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels),
        )
        self.weight_g = normalize_weight(weight_init, except_dim=2)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"in_channels={self.weight_v.shape[2]}, out_channels={self.weight_v.shape[0]}, "
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"output_padding={self.output_padding}, bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = (
            self.weight_g
            * self.weight_v
            / normalize_weight(self.weight_v, except_dim=2)
        )
        y = mx.conv_transpose1d(
            x, weight, self.stride, self.padding, self.dilation, self.output_padding
        )
        nn.ConvTranspose1d
        if self.bias is not None:
            y = y + self.bias
        return y



================================================
FILE: mlx_audio/codec/models/bigvgan/resample.py
================================================
import math
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np


def sinc(x: mx.array):
    return mx.where(
        x == 0,
        mx.array(1.0, dtype=x.dtype),
        mx.sin(math.pi * x) / math.pi / x,
    )


def kaiser_sinc_filter1d(
    cutoff: float, half_width: float, kernel_size: int
) -> mx.array:  # return filter [1,kernel_size,1]
    even = kernel_size % 2 == 0
    half_size = kernel_size // 2

    # For kaiser window
    delta_f = 4 * half_width
    A = 2.285 * (half_size - 1) * math.pi * delta_f + 7.95
    if A > 50.0:
        beta = 0.1102 * (A - 8.7)
    elif A >= 21.0:
        beta = 0.5842 * (A - 21) ** 0.4 + 0.07886 * (A - 21.0)
    else:
        beta = 0.0
    window = mx.array(np.kaiser(kernel_size, beta=beta))

    # ratio = 0.5/cutoff -> 2 * cutoff = 1 / ratio
    if even:
        time = mx.arange(-half_size, half_size) + 0.5
    else:
        time = mx.arange(kernel_size) - half_size
    if cutoff == 0:
        filter = mx.zeros_like(time).reshape(1, kernel_size, 1)
    else:
        filter_ = 2 * cutoff * window * sinc(2 * cutoff * time)
        filter_ /= filter_.sum()
        filter = filter_.reshape(1, kernel_size, 1)

    return filter


class LowPassFilter1d(nn.Module):
    def __init__(
        self,
        cutoff: float = 0.5,
        half_width: float = 0.6,
        stride: int = 1,
        padding: bool = True,
        padding_mode: str = "edge",
        kernel_size: int = 12,
    ):
        super().__init__()

        if cutoff < -0.0:
            raise ValueError("Minimum cutoff must be larger than zero.")
        if cutoff > 0.5:
            raise ValueError("A cutoff above 0.5 does not make sense.")

        self.even = kernel_size % 2 == 0
        self.stride = stride

        self.pad_left = kernel_size // 2 - int(self.even)
        self.pad_right = kernel_size // 2
        self.padding = padding
        self.padding_mode = padding_mode

        self.filter = kaiser_sinc_filter1d(
            cutoff, half_width, kernel_size
        )  # (1, kernel_size, 1)
        mx.eval(self.filter)

    def __call__(self, x: mx.array):  # (b, t, c)
        _, _, C = x.shape

        if self.padding:
            x = mx.pad(
                x,
                ((0, 0), (self.pad_left, self.pad_right), (0, 0)),
                mode=self.padding_mode,
            )

        expanded_filter = mx.broadcast_to(self.filter, (C, *self.filter.shape[1:]))

        out = mx.conv1d(
            x,
            expanded_filter,
            stride=self.stride,
            groups=C,
        )

        return out


class UpSample1d(nn.Module):
    def __init__(self, ratio: int = 2, kernel_size: Optional[int] = None):
        super().__init__()

        self.ratio = ratio
        self.kernel_size = (
            int(6 * ratio // 2) * 2 if kernel_size is None else kernel_size
        )
        self.stride = ratio

        self.pad = self.kernel_size // ratio - 1
        self.pad_left = self.pad * self.stride + (self.kernel_size - self.stride) // 2
        self.pad_right = (
            self.pad * self.stride + (self.kernel_size - self.stride + 1) // 2
        )

        self.filter = kaiser_sinc_filter1d(
            cutoff=0.5 / ratio, half_width=0.6 / ratio, kernel_size=self.kernel_size
        )
        mx.eval(self.filter)

    def __call__(self, x: mx.array) -> mx.array:  # (b, t, c)
        _, _, C = x.shape

        x = mx.pad(x, ((0, 0), (self.pad, self.pad), (0, 0)), mode="edge")

        expanded_filter = mx.broadcast_to(self.filter, (C, *self.filter.shape[1:]))

        x = self.ratio * mx.conv_transpose1d(
            x,
            expanded_filter,
            stride=self.stride,
            groups=C,
        )

        return x[:, self.pad_left : -self.pad_right, :]


class DownSample1d(nn.Module):
    def __init__(self, ratio: int = 2, kernel_size: Optional[int] = None):
        super().__init__()
        self.ratio = ratio
        self.kernel_size = (
            int(6 * ratio // 2) * 2 if kernel_size is None else kernel_size
        )
        self.lowpass = LowPassFilter1d(
            cutoff=0.5 / ratio,
            half_width=0.6 / ratio,
            stride=ratio,
            kernel_size=self.kernel_size,
        )

    def __call__(self, x: mx.array) -> mx.array:  # (b, t, c)
        return self.lowpass(x)


class Activation1d(nn.Module):
    def __init__(
        self,
        activation: nn.Module,
        up_ratio: int = 2,
        down_ratio: int = 2,
        up_kernel_size: int = 12,
        down_kernel_size: int = 12,
    ):
        super().__init__()
        self.up_ratio = up_ratio
        self.down_ratio = down_ratio
        self.act = activation
        self.upsample = UpSample1d(up_ratio, up_kernel_size)
        self.downsample = DownSample1d(down_ratio, down_kernel_size)

    def __call__(self, x):
        x = self.upsample(x)
        x = self.act(x)
        x = self.downsample(x)
        return x



================================================
FILE: mlx_audio/codec/models/descript/__init__.py
================================================
from .dac import DAC



================================================
FILE: mlx_audio/codec/models/descript/base.py
================================================
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import soundfile as sf
from einops.array_api import rearrange

SUPPORTED_VERSIONS = ["1.0.0"]


@dataclass
class DACFile:
    codes: mx.array

    # Metadata
    chunk_length: int
    original_length: int
    input_db: float
    channels: int
    sample_rate: int
    padding: bool
    dac_version: str

    def save(self, path):
        artifacts = {
            "codes": np.array(self.codes).astype(np.uint16),
            "metadata": {
                "input_db": self.input_db,
                "original_length": self.original_length,
                "sample_rate": self.sample_rate,
                "chunk_length": self.chunk_length,
                "channels": self.channels,
                "padding": self.padding,
                "dac_version": SUPPORTED_VERSIONS[-1],
            },
        }
        path = Path(path).with_suffix(".dac")
        with open(path, "wb") as f:
            np.save(f, artifacts)
        return path

    @classmethod
    def load(cls, path):
        artifacts = np.load(path, allow_pickle=True)[()]
        codes = mx.array(artifacts["codes"], dtype=mx.int32)
        if artifacts["metadata"].get("dac_version", None) not in SUPPORTED_VERSIONS:
            raise RuntimeError(
                f"Given file {path} can't be loaded with this version of descript-audio-codec."
            )
        return cls(codes=codes, **artifacts["metadata"])


class CodecMixin:
    @property
    def padding(self):
        if not hasattr(self, "_padding"):
            self._padding = True
        return self._padding

    @padding.setter
    def padding(self, value):
        assert isinstance(value, bool)

        layers = [
            layer
            for layer in self.modules()
            if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d))
        ]

        for layer in layers:
            if value:
                if hasattr(layer, "original_padding"):
                    layer.padding = layer.original_padding
            else:
                layer.original_padding = layer.padding
                layer.padding = tuple(0 for _ in range(len(layer.padding)))

        self._padding = value

    def get_delay(self):
        l_out = self.get_output_length(0)
        L = l_out

        layers = []
        for layer in self.modules():
            if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):
                layers.append(layer)

        for layer in reversed(layers):
            d = layer.dilation
            k = layer.weight.shape[1]
            s = layer.stride

            if isinstance(layer, nn.ConvTranspose1d):
                L = ((L - d * (k - 1) - 1) / s) + 1
            elif isinstance(layer, nn.Conv1d):
                L = (L - 1) * s + d * (k - 1) + 1

            L = math.ceil(L)

        l_in = L

        return (l_in - l_out) // 2

    def get_output_length(self, input_length):
        L = input_length
        for layer in self.modules():
            if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):
                d = layer.dilation
                k = layer.weight.shape[1]
                s = layer.stride

                if isinstance(layer, nn.Conv1d):
                    L = ((L - d * (k - 1) - 1) / s) + 1
                elif isinstance(layer, nn.ConvTranspose1d):
                    L = (L - 1) * s + d * (k - 1) + 1

                L = math.floor(L)
        return L

    def compress(
        self,
        audio_path: Union[str, Path],
        win_duration: float = 1.0,
        normalize_db: float = -16,
        n_quantizers: int = None,
    ) -> DACFile:
        audio_signal, original_sr = sf.read(audio_path)
        signal_duration = audio_signal.shape[-1] / original_sr

        original_padding = self.padding
        if original_sr != self.sample_rate:
            raise ValueError(
                f"Sample rate of the audio signal ({original_sr}) does not match the sample rate of the model ({self.sample_rate})."
            )

        audio_data = mx.array(audio_signal)

        rms = mx.sqrt(mx.mean(mx.power(audio_data, 2), axis=-1) + 1e-12)
        input_db = 20 * mx.log10(rms / 1.0 + 1e-12)

        if normalize_db is not None:
            audio_data = audio_data * mx.power(10, (normalize_db - input_db) / 20)

        audio_data = rearrange(audio_data, "n -> 1 1 n")
        nb, nac, nt = audio_data.shape
        audio_data = rearrange(audio_data, "nb nac nt -> (nb nac) 1 nt")

        win_duration = signal_duration if win_duration is None else win_duration

        if signal_duration <= win_duration:
            self.padding = True
            n_samples = nt
            hop = nt
        else:
            self.padding = False
            audio_data = mx.pad(audio_data, [(0, 0), (0, 0), (self.delay, self.delay)])

            n_samples = int(win_duration * self.sample_rate)
            n_samples = int(math.ceil(n_samples / self.hop_length) * self.hop_length)
            hop = self.get_output_length(n_samples)

        codes = []
        for i in range(0, nt, hop):
            x = audio_data[..., i : i + n_samples]
            x = mx.pad(x, [(0, 0), (0, 0), (0, max(0, n_samples - x.shape[-1]))])

            x = self.preprocess(x, self.sample_rate)
            _, c, _, _, _ = self.encode(x, n_quantizers)
            codes.append(c)
            chunk_length = c.shape[-1]

        codes = mx.concatenate(codes, axis=-1)

        dac_file = DACFile(
            codes=codes,
            chunk_length=chunk_length,
            original_length=signal_duration,
            input_db=input_db,
            channels=nac,
            sample_rate=original_sr,
            padding=self.padding,
            dac_version=SUPPORTED_VERSIONS[-1],
        )

        if n_quantizers is not None:
            codes = codes[:, :n_quantizers, :]

        self.padding = original_padding
        return dac_file

    def decompress(self, obj: Union[str, Path, DACFile]) -> mx.array:
        if isinstance(obj, (str, Path)):
            obj = DACFile.load(obj)

        if self.sample_rate != obj.sample_rate:
            raise ValueError(
                f"Sample rate of the audio signal ({obj.sample_rate}) does not match the sample rate of the model ({self.sample_rate})."
            )

        original_padding = self.padding
        self.padding = obj.padding

        codes = obj.codes
        chunk_length = obj.chunk_length
        recons = []

        for i in range(0, codes.shape[-1], chunk_length):
            c = codes[..., i : i + chunk_length]
            z = self.quantizer.from_codes(c)[0]
            r = self.decode(z)
            recons.append(r)

        recons = mx.concatenate(recons, axis=1)
        recons = rearrange(recons, "1 n 1 -> 1 n")

        target_db = obj.input_db
        normalize_db = -16

        if normalize_db is not None:
            recons = recons * mx.power(10, (target_db - normalize_db) / 20)

        self.padding = original_padding
        return recons



================================================
FILE: mlx_audio/codec/models/descript/dac.py
================================================
import json
import math
from pathlib import Path
from typing import List, Literal, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from huggingface_hub import snapshot_download

from .base import CodecMixin
from .nn.layers import Snake1d, WNConv1d, WNConvTranspose1d
from .nn.quantize import ResidualVectorQuantize


class ResidualUnit(nn.Module):
    def __init__(self, dim: int = 16, dilation: int = 1):
        super().__init__()
        pad = ((7 - 1) * dilation) // 2

        self.block = nn.Sequential(
            Snake1d(dim),
            WNConv1d(dim, dim, kernel_size=7, dilation=dilation, padding=pad),
            Snake1d(dim),
            WNConv1d(dim, dim, kernel_size=1),
        )

    def __call__(self, x):
        y = self.block(x)
        pad = (x.shape[-1] - y.shape[-1]) // 2
        if pad > 0:
            x = x[..., pad:-pad]
        return x + y


class EncoderBlock(nn.Module):
    def __init__(self, dim: int = 16, stride: int = 1):
        super().__init__()
        self.block = nn.Sequential(
            ResidualUnit(dim // 2, dilation=1),
            ResidualUnit(dim // 2, dilation=3),
            ResidualUnit(dim // 2, dilation=9),
            Snake1d(dim // 2),
            WNConv1d(
                dim // 2,
                dim,
                kernel_size=2 * stride,
                stride=stride,
                padding=math.ceil(stride / 2),
            ),
        )

    def __call__(self, x):
        return self.block(x)


class Encoder(nn.Module):
    def __init__(
        self,
        d_model: int = 64,
        strides: list = [2, 4, 8, 8],
        d_latent: int = 64,
    ):
        super().__init__()
        self.block = [WNConv1d(1, d_model, kernel_size=7, padding=3)]

        for stride in strides:
            d_model *= 2
            self.block += [EncoderBlock(d_model, stride=stride)]

        self.block += [
            Snake1d(d_model),
            WNConv1d(d_model, d_latent, kernel_size=3, padding=1),
        ]

        self.block = nn.Sequential(*self.block)
        self.enc_dim = d_model

    def __call__(self, x):
        return self.block(x).moveaxis(1, 2)


class DecoderBlock(nn.Module):
    def __init__(self, input_dim: int = 16, output_dim: int = 8, stride: int = 1):
        super().__init__()
        self.block = nn.Sequential(
            Snake1d(input_dim),
            WNConvTranspose1d(
                input_dim,
                output_dim,
                kernel_size=2 * stride,
                stride=stride,
                padding=math.ceil(stride / 2),
            ),
            ResidualUnit(output_dim, dilation=1),
            ResidualUnit(output_dim, dilation=3),
            ResidualUnit(output_dim, dilation=9),
        )

    def __call__(self, x):
        return self.block(x)


class Decoder(nn.Module):
    def __init__(
        self,
        input_channel,
        channels,
        rates,
        d_out: int = 1,
    ):
        super().__init__()
        layers = [WNConv1d(input_channel, channels, kernel_size=7, padding=3)]

        for i, stride in enumerate(rates):
            input_dim = channels // 2**i
            output_dim = channels // 2 ** (i + 1)
            layers += [DecoderBlock(input_dim, output_dim, stride)]

        layers += [
            Snake1d(output_dim),
            WNConv1d(output_dim, d_out, kernel_size=7, padding=3),
            nn.Tanh(),
        ]

        self.model = nn.Sequential(*layers)

    def __call__(self, x):
        return self.model(x)


class DAC(nn.Module, CodecMixin):
    def __init__(
        self,
        encoder_dim: int = 64,
        encoder_rates: List[int] = [2, 4, 5, 8],
        latent_dim: int = None,
        decoder_dim: int = 1536,
        decoder_rates: List[int] = [8, 5, 4, 2],
        n_codebooks: int = 32,
        codebook_size: int = 1024,
        codebook_dim: Union[int, list] = 8,
        sample_rate: int = 44100,
        **kwargs,
    ):
        super().__init__()

        self.encoder_dim = encoder_dim
        self.encoder_rates = encoder_rates
        self.decoder_dim = decoder_dim
        self.decoder_rates = decoder_rates
        self.sample_rate = sample_rate

        if latent_dim is None:
            latent_dim = encoder_dim * (2 ** len(encoder_rates))

        self.latent_dim = latent_dim

        self.hop_length = np.prod(encoder_rates)
        self.encoder = Encoder(encoder_dim, encoder_rates, latent_dim)

        self.n_codebooks = n_codebooks
        self.codebook_size = codebook_size
        self.codebook_dim = codebook_dim
        self.quantizer = ResidualVectorQuantize(
            input_dim=latent_dim,
            n_codebooks=n_codebooks,
            codebook_size=codebook_size,
            codebook_dim=codebook_dim,
        )

        self.decoder = Decoder(
            latent_dim,
            decoder_dim,
            decoder_rates,
        )

        self.sample_rate = sample_rate

        self.delay = self.get_delay()

    def preprocess(self, audio_data, sample_rate):
        if sample_rate is None:
            sample_rate = self.sample_rate
        assert sample_rate == self.sample_rate

        length = audio_data.shape[-1]
        right_pad = math.ceil(length / self.hop_length) * self.hop_length - length
        audio_data = mx.pad(audio_data, [(0, 0), (0, 0), (0, right_pad)])

        return audio_data

    def encode(
        self,
        audio_data: mx.array,
        n_quantizers: int = None,
    ):
        z = self.encoder(audio_data.moveaxis(1, 2))
        z, codes, latents, commitment_loss, codebook_loss = self.quantizer(
            z, n_quantizers
        )
        return z, codes, latents, commitment_loss, codebook_loss

    def decode(self, z: mx.array):
        return self.decoder(z.moveaxis(1, 2))

    def _extra_repr(self):
        return (
            f"encoder_dim={self.encoder_dim}, "
            f"encoder_rates={self.encoder_rates}, "
            f"latent_dim={self.latent_dim}, "
            f"decoder_dim={self.decoder_dim}, "
            f"decoder_rates={self.decoder_rates}, "
            f"n_codebooks={self.n_codebooks}, "
            f"codebook_size={self.codebook_size}, "
            f"codebook_dim={self.codebook_dim}"
        )

    def __call__(
        self,
        audio_data: mx.array,
        sample_rate: int = None,
        n_quantizers: int = None,
        use_rvq: bool = True,
        return_loss: bool = False,
    ):
        length = audio_data.shape[-1]
        audio_data = self.preprocess(audio_data, sample_rate)

        if use_rvq:
            z, codes, latents, commitment_loss, codebook_loss = self.encode(
                audio_data, n_quantizers
            )
        else:
            z = self.encoder(audio_data.moveaxis(1, 2))

        x = self.decode(z)

        if return_loss:
            return mx.losses.mse(x, audio_data)

        return {
            "audio": x[..., :length],
            "z": z,
            "codes": codes,
            "latents": latents,
            "vq/commitment_loss": commitment_loss,
            "vq/codebook_loss": codebook_loss,
        }

    @classmethod
    def from_pretrained(
        cls,
        repo_id: str,
    ) -> "DAC":
        path = fetch_from_hub(repo_id)
        if path is None:
            raise ValueError(f"Could not find model {path}")

        model_path = path / "model.safetensors"
        config_path = path / "config.json"

        with open(config_path) as f:
            config = json.load(f)

        dac = DAC(**config)

        weights = mx.load(model_path.as_posix(), format="safetensors")
        dac.load_weights(list(weights.items()))
        mx.eval(dac.parameters())

        return dac


# fetch model from hub


def fetch_from_hub(hf_repo: str) -> Path:
    model_path = Path(
        snapshot_download(
            repo_id=hf_repo,
            allow_patterns=["*.safetensors", "*.json"],
        )
    )
    return model_path



================================================
FILE: mlx_audio/codec/models/descript/nn/__init__.py
================================================
from . import layers, quantize



================================================
FILE: mlx_audio/codec/models/descript/nn/layers.py
================================================
import math

import mlx.core as mx
import mlx.nn as nn


def normalize_weight(x, except_dim=0):
    if x.ndim != 3:
        raise ValueError("Input tensor must have 3 dimensions")

    axes = tuple(i for i in range(x.ndim) if i != except_dim)
    return mx.sqrt(mx.sum(mx.power(x, 2), axis=axes, keepdims=True))


class WNConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        bias: bool = True,
        groups: int = 1,
    ):
        super().__init__()

        if bias:
            self.bias = mx.zeros((out_channels,))

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels),
        )
        self.weight_g = normalize_weight(weight_init)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"in_channels={self.weight_v.shape[2]}, out_channels={self.weight_v.shape[0]}, "
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = self.weight_g * self.weight_v / normalize_weight(self.weight_v)
        y = mx.conv1d(x, weight, self.stride, self.padding, self.dilation, self.groups)
        if "bias" in self:
            y = y + self.bias
        return y


class WNConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        self.bias = mx.zeros((out_channels,)) if bias else None

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels // groups),
        )
        self.weight_g = normalize_weight(weight_init, except_dim=2)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"in_channels={self.weight_v.shape[2] * self.groups}, out_channels={self.weight_v.shape[0]}, "
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"groups={self.groups}, bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = (
            self.weight_g
            * self.weight_v
            / normalize_weight(self.weight_v, except_dim=2)
        )
        y = mx.conv_transpose1d(
            x, weight, self.stride, self.padding, self.dilation, self.groups
        )
        if self.bias is not None:
            y = y + self.bias
        return y


def snake(x, alpha):
    recip = mx.reciprocal(alpha + 1e-9)
    x = x + recip * mx.power(mx.sin(alpha * x), 2)
    return x


class Snake1d(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.alpha = mx.ones((1, 1, channels))

    def __call__(self, x):
        x = snake(x, self.alpha)
        return x



================================================
FILE: mlx_audio/codec/models/descript/nn/quantize.py
================================================
from typing import Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from einops.array_api import rearrange

from .layers import WNConv1d


def normalize(input, p=2.0, dim=1, eps=1e-12):
    norm = mx.power(mx.sum(mx.power(mx.abs(input), p), axis=dim, keepdims=True), 1 / p)
    return input / mx.maximum(norm, eps)


class VectorQuantize(nn.Module):
    def __init__(self, input_dim: int, codebook_size: int, codebook_dim: int):
        super().__init__()
        self.codebook_size = codebook_size
        self.codebook_dim = codebook_dim

        self.in_proj = WNConv1d(input_dim, codebook_dim, kernel_size=1)
        self.out_proj = WNConv1d(codebook_dim, input_dim, kernel_size=1)
        self.codebook = nn.Embedding(codebook_size, codebook_dim)

    def __call__(self, z):
        z_e = self.in_proj(z.moveaxis(1, 2)).moveaxis(1, 2)  # z_e : (B x D x T)
        z_q, indices = self.decode_latents(z_e)

        commitment_loss = nn.losses.mse_loss(z_e, z_q, reduction="none").mean([1, 2])
        codebook_loss = nn.losses.mse_loss(z_q, z_e, reduction="none").mean([1, 2])

        z_q = z_e + (
            z_q - z_e
        )  # noop in forward pass, straight-through gradient estimator in backward pass
        z_q = self.out_proj(z_q.moveaxis(1, 2)).moveaxis(1, 2)

        return z_q, commitment_loss, codebook_loss, indices, z_e

    def embed_code(self, embed_id):
        return self.codebook.weight[embed_id]

    def decode_code(self, embed_id):
        return self.embed_code(embed_id).moveaxis(1, 2)

    def decode_latents(self, latents):
        encodings = rearrange(latents, "b d t -> (b t) d")
        codebook = self.codebook.weight  # codebook: (N x D)

        encodings = normalize(encodings)
        codebook = normalize(codebook)

        dist = (
            mx.power(encodings, 2).sum(1, keepdims=True)
            - 2 * encodings @ codebook.T
            + mx.power(codebook, 2).sum(1, keepdims=True).T
        )
        min_dist = (-dist).argmax(1)
        indices = rearrange(min_dist, "(b t) -> b t", b=latents.shape[0])
        z_q = self.decode_code(indices)
        return z_q, indices


class ResidualVectorQuantize(nn.Module):
    def __init__(
        self,
        input_dim: int = 512,
        n_codebooks: int = 9,
        codebook_size: int = 1024,
        codebook_dim: Union[int, list] = 8,
    ):
        super().__init__()
        if isinstance(codebook_dim, int):
            codebook_dim = [codebook_dim for _ in range(n_codebooks)]

        self.n_codebooks = n_codebooks
        self.codebook_dim = codebook_dim
        self.codebook_size = codebook_size

        self.quantizers = [
            VectorQuantize(input_dim, codebook_size, codebook_dim[i])
            for i in range(n_codebooks)
        ]

    def __call__(self, z, n_quantizers: int = None):
        z_q = 0
        residual = z
        commitment_loss = 0
        codebook_loss = 0

        codebook_indices = []
        latents = []

        if n_quantizers is None:
            n_quantizers = self.n_codebooks

        for i, quantizer in enumerate(self.quantizers):
            if i >= n_quantizers:
                break

            z_q_i, commitment_loss_i, codebook_loss_i, indices_i, z_e_i = quantizer(
                residual
            )

            mask = mx.full((z.shape[0],), vals=i) < n_quantizers
            z_q = z_q + z_q_i * mask[:, None, None]
            residual = residual - z_q_i

            commitment_loss += (commitment_loss_i * mask).mean()
            codebook_loss += (codebook_loss_i * mask).mean()

            codebook_indices.append(indices_i)
            latents.append(z_e_i)

        codes = mx.stack(codebook_indices, axis=1)
        latents = mx.concatenate(latents, axis=1)

        return z_q, codes, latents, commitment_loss, codebook_loss

    def from_codes(self, codes: mx.array):
        z_q = 0.0
        z_p = []
        n_codebooks = codes.shape[1]
        for i in range(n_codebooks):
            z_p_i = self.quantizers[i].decode_code(codes[:, i, :])
            z_p.append(z_p_i)
            z_q_i = self.quantizers[i].out_proj(z_p_i.moveaxis(1, 2)).moveaxis(1, 2)
            z_q = z_q + z_q_i
        return z_q, mx.concatenate(z_p, axis=1), codes

    def from_latents(self, latents: mx.array):
        z_q = 0
        z_p = []
        codes = []
        dims = np.cumsum([0] + [q.codebook_dim for q in self.quantizers])

        n_codebooks = np.where(dims <= latents.shape[1])[0].max(axis=0, keepdims=True)[
            0
        ]
        for i in range(n_codebooks):
            j, k = dims[i], dims[i + 1]
            z_p_i, codes_i = self.quantizers[i].decode_latents(latents[:, j:k, :])
            z_p.append(z_p_i)
            codes.append(codes_i)

            z_q_i = self.quantizers[i].out_proj(z_p_i.moveaxis(1, 2)).moveaxis(1, 2)
            z_q = z_q + z_q_i

        return z_q, mx.concatenate(z_p, axis=1), mx.stack(codes, axis=1)



================================================
FILE: mlx_audio/codec/models/encodec/__init__.py
================================================
from .encodec import Encodec, EncodecConfig



================================================
FILE: mlx_audio/codec/models/encodec/encodec.py
================================================
import functools
import json
import math
from dataclasses import dataclass
from pathlib import Path
from types import SimpleNamespace
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from huggingface_hub import snapshot_download


def filter_dataclass_fields(data_dict, dataclass_type):
    """Filter a dictionary to only include keys that are fields in the dataclass."""
    valid_fields = {f.name for f in dataclass_type.__dataclass_fields__.values()}
    return {k: v for k, v in data_dict.items() if k in valid_fields}


@dataclass
class EncodecConfig:
    model_type: str = "encodec"
    audio_channels: int = 1
    num_filters: int = 32
    kernel_size: int = 7
    num_residual_layers: int = 1
    dilation_growth_rate: int = 2
    codebook_size: int = 1024
    codebook_dim: int = 128
    hidden_size: int = 128
    num_lstm_layers: int = 2
    residual_kernel_size: int = 3
    use_causal_conv: bool = True
    normalize: bool = False
    pad_mode: str = "reflect"
    norm_type: str = "weight_norm"
    last_kernel_size: int = 7
    trim_right_ratio: float = 1.0
    compress: int = 2
    upsampling_ratios: List[int] = None
    target_bandwidths: List[float] = None
    sampling_rate: int = 24000
    chunk_length_s: Optional[float] = None
    overlap: Optional[float] = None
    architectures: List[str] = None


def preprocess_audio(
    raw_audio: Union[mx.array, List[mx.array]],
    sampling_rate: int = 24000,
    chunk_length: Optional[int] = None,
    chunk_stride: Optional[int] = None,
):
    r"""
    Prepare inputs for the EnCodec model.

    Args:
        raw_audio (mx.array or List[mx.array]): The sequence or batch of
            sequences to be processed.
        sampling_rate (int): The sampling rate at which the audio waveform
            should be digitalized.
        chunk_length (int, optional): The model's chunk length.
        chunk_stride (int, optional): The model's chunk stride.
    """
    if not isinstance(raw_audio, list):
        raw_audio = [raw_audio]

    raw_audio = [x[..., None] if x.ndim == 1 else x for x in raw_audio]

    max_length = max(array.shape[0] for array in raw_audio)
    if chunk_length is not None:
        max_length += chunk_length - (max_length % chunk_stride)

    inputs = []
    masks = []
    for x in raw_audio:
        length = x.shape[0]
        mask = mx.ones((length,), dtype=mx.bool_)
        difference = max_length - length
        if difference > 0:
            mask = mx.pad(mask, (0, difference))
            x = mx.pad(x, ((0, difference), (0, 0)))
        inputs.append(x)
        masks.append(mask)
    return mx.stack(inputs), mx.stack(masks)


_lstm_kernel = mx.fast.metal_kernel(
    name="lstm",
    input_names=["x", "h_in", "cell", "hidden_size", "time_step", "num_time_steps"],
    output_names=["hidden_state", "cell_state"],
    header="""
    template <typename T>
    T sigmoid(T x) {
        auto y = 1 / (1 + metal::exp(-metal::abs(x)));
        return (x < 0) ? 1 - y : y;
    }
    """,
    source="""
        uint b = thread_position_in_grid.x;
        uint d = hidden_size * 4;

        uint elem = b * d + thread_position_in_grid.y;
        uint index = elem;
        uint x_index = b * num_time_steps * d + time_step * d + index;

        auto i = sigmoid(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto f = sigmoid(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto g = metal::precise::tanh(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto o = sigmoid(h_in[index] + x[x_index]);

        cell_state[elem] = f * cell[elem] + i * g;
        hidden_state[elem] = o * metal::precise::tanh(cell_state[elem]);
    """,
)


def lstm_custom(x, h_in, cell, time_step):
    assert x.ndim == 3, "Input to LSTM must have 3 dimensions."
    out_shape = cell.shape
    return _lstm_kernel(
        inputs=[x, h_in, cell, out_shape[-1], time_step, x.shape[-2]],
        output_shapes=[out_shape, out_shape],
        output_dtypes=[h_in.dtype, h_in.dtype],
        grid=(x.shape[0], h_in.size // 4, 1),
        threadgroup=(256, 1, 1),
    )


class LSTM(nn.Module):
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        bias: bool = True,
    ):
        super().__init__()

        self.hidden_size = hidden_size
        self.Wx = mx.zeros((4 * hidden_size, input_size))
        self.Wh = mx.zeros((4 * hidden_size, hidden_size))
        self.bias = mx.zeros((4 * hidden_size,)) if bias else None

    def __call__(self, x, hidden=None, cell=None):
        if self.bias is not None:
            x = mx.addmm(self.bias, x, self.Wx.T)
        else:
            x = x @ self.Wx.T

        all_hidden = []

        B = x.shape[0]
        cell = cell or mx.zeros((B, self.hidden_size), x.dtype)
        for t in range(x.shape[-2]):
            if hidden is None:
                hidden = mx.zeros((B, self.hidden_size * 4), x.dtype)
            else:
                hidden = hidden @ self.Wh.T
            hidden, cell = lstm_custom(x, hidden, cell, t)
            all_hidden.append(hidden)

        return mx.stack(all_hidden, axis=-2)


class EncodecConv1d(nn.Module):
    """Conv1d with asymmetric or causal padding and normalization."""

    def __init__(
        self,
        config,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
    ):
        super().__init__()
        self.causal = config.use_causal_conv
        self.pad_mode = config.pad_mode
        self.norm_type = config.norm_type

        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride, dilation=dilation
        )
        if self.norm_type == "time_group_norm":
            self.norm = nn.GroupNorm(1, out_channels, pytorch_compatible=True)

        self.stride = stride

        # Effective kernel size with dilations.
        self.kernel_size = (kernel_size - 1) * dilation + 1

        self.padding_total = kernel_size - stride

    def _get_extra_padding_for_conv1d(
        self,
        hidden_states: mx.array,
    ) -> mx.array:
        length = hidden_states.shape[1]
        n_frames = (length - self.kernel_size + self.padding_total) / self.stride + 1
        n_frames = int(math.ceil(n_frames)) - 1
        ideal_length = n_frames * self.stride + self.kernel_size - self.padding_total
        return ideal_length - length

    def _pad1d(
        self,
        hidden_states: mx.array,
        paddings: Tuple[int, int],
        mode: str = "zero",
        value: float = 0.0,
    ):
        if mode != "reflect":
            return mx.pad(
                hidden_states, paddings, mode="constant", constant_values=value
            )

        length = hidden_states.shape[1]
        prefix = hidden_states[:, 1 : paddings[0] + 1][:, ::-1]
        suffix = hidden_states[:, max(length - (paddings[1] + 1), 0) : -1][:, ::-1]
        return mx.concatenate([prefix, hidden_states, suffix], axis=1)

    def __call__(self, hidden_states):
        extra_padding = self._get_extra_padding_for_conv1d(hidden_states)

        if self.causal:
            # Left padding for causal
            hidden_states = self._pad1d(
                hidden_states, (self.padding_total, extra_padding), mode=self.pad_mode
            )
        else:
            # Asymmetric padding required for odd strides
            padding_right = self.padding_total // 2
            padding_left = self.padding_total - padding_right
            hidden_states = self._pad1d(
                hidden_states,
                (padding_left, padding_right + extra_padding),
                mode=self.pad_mode,
            )

        hidden_states = self.conv(hidden_states)

        if self.norm_type == "time_group_norm":
            hidden_states = self.norm(hidden_states)

        return hidden_states


class EncodecConvTranspose1d(nn.Module):
    """ConvTranspose1d with asymmetric or causal padding and normalization."""

    def __init__(
        self,
        config,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
    ):
        super().__init__()
        self.causal = config.use_causal_conv
        self.trim_right_ratio = config.trim_right_ratio
        self.norm_type = config.norm_type
        self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride)
        if config.norm_type == "time_group_norm":
            self.norm = nn.GroupNorm(1, out_channels, pytorch_compatible=True)
        self.padding_total = kernel_size - stride

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states)

        if self.norm_type == "time_group_norm":
            hidden_states = self.norm(hidden_states)

        if self.causal:
            padding_right = math.ceil(self.padding_total * self.trim_right_ratio)
        else:
            padding_right = self.padding_total // 2

        padding_left = self.padding_total - padding_right

        end = hidden_states.shape[1] - padding_right
        hidden_states = hidden_states[:, padding_left:end, :]
        return hidden_states


class EncodecLSTM(nn.Module):
    def __init__(self, config, dimension):
        super().__init__()
        self.lstm = [LSTM(dimension, dimension) for _ in range(config.num_lstm_layers)]

    def __call__(self, hidden_states):
        h = hidden_states
        for lstm in self.lstm:
            h = lstm(h)
        return h + hidden_states


class EncodecResnetBlock(nn.Module):
    """
    Residual block from SEANet model as used by EnCodec.
    """

    def __init__(self, config, dim: int, dilations: List[int]):
        super().__init__()
        kernel_sizes = (config.residual_kernel_size, 1)
        if len(kernel_sizes) != len(dilations):
            raise ValueError("Number of kernel sizes should match number of dilations")

        hidden = dim // config.compress
        block = []
        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):
            in_chs = dim if i == 0 else hidden
            out_chs = dim if i == len(kernel_sizes) - 1 else hidden
            block += [nn.ELU()]
            block += [
                EncodecConv1d(config, in_chs, out_chs, kernel_size, dilation=dilation)
            ]
        self.block = block

        if getattr(config, "use_conv_shortcut", True):
            self.shortcut = EncodecConv1d(config, dim, dim, kernel_size=1)
        else:
            self.shortcut = nn.Identity()

    def __call__(self, hidden_states):
        residual = hidden_states
        for layer in self.block:
            hidden_states = layer(hidden_states)

        return self.shortcut(residual) + hidden_states


class EncodecEncoder(nn.Module):
    """SEANet encoder as used by EnCodec."""

    def __init__(self, config):
        super().__init__()
        model = [
            EncodecConv1d(
                config, config.audio_channels, config.num_filters, config.kernel_size
            )
        ]
        scaling = 1

        for ratio in reversed(config.upsampling_ratios):
            current_scale = scaling * config.num_filters
            for j in range(config.num_residual_layers):
                model += [
                    EncodecResnetBlock(
                        config, current_scale, [config.dilation_growth_rate**j, 1]
                    )
                ]
            model += [nn.ELU()]
            model += [
                EncodecConv1d(
                    config,
                    current_scale,
                    current_scale * 2,
                    kernel_size=ratio * 2,
                    stride=ratio,
                )
            ]
            scaling *= 2

        model += [EncodecLSTM(config, scaling * config.num_filters)]
        model += [nn.ELU()]
        model += [
            EncodecConv1d(
                config,
                scaling * config.num_filters,
                config.hidden_size,
                config.last_kernel_size,
            )
        ]

        self.layers = model

    def __call__(self, hidden_states):
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        return hidden_states


class EncodecDecoder(nn.Module):
    """SEANet decoder as used by EnCodec."""

    def __init__(self, config):
        super().__init__()
        scaling = int(2 ** len(config.upsampling_ratios))
        model = [
            EncodecConv1d(
                config,
                config.hidden_size,
                scaling * config.num_filters,
                config.kernel_size,
            )
        ]

        model += [EncodecLSTM(config, scaling * config.num_filters)]

        for ratio in config.upsampling_ratios:
            current_scale = scaling * config.num_filters
            model += [nn.ELU()]
            model += [
                EncodecConvTranspose1d(
                    config,
                    current_scale,
                    current_scale // 2,
                    kernel_size=ratio * 2,
                    stride=ratio,
                )
            ]
            for j in range(config.num_residual_layers):
                model += [
                    EncodecResnetBlock(
                        config, current_scale // 2, (config.dilation_growth_rate**j, 1)
                    )
                ]
            scaling //= 2

        model += [nn.ELU()]
        model += [
            EncodecConv1d(
                config,
                config.num_filters,
                config.audio_channels,
                config.last_kernel_size,
            )
        ]
        self.layers = model

    def __call__(self, hidden_states):
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        return hidden_states


class EncodecEuclideanCodebook(nn.Module):
    """Codebook with Euclidean distance."""

    def __init__(self, config):
        super().__init__()
        self.embed = mx.zeros((config.codebook_size, config.codebook_dim))

    def quantize(self, hidden_states):
        embed = self.embed.T
        scaled_states = hidden_states.square().sum(axis=1, keepdims=True)
        dist = -(
            scaled_states
            - 2 * hidden_states @ embed
            + embed.square().sum(axis=0, keepdims=True)
        )
        embed_ind = dist.argmax(axis=-1)
        return embed_ind

    def encode(self, hidden_states):
        shape = hidden_states.shape
        hidden_states = hidden_states.reshape((-1, shape[-1]))
        embed_ind = self.quantize(hidden_states)
        embed_ind = embed_ind.reshape(*shape[:-1])
        return embed_ind

    def decode(self, embed_ind):
        return self.embed[embed_ind]


class EncodecVectorQuantization(nn.Module):
    """
    Vector quantization implementation. Currently supports only euclidean distance.
    """

    def __init__(self, config):
        super().__init__()
        self.codebook = EncodecEuclideanCodebook(config)

    def encode(self, hidden_states):
        return self.codebook.encode(hidden_states)

    def decode(self, embed_ind):
        return self.codebook.decode(embed_ind)


class EncodecResidualVectorQuantizer(nn.Module):
    """Residual Vector Quantizer."""

    def __init__(self, config):
        super().__init__()
        self.codebook_size = config.codebook_size

        hop_length = np.prod(config.upsampling_ratios)
        self.frame_rate = math.ceil(config.sampling_rate / hop_length)
        self.num_quantizers = int(
            1000 * config.target_bandwidths[-1] // (self.frame_rate * 10)
        )
        self.layers = [
            EncodecVectorQuantization(config) for _ in range(self.num_quantizers)
        ]

    def get_num_quantizers_for_bandwidth(
        self, bandwidth: Optional[float] = None
    ) -> int:
        """Return num_quantizers based on specified target bandwidth."""
        bw_per_q = math.log2(self.codebook_size) * self.frame_rate
        num_quantizers = self.num_quantizers
        if bandwidth is not None and bandwidth > 0.0:
            num_quantizers = int(max(1, math.floor(bandwidth * 1000 / bw_per_q)))
        return num_quantizers

    def encode(
        self, embeddings: mx.array, bandwidth: Optional[float] = None
    ) -> mx.array:
        """
        Encode a given input array with the specified frame rate at the given
        bandwidth. The RVQ encode method sets the appropriate number of
        quantizers to use and returns indices for each quantizer.
        """
        num_quantizers = self.get_num_quantizers_for_bandwidth(bandwidth)
        residual = embeddings
        all_indices = []
        for layer in self.layers[:num_quantizers]:
            indices = layer.encode(residual)
            quantized = layer.decode(indices)
            residual = residual - quantized
            all_indices.append(indices)
        out_indices = mx.stack(all_indices, axis=1)
        return out_indices

    def decode(self, codes: mx.array) -> mx.array:
        """Decode the given codes to the quantized representation."""
        quantized_out = None
        for i, indices in enumerate(codes.split(codes.shape[1], axis=1)):
            layer = self.layers[i]
            quantized = layer.decode(indices.squeeze(1))
            if quantized_out is None:
                quantized_out = quantized
            else:
                quantized_out = quantized + quantized_out
        return quantized_out


class Encodec(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.encoder = EncodecEncoder(self.config)
        self.decoder = EncodecDecoder(self.config)
        self.quantizer = EncodecResidualVectorQuantizer(self.config)

    def _encode_frame(
        self, input_values: mx.array, bandwidth: float, padding_mask: mx.array
    ) -> Tuple[mx.array, Optional[mx.array]]:
        """
        Encodes the given input using the underlying VQVAE.
        """
        length = input_values.shape[1]
        duration = length / self.config.sampling_rate

        if (
            self.config.chunk_length_s is not None
            and duration > 1e-5 + self.config.chunk_length_s
        ):
            raise RuntimeError(
                f"Duration of frame ({duration}) is longer than chunk {self.config.chunk_length_s}"
            )

        scale = None
        if self.config.normalize:
            # if the padding is non zero
            input_values = input_values * padding_mask[..., None]
            mono = mx.sum(input_values, axis=2, keepdims=True) / input_values.shape[2]
            scale = mono.square().mean(axis=1, keepdims=True).sqrt() + 1e-8
            input_values = input_values / scale

        embeddings = self.encoder(input_values)
        codes = self.quantizer.encode(embeddings, bandwidth)
        return codes, scale

    def encode(
        self,
        input_values: mx.array,
        padding_mask: mx.array = None,
        bandwidth: Optional[float] = None,
    ) -> Tuple[mx.array, Optional[mx.array]]:
        """
        Encodes the input audio waveform into discrete codes.

        Args:
            input_values (mx.array): The input audio waveform with shape
                ``(batch_size, channels, sequence_length)``.
            padding_mask (mx.array): Padding mask used to pad the ``input_values``.
            bandwidth (float, optional): The target bandwidth. Must be one of
                ``config.target_bandwidths``. If ``None``, uses the smallest
                possible bandwidth. bandwidth is represented as a thousandth of
                what it is, e.g. 6kbps bandwidth is represented as bandwidth == 6.0

        Returns:
            A list of frames containing the discrete encoded codes for the
            input audio waveform, along with rescaling factors for each chunk
            when ``config.normalize==True``. Each frame is a tuple ``(codebook,
            scale)``, with ``codebook`` of shape ``(batch_size, num_codebooks,
            frames)``.
        """

        if bandwidth is None:
            bandwidth = self.config.target_bandwidths[0]
        if bandwidth not in self.config.target_bandwidths:
            raise ValueError(
                f"This model doesn't support the bandwidth {bandwidth}. Select one of {self.config.target_bandwidths}."
            )

        _, input_length, channels = input_values.shape

        if channels < 1 or channels > 2:
            raise ValueError(
                f"Number of audio channels must be 1 or 2, but got {channels}"
            )

        chunk_length = self.chunk_length
        if chunk_length is None:
            chunk_length = input_length
            stride = input_length
        else:
            stride = self.chunk_stride

        if padding_mask is None:
            padding_mask = mx.ones(input_values.shape[:2], dtype=mx.bool_)
        encoded_frames = []
        scales = []

        step = chunk_length - stride
        if (input_length % stride) != step:
            raise ValueError(
                "The input length is not properly padded for batched chunked encoding. Make sure to pad the input correctly."
            )

        for offset in range(0, input_length - step, stride):
            mask = padding_mask[:, offset : offset + chunk_length].astype(mx.bool_)
            frame = input_values[:, offset : offset + chunk_length]
            encoded_frame, scale = self._encode_frame(frame, bandwidth, mask)
            encoded_frames.append(encoded_frame)
            scales.append(scale)

        encoded_frames = mx.stack(encoded_frames)

        return (encoded_frames, scales)

    @staticmethod
    def _linear_overlap_add(frames: List[mx.array], stride: int):
        if len(frames) == 0:
            raise ValueError("`frames` cannot be an empty list.")

        dtype = frames[0].dtype
        N, frame_length, C = frames[0].shape
        total_size = stride * (len(frames) - 1) + frames[-1].shape[1]

        time_vec = mx.linspace(0, 1, frame_length + 2, dtype=dtype)[1:-1]
        weight = 0.5 - (time_vec - 0.5).abs()

        weight = weight[:, None]
        sum_weight = mx.zeros((total_size, 1), dtype=dtype)
        out = mx.zeros((N, total_size, C), dtype=dtype)
        offset = 0

        for frame in frames:
            frame_length = frame.shape[1]
            out[:, offset : offset + frame_length] += weight[:frame_length] * frame
            sum_weight[offset : offset + frame_length] += weight[:frame_length]
            offset += stride

        return out / sum_weight

    def _decode_frame(
        self, codes: mx.array, scale: Optional[mx.array] = None
    ) -> mx.array:
        embeddings = self.quantizer.decode(codes)
        outputs = self.decoder(embeddings)
        if scale is not None:
            outputs = outputs * scale
        return outputs

    @property
    def channels(self):
        return self.config.audio_channels

    @property
    def sampling_rate(self):
        return self.config.sampling_rate

    @property
    def chunk_length(self):
        if self.config.chunk_length_s is None:
            return None
        else:
            return int(self.config.chunk_length_s * self.config.sampling_rate)

    @property
    def chunk_stride(self):
        if self.config.chunk_length_s is None or self.config.overlap is None:
            return None
        else:
            return max(1, int((1.0 - self.config.overlap) * self.chunk_length))

    @classmethod
    def from_pretrained(cls, path_or_repo: str):
        """
        Load the model and audo preprocessor.
        """
        path = Path(path_or_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_repo,
                    allow_patterns=["*.json", "*.safetensors", "*.model"],
                )
            )

        with open(path / "config.json", "r") as f:
            config = json.load(f)

        filtered_config = filter_dataclass_fields(config, EncodecConfig)
        config = EncodecConfig(**filtered_config)
        model = cls(config)
        model.load_weights(str(path / "model.safetensors"))
        processor = functools.partial(
            preprocess_audio,
            sampling_rate=config.sampling_rate,
            chunk_length=model.chunk_length,
            chunk_stride=model.chunk_stride,
        )
        mx.eval(model)
        return model, processor

    def decode(
        self,
        audio_codes: mx.array,
        audio_scales: Union[mx.array, List[mx.array]],
        padding_mask: Optional[mx.array] = None,
    ) -> Tuple[mx.array, mx.array]:
        """
        Decodes the given frames into an output audio waveform.

        Note that the output might be a bit bigger than the input. In that
        case, any extra steps at the end should be trimmed.

        Args:
            audio_codes (mx.array): Discret code embeddings of shape
                ``(batch_size, nb_chunks, chunk_length)``.
            audio_scales (mx.array): Scaling factor for each input.
            padding_mask (mx.array): Padding mask.
        """
        chunk_length = self.chunk_length
        if chunk_length is None:
            if audio_codes.shape[1] != 1:
                raise ValueError(f"Expected one frame, got {len(audio_codes)}")
            audio_values = self._decode_frame(audio_codes[:, 0], audio_scales[0])
        else:
            decoded_frames = []

            for frame, scale in zip(audio_codes, audio_scales):
                frames = self._decode_frame(frame, scale)
                decoded_frames.append(frames)

            audio_values = self._linear_overlap_add(
                decoded_frames, self.chunk_stride or 1
            )

        # truncate based on padding mask
        if padding_mask is not None and padding_mask.shape[1] < audio_values.shape[1]:
            audio_values = audio_values[:, : padding_mask.shape[1]]
        return audio_values



================================================
FILE: mlx_audio/codec/models/mimi/__init__.py
================================================
from .mimi import Mimi, MimiStreamingDecoder



================================================
FILE: mlx_audio/codec/models/mimi/mimi.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import hf_hub_download

from .modules import (
    ConvDownsample1d,
    ConvTranspose1d,
    ConvTrUpsample1d,
    EuclideanCodebook,
    ProjectedTransformer,
    SeanetConfig,
    SeanetDecoder,
    SeanetEncoder,
    SplitResidualVectorQuantizer,
    TransformerConfig,
)


@dataclass
class MimiConfig:
    channels: int
    sample_rate: float
    frame_rate: float
    renormalize: bool
    seanet: SeanetConfig
    transformer: TransformerConfig
    quantizer_nq: int
    quantizer_bins: int
    quantizer_dim: int


def mimi_202407(num_codebooks: int) -> MimiConfig:
    seanet = SeanetConfig(
        dimension=512,
        channels=1,
        causal=True,
        nfilters=64,
        nresidual_layers=1,
        ratios=[8, 6, 5, 4],
        ksize=7,
        residual_ksize=3,
        last_ksize=3,
        dilation_base=2,
        pad_mode="constant",
        true_skip=True,
        compress=2,
    )
    transformer = TransformerConfig(
        d_model=seanet.dimension,
        num_heads=8,
        num_layers=8,
        causal=True,
        norm_first=True,
        bias_ff=False,
        bias_attn=False,
        layer_scale=0.01,
        positional_embedding="rope",
        use_conv_bias=True,
        gating=False,
        norm="layer_norm",
        context=250,
        max_period=10000,
        max_seq_len=8192,
        kv_repeat=1,
        dim_feedforward=2048,
        conv_layout=True,
        use_conv_block=False,
        cross_attention=False,
        conv_kernel_size=3,
    )
    return MimiConfig(
        channels=1,
        sample_rate=24000,
        frame_rate=12.5,
        renormalize=True,
        seanet=seanet,
        transformer=transformer,
        quantizer_nq=num_codebooks,
        quantizer_bins=2048,
        quantizer_dim=256,
    )


class Mimi(nn.Module):
    def __init__(self, cfg: MimiConfig):
        super().__init__()
        dim = cfg.seanet.dimension
        self.cfg = cfg
        encoder_frame_rate = cfg.sample_rate / math.prod(cfg.seanet.ratios)
        downsample_stride = int(encoder_frame_rate / cfg.frame_rate)
        self.encoder = SeanetEncoder(cfg.seanet)
        self.decoder = SeanetDecoder(cfg.seanet)
        self.quantizer = SplitResidualVectorQuantizer(
            dim=cfg.quantizer_dim,
            input_dim=dim,
            output_dim=dim,
            nq=cfg.quantizer_nq,
            bins=cfg.quantizer_bins,
        )
        self.encoder_transformer = ProjectedTransformer(
            cfg.transformer,
            input_dim=dim,
            output_dims=[dim],
        )
        self.decoder_transformer = ProjectedTransformer(
            cfg.transformer,
            input_dim=dim,
            output_dims=[dim],
        )
        self.downsample = ConvDownsample1d(
            stride=downsample_stride,
            dim=dim,
            causal=True,
        )
        self.upsample = ConvTrUpsample1d(
            stride=downsample_stride,
            dim=dim,
            causal=True,
        )
        self.encoder_cache = self.encoder_transformer.make_cache()
        self.decoder_cache = self.decoder_transformer.make_cache()

    def reset_state(self):
        self.encoder.reset_state()
        self.decoder.reset_state()
        for c in self.decoder_cache:
            c.reset()
        for c in self.encoder_cache:
            c.reset()

    def encode(self, xs: mx.array) -> mx.array:
        self.encoder.reset_state()
        for c in self.encoder_cache:
            c.reset()
        xs = self.encoder(xs)
        xs = self.encoder_transformer(xs, cache=self.encoder_cache)[0]
        xs = self.downsample(xs)
        return self.quantizer.encode(xs)

    def decode(self, xs: mx.array) -> mx.array:
        self.decoder.reset_state()
        for c in self.decoder_cache:
            c.reset()
        xs = self.quantizer.decode(xs)
        xs = self.upsample(xs)
        xs = self.decoder_transformer(xs, cache=self.decoder_cache)[0]
        return self.decoder(xs)

    def encode_step(self, xs: mx.array) -> mx.array:
        xs = self.encoder.step(xs)
        xs = self.encoder_transformer(xs, cache=self.encoder_cache)[0]
        xs = self.downsample.step(xs)
        xs = self.quantizer.encode(xs)
        return xs

    def decode_step(self, xs: mx.array) -> mx.array:
        xs = self.quantizer.decode(xs)
        xs = self.upsample.step(xs)
        xs = self.decoder_transformer(xs, cache=self.decoder_cache)[0]
        xs = self.decoder.step(xs)
        return xs

    def warmup(self):
        pcm = mx.zeros((1, 1, 1920 * 4))
        codes = self.encode(pcm)
        pcm_out = self.decode(codes)
        mx.eval(pcm_out)

    @property
    def frame_rate(self) -> float:
        return self.cfg.frame_rate

    @property
    def sample_rate(self) -> float:
        return self.cfg.sample_rate

    def load_pytorch_weights(
        self,
        file: str,
        strict: bool = True,
    ) -> nn.Module:
        weights = []
        for k, v in mx.load(file).items():
            v: mx.array = v
            k: str = ".".join([s.removeprefix("_") for s in k.split(".")])
            if k.startswith("encoder.model."):
                k = k.replace("encoder.model.", "encoder.")
            if k.startswith("decoder.model."):
                k = k.replace("decoder.model.", "decoder.")
            if k.endswith(".in_proj_weight"):
                k = k.replace(".in_proj_weight", ".in_proj.weight")
            if k.endswith(".linear1.weight"):
                k = k.replace(".linear1.weight", ".gating.linear1.weight")
            if k.endswith(".linear2.weight"):
                k = k.replace(".linear2.weight", ".gating.linear2.weight")
            # Awfully hardcoded matching between the pytorch layers and their mlx equivalent :(
            for layerIdx, decoderIdx in enumerate([2, 5, 8, 11]):
                k = k.replace(
                    f"decoder.{decoderIdx}.", f"decoder.layers.{layerIdx}.upsample."
                )
                k = k.replace(
                    f"decoder.{decoderIdx + 1}.",
                    f"decoder.layers.{layerIdx}.residuals.0.",
                )
            for layerIdx, encoderIdx in enumerate([1, 4, 7, 10]):
                k = k.replace(
                    f"encoder.{encoderIdx}.", f"encoder.layers.{layerIdx}.residuals.0."
                )
                k = k.replace(
                    f"encoder.{encoderIdx + 2}.",
                    f"encoder.layers.{layerIdx}.downsample.",
                )

            k = k.replace("decoder.0.", "decoder.init_conv1d.")
            k = k.replace("decoder.14.", "decoder.final_conv1d.")
            k = k.replace("encoder.0.", "encoder.init_conv1d.")
            k = k.replace("encoder.14.", "encoder.final_conv1d.")
            k = k.replace(".block.1.", ".block.0.")
            k = k.replace(".block.3.", ".block.1.")

            # PyTorch layout for conv weights is outC, inC, kSize, for MLX it's outC, kSize, inC
            if (
                k.endswith(".conv.weight")
                or k.endswith(".output_proj.weight")
                or k.endswith(".input_proj.weight")
            ):
                v = v.swapaxes(-1, -2)
            # PyTorch layout for conv-transposed weights is inC, outC, kSize, for MLX it's outC, kSize, inC
            if k.endswith(".convtr.weight"):
                v = v.transpose(1, 2, 0)
            weights.append((k, v))
        m = self.load_weights(weights, strict=strict)

        def _filter_fn(module, name, _):
            if isinstance(module, EuclideanCodebook) and name == "initialized":
                module.update_in_place()
            if isinstance(module, ConvTranspose1d) and name == "weight":
                module.update_in_place()
            return True

        m.filter_and_map(_filter_fn)
        return m

    @classmethod
    def from_pretrained(
        cls,
        repo_id: str,
        filename: str = "tokenizer-e351c8d8-checkpoint125.safetensors",
    ) -> nn.Module:
        cfg = mimi_202407(32)
        model = cls(cfg)
        model_file = hf_hub_download(repo_id, filename)
        model.load_pytorch_weights(model_file, strict=True)
        return model


class MimiStreamingDecoder:
    """Incremental decoder wrapper for the Mimi codec.

    This helper keeps the internal state of the Mimi model across calls and
    decodes audio tokens frame by frame using ``decode_step``.
    """

    def __init__(self, mimi: "Mimi") -> None:  # noqa: F821 - Mimi defined below
        self._mimi = mimi
        self.reset()

    def reset(self) -> None:
        """Reset the underlying codec state."""
        self._mimi.decoder.reset_state()
        self._mimi.upsample.reset_state()
        for c in self._mimi.decoder_cache:
            c.reset()

    def decode_frames(self, tokens: mx.array) -> mx.array:
        """Decode a sequence of audio tokens incrementally.

        Parameters
        ----------
        tokens:
            Array of shape ``(B, C, T)`` or ``(C, T)`` containing the audio
            tokens to decode. ``B`` is the batch dimension, ``C`` is the number
            of codebooks and ``T`` the number of frames.

        Returns
        -------
        mx.array
            The decoded waveform for the provided frames.
        """

        if tokens.ndim == 2:
            tokens = mx.expand_dims(tokens, 0)

        pcm = []
        for t in range(tokens.shape[-1]):
            step_tokens = tokens[:, :, t : t + 1]
            pcm.append(self._mimi.decode_step(step_tokens))

        return mx.concat(pcm, axis=-1)



================================================
FILE: mlx_audio/codec/models/mimi/modules/__init__.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# flake8: noqa
"""Modules used for building the models."""

from .conv import (
    Conv1d,
    ConvDownsample1d,
    ConvTranspose1d,
    ConvTrUpsample1d,
    NormConv1d,
    NormConvTranspose1d,
    StreamableConv1d,
    StreamableConvTranspose1d,
)
from .kv_cache import KVCache, RotatingKVCache
from .quantization import EuclideanCodebook, SplitResidualVectorQuantizer
from .seanet import SeanetConfig, SeanetDecoder, SeanetEncoder
from .transformer import ProjectedTransformer, Transformer, TransformerConfig



================================================
FILE: mlx_audio/codec/models/mimi/modules/conv.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math

import mlx.core as mx
import mlx.nn as nn


class Conv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int = 1,
        padding: int = 0,
        groups: int = 1,
        dilation: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        nn.Conv1d
        scale = 1 / (in_channels * ksize)
        self.weight = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, ksize, in_channels // groups),
        )
        self.bias = None
        if bias:
            self.bias = mx.zeros(out_channels)
        self._padding = padding
        self._groups = groups
        self._stride = stride
        self._dilation = dilation

    def __call__(self, xs: mx.array) -> mx.array:
        # MLX uses NLC whereas pytorch/candle use NCL
        y = mx.conv1d(
            xs.swapaxes(-1, -2),
            self.weight,
            stride=self._stride,
            padding=self._padding,
            dilation=self._dilation,
            groups=self._groups,
        )
        if self.bias is not None:
            y = y + self.bias
        return y.swapaxes(-1, -2)


class ConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int = 1,
        padding: int = 0,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        nn.Conv1d
        scale = 1 / (in_channels * ksize)
        self.weight = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels // groups, ksize, in_channels),
        )
        self.bias = None
        if bias:
            self.bias = mx.zeros(out_channels)
        self._padding = padding
        self._groups = groups
        self._stride = stride
        self._ksize = ksize
        self._in_channels = in_channels
        self._out_channels = out_channels
        if groups == in_channels and groups == out_channels:
            eye = (
                mx.eye(out_channels)
                .astype(self.weight.dtype)
                .reshape((out_channels, 1, out_channels))
            )
            eye = mx.repeat(eye, repeats=ksize, axis=1)
            self._expanded_weight = mx.repeat(self.weight, repeats=groups, axis=0) * eye
            self._expanded_groups = 1
        elif groups > 1:
            raise ValueError("groups are not supported in ConvTranspose1d")
        else:
            self._expanded_weight = self.weight
            self._expanded_groups = groups

    def update_in_place(self):
        groups = self._groups
        in_channels = self._in_channels
        out_channels = self._out_channels
        ksize = self._ksize
        if groups == in_channels and groups == out_channels:
            eye = (
                mx.eye(out_channels)
                .astype(self.weight.dtype)
                .reshape((out_channels, 1, out_channels))
            )
            eye = mx.repeat(eye, repeats=ksize, axis=1)
            self._expanded_weight = mx.repeat(self.weight, repeats=groups, axis=0) * eye
            self._expanded_groups = 1
        elif groups > 1:
            raise ValueError("groups are not supported in ConvTranspose1d")
        else:
            self._expanded_weight = self.weight
            self._expanded_groups = groups

    def update(self, parameters: dict) -> nn.Module:
        super().update(parameters)
        self.update_in_place()
        return self

    def __call__(self, xs: mx.array) -> mx.array:
        y = mx.conv_transpose1d(
            xs.swapaxes(-1, -2),
            self._expanded_weight,
            stride=self._stride,
            padding=self._padding,
            groups=self._expanded_groups,
        )
        if self.bias is not None:
            y = y + self.bias
        return y.swapaxes(-1, -2)


class NormConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int = 1,
        padding: int = 0,
        groups: int = 1,
        dilation: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.conv = Conv1d(
            in_channels,
            out_channels,
            ksize,
            stride=stride,
            padding=padding,
            groups=groups,
            dilation=dilation,
            bias=bias,
        )

    def __call__(self, xs: mx.array) -> mx.array:
        return self.conv(xs)


class NormConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int = 1,
        padding: int = 0,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.convtr = ConvTranspose1d(
            in_channels,
            out_channels,
            ksize,
            stride=stride,
            padding=padding,
            groups=groups,
            bias=bias,
        )

    def __call__(self, xs: mx.array) -> mx.array:
        return self.convtr(xs)


def get_extra_padding_for_conv1d(
    xs: mx.array,
    ksize: int,
    stride: int,
    padding_total: int,
) -> int:
    len_ = xs.shape[-1]
    nframes = max(len_ + padding_total - ksize, 0) / stride + 1.0
    ideal_len = (int(math.ceil(nframes)) - 1) * stride + ksize - padding_total
    return max(0, ideal_len - len_)


def unpad1d(xs: mx.array, unpad_l: int, unpad_r: int) -> mx.array:
    left = unpad_l
    right = xs.shape[-1] - unpad_r
    return xs[..., left:right]


# TODO(laurent): add a streaming module abstract class?


class StreamableConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int,
        dilation: int,
        groups: int,
        bias: bool,
        causal: bool,
        pad_mode: str,
    ):
        super().__init__()
        self._causal = causal
        self._pad_mode = pad_mode
        self._ksize = ksize
        self.conv = NormConv1d(
            in_channels,
            out_channels,
            ksize,
            stride=stride,
            groups=groups,
            dilation=dilation,
            bias=bias,
        )
        self._prev_xs = None
        self._left_pad_applied = False
        self._out_channels = out_channels

    def reset_state(self):
        self._prev_xs = None
        self._left_pad_applied = False

    def __call__(self, xs: mx.array) -> mx.array:
        ksize = self._ksize
        ksize = (ksize - 1) * self.conv.conv._dilation + 1
        padding_total = ksize - self.conv.conv._stride
        extra_padding = get_extra_padding_for_conv1d(
            xs,
            ksize=ksize,
            stride=self.conv.conv._stride,
            padding_total=padding_total,
        )
        z = 0, 0
        if self._causal:
            padding_left = padding_total
            padding_right = 0
        else:
            padding_right = padding_total // 2
            padding_left = padding_total - padding_right
        widths = [z, z, (padding_left, padding_right + extra_padding)]
        pd = mx.pad(xs, pad_width=widths, mode=self._pad_mode)
        return self.conv(pd)

    def step(self, xs: mx.array) -> mx.array:
        b, _, len_ = xs.shape
        if len_ == 0:
            return mx.zeros((b, self._out_channels, 0))
        stride = self.conv.conv._stride
        dilation = self.conv.conv._dilation
        ksize = (self._ksize - 1) * dilation + 1
        if not self._left_pad_applied:
            self._left_pad_applied = True
            padding_total = ksize - stride
            xs = mx.pad(
                xs, pad_width=((0, 0), (0, 0), (padding_total, 0)), mode=self._pad_mode
            )
        if self._prev_xs is not None:
            xs = mx.concat([self._prev_xs, xs], axis=-1)
        len_ = xs.shape[-1]
        nframes = max(len_ + stride - ksize, 0) // stride
        if nframes > 0:
            offset = nframes * stride
            self._prev_xs = xs[..., offset:]
            in_l = (nframes - 1) * stride + ksize
            if in_l > 0:
                xs = xs[..., 0:in_l]
                return self.conv(xs)
            else:
                return mx.zeros((b, self._out_channels, 0))
        else:
            self._prev_xs = xs
            return mx.zeros((b, self._out_channels, 0))


class StreamableConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        ksize: int,
        stride: int,
        groups: int,
        bias: bool,
        causal: bool,
    ):
        super().__init__()
        self._causal = causal
        self._ksize = ksize
        self.convtr = NormConvTranspose1d(
            in_channels,
            out_channels,
            ksize,
            stride=stride,
            groups=groups,
            bias=bias,
        )
        self._prev_ys = None

    def reset_state(self):
        self._prev_ys = None

    def __call__(self, xs: mx.array) -> mx.array:
        stride = self.convtr.convtr._stride
        padding_total = max(self._ksize - stride, 0)
        xs = self.convtr(xs)
        if self._causal:
            unpad_l = 0
            unpad_r = padding_total
        else:
            unpad_r = padding_total // 2
            unpad_l = padding_total - unpad_r
        return unpad1d(xs, unpad_l=unpad_l, unpad_r=unpad_r)

    def step(self, xs: mx.array) -> mx.array:
        b, _, len_ = xs.shape
        if len_ == 0:
            return mx.zeros((b, self._out_channels, 0))
        stride = self.convtr.convtr._stride
        ys = self.convtr(xs)
        ot = ys.shape[-1]
        if self._prev_ys is not None:
            prev_ys = self._prev_ys
            pt = prev_ys.shape[-1]
            if self.convtr.convtr.bias is not None:
                prev_ys = prev_ys - self.convtr.convtr.bias[None, :, None]
            ys1, ys2 = ys[..., :pt] + prev_ys, ys[..., pt:]
            ys = mx.concat([ys1, ys2], axis=-1)
        invalid_steps = self._ksize - stride
        ys, self._prev_ys = ys[..., : ot - invalid_steps], ys[..., ot - invalid_steps :]
        return ys


class ConvDownsample1d(nn.Module):
    def __init__(self, stride: int, dim: int, causal: bool):
        super().__init__()
        self.conv = StreamableConv1d(
            in_channels=dim,
            out_channels=dim,
            ksize=2 * stride,
            stride=stride,
            dilation=1,
            groups=1,
            bias=False,
            causal=causal,
            pad_mode="edge",
        )

    def reset_state(self):
        self.conv.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        return self.conv(xs)

    def step(self, xs: mx.array) -> mx.array:
        return self.conv.step(xs)


class ConvTrUpsample1d(nn.Module):
    def __init__(self, stride: int, dim: int, causal: bool):
        super().__init__()
        self.convtr = StreamableConvTranspose1d(
            in_channels=dim,
            out_channels=dim,
            ksize=2 * stride,
            stride=stride,
            groups=dim,
            bias=False,
            causal=causal,
        )

    def reset_state(self):
        self.convtr.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        xs = self.convtr(xs)
        return xs

    def step(self, xs: mx.array) -> mx.array:
        xs = self.convtr.step(xs)
        return xs



================================================
FILE: mlx_audio/codec/models/mimi/modules/kv_cache.py
================================================
# Most of the code below comes from:
# https://github.com/ml-explore/mlx-examples/blob/6c2369e4b97f49fb5906ec46033497b39931b25d/llms/mlx_lm/models/base.py#L1
# Copyright Â© 2023-2024 Apple Inc.

from __future__ import annotations

import inspect
from dataclasses import dataclass
from typing import Any

import mlx.core as mx


class KVCache:

    def __init__(self, head_dim, n_kv_heads):
        self.n_kv_heads = n_kv_heads
        if isinstance(head_dim, int):
            self.k_head_dim = self.v_head_dim = head_dim
        elif isinstance(head_dim, tuple) and len(head_dim) == 2:
            self.k_head_dim, self.v_head_dim = head_dim
        else:
            raise ValueError("head_dim must be an int or a tuple of two ints")
        self.keys = None
        self.values = None
        self.offset = 0
        self.step = 256

    def update_and_fetch(self, keys, values) -> tuple[mx.array, mx.array]:
        prev = self.offset
        if self.keys is None or (prev + keys.shape[2]) > self.keys.shape[2]:
            B = keys.shape[0]
            n_steps = (self.step + keys.shape[2] - 1) // self.step
            k_shape = (B, self.n_kv_heads, n_steps * self.step, self.k_head_dim)
            v_shape = (B, self.n_kv_heads, n_steps * self.step, self.v_head_dim)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)
            if self.keys is not None:
                assert self.values is not None
                if prev % self.step != 0:
                    self.keys = self.keys[..., :prev, :]
                    self.values = self.values[..., :prev, :]
                self.keys = mx.concatenate([self.keys, new_k], axis=2)
                self.values = mx.concatenate([self.values, new_v], axis=2)
            else:
                self.keys, self.values = new_k, new_v

        self.offset += keys.shape[2]
        self.keys[..., prev : self.offset, :] = keys
        assert self.values is not None
        self.values[..., prev : self.offset, :] = values
        return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]

    def reset(self):
        self.offset = 0
        self.keys = None
        self.values = None

    @property
    def state(self):
        return self.keys, self.values


class RotatingKVCache:

    def __init__(self, head_dim, n_kv_heads, max_size, keep=0, step=256):
        self.n_kv_heads = n_kv_heads
        if isinstance(head_dim, int):
            self.k_head_dim = self.v_head_dim = head_dim
        elif isinstance(head_dim, tuple) and len(head_dim) == 2:
            self.k_head_dim, self.v_head_dim = head_dim
        else:
            raise ValueError("head_dim must be an int or a tuple of two ints")
        self.keep = keep
        self.keys = None
        self.values = None
        self.offset = 0
        self.max_size = max_size
        self.step = step
        self._idx = 0

    def _trim(self, trim_size, v, append=None):
        to_cat = []
        if trim_size > 0:
            to_cat = [v[..., : self.keep, :], v[..., trim_size + self.keep :, :]]
        else:
            to_cat = [v]
        if append is not None:
            to_cat.append(append)
        return mx.concatenate(to_cat, axis=2)

    def update_and_fetch(self, keys, values) -> tuple[mx.array, mx.array]:
        prev = self.offset
        B, _, S = keys.shape[:3]

        # Prefill mode
        if S > 1:
            if self.keys is None:
                self.keys = keys
                self.values = values
            else:
                # The largest size is self.max_size + S - 1 to ensure
                # every token gets at least self.max_size context
                trim_size = self.keys.shape[2] - self.max_size + 1
                self.keys = self._trim(trim_size, self.keys, keys)
                self.values = self._trim(trim_size, self.values, values)
            self.offset += S
            self._idx = self.keys.shape[2]
            return self.keys, self.values

        # Generation mode
        # May not have hit the max size yet, so potentially
        # keep growing the cache
        if self.keys is None or (
            prev >= self.keys.shape[2] and self.keys.shape[2] < self.max_size
        ):
            new_size = min(self.step, self.max_size - prev)
            k_shape = (B, self.n_kv_heads, new_size, self.k_head_dim)
            v_shape = (B, self.n_kv_heads, new_size, self.v_head_dim)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)
            if self.keys is not None:
                assert self.values is not None
                self.keys = mx.concatenate([self.keys, new_k], axis=2)
                self.values = mx.concatenate([self.values, new_v], axis=2)
            else:
                self.keys, self.values = new_k, new_v
            self._idx = prev

        # Trim if needed
        trim_size = self.keys.shape[2] - self.max_size
        if trim_size > 0:
            self.keys = self._trim(trim_size, self.keys)
            self.values = self._trim(trim_size, self.values)
            self._idx = self.max_size

        # Rotate
        if self._idx == self.max_size:
            self._idx = self.keep

        # Assign
        self.keys[..., self._idx : self._idx + 1, :] = keys
        assert self.values is not None
        self.values[..., self._idx : self._idx + 1, :] = values
        self.offset += 1
        self._idx += 1

        # If the buffer is not full, slice off the end
        if self.offset < self.max_size:
            return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]
        return self.keys, self.values

    def reset(self):
        self.offset = 0
        self._idx = 0
        self.keys = None
        self.values = None

    @property
    def state(self):
        return self.keys, self.values


@dataclass
class BaseModelArgs:
    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


def create_additive_causal_mask(N: int, offset: int = 0):
    rinds = mx.arange(offset + N)
    linds = mx.arange(offset, offset + N) if offset else rinds
    mask = linds[:, None] < rinds[None]
    return mask * -1e9


def create_attention_mask(h: mx.array, cache: Any | None = None):
    T = h.shape[1]
    if T > 1:
        if cache is not None and cache[0] is not None:
            c = cache[0]
            if isinstance(c, RotatingKVCache):
                offset = min(c.max_size - 1, c.offset)
            else:
                offset = c.offset
        else:
            offset = 0
        mask = create_additive_causal_mask(T, offset)
        mask = mask.astype(h.dtype)
    else:
        mask = None
    return mask



================================================
FILE: mlx_audio/codec/models/mimi/modules/quantization.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from __future__ import annotations

import mlx.core as mx
import mlx.nn as nn

from .conv import Conv1d


class EuclideanCodebook(nn.Module):
    def __init__(self, dim: int, codebook_size: int):
        super().__init__()
        self._epsilon = 1e-5
        self._dim = dim
        self.initialized = mx.zeros([1], dtype=mx.float32)
        self.embedding_sum = mx.zeros([codebook_size, dim], dtype=mx.float32)
        self.cluster_usage = mx.zeros([codebook_size], dtype=mx.float32)
        cluster_usage = mx.maximum(self.cluster_usage, self._epsilon)[:, None]
        self._embedding = self.embedding_sum / cluster_usage
        self._c2 = self._embedding.square().sum(axis=-1) / 2

    def update_in_place(self):
        cluster_usage = mx.maximum(self.cluster_usage, self._epsilon)[:, None]
        self._embedding = self.embedding_sum / cluster_usage
        self._c2 = self._embedding.square().sum(axis=-1) / 2

    def update(self, parameters: dict) -> nn.Module:
        super().update(parameters)
        self.update_in_place()
        return self

    def encode(self, xs: mx.array) -> mx.array:
        target_shape = xs.shape[:-1]
        xs = xs.flatten(end_axis=-2)
        dot_prod = xs @ self._embedding.swapaxes(-1, -2)
        return (self._c2 - dot_prod).argmin(axis=-1).reshape(target_shape)

    def decode(self, xs: mx.array) -> mx.array:
        target_shape = list(xs.shape) + [self._dim]
        return mx.take(self._embedding, xs.flatten(), axis=0).reshape(target_shape)


class VectorQuantization(nn.Module):
    def __init__(self, dim: int, codebook_size: int, codebook_dim: int | None):
        super().__init__()
        codebook_dim = dim if codebook_dim is None else codebook_dim
        if dim == codebook_dim:
            self.project_in = None
            self.project_out = None
        else:
            self.project_in = nn.Linear(dim, codebook_dim)
            self.project_out = nn.Linear(codebook_dim, dim)
        self.codebook = EuclideanCodebook(dim=codebook_dim, codebook_size=codebook_size)

    def encode(self, xs: mx.array) -> mx.array:
        xs = xs.swapaxes(-1, -2)
        if self.project_in is not None:
            xs = self.project_in(xs)
        return self.codebook.encode(xs)

    def decode(self, xs: mx.array) -> mx.array:
        xs = self.codebook.decode(xs)
        if self.project_out is not None:
            xs = self.project_out(xs)
        return xs.swapaxes(-1, -2)


class ResidualVectorQuantization(nn.Module):
    def __init__(self, nq: int, dim: int, codebook_size: int, codebook_dim: int | None):
        super().__init__()
        layers = []
        for _ in range(nq):
            vq = VectorQuantization(
                dim=dim,
                codebook_size=codebook_size,
                codebook_dim=codebook_dim,
            )
            layers.append(vq)
        self.layers = layers

    def encode(self, xs: mx.array) -> mx.array:
        codes = []
        residual = xs
        for layer in self.layers:
            indices = layer.encode(residual)
            quantized = layer.decode(indices)
            residual = residual - quantized
            codes.append(indices)
        return mx.stack(codes, axis=0)

    def decode(self, xs: mx.array) -> mx.array:
        seq_len = xs.shape[0]
        quantized = self.layers[0].decode(xs[0])
        for i in range(1, seq_len):
            quantized = quantized + self.layers[i].decode(xs[i])
        return quantized


class ResidualVectorQuantizer(nn.Module):
    def __init__(
        self,
        dim: int,
        input_dim: int | None,
        output_dim: int | None,
        nq: int,
        bins: int,
        force_projection: bool,
    ):
        super().__init__()
        input_dim = dim if input_dim is None else input_dim
        output_dim = dim if output_dim is None else output_dim
        if input_dim == dim and not force_projection:
            self.input_proj = None
        else:
            self.input_proj = Conv1d(input_dim, dim, 1, bias=False)
        if output_dim == dim and not force_projection:
            self.output_proj = None
        else:
            self.output_proj = Conv1d(dim, output_dim, 1, bias=False)
        self.vq = ResidualVectorQuantization(
            nq=nq,
            dim=dim,
            codebook_size=bins,
            codebook_dim=None,
        )

    def encode(self, xs: mx.array) -> mx.array:
        if self.input_proj is not None:
            xs = self.input_proj(xs)
        return self.vq.encode(xs).swapaxes(0, 1)

    def decode(self, xs: mx.array) -> mx.array:
        xs = xs.swapaxes(0, 1)
        quantized = self.vq.decode(xs)
        if self.output_proj is not None:
            quantized = self.output_proj(quantized)
        return quantized


class SplitResidualVectorQuantizer(nn.Module):
    def __init__(
        self,
        dim: int,
        input_dim: int | None,
        output_dim: int | None,
        nq: int,
        bins: int,
    ):
        super().__init__()
        self._nq = nq
        self.rvq_first = ResidualVectorQuantizer(
            dim=dim,
            input_dim=input_dim,
            output_dim=output_dim,
            nq=1,
            bins=bins,
            force_projection=True,
        )
        self.rvq_rest = ResidualVectorQuantizer(
            dim=dim,
            input_dim=input_dim,
            output_dim=output_dim,
            nq=nq - 1,
            bins=bins,
            force_projection=True,
        )

    def encode(self, xs: mx.array) -> mx.array:
        codes = self.rvq_first.encode(xs)
        if self._nq > 1:
            rest_codes = self.rvq_rest.encode(xs)
            codes = mx.concat([codes, rest_codes], axis=1)
        return codes

    def decode(self, xs: mx.array) -> mx.array:
        quantized = self.rvq_first.decode(xs[:, :1])
        if self._nq > 1:
            quantized = quantized + self.rvq_rest.decode(xs[:, 1:])
        return quantized



================================================
FILE: mlx_audio/codec/models/mimi/modules/seanet.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from .conv import StreamableConv1d, StreamableConvTranspose1d


@dataclass
class SeanetConfig:
    dimension: int
    channels: int
    causal: bool
    nfilters: int
    nresidual_layers: int
    ratios: list[int]
    ksize: int
    residual_ksize: int
    last_ksize: int
    dilation_base: int
    pad_mode: str
    true_skip: bool
    compress: int


class StreamingAdd(nn.Module):
    def __init__(self):
        super().__init__()
        self._lhs = None
        self._rhs = None

    def step(self, lhs: mx.array, rhs: mx.array) -> mx.array:
        if self._lhs is not None:
            lhs = mx.concat([self._lhs, lhs], axis=-1)
            self._lhs = None
        if self._rhs is not None:
            rhs = mx.concat([self._rhs, rhs], axis=-1)
            self._rhs = None
        lhs_l = lhs.shape[-1]
        rhs_l = rhs.shape[-1]
        if lhs_l == rhs_l:
            return lhs + rhs
        elif lhs_l < rhs_l:
            self._rhs = rhs[..., lhs_l:]
            return lhs + rhs[..., :lhs_l]
        else:
            self._lhs = lhs[..., rhs_l:]
            return lhs[..., :rhs_l] + rhs


class SeanetResnetBlock(nn.Module):
    def __init__(self, cfg: SeanetConfig, dim: int, ksizes_and_dilations: list):
        super().__init__()
        block = []
        hidden = dim // cfg.compress
        for i, (ksize, dilation) in enumerate(ksizes_and_dilations):
            in_channels = dim if i == 0 else hidden
            out_channels = dim if i == len(ksizes_and_dilations) - 1 else hidden
            c = StreamableConv1d(
                in_channels=in_channels,
                out_channels=out_channels,
                ksize=ksize,
                stride=1,
                dilation=dilation,
                groups=1,
                bias=True,
                causal=cfg.causal,
                pad_mode=cfg.pad_mode,
            )
            block.append(c)
        self.block = block
        self.streaming_add = StreamingAdd()

        if cfg.true_skip:
            self.shortcut = None
        else:
            self.shortcut = StreamableConv1d(
                in_channels=dim,
                out_channels=dim,
                ksize=1,
                stride=1,
                dilation=1,
                groups=1,
                bias=True,
                causal=cfg.causal,
                pad_mode=cfg.pad_mode,
            )

    def reset_state(self):
        if self.shortcut is not None:
            self.shortcut.reset_state()
        for b in self.block:
            b.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        residual = xs
        for b in self.block:
            xs = b(nn.elu(xs, alpha=1.0))
        if self.shortcut is None:
            xs = xs + residual
        else:
            xs = xs + self.shortcut(residual)
        return xs

    def step(self, xs: mx.array) -> mx.array:
        residual = xs
        for b in self.block:
            xs = b.step(nn.elu(xs, alpha=1.0))
        if self.shortcut is None:
            xs = self.streaming_add.step(xs, residual)
        else:
            xs = self.streaming_add.step(xs, self.shortcut.step(residual))
        return xs


class EncoderLayer(nn.Module):
    def __init__(self, cfg: SeanetConfig, ratio: int, mult: int):
        super().__init__()
        residuals = []
        dilation = 1
        for _ in range(cfg.nresidual_layers):
            b = SeanetResnetBlock(
                cfg,
                dim=mult * cfg.nfilters,
                ksizes_and_dilations=[(cfg.residual_ksize, dilation), (1, 1)],
            )
            residuals.append(b)
            dilation *= cfg.dilation_base
        self.residuals = residuals
        self.downsample = StreamableConv1d(
            in_channels=mult * cfg.nfilters,
            out_channels=mult * cfg.nfilters * 2,
            ksize=ratio * 2,
            stride=ratio,
            dilation=1,
            groups=1,
            bias=True,
            causal=True,
            pad_mode=cfg.pad_mode,
        )

    def reset_state(self):
        self.downsample.reset_state()
        for r in self.residuals:
            r.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        for r in self.residuals:
            xs = r(xs)
        return self.downsample(nn.elu(xs, alpha=1.0))

    def step(self, xs: mx.array) -> mx.array:
        for r in self.residuals:
            xs = r.step(xs)
        return self.downsample.step(nn.elu(xs, alpha=1.0))


class SeanetEncoder(nn.Module):
    def __init__(self, cfg: SeanetConfig):
        super().__init__()
        mult = 1
        self.init_conv1d = StreamableConv1d(
            in_channels=cfg.channels,
            out_channels=mult * cfg.nfilters,
            ksize=cfg.ksize,
            stride=1,
            dilation=1,
            groups=1,
            bias=True,
            causal=cfg.causal,
            pad_mode=cfg.pad_mode,
        )
        layers = []
        for ratio in reversed(cfg.ratios):
            layers.append(EncoderLayer(cfg, ratio=ratio, mult=mult))
            mult *= 2
        self.layers = layers
        self.final_conv1d = StreamableConv1d(
            in_channels=mult * cfg.nfilters,
            out_channels=cfg.dimension,
            ksize=cfg.last_ksize,
            stride=1,
            dilation=1,
            groups=1,
            bias=True,
            causal=cfg.causal,
            pad_mode=cfg.pad_mode,
        )

    def reset_state(self):
        self.init_conv1d.reset_state()
        self.final_conv1d.reset_state()
        for layer in self.layers:
            layer.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        xs = self.init_conv1d(xs)
        for layer in self.layers:
            xs = layer(xs)
        xs = nn.elu(xs, alpha=1.0)
        return self.final_conv1d(xs)

    def step(self, xs: mx.array) -> mx.array:
        xs = self.init_conv1d.step(xs)
        for layer in self.layers:
            xs = layer.step(xs)
        xs = nn.elu(xs, alpha=1.0)
        return self.final_conv1d.step(xs)


class DecoderLayer(nn.Module):
    def __init__(self, cfg: SeanetConfig, ratio: int, mult: int):
        super().__init__()
        self.upsample = StreamableConvTranspose1d(
            in_channels=mult * cfg.nfilters,
            out_channels=mult * cfg.nfilters // 2,
            ksize=ratio * 2,
            stride=ratio,
            groups=1,
            bias=True,
            causal=cfg.causal,
        )
        residuals = []
        dilation = 1
        for _ in range(cfg.nresidual_layers):
            r = SeanetResnetBlock(
                cfg,
                dim=mult * cfg.nfilters // 2,
                ksizes_and_dilations=[(cfg.residual_ksize, dilation), (1, 1)],
            )
            residuals.append(r)
            dilation *= cfg.dilation_base
        self.residuals = residuals

    def reset_state(self):
        self.upsample.reset_state()
        for r in self.residuals:
            r.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        xs = self.upsample(nn.elu(xs, alpha=1.0))
        for r in self.residuals:
            xs = r(xs)
        return xs

    def step(self, xs: mx.array) -> mx.array:
        xs = self.upsample.step(nn.elu(xs, alpha=1.0))
        for r in self.residuals:
            xs = r.step(xs)
        return xs


class SeanetDecoder(nn.Module):
    def __init__(self, cfg: SeanetConfig):
        super().__init__()
        mult = 1 << len(cfg.ratios)
        self.init_conv1d = StreamableConv1d(
            in_channels=cfg.dimension,
            out_channels=mult * cfg.nfilters,
            ksize=cfg.ksize,
            stride=1,
            dilation=1,
            groups=1,
            bias=True,
            causal=cfg.causal,
            pad_mode=cfg.pad_mode,
        )
        layers = []
        for ratio in cfg.ratios:
            layers.append(DecoderLayer(cfg, ratio=ratio, mult=mult))
            mult //= 2
        self.layers = layers
        self.final_conv1d = StreamableConv1d(
            in_channels=cfg.nfilters,
            out_channels=cfg.channels,
            ksize=cfg.last_ksize,
            stride=1,
            dilation=1,
            groups=1,
            bias=True,
            causal=cfg.causal,
            pad_mode=cfg.pad_mode,
        )

    def reset_state(self):
        self.init_conv1d.reset_state()
        self.final_conv1d.reset_state()
        for layer in self.layers:
            layer.reset_state()

    def __call__(self, xs: mx.array) -> mx.array:
        xs = self.init_conv1d(xs)
        for layer in self.layers:
            xs = layer(xs)
        xs = nn.elu(xs, alpha=1.0)
        return self.final_conv1d(xs)

    def step(self, xs: mx.array) -> mx.array:
        xs = self.init_conv1d.step(xs)
        for layer in self.layers:
            xs = layer.step(xs)
        xs = nn.elu(xs, alpha=1.0)
        return self.final_conv1d.step(xs)


class Seanet(nn.Module):
    def __init__(self, cfg: SeanetConfig):
        super().__init__()
        self.encoder = SeanetEncoder(cfg)
        self.decoder = SeanetDecoder(cfg)



================================================
FILE: mlx_audio/codec/models/mimi/modules/transformer.py
================================================
# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from __future__ import annotations

from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from .kv_cache import KVCache, RotatingKVCache


@dataclass
class TransformerConfig:
    d_model: int
    num_heads: int
    num_layers: int
    causal: bool
    norm_first: bool
    bias_ff: bool
    bias_attn: bool
    layer_scale: float | None
    positional_embedding: str
    use_conv_block: bool
    cross_attention: bool
    conv_kernel_size: int
    use_conv_bias: bool
    gating: bool
    norm: str
    context: int
    max_period: int
    max_seq_len: int
    kv_repeat: int
    dim_feedforward: int
    conv_layout: bool

    @property
    def head_dim(self) -> int:
        return self.d_model // self.num_heads


class Id(nn.Module):
    def __init__(self):
        super().__init__()

    def __call__(self, xs: mx.array) -> mx.array:
        return xs


class LayerScale(nn.Module):
    def __init__(self, dim: int):
        super().__init__()

        self.scale = mx.ones(dim)

    def __call__(self, xs: mx.array) -> mx.array:
        return xs * self.scale


class Attention(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()

        num_kv = cfg.num_heads // cfg.kv_repeat
        out_dim = cfg.d_model + 2 * num_kv * cfg.d_model // cfg.num_heads
        self.cfg = cfg
        self.in_proj = nn.Linear(cfg.d_model, out_dim, bias=cfg.bias_attn)
        self.out_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.bias_attn)
        self.scale = cfg.head_dim ** (-0.5)
        self.rope = None
        if cfg.positional_embedding == "rope":
            self.rope = nn.RoPE(cfg.head_dim, traditional=True, base=cfg.max_period)

    def __call__(
        self,
        xs: mx.array,
        cache: KVCache | RotatingKVCache,
        mask: mx.array | None = None,
    ) -> mx.array:
        assert self.cfg.kv_repeat == 1, "only kv_repeat==1 is supported"

        b, t, hd = xs.shape
        qkv = self.in_proj(xs).reshape(b, t, 3, self.cfg.num_heads, self.cfg.head_dim)
        q = qkv[:, :, 0].transpose(0, 2, 1, 3)
        k = qkv[:, :, 1].transpose(0, 2, 1, 3)
        v = qkv[:, :, 2].transpose(0, 2, 1, 3)
        if self.rope is not None:
            q = self.rope(q, offset=cache.offset)
            k = self.rope(k, offset=cache.offset)

        k, v = cache.update_and_fetch(k, v)
        k_len = k.shape[2]
        k_target_len = t + min(self.cfg.context, k_len - t)
        if k_target_len < k_len:
            k = k[:, :, k_len - k_target_len :]
            v = v[:, :, k_len - k_target_len :]

        xs = mx.fast.scaled_dot_product_attention(q, k, v, scale=self.scale, mask=mask)
        xs = xs.transpose(0, 2, 1, 3).reshape(b, t, hd)
        xs = self.out_proj(xs)
        return xs


class MlpGating(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()

        hidden = 2 * cfg.dim_feedforward // 3
        if cfg.dim_feedforward == 4 * cfg.d_model:
            hidden = 11 * cfg.d_model // 4

        self.linear_in = nn.Linear(cfg.d_model, 2 * hidden, bias=cfg.bias_ff)
        self.linear_out = nn.Linear(hidden, cfg.d_model, bias=cfg.bias_ff)

    def __call__(self, xs: mx.array) -> mx.array:
        xs = self.linear_in(xs)
        b, t, _ = xs.shape
        xs = xs.reshape(b, t, 2, -1)
        return self.linear_out(nn.silu(xs[:, :, 0]) * xs[:, :, 1])


class MlpNoGating(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()

        self.linear1 = nn.Linear(cfg.d_model, cfg.dim_feedforward, bias=cfg.bias_ff)
        self.linear2 = nn.Linear(cfg.dim_feedforward, cfg.d_model, bias=cfg.bias_ff)

    def __call__(self, xs: mx.array) -> mx.array:
        return self.linear2(nn.gelu_approx(self.linear1(xs)))


class TransformerLayer(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()

        assert not cfg.use_conv_block, "conv-block is not supported"
        assert not cfg.cross_attention, "cross-attn is not supported"
        if cfg.gating:
            self.gating = MlpGating(cfg)
        else:
            # TODO: Use a better name?
            self.gating = MlpNoGating(cfg)

        if cfg.norm == "layer_norm":
            self.norm1 = nn.LayerNorm(cfg.d_model, 1e-5)
            self.norm2 = nn.LayerNorm(cfg.d_model, 1e-5)
        elif cfg.norm == "rms_norm":
            self.norm1 = nn.RMSNorm(cfg.d_model, 1e-8)
            self.norm2 = nn.RMSNorm(cfg.d_model, 1e-8)
        else:
            raise ValueError(f"unsupported norm type {cfg.norm}")

        if cfg.layer_scale is not None:
            self.layer_scale_1 = LayerScale(cfg.d_model)
            self.layer_scale_2 = LayerScale(cfg.d_model)
        else:
            self.layer_scale_1 = Id()
            self.layer_scale_2 = Id()
        self.self_attn = Attention(cfg)

    def __call__(
        self,
        xs: mx.array,
        cache: KVCache | RotatingKVCache,
    ) -> mx.array:
        n1 = self.norm1(xs)
        n1 = self.self_attn(n1, cache=cache)
        xs = xs + self.layer_scale_1(n1)
        xs = xs + self.layer_scale_2(self.gating(self.norm2(xs)))
        return xs


class Transformer(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()

        self.cfg = cfg
        self.layers = [TransformerLayer(cfg=cfg) for _ in range(cfg.num_layers)]

    def __call__(
        self,
        xs: mx.array,
        cache: list[KVCache] | list[RotatingKVCache],
    ) -> mx.array:
        for layer, c in zip(self.layers, cache):
            xs = layer(xs, cache=c)
        return xs

    def make_cache(self) -> list[KVCache]:
        num_kv_heads = self.cfg.num_heads // self.cfg.kv_repeat
        return [
            KVCache(head_dim=self.cfg.head_dim, n_kv_heads=num_kv_heads)
            for _ in self.layers
        ]

    def make_rot_cache(self) -> list[RotatingKVCache]:
        num_kv_heads = self.cfg.num_heads // self.cfg.kv_repeat
        return [
            RotatingKVCache(
                head_dim=self.cfg.head_dim,
                n_kv_heads=num_kv_heads,
                max_size=self.cfg.max_seq_len,
            )
            for _ in self.layers
        ]


class ProjectedTransformer(nn.Module):
    def __init__(self, cfg: TransformerConfig, input_dim: int, output_dims: list[int]):
        super().__init__()

        self.conv_layout = cfg.conv_layout
        self.transformer = Transformer(cfg)
        if input_dim == cfg.d_model:
            self.input_proj = None
        else:
            self.input_proj = nn.Linear(input_dim, cfg.d_model, bias=False)

        output_projs = []
        for output_dim in output_dims:
            if output_dim == cfg.d_model:
                p = None
            else:
                p = nn.Linear(cfg.d_model, output_dim, bias=False)
            output_projs.append(p)
        self.output_projs = output_projs

    def __call__(
        self,
        xs: mx.array,
        cache: list[KVCache] | list[RotatingKVCache],
    ) -> list[mx.array]:
        if self.conv_layout:
            xs = xs.swapaxes(1, 2)
        if self.input_proj is not None:
            xs = self.input_proj(xs)
        xs = self.transformer(xs, cache=cache)
        outs = []
        for output_proj in self.output_projs:
            if output_proj is None:
                out = xs
            else:
                out = output_proj(xs)
            if self.conv_layout:
                out = out.swapaxes(1, 2)
            outs.append(out)
        return outs

    def make_cache(self) -> list[KVCache]:
        return self.transformer.make_cache()

    def make_rot_cache(self) -> list[RotatingKVCache]:
        return self.transformer.make_rot_cache()



================================================
FILE: mlx_audio/codec/models/s3/__init__.py
================================================
from .model_v2 import S3TokenizerV2



================================================
FILE: mlx_audio/codec/models/s3/model.py
================================================
from dataclasses import dataclass
from typing import Iterable, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from einops.array_api import rearrange

from .utils import make_non_pad_mask, mask_to_bias


@dataclass
class ModelConfig:
    n_mels: int = 128
    n_audio_ctx: int = 1500
    n_audio_state: int = 1280
    n_audio_head: int = 20
    n_audio_layer: int = 6
    n_codebook_size: int = 4096


def sinusoids(length: int, channels: int, max_timescale: float = 10000) -> mx.array:
    """Returns sinusoids for positional embedding"""
    assert channels % 2 == 0
    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)
    inv_timescales = mx.exp(-log_timescale_increment * mx.arange(channels // 2))
    scaled_time = mx.arange(length)[:, None] * inv_timescales[None, :]
    return mx.concatenate([mx.sin(scaled_time), mx.cos(scaled_time)], axis=1)


class MultiHeadAttention(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()
        self.n_head = n_head
        self.query = nn.Linear(n_state, n_state)
        self.key = nn.Linear(n_state, n_state, bias=False)
        self.value = nn.Linear(n_state, n_state)
        self.out = nn.Linear(n_state, n_state)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
    ) -> Tuple[mx.array, mx.array]:
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        wv, qk = self.qkv_attention(q, k, v, mask)
        return self.out(wv), qk

    def qkv_attention(
        self, q: mx.array, k: mx.array, v: mx.array, mask: Optional[mx.array] = None
    ) -> Tuple[mx.array, mx.array | None]:
        B, T, D = q.shape
        scale = (D // self.n_head) ** -0.25

        q = q.reshape(B, T, self.n_head, -1).transpose(0, 2, 1, 3) * scale
        k = k.reshape(B, T, self.n_head, -1).transpose(0, 2, 1, 3) * scale
        v = v.reshape(B, T, self.n_head, -1).transpose(0, 2, 1, 3)

        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=1, mask=mask)
        output = output.transpose(0, 2, 1, 3).reshape(B, T, D)
        return output, None


class ResidualAttentionBlock(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()

        self.attn = MultiHeadAttention(n_state, n_head)
        self.attn_ln = nn.LayerNorm(n_state)

        n_mlp = n_state * 4
        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_mlp), nn.GELU(), nn.Linear(n_mlp, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        x = x + self.attn(self.attn_ln(x), mask=mask)[0]
        x = x + self.mlp(self.mlp_ln(x))
        return x


class AudioEncoder(nn.Module):
    def __init__(
        self,
        n_mels: int,
        n_ctx: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        stride: int,
    ):
        super().__init__()
        self.stride = stride

        self.conv1 = nn.Conv1d(
            in_channels=n_mels,
            out_channels=n_state,
            kernel_size=3,
            stride=stride,
            padding=1,
        )
        self.conv2 = nn.Conv1d(
            in_channels=n_state,
            out_channels=n_state,
            kernel_size=3,
            stride=2,
            padding=1,
        )

        self.positional_embedding = sinusoids(n_ctx, n_state)

        self.blocks = [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]

    def __call__(self, x: mx.array, x_len: mx.array) -> Tuple[mx.array, mx.array]:
        """
        x : mx.array, shape = (batch_size, n_mels, T)
            the mel spectrogram of the audio
        x_len: mx.array, shape = (batch_size,)
            length of each audio in x
        """
        mask = make_non_pad_mask(x_len)
        mask = mx.expand_dims(mask, axis=1)  # (B, 1, T)

        x = x.transpose(0, 2, 1)  # (B, T, n_mels)
        mask_transposed = mask.transpose(0, 2, 1)  # (B, T, 1)

        x = self.conv1(x * mask_transposed)
        x = nn.gelu(x)
        x_len = (x_len + 2 - 1 * (3 - 1) - 1) // self.stride + 1

        mask = make_non_pad_mask(x_len)
        mask_transposed = mx.expand_dims(mask, axis=-1)  # (B, T, 1)
        x = self.conv2(x * mask_transposed)
        x = nn.gelu(x)

        x_len = (x_len + 2 - 1 * (3 - 1) - 1) // 2 + 1

        mask = make_non_pad_mask(x_len)
        mask = mask_to_bias(mask, x.dtype)
        mask = mx.expand_dims(mask, axis=1)  # (B, 1, T)

        x = x + self.positional_embedding[: x.shape[1], :]

        for block in self.blocks:
            x = block(x, mx.expand_dims(mask, axis=1))

        return x, x_len


class EuclideanCodebook(nn.Module):
    """Codebook with Euclidean distance.
    Args:
        dim (int): Dimension.
        codebook_size (int): Codebook size.
    """

    def __init__(self, dim: int, codebook_size: int):
        super().__init__()
        self.codebook_size = codebook_size
        self.embed = mx.zeros((codebook_size, dim))

    def preprocess(self, x: mx.array) -> mx.array:
        x = rearrange(x, "... d -> (...) d")
        return x

    def quantize(self, x: mx.array) -> mx.array:
        embed = self.embed.T
        dist = -(
            mx.sum(x.astype(mx.float32) ** 2, axis=1, keepdims=True)
            - 2 * x @ embed
            + mx.sum(embed.astype(mx.float32) ** 2, axis=0, keepdims=True)
        )
        embed_ind = mx.argmax(dist, axis=-1)
        return embed_ind

    def postprocess_emb(self, embed_ind: mx.array, shape: tuple) -> mx.array:
        return embed_ind.reshape(*shape[:-1])

    def dequantize(self, embed_ind: mx.array) -> mx.array:
        quantize = self.embed[embed_ind]
        return quantize

    def encode(self, x: mx.array) -> mx.array:
        shape = x.shape
        # pre-process
        x = self.preprocess(x)
        # quantize
        embed_ind = self.quantize(x)
        # post-process
        embed_ind = self.postprocess_emb(embed_ind, shape)
        return embed_ind

    def decode(self, embed_ind: mx.array) -> mx.array:
        quantize = self.dequantize(embed_ind)
        return quantize


class VectorQuantization(nn.Module):
    """Vector quantization implementation
    Args:
        dim (int): Dimension
        codebook_size (int): Codebook size
    """

    def __init__(self, dim: int, codebook_size: int):
        super().__init__()
        self._codebook = EuclideanCodebook(dim=dim, codebook_size=codebook_size)
        self.codebook_size = codebook_size

    @property
    def codebook(self):
        return self._codebook.embed

    def encode(self, x: mx.array) -> mx.array:
        x = x / mx.sqrt(mx.sum(x**2, axis=-1, keepdims=True) + 1e-8)
        embed_in = self._codebook.encode(x)
        return embed_in

    def decode(self, embed_ind: mx.array) -> mx.array:
        quantize = self._codebook.decode(embed_ind)
        quantize = rearrange(quantize, "b n d -> b d n")
        return quantize


class S3Tokenizer(nn.Module):
    """S3 tokenizer implementation
    Args:
        config (ModelConfig): Config
    """

    def __init__(self, name: str, config: ModelConfig = ModelConfig()):
        super().__init__()
        self.config = config
        self.encoder = AudioEncoder(
            self.config.n_mels,
            self.config.n_audio_ctx,
            self.config.n_audio_state,
            self.config.n_audio_head,
            self.config.n_audio_layer,
            2 if name == "speech_tokenizer_v1_25hz" else 1,
        )
        self.quantizer = VectorQuantization(
            self.config.n_audio_state, self.config.n_codebook_size
        )

    def __call__(self, mel: mx.array, mel_len: mx.array) -> Tuple[mx.array, mx.array]:
        return self.quantize(mel, mel_len)

    def quantize(self, mel: mx.array, mel_len: mx.array) -> Tuple[mx.array, mx.array]:
        hidden, code_len = self.encoder(mel, mel_len)
        code = self.quantizer.encode(hidden)
        return code, code_len



================================================
FILE: mlx_audio/codec/models/s3/model_v2.py
================================================
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from einops.array_api import rearrange
from huggingface_hub import snapshot_download

from .model import MultiHeadAttention
from .utils import make_non_pad_mask, mask_to_bias


@dataclass
class ModelConfig:
    n_mels: int = 128
    n_audio_ctx: int = 1500
    n_audio_state: int = 1280
    n_audio_head: int = 20
    n_audio_layer: int = 6
    n_codebook_size: int = 3**8


def precompute_freqs_cis(
    dim: int, end: int, theta: float = 10000.0, scaling: Optional[float] = None
) -> mx.array:
    """Precompute frequency tensor for rotary embeddings"""
    freqs = 1.0 / (
        theta ** (mx.arange(0, dim, 2)[: (dim // 2)].astype(mx.float32) / dim)
    )
    t = mx.arange(end)
    if scaling is not None:
        t = t * scaling
    freqs = mx.outer(t, freqs).astype(mx.float32)
    cos_freqs = mx.cos(freqs)
    sin_freqs = mx.sin(freqs)
    cos_freqs = mx.concatenate([cos_freqs, cos_freqs], axis=-1)
    sin_freqs = mx.concatenate([sin_freqs, sin_freqs], axis=-1)
    return cos_freqs, sin_freqs


def apply_rotary_emb(
    xq: mx.array,
    xk: mx.array,
    cos: mx.array,
    sin: mx.array,
) -> Tuple[mx.array, mx.array]:
    """Apply rotary embeddings to query and key tensors"""
    # Expand dimensions for broadcasting
    cos = mx.expand_dims(mx.expand_dims(cos, axis=0), axis=2)
    sin = mx.expand_dims(mx.expand_dims(sin, axis=0), axis=2)

    D = xq.shape[-1]
    # Split and rotate
    xq_half_l, xq_half_r = xq[..., : D // 2], xq[..., D // 2 :]
    xq_rotated = mx.concatenate([-xq_half_r, xq_half_l], axis=-1)

    xk_half_l, xk_half_r = xk[..., : D // 2], xk[..., D // 2 :]
    xk_rotated = mx.concatenate([-xk_half_r, xk_half_l], axis=-1)

    # Apply rotation
    xq_out = xq * cos + xq_rotated * sin
    xk_out = xk * cos + xk_rotated * sin

    return xq_out, xk_out


class FSQCodebook(nn.Module):
    """Finite Scalar Quantization Codebook"""

    def __init__(self, dim: int, level: int = 3):
        super().__init__()
        self.project_down = nn.Linear(dim, 8)
        self.level = level
        self.embed = None

    def preprocess(self, x: mx.array) -> mx.array:
        x = rearrange(x, "... d -> (...) d")
        return x

    def encode(self, x: mx.array) -> mx.array:
        x_shape = x.shape
        # pre-process
        x = self.preprocess(x)
        # quantize
        h = self.project_down(x).astype(mx.float32)
        h = mx.tanh(h)
        h = h * 0.9990000128746033
        h = mx.round(h) + 1

        # Create powers for base conversion
        powers = mx.power(self.level, mx.arange(2**self.level, dtype=h.dtype))
        mu = mx.sum(h * mx.expand_dims(powers, axis=0), axis=-1)
        ind = mu.reshape(x_shape[0], x_shape[1]).astype(mx.int32)
        return ind

    def decode(self, embed_ind: mx.array) -> mx.array:
        raise NotImplementedError("There is no official up project component provided")


class FSQVectorQuantization(nn.Module):
    """Finite Scalar Quantization Vector Quantization"""

    def __init__(
        self,
        dim: int,
        codebook_size: int,
    ):
        super().__init__()
        assert 3**8 == codebook_size
        self.fsq_codebook = FSQCodebook(dim=dim, level=3)
        self.codebook_size = codebook_size

    @property
    def codebook(self):
        return self.fsq_codebook.embed

    def encode(self, x: mx.array) -> mx.array:
        return self.fsq_codebook.encode(x)

    def decode(self, embed_ind: mx.array) -> mx.array:
        quantize = self.fsq_codebook.decode(embed_ind)
        quantize = rearrange(quantize, "b n d -> b d n")
        return quantize


class FSMNMultiHeadAttention(MultiHeadAttention):
    """Multi-head attention with FSMN (Feedforward Sequential Memory Network)"""

    def __init__(
        self,
        n_state: int,
        n_head: int,
        kernel_size: int = 31,
    ):
        super().__init__(n_state, n_head)

        self.fsmn_block = nn.Conv1d(
            in_channels=n_state,
            out_channels=n_state,
            kernel_size=kernel_size,
            stride=1,
            padding=0,
            groups=n_state,
            bias=False,
        )
        self.left_padding = (kernel_size - 1) // 2
        self.right_padding = kernel_size - 1 - self.left_padding

    def forward_fsmn(
        self, inputs: mx.array, mask: Optional[mx.array] = None
    ) -> mx.array:
        b, t, n, d = inputs.shape
        inputs = inputs.reshape(b, t, -1)

        if mask is not None and mask.shape[2] > 0:
            inputs = inputs * mask

        pad_left = mx.zeros((b, self.left_padding, inputs.shape[2]), dtype=inputs.dtype)
        pad_right = mx.zeros(
            (b, self.right_padding, inputs.shape[2]), dtype=inputs.dtype
        )
        x_padded = mx.concatenate([pad_left, inputs, pad_right], axis=1)
        x = self.fsmn_block(x_padded)
        x = x + inputs

        if mask is not None:
            x = x * mask

        return x

    def qkv_attention(
        self,
        q: mx.array,
        k: mx.array,
        v: mx.array,
        mask: Optional[mx.array] = None,
        mask_pad: Optional[mx.array] = None,
        freqs_cis: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> Tuple[mx.array, mx.array | None, mx.array]:
        B, T, D = q.shape
        scale = (D // self.n_head) ** -0.25

        q = q.reshape(B, T, self.n_head, -1)
        k = k.reshape(B, T, self.n_head, -1)
        v = v.reshape(B, T, self.n_head, -1)

        if freqs_cis is not None:
            cos, sin = freqs_cis
            q, k = apply_rotary_emb(q, k, cos[:T], sin[:T])

        fsm_memory = self.forward_fsmn(v, mask_pad)

        q = q.transpose(0, 2, 1, 3) * scale
        k = k.transpose(0, 2, 1, 3) * scale
        v = v.transpose(0, 2, 1, 3)

        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=1, mask=mask)
        output = output.transpose(0, 2, 1, 3).reshape(B, T, D)

        return output, None, fsm_memory

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        mask_pad: Optional[mx.array] = None,
        freqs_cis: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> Tuple[mx.array, mx.array | None]:
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        wv, qk, fsm_memory = self.qkv_attention(q, k, v, mask, mask_pad, freqs_cis)
        return self.out(wv) + fsm_memory, qk


class ResidualAttentionBlock(nn.Module):
    """Residual attention block with FSMN"""

    def __init__(
        self,
        n_state: int,
        n_head: int,
        kernel_size: int = 31,
    ):
        super().__init__()

        self.attn = FSMNMultiHeadAttention(n_state, n_head, kernel_size)
        self.attn_ln = nn.LayerNorm(n_state, eps=1e-6)

        n_mlp = n_state * 4
        self.mlp = nn.Sequential(
            nn.Linear(n_state, n_mlp), nn.GELU(), nn.Linear(n_mlp, n_state)
        )
        self.mlp_ln = nn.LayerNorm(n_state)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        mask_pad: Optional[mx.array] = None,
        freqs_cis: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        x = (
            x
            + self.attn(
                self.attn_ln(x), mask=mask, mask_pad=mask_pad, freqs_cis=freqs_cis
            )[0]
        )

        x = x + self.mlp(self.mlp_ln(x))
        return x


class AudioEncoderV2(nn.Module):
    def __init__(
        self,
        n_mels: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        stride: int,
    ):
        super().__init__()
        self.stride = stride

        self.conv1 = nn.Conv1d(
            in_channels=n_mels,
            out_channels=n_state,
            kernel_size=3,
            stride=stride,
            padding=1,
        )
        self.conv2 = nn.Conv1d(
            in_channels=n_state,
            out_channels=n_state,
            kernel_size=3,
            stride=2,
            padding=1,
        )

        self._freqs_cis = precompute_freqs_cis(64, 1024 * 2)

        self.blocks = [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]

    def __call__(self, x: mx.array, x_len: mx.array) -> Tuple[mx.array, mx.array]:
        """
        x : mx.array, shape = (batch_size, n_mels, T)
            the mel spectrogram of the audio
        x_len: mx.array, shape = (batch_size,)
            length of each audio in x
        """
        mask = make_non_pad_mask(x_len)
        mask = mx.expand_dims(mask, axis=1)  # (B, 1, T)

        x = x.transpose(0, 2, 1)  # (B, T, n_mels)
        mask_transposed = mask.transpose(0, 2, 1)  # (B, T, 1)

        x = self.conv1(x * mask_transposed)
        x = nn.gelu(x)
        x_len = (x_len + 2 - 1 * (3 - 1) - 1) // self.stride + 1

        mask = make_non_pad_mask(x_len)
        mask_transposed = mx.expand_dims(mask, axis=-1)  # (B, T, 1)

        x = self.conv2(x * mask_transposed)
        x = nn.gelu(x)
        x_len = (x_len + 2 - 1 * (3 - 1) - 1) // 2 + 1

        mask = make_non_pad_mask(x_len)
        mask_pad = mx.expand_dims(mask, axis=-1)  # (B, T, 1)
        mask = mask_to_bias(mask, x.dtype)
        mask = mx.expand_dims(mask, axis=1)  # (B, 1, T)

        for block in self.blocks:
            x = block(x, mask, mask_pad, self._freqs_cis)

        return x, x_len


class S3TokenizerV2(nn.Module):
    """S3 tokenizer v2 implementation.
    Args:
        config (ModelConfig): Config
    """

    def __init__(self, name: str, config: ModelConfig = ModelConfig()):
        super().__init__()
        if "v1" not in name:
            assert "v2" in name
            config.n_codebook_size = 3**8
        self.config = config
        self.encoder = AudioEncoderV2(
            self.config.n_mels,
            self.config.n_audio_state,
            self.config.n_audio_head,
            self.config.n_audio_layer,
            2,
        )
        self.quantizer = FSQVectorQuantization(
            self.config.n_audio_state,
            self.config.n_codebook_size,
        )

    def __call__(self, mel: mx.array, mel_len: mx.array) -> Tuple[mx.array, mx.array]:
        return self.quantize(mel, mel_len)

    def quantize(self, mel: mx.array, mel_len: mx.array) -> Tuple[mx.array, mx.array]:
        hidden, code_len = self.encoder(mel, mel_len)
        code = self.quantizer.encode(hidden)
        return code, code_len

    @classmethod
    def from_pretrained(
        cls,
        name: str = "speech_tokenizer_v2_25hz",
        repo_id: str = "mlx-community/CosyVoice2-0.5B-S3Tokenizer",
    ) -> "S3TokenizerV2":
        path = fetch_from_hub(repo_id)
        if path is None:
            raise ValueError(f"Could not find model {path}")

        model = S3TokenizerV2(name)
        model_path = path / f"{name}.safetensors"
        weights = mx.load(model_path.as_posix(), format="safetensors")
        model.load_weights(list(weights.items()))
        mx.eval(model.parameters())

        return model


# fetch model from hub


def fetch_from_hub(hf_repo: str) -> Path:
    model_path = Path(
        snapshot_download(
            repo_id=hf_repo,
            allow_patterns=["*.safetensors", "*.json"],
        )
    )
    return model_path



================================================
FILE: mlx_audio/codec/models/s3/utils.py
================================================
from typing import List

import mlx.core as mx

from mlx_audio.utils import hanning, mel_filters, stft


def log_mel_spectrogram(
    audio: mx.array,
    sample_rate: int = 16_000,
    n_mels: int = 128,
    n_fft: int = 400,
    hop_length: int = 160,
    padding: int = 0,
):
    if not isinstance(audio, mx.array):
        audio = mx.array(audio)

    if padding > 0:
        audio = mx.pad(audio, (0, padding))

    window = hanning(n_fft + 1)[:-1]
    freqs = stft(
        audio,
        window=window,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=n_fft,
    ).swapaxes(0, 1)
    magnitudes = freqs.abs() ** 2
    filters = mel_filters(
        sample_rate=sample_rate,
        n_fft=n_fft,
        n_mels=n_mels,
        norm="slaney",
        mel_scale="slaney",
    )
    mel_spec = filters @ magnitudes
    log_spec = mx.maximum(mel_spec, 1e-10).log10()
    log_spec = mx.maximum(log_spec, log_spec.max() - 8.0)
    log_spec = (log_spec + 4.0) / 4.0
    return log_spec


def make_non_pad_mask(lengths: mx.array, max_len: int = 0) -> mx.array:
    """Make mask tensor containing indices of non-padded part.

    The sequences in a batch may have different lengths. To enable
    batch computing, padding is need to make all sequence in same
    size. To avoid the padding part pass value to context dependent
    block such as attention or convolution, this padding part is
    masked.

    1 for non-padded part and 0 for padded part.

    Parameters
    ----------
        lengths (mx.array): Batch of lengths (B,).
        max_len (int): Maximum length. If 0, use the maximum length in batch.

    Returns:
    -------
        mx.array: Mask tensor containing indices of padded part (B, max_T).

    Examples:
        >>> import mlx.core as mx
        >>> lengths = mx.array([5, 3, 2])
        >>> masks = make_non_pad_mask(lengths)
        masks = [[1, 1, 1, 1, 1],
                 [1, 1, 1, 0, 0],
                 [1, 1, 0, 0, 0]]
    """
    batch_size = lengths.shape[0]
    max_len = max_len if max_len > 0 else int(mx.max(lengths).item())
    seq_range = mx.arange(0, max_len, dtype=mx.int32)
    seq_range_expand = mx.expand_dims(seq_range, axis=0)  # (1, max_len)
    seq_range_expand = mx.broadcast_to(seq_range_expand, (batch_size, max_len))
    seq_length_expand = mx.expand_dims(lengths, axis=-1)  # (B, 1)
    mask = seq_range_expand >= seq_length_expand
    return mx.logical_not(mask)


def mask_to_bias(mask: mx.array, dtype: mx.Dtype = mx.float32) -> mx.array:
    assert mask.dtype == mx.bool_, "Input mask must be boolean type"
    assert dtype in [
        mx.float32,
        mx.bfloat16,
        mx.float16,
    ], "dtype must be a floating point type"
    mask = mask.astype(dtype)
    mask = (1.0 - mask) * -1.0e10
    return mask


def padding(data: List[mx.array]) -> tuple[mx.array, mx.array]:
    """Padding the data into batch data

    Parameters
    ----------
        data: List[mx.array], shape of each array (128, T)

    Returns:
    -------
        Tuple of (padded_feats, feats_lengths)
        - padded_feats: shape (B, 128, max_T)
        - feats_lengths: shape (B,)
    """
    assert isinstance(data, list), "Input must be a list of arrays"

    feats_lengths = mx.array([s.shape[1] for s in data], dtype=mx.int32)

    max_len = max(s.shape[1] for s in data)
    batch_size = len(data)
    n_mels = data[0].shape[0]

    padded_feats = mx.zeros((batch_size, n_mels, max_len), dtype=data[0].dtype)

    for i, feat in enumerate(data):
        seq_len = feat.shape[1]
        padded_feats[i, :, :seq_len] = feat

    return padded_feats, feats_lengths



================================================
FILE: mlx_audio/codec/models/snac/__init__.py
================================================
from .snac import SNAC



================================================
FILE: mlx_audio/codec/models/snac/attention.py
================================================
import mlx.core as mx
import mlx.nn as nn
from einops.array_api import rearrange


class LocalMHA(nn.Module):
    def __init__(self, dim=1024, window_size=32, dim_head=64, use_rotary_pos_emb=True):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.heads = dim // dim_head
        self.window_size = window_size
        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)
        if use_rotary_pos_emb:
            self.rel_pos = SinusoidalEmbeddings(dim_head, scale_base=window_size // 2)
        else:
            self.rel_pos = None
        self.to_out = nn.Linear(dim, dim, bias=False)

    def __call__(self, x):
        B, C, T = x.shape
        residual = x
        x = self.norm(x.moveaxis(1, 2))
        windows = T // self.window_size

        qkv = self.to_qkv(x)
        q, k, v = mx.split(qkv, 3, axis=-1)

        q = rearrange(q, "b (w n) (h d) -> b h w n d", w=windows, h=self.heads)
        k = rearrange(k, "b (w n) (h d) -> b h w n d", w=windows, h=self.heads)
        v = rearrange(v, "b (w n) (h d) -> b h w n d", w=windows, h=self.heads)

        if self.rel_pos is not None:
            pos_emb, scale = self.rel_pos(k)
            q, k = apply_rotary_pos_emb(q, k, pos_emb, scale)

        scale = mx.sqrt(mx.array(q.shape[-1], dtype=mx.float32))
        scores = mx.matmul(q, k.transpose(0, 1, 2, 4, 3)) / scale
        attn_weights = mx.softmax(scores, axis=-1)
        out = mx.matmul(attn_weights, v)

        out = rearrange(out, "b h w n d -> b (w n) (h d)")
        out = self.to_out(out)
        return out.moveaxis(1, 2) + residual


class SinusoidalEmbeddings(nn.Module):
    def __init__(self, dim, scale_base=None, use_xpos=False):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (mx.arange(0, dim, 2, dtype=mx.float32) / dim))
        self.inv_freq = inv_freq

        # xpos related
        self.use_xpos = use_xpos
        self.scale_base = scale_base
        assert not (
            use_xpos and scale_base is None
        ), "scale base must be defined if using xpos"

        if use_xpos:
            scale = (mx.arange(0, dim, 2, dtype=mx.float32) + 0.4 * dim) / (1.4 * dim)
            self.scale = scale

    def __call__(self, x):
        seq_len = x.shape[-2]
        t = mx.arange(seq_len, dtype=self.inv_freq.dtype)
        freqs = mx.einsum("i,j->ij", t, self.inv_freq)
        freqs = mx.concatenate([freqs, freqs], axis=-1)

        if not self.use_xpos:
            return freqs, mx.ones((1,))

        power = (t - (seq_len // 2)) / self.scale_base
        power_reshaped = rearrange(power, "n -> n 1")
        scale = mx.power(self.scale, power_reshaped)
        scale = mx.concatenate([scale, scale], axis=-1)

        return freqs, scale


def rotate_half(x):
    x = rearrange(x, "b ... (r d) -> b ... r d", r=2)
    x1, x2 = mx.split(x, 2, axis=-2)
    return mx.concatenate([-x2, x1], axis=-1)


def apply_rotary_pos_emb(q, k, freqs, scale=1):
    q_len = q.shape[-2]
    q_freqs = freqs[..., -q_len:, :]
    inv_scale = mx.power(scale, -1)

    if scale.ndim == 2:
        scale = scale[-q_len:, :]

    q = (q * mx.cos(q_freqs) * scale) + (rotate_half(q) * mx.sin(q_freqs) * scale)
    k = (k * mx.cos(freqs) * inv_scale) + (rotate_half(k) * mx.sin(freqs) * inv_scale)

    return q, k



================================================
FILE: mlx_audio/codec/models/snac/layers.py
================================================
import math

import mlx.core as mx
import mlx.nn as nn

from .attention import LocalMHA


def normalize_weight(x, except_dim=0):
    if x.ndim != 3:
        raise ValueError("Input tensor must have 3 dimensions")

    axes = tuple(i for i in range(x.ndim) if i != except_dim)
    return mx.sqrt(mx.sum(mx.power(x, 2), axis=axes, keepdims=True))


class WNConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        if bias:
            self.bias = mx.zeros((out_channels,))

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels // groups),
        )
        self.weight_g = normalize_weight(weight_init)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"groups={self.groups}, bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = self.weight_g * self.weight_v / normalize_weight(self.weight_v)
        y = mx.conv1d(x, weight, self.stride, self.padding, self.dilation, self.groups)
        if hasattr(self, "bias"):
            y = y + self.bias
        return y


class WNConvTranspose1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        self.bias = mx.zeros((out_channels,)) if bias else None

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.output_padding = output_padding
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(in_channels, kernel_size, out_channels // groups),
        )
        self.weight_g = normalize_weight(weight_init, except_dim=0)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"output_padding={self.output_padding}, "
            f"groups={self.groups}, bias={self.bias is not None}"
        )

    def __call__(self, x):
        weight = (
            self.weight_g
            * self.weight_v
            / normalize_weight(self.weight_v, except_dim=0)
        )
        y = mx.conv_transpose1d(
            x,
            weight.swapaxes(0, 2),  # mlx uses (out_channels, kernel_size, in_channels)
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
        )
        if self.bias is not None:
            y = y + self.bias
        return y


def snake(x, alpha):
    shape = x.shape
    x = x.reshape(shape[0], shape[1], -1)
    recip = mx.reciprocal(alpha + 1e-9)
    x = x + recip * mx.power(mx.sin(alpha * x), 2)
    x = x.reshape(shape)
    return x


class Encoder(nn.Module):
    def __init__(
        self,
        d_model=64,
        strides=[3, 3, 7, 7],
        depthwise=False,
        attn_window_size=32,
    ):
        super().__init__()
        layers = [WNConv1d(1, d_model, kernel_size=7, padding=3)]
        for stride in strides:
            d_model *= 2
            groups = d_model // 2 if depthwise else 1
            layers += [EncoderBlock(output_dim=d_model, stride=stride, groups=groups)]
        if attn_window_size is not None:
            layers += [LocalMHA(dim=d_model, window_size=attn_window_size)]
        groups = d_model if depthwise else 1
        layers += [
            WNConv1d(d_model, d_model, kernel_size=7, padding=3, groups=groups),
        ]
        self.block = nn.Sequential(*layers)

    def __call__(self, x):
        return self.block(x).moveaxis(1, 2)


class Decoder(nn.Module):
    def __init__(
        self,
        input_channel,
        channels,
        rates,
        noise=False,
        depthwise=False,
        attn_window_size=32,
        d_out=1,
    ):
        super().__init__()
        if depthwise:
            layers = [
                WNConv1d(
                    input_channel,
                    input_channel,
                    kernel_size=7,
                    padding=3,
                    groups=input_channel,
                ),
                WNConv1d(input_channel, channels, kernel_size=1),
            ]
        else:
            layers = [WNConv1d(input_channel, channels, kernel_size=7, padding=3)]

        if attn_window_size is not None:
            layers += [LocalMHA(dim=channels, window_size=attn_window_size)]

        for i, stride in enumerate(rates):
            input_dim = channels // (2**i)
            output_dim = channels // (2 ** (i + 1))
            groups = output_dim if depthwise else 1
            layers.append(
                DecoderBlock(input_dim, output_dim, stride, noise, groups=groups)
            )

        layers += [
            Snake1d(output_dim),
            WNConv1d(output_dim, d_out, kernel_size=7, padding=3),
            nn.Tanh(),
        ]
        self.model = nn.Sequential(*layers)

    def __call__(self, x):
        x = self.model(x)
        return x


class ResidualUnit(nn.Module):
    def __init__(self, dim=16, dilation=1, kernel=7, groups=1):
        super().__init__()
        pad = ((kernel - 1) * dilation) // 2
        self.block = nn.Sequential(
            Snake1d(dim),
            WNConv1d(
                dim,
                dim,
                kernel_size=kernel,
                dilation=dilation,
                padding=pad,
                groups=groups,
            ),
            Snake1d(dim),
            WNConv1d(dim, dim, kernel_size=1),
        )

    def __call__(self, x):
        y = self.block(x)
        pad = (x.shape[-1] - y.shape[-1]) // 2
        if pad > 0:
            x = x[..., pad:-pad]
        return x + y


class EncoderBlock(nn.Module):
    def __init__(self, output_dim=16, input_dim=None, stride=1, groups=1):
        super().__init__()
        input_dim = input_dim or output_dim // 2
        self.block = nn.Sequential(
            ResidualUnit(input_dim, dilation=1, groups=groups),
            ResidualUnit(input_dim, dilation=3, groups=groups),
            ResidualUnit(input_dim, dilation=9, groups=groups),
            Snake1d(input_dim),
            WNConv1d(
                input_dim,
                output_dim,
                kernel_size=2 * stride,
                stride=stride,
                padding=math.ceil(stride / 2),
            ),
        )

    def __call__(self, x):
        return self.block(x)


class NoiseBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.linear = WNConv1d(dim, dim, kernel_size=1, bias=False)

    def __call__(self, x):
        B, C, T = x.shape
        noise = mx.random.normal((B, 1, T))
        h = self.linear(x)
        n = noise * h
        x = x + n
        return x


class DecoderBlock(nn.Module):
    def __init__(self, input_dim=16, output_dim=8, stride=1, noise=False, groups=1):
        super().__init__()

        layers = [
            Snake1d(input_dim),
            WNConvTranspose1d(
                input_dim,
                output_dim,
                kernel_size=2 * stride,
                stride=stride,
                padding=math.ceil(stride / 2),
                output_padding=stride % 2,
            ),
        ]
        if noise:
            layers.append(NoiseBlock(output_dim))
        layers.extend(
            [
                ResidualUnit(output_dim, dilation=1, groups=groups),
                ResidualUnit(output_dim, dilation=3, groups=groups),
                ResidualUnit(output_dim, dilation=9, groups=groups),
            ]
        )
        self.block = nn.Sequential(*layers)

    def __call__(self, x):
        return self.block(x)


class Snake1d(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.alpha = mx.ones((1, channels, 1))

    def __call__(self, x):
        return snake(x, self.alpha.swapaxes(1, 2))



================================================
FILE: mlx_audio/codec/models/snac/snac.py
================================================
import json
import math
from pathlib import Path
from typing import List, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from huggingface_hub import snapshot_download

from .layers import Decoder, Encoder
from .vq import ResidualVectorQuantize


class SNAC(nn.Module):
    def __init__(
        self,
        sampling_rate=44100,
        encoder_dim=64,
        encoder_rates=[3, 3, 7, 7],
        latent_dim=None,
        decoder_dim=1536,
        decoder_rates=[7, 7, 3, 3],
        attn_window_size=32,
        codebook_size=4096,
        codebook_dim=8,
        vq_strides=[8, 4, 2, 1],
        noise=True,
        depthwise=True,
    ):
        super().__init__()
        self.sampling_rate = sampling_rate
        self.encoder_dim = encoder_dim
        self.encoder_rates = encoder_rates
        self.decoder_dim = decoder_dim
        self.decoder_rates = decoder_rates
        if latent_dim is None:
            latent_dim = encoder_dim * (2 ** len(encoder_rates))
        self.latent_dim = latent_dim
        self.hop_length = np.prod(encoder_rates)
        self.encoder = Encoder(
            encoder_dim,
            encoder_rates,
            depthwise=depthwise,
            attn_window_size=attn_window_size,
        )
        self.n_codebooks = len(vq_strides)
        self.codebook_size = codebook_size
        self.codebook_dim = codebook_dim
        self.vq_strides = vq_strides
        self.attn_window_size = attn_window_size
        self.quantizer = ResidualVectorQuantize(
            input_dim=latent_dim,
            codebook_size=codebook_size,
            codebook_dim=codebook_dim,
            vq_strides=vq_strides,
        )
        self.decoder = Decoder(
            latent_dim,
            decoder_dim,
            decoder_rates,
            noise,
            depthwise=depthwise,
            attn_window_size=attn_window_size,
        )

    def preprocess(self, audio_data):
        length = audio_data.shape[-1]

        def lcm(a, b):
            return abs(a * b) // math.gcd(a, b)

        lcm_value = self.vq_strides[0]
        for i in range(1, len(self.vq_strides)):
            lcm_value = lcm(lcm_value, self.vq_strides[i])

        if self.attn_window_size:
            lcm_value = lcm(lcm_value, self.attn_window_size)

        pad_to = self.hop_length * lcm_value
        right_pad = math.ceil(length / pad_to) * pad_to - length

        # Pad the audio data
        audio_data = mx.pad(audio_data, [(0, 0), (0, 0), (0, right_pad)])
        return audio_data

    def __call__(self, audio_data: mx.array) -> Tuple[mx.array, List[mx.array]]:
        length = audio_data.shape[-1]
        audio_data = self.preprocess(audio_data)
        z = self.encoder(audio_data.moveaxis(1, 2))
        z_q, codes = self.quantizer(z)
        audio_hat = self.decoder(z_q)
        return audio_hat[..., :length], codes

    def encode(self, audio_data: mx.array) -> List[mx.array]:
        audio_data = self.preprocess(audio_data)
        z = self.encoder(audio_data.moveaxis(1, 2))
        _, codes = self.quantizer(z)
        return codes

    def decode(self, codes: List[mx.array]) -> mx.array:
        z_q = self.quantizer.from_codes(codes)
        audio_hat = self.decoder(z_q.moveaxis(1, 2))
        return audio_hat

    def _extra_repr(self):
        return (
            f"sampling_rate={self.sampling_rate}, "
            f"encoder_dim={self.encoder_dim}, "
            f"encoder_rates={self.encoder_rates}, "
            f"latent_dim={self.latent_dim}, "
            f"decoder_dim={self.decoder_dim}, "
            f"decoder_rates={self.decoder_rates}, "
            f"codebook_size={self.codebook_size}, "
            f"codebook_dim={self.codebook_dim}, "
            f"vq_strides={self.vq_strides}"
        )

    @classmethod
    def from_config(cls, config_path):
        with open(config_path, "r") as f:
            config = json.load(f)
        model = cls(**config)
        return model

    @classmethod
    def from_pretrained(cls, repo_id, **kwargs):
        path = fetch_from_hub(repo_id)

        if path is None:
            raise ValueError(f"Could not find model {path}")

        model_path = path / "model.safetensors"
        config_path = path / "config.json"
        snac = cls.from_config(config_path)

        weights = mx.load(model_path.as_posix(), format="safetensors")
        snac.load_weights(list(weights.items()))
        mx.eval(snac.parameters())

        return snac


# fetch model from hub


def fetch_from_hub(hf_repo: str) -> Path:
    model_path = Path(
        snapshot_download(
            repo_id=hf_repo,
            allow_patterns=["*.safetensors", "*.json"],
        )
    )
    return model_path



================================================
FILE: mlx_audio/codec/models/snac/vq.py
================================================
from typing import List

import mlx.core as mx
import mlx.nn as nn
from einops.array_api import rearrange

from .layers import WNConv1d


class VectorQuantize(nn.Module):
    def __init__(
        self, input_dim: int, codebook_size: int, codebook_dim: int, stride: int = 1
    ):
        super().__init__()
        self.codebook_size = codebook_size
        self.codebook_dim = codebook_dim
        self.stride = stride

        self.in_proj = WNConv1d(input_dim, codebook_dim, kernel_size=1)
        self.out_proj = WNConv1d(codebook_dim, input_dim, kernel_size=1)
        self.codebook = nn.Embedding(codebook_size, codebook_dim)

    def __call__(self, z):
        z = z.moveaxis(1, 2)

        if self.stride > 1:
            kernel_size = self.stride
            stride = self.stride
            kernel = mx.ones((z.shape[2], kernel_size, 1)) / kernel_size
            z = mx.conv1d(z, kernel, stride=stride, padding=0, groups=z.shape[2])

        # Factorized codes - Project input into low-dimensional space
        z_e = self.in_proj(z).moveaxis(1, 2)  # z_e : (B x D x T)
        z_q, indices = self.decode_latents(z_e)

        # Straight-through estimator: z_e + stop_gradient(z_q - z_e)
        z_q = z_e + (z_q - z_e)

        z_q = self.out_proj(z_q.moveaxis(1, 2)).moveaxis(1, 2)

        if self.stride > 1:
            # Implement repeat_interleave
            shape = list(z_q.shape)
            shape[-1] *= self.stride
            # Create a new tensor with the expanded shape
            expanded = mx.zeros(shape)

            # Fill the expanded tensor with repeated values
            for i in range(self.stride):
                expanded[..., i :: self.stride] = z_q

            z_q = expanded

        return z_q, indices

    def embed_code(self, embed_id):
        return self.codebook.weight[embed_id]

    def decode_code(self, embed_id):
        return self.embed_code(embed_id).moveaxis(1, 2)

    def decode_latents(self, latents):
        encodings = rearrange(latents, "b d t -> (b t) d")
        codebook = self.codebook.weight  # codebook: (N x D)

        encodings = normalize(encodings)
        codebook = normalize(codebook)

        dist = (
            mx.power(encodings, 2).sum(1, keepdims=True)
            - 2 * encodings @ codebook.T
            + mx.power(codebook, 2).sum(1, keepdims=True).T
        )
        min_dist = (-dist).argmax(1)
        indices = rearrange(min_dist, "(b t) -> b t", b=latents.shape[0])
        z_q = self.decode_code(indices)
        return z_q, indices


class ResidualVectorQuantize(nn.Module):
    def __init__(
        self,
        input_dim: int = 512,
        codebook_size: int = 1024,
        codebook_dim: int = 8,
        vq_strides: List[int] = [1, 1, 1, 1],
    ):
        super().__init__()
        self.n_codebooks = len(vq_strides)
        self.codebook_dim = codebook_dim
        self.codebook_size = codebook_size
        self.quantizers = [
            VectorQuantize(input_dim, codebook_size, codebook_dim, stride)
            for stride in vq_strides
        ]

    def __call__(self, z):
        z_q = 0
        residual = z
        codes = []
        for i, quantizer in enumerate(self.quantizers):
            z_q_i, indices_i = quantizer(residual)
            z_q = z_q + z_q_i
            residual = residual - z_q_i
            codes.append(indices_i)

        return z_q, codes

    def from_codes(self, codes: List[mx.array]) -> mx.array:
        z_q = 0.0
        for i in range(self.n_codebooks):
            z_p_i = self.quantizers[i].decode_code(codes[i])
            z_q_i = self.quantizers[i].out_proj(z_p_i.moveaxis(1, 2)).moveaxis(1, 2)

            # Handle repeat_interleave for stride > 1
            if self.quantizers[i].stride > 1:
                stride = self.quantizers[i].stride
                shape = list(z_q_i.shape)
                shape[-1] *= stride
                expanded = mx.zeros(shape)

                for j in range(stride):
                    expanded[..., j::stride] = z_q_i

                z_q_i = expanded

            z_q += z_q_i

        return z_q


def normalize(x, p=2.0, dim=1, eps=1e-12):
    """L2 normalization function"""
    norm = mx.power(mx.sum(mx.power(mx.abs(x), p), axis=dim, keepdims=True), 1 / p)
    return x / mx.maximum(norm, eps)



================================================
FILE: mlx_audio/codec/models/vocos/__init__.py
================================================
from .vocos import Vocos



================================================
FILE: mlx_audio/codec/models/vocos/mel.py
================================================
from __future__ import annotations

import mlx.core as mx

from mlx_audio.utils import hanning, mel_filters, stft


def log_mel_spectrogram(
    audio: mx.array,
    sample_rate: int = 24_000,
    n_mels: int = 100,
    n_fft: int = 1024,
    hop_length: int = 256,
    padding: int = 0,
):
    if not isinstance(audio, mx.array):
        audio = mx.array(audio)

    if padding > 0:
        audio = mx.pad(audio, (0, padding))

    freqs = stft(audio, window=hanning(n_fft), n_fft=n_fft, win_length=hop_length)
    magnitudes = freqs[:-1, :].abs()
    filters = mel_filters(
        sample_rate=sample_rate,
        n_fft=n_fft,
        n_mels=n_mels,
        norm=None,
        mel_scale="htk",
    )
    mel_spec = magnitudes @ filters.T
    log_spec = mx.maximum(mel_spec, 1e-5).log()
    return mx.expand_dims(log_spec, axis=0)



================================================
FILE: mlx_audio/codec/models/vocos/vocos.py
================================================
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from typing import Any, List, Optional

import mlx.core as mx
import mlx.nn as nn
import yaml
from huggingface_hub import snapshot_download

from mlx_audio.utils import hanning, istft

from ..encodec import Encodec
from .mel import log_mel_spectrogram


class FeatureExtractor(nn.Module):
    """Base class for feature extractors."""

    def __call__(self, audio: mx.array, **kwargs) -> mx.array:
        raise NotImplementedError("Subclasses must implement the forward method.")


class MelSpectrogramFeatures(FeatureExtractor):
    def __init__(
        self,
        sample_rate=24_000,
        n_fft=1024,
        hop_length=256,
        n_mels=100,
        padding="center",
    ):
        super().__init__()
        if padding not in ["center", "same"]:
            raise ValueError("Padding must be 'center' or 'same'.")
        self.padding = padding
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels

    def __call__(self, audio: mx.array, **kwargs):
        return log_mel_spectrogram(
            audio,
            sample_rate=self.sample_rate,
            n_mels=self.n_mels,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            padding=0,
        )


class EncodecFeatures(FeatureExtractor):
    def __init__(
        self,
        encodec_model: str = "encodec_24khz",
        bandwidths: List[float] = [1.5, 3.0, 6.0, 12.0],
        train_codebooks: bool = False,
    ):
        super().__init__()

        if encodec_model == "encodec_24khz":
            encodec, preprocessor = Encodec.from_pretrained(
                "mlx-community/encodec-24khz-float32"
            )
        elif encodec_model == "encodec_48khz":
            encodec, preprocessor = Encodec.from_pretrained(
                "mlx-community/encodec-48khz-float32"
            )
        else:
            raise ValueError(
                f"Unsupported encodec_model: {encodec_model}. Supported options are 'encodec_24khz' and 'encodec_48khz'."
            )

        self.encodec = encodec
        self.preprocessor = preprocessor
        self.num_q = self.encodec.quantizer.get_num_quantizers_for_bandwidth(
            bandwidth=max(bandwidths)
        )
        self.codebook_weights = mx.concatenate(
            [vq.codebook.embed for vq in self.encodec.quantizer.layers[: self.num_q]]
        )
        self.bandwidths = bandwidths

    def get_encodec_codes(self, audio: mx.array, bandwidth_id: int) -> mx.array:
        features, mask = self.preprocessor(audio)

        if isinstance(bandwidth_id, mx.array):
            bandwidth_id = int(bandwidth_id.flatten().tolist()[0])
        elif isinstance(bandwidth_id, list):
            bandwidth_id = bandwidth_id[0]

        codes, _ = self.encodec.encode(
            features, mask, bandwidth=self.bandwidths[bandwidth_id]
        )
        return mx.reshape(codes, (codes.shape[-2], 1, codes.shape[-1]))

    def get_features_from_codes(self, codes: mx.array) -> mx.array:
        offsets = mx.arange(
            0,
            self.encodec.quantizer.codebook_size * codes.shape[0],
            self.encodec.quantizer.codebook_size,
        )
        embeddings_idxs = codes + mx.reshape(offsets, (offsets.shape[0], 1, 1))
        embeddings = self.codebook_weights[embeddings_idxs]
        features = mx.sum(embeddings, axis=0)
        return features

    def __call__(self, audio: mx.array, **kwargs) -> mx.array:
        bandwidth_id = kwargs.get("bandwidth_id")
        if bandwidth_id is None:
            raise ValueError("The 'bandwidth_id' argument is required")

        codes = self.get_encodec_codes(audio, bandwidth_id=bandwidth_id)
        return self.get_features_from_codes(codes)


class ISTFTHead(nn.Module):
    def __init__(self, dim: int, n_fft: int, hop_length: int, padding: str = "center"):
        super().__init__()
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.out = nn.Linear(dim, n_fft + 2)

    def __call__(self, x: mx.array) -> mx.array:
        x = self.out(x).swapaxes(1, 2)
        mag, p = x.split(2, axis=1)
        mag = mx.exp(mag)
        mag = mx.clip(mag, None, 1e2)
        x = mx.cos(p)
        y = mx.sin(p)
        S = mag * (x + 1j * y)
        audio = istft(
            S.squeeze(0),
            window=hanning(self.n_fft),
            hop_length=self.hop_length,
            win_length=self.n_fft,
        )
        return audio


class ConvNeXtBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        layer_scale_init_value: float,
        adanorm_num_embeddings: Optional[int] = None,
    ):
        super().__init__()

        # depthwise conv
        self.dwconv = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)
        self.adanorm = adanorm_num_embeddings is not None
        if adanorm_num_embeddings:
            self.norm = AdaLayerNorm(adanorm_num_embeddings, dim, eps=1e-6)
        else:
            self.norm = nn.LayerNorm(dim, eps=1e-6)

        # pointwise/1x1 convs, implemented with linear layers
        self.pwconv1 = nn.Linear(dim, intermediate_dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)
        self.gamma = (
            layer_scale_init_value * mx.ones(dim)
            if layer_scale_init_value > 0
            else None
        )

    def __call__(
        self, x: mx.array, cond_embedding_id: Optional[mx.array] = None
    ) -> mx.array:
        residual = x

        x = self.dwconv(x)
        if self.adanorm:
            assert cond_embedding_id is not None
            x = self.norm(x, cond_embedding_id)
        else:
            x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = residual + x
        return x


class AdaLayerNorm(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.dim = embedding_dim

        self.scale = nn.Linear(num_embeddings, embedding_dim)
        self.shift = nn.Linear(num_embeddings, embedding_dim)
        self.scale.weight = mx.ones(self.scale.weight.shape)
        self.shift.weight = mx.zeros(self.shift.weight.shape)

    def __call__(self, x: mx.array, cond_embedding: mx.array) -> mx.array:
        scale = self.scale(cond_embedding)
        shift = self.shift(cond_embedding)
        x = mx.fast.layer_norm(x, weight=None, bias=None, eps=self.eps)
        x = x * scale[:, None, :] + shift[:, None, :]
        return x


class VocosBackbone(nn.Module):
    def __init__(
        self,
        input_channels: int,
        dim: int,
        intermediate_dim: int,
        num_layers: int,
        layer_scale_init_value: Optional[float] = None,
        adanorm_num_embeddings: Optional[int] = None,
        bias: bool = True,
    ):
        super().__init__()
        self.input_channels = input_channels
        self.embed = nn.Conv1d(input_channels, dim, kernel_size=7, padding=3)
        self.adanorm = adanorm_num_embeddings is not None
        if adanorm_num_embeddings:
            self.norm = AdaLayerNorm(adanorm_num_embeddings, dim, eps=1e-6)
        else:
            self.norm = nn.LayerNorm(dim, eps=1e-6)
        layer_scale_init_value = layer_scale_init_value or 1 / num_layers
        self.convnext = [
            ConvNeXtBlock(
                dim=dim,
                intermediate_dim=intermediate_dim,
                layer_scale_init_value=layer_scale_init_value,
                adanorm_num_embeddings=adanorm_num_embeddings,
            )
            for _ in range(num_layers)
        ]
        self.final_layer_norm = nn.LayerNorm(dim, eps=1e-6, bias=bias)

    def __call__(self, x: mx.array, **kwargs) -> mx.array:
        bandwidth_id = kwargs.get("bandwidth_id", None)

        # Transpose if the input is not in the correct shape
        if x.shape[-1] != self.input_channels:
            x = x.transpose(0, 2, 1)

        x = self.embed(x)

        if self.adanorm:
            assert bandwidth_id is not None
            x = self.norm(x, bandwidth_id)

        else:
            x = self.norm(x)

        for conv_block in self.convnext:
            x = conv_block(x, cond_embedding_id=bandwidth_id)
        x = self.final_layer_norm(x)
        return x


class Vocos(nn.Module):
    def __init__(
        self,
        feature_extractor: FeatureExtractor,
        backbone: VocosBackbone,
        head: ISTFTHead,
    ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.backbone = backbone
        self.head = head

    @classmethod
    def from_hparams(cls, config: dict) -> Vocos:
        """
        Class method to create a new Vocos model instance from hyperparameters stored in a yaml configuration file.
        """
        config = SimpleNamespace(**config)

        if "MelSpectrogramFeatures" in config.feature_extractor["class_path"]:
            feature_extractor_init_args = config.feature_extractor["init_args"]
            feature_extractor = MelSpectrogramFeatures(**feature_extractor_init_args)
        elif "EncodecFeatures" in config.feature_extractor["class_path"]:
            feature_extractor = EncodecFeatures(**config.feature_extractor["init_args"])
        backbone = VocosBackbone(**config.backbone["init_args"])
        head = ISTFTHead(**config.head["init_args"])
        model = cls(feature_extractor=feature_extractor, backbone=backbone, head=head)
        return model

    @classmethod
    def from_pretrained(cls, path_or_repo: str) -> Vocos:
        """
        Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
        """

        path = Path(path_or_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_repo,
                    allow_patterns=["*.yaml", "*.safetensors"],
                )
            )

        model_path = path / "model.safetensors"
        with open(model_path, "rb") as f:
            weights = mx.load(f)

        config_path = path / "config.yaml"
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        model = cls.from_hparams(config)

        # remove unused weights
        try:
            del weights["feature_extractor.mel_spec.spectrogram.window"]
            del weights["head.istft.window"]
        except KeyError:
            pass

        # transpose weights as needed
        new_weights = {}
        for k, v in weights.items():
            basename, pname = k.rsplit(".", 1)
            if "backbone.embed" in basename and pname == "weight":
                new_weights[k] = v.moveaxis(1, 2)
            elif "dwconv" in basename and pname == "weight":
                new_weights[k] = v.moveaxis(1, 2)
            else:
                new_weights[k] = v

        # use strict = False to avoid the encodec weights
        model.load_weights(list(new_weights.items()), strict=False)
        model.eval()

        return model

    def __call__(self, audio_input: mx.array, **kwargs: Any) -> mx.array:
        features = self.feature_extractor(audio_input, **kwargs)
        audio_output = self.decode(features, **kwargs)
        return audio_output

    def get_encodec_codes(self, audio_input: mx.array, bandwidth_id: int) -> mx.array:
        if not isinstance(self.feature_extractor, EncodecFeatures):
            raise ValueError("This model does not support getting encodec codes.")

        return self.feature_extractor.get_encodec_codes(audio_input, bandwidth_id)

    def decode(self, features_input: mx.array, **kwargs: Any) -> mx.array:
        x = self.backbone(features_input, **kwargs)
        audio_output = self.head(x)
        return audio_output

    def decode_from_codes(self, codes: mx.array, **kwargs: Any) -> mx.array:
        features = self.feature_extractor.get_features_from_codes(codes)
        audio_output = self.decode(features, **kwargs)
        return audio_output



================================================
FILE: mlx_audio/codec/tests/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/codec/tests/test_bigvgan.py
================================================
import math
import unittest

import mlx.core as mx

from mlx_audio.codec.models.bigvgan.bigvgan import BigVGAN, BigVGANConfig


class TestBigVGAN(unittest.TestCase):
    def test_bigvgan_22khz_80bands(self):
        cfg = BigVGANConfig(
            num_mels=80,
            upsample_rates=[4, 4, 2, 2, 2, 2],
            upsample_kernel_sizes=[8, 8, 4, 4, 4, 4],
            upsample_initial_channel=1536,
            resblock="1",
            resblock_kernel_sizes=[3, 7, 11],
            resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
            activation="snakebeta",
            snake_logscale=True,
            use_bias_at_final=True,
            use_tanh_at_final=True,
        )
        model = BigVGAN(cfg)

        audio = mx.zeros((1, 80, 800))
        y = model(audio)

        self.assertEqual(y.shape, (1, 1, 800 * math.prod(cfg.upsample_rates)))

    def test_bigvgan_44khz_128bands_512x(self):
        cfg = BigVGANConfig(
            num_mels=128,
            upsample_rates=[8, 4, 2, 2, 2, 2],
            upsample_kernel_sizes=[16, 8, 4, 4, 4, 4],
            upsample_initial_channel=1536,
            resblock="1",
            resblock_kernel_sizes=[3, 7, 11],
            resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
            activation="snakebeta",
            snake_logscale=True,
            use_bias_at_final=False,
            use_tanh_at_final=False,
        )
        model = BigVGAN(cfg)

        audio = mx.zeros((1, 128, 800))
        y = model(audio)

        self.assertEqual(y.shape, (1, 1, 800 * math.prod(cfg.upsample_rates)))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/codec/tests/test_descript.py
================================================
import unittest

import mlx.core as mx

from ..models.descript import DAC


class TestDescript(unittest.TestCase):
    """Test Descript model encoding and decoding."""

    def test_descript_16khz(self):
        audio = mx.zeros((1, 1, 80_000))

        encoder_dim = 64
        encoder_rates = [2, 4, 5, 8]
        decoder_dim = 1536
        decoder_rates = [8, 5, 4, 2]
        n_codebooks = 12
        codebook_size = 1024
        codebook_dim = 8
        sample_rate = 16_000

        model = DAC(
            encoder_dim=encoder_dim,
            encoder_rates=encoder_rates,
            decoder_dim=decoder_dim,
            decoder_rates=decoder_rates,
            n_codebooks=n_codebooks,
            codebook_size=codebook_size,
            codebook_dim=codebook_dim,
            sample_rate=sample_rate,
        )

        x = model.preprocess(audio, sample_rate)
        z, codes, latents, _, _ = model.encode(x)
        self.assertEqual(z.shape, (1, 1024, 250))
        self.assertEqual(codes.shape, (1, 12, 250))
        self.assertEqual(latents.shape, (1, 96, 250))

        y = model.decode(z).squeeze(-1)
        self.assertEqual(y.shape, (1, 80_043))

    def test_descript_24khz(self):
        audio = mx.zeros((1, 1, 120_000))

        encoder_dim = 64
        encoder_rates = [2, 4, 5, 8]
        decoder_dim = 1536
        decoder_rates = [8, 5, 4, 2]
        n_codebooks = 32
        codebook_size = 1024
        codebook_dim = 8
        sample_rate = 24_000

        model = DAC(
            encoder_dim=encoder_dim,
            encoder_rates=encoder_rates,
            decoder_dim=decoder_dim,
            decoder_rates=decoder_rates,
            n_codebooks=n_codebooks,
            codebook_size=codebook_size,
            codebook_dim=codebook_dim,
            sample_rate=sample_rate,
        )

        x = model.preprocess(audio, sample_rate)
        z, codes, latents, _, _ = model.encode(x)
        self.assertEqual(z.shape, (1, 1024, 375))
        self.assertEqual(codes.shape, (1, 32, 375))
        self.assertEqual(latents.shape, (1, 256, 375))

        y = model.decode(z).squeeze(-1)
        self.assertEqual(y.shape, (1, 120_043))

    def test_descript_44khz(self):
        audio = mx.zeros((1, 1, 220_000))

        encoder_dim = 64
        encoder_rates = [2, 4, 8, 8]
        decoder_dim = 1536
        decoder_rates = [8, 8, 4, 2]
        n_codebooks = 9
        codebook_size = 1024
        codebook_dim = 8
        sample_rate = 44_100

        model = DAC(
            encoder_dim=encoder_dim,
            encoder_rates=encoder_rates,
            decoder_dim=decoder_dim,
            decoder_rates=decoder_rates,
            n_codebooks=n_codebooks,
            codebook_size=codebook_size,
            codebook_dim=codebook_dim,
            sample_rate=sample_rate,
        )

        x = model.preprocess(audio, sample_rate)
        z, codes, latents, _, _ = model.encode(x)
        self.assertEqual(z.shape, (1, 1024, 430))
        self.assertEqual(codes.shape, (1, 9, 430))
        self.assertEqual(latents.shape, (1, 72, 430))

        y = model.decode(z).squeeze(-1)
        self.assertEqual(y.shape, (1, 220_235))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/codec/tests/test_encodec.py
================================================
import unittest

import mlx.core as mx

from ..models.encodec import Encodec, EncodecConfig

config = EncodecConfig(
    audio_channels=1,
    chunk_length_s=None,
    codebook_dim=128,
    codebook_size=1024,
    compress=2,
    dilation_growth_rate=2,
    hidden_size=128,
    kernel_size=7,
    last_kernel_size=7,
    model_type="encodec",
    norm_type="weight_norm",
    normalize=False,
    num_filters=32,
    num_lstm_layers=2,
    num_residual_layers=1,
    overlap=None,
    pad_mode="reflect",
    residual_kernel_size=3,
    sampling_rate=24000,
    target_bandwidths=[1.5, 3.0, 6.0, 12.0, 24.0],
    trim_right_ratio=1.0,
    upsampling_ratios=[8, 5, 4, 2],
    use_causal_conv=True,
)


class TestEncodec(unittest.TestCase):
    """Test EnCodec model encoding and decoding."""

    def test_encodec_24khz(self):
        model = Encodec(config)

        audio = mx.zeros((1, 120_000, 1))

        # default bandwidth
        (codes, scales) = model.encode(audio)
        self.assertEqual(codes.shape, (1, 1, 2, 375))

        audio_out = model.decode(codes, scales)
        self.assertEqual(audio_out.shape, (1, 120_000, 1))

        # 6kbps bandwidth
        (codes, scales) = model.encode(audio, bandwidth=6)
        self.assertEqual(codes.shape, (1, 1, 8, 375))

        audio_out = model.decode(codes, scales)
        self.assertEqual(audio_out.shape, (1, 120_000, 1))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/codec/tests/test_mimi.py
================================================
import unittest

import mlx.core as mx

from ..models.mimi.mimi import Mimi, mimi_202407


class TestMimi(unittest.TestCase):
    def test_mimi_model(self):
        """Test Mimi model encoding and decoding."""
        model = Mimi(mimi_202407(32))

        audio = mx.zeros((1, 1, 120_000))
        codes = model.encode(audio)
        self.assertEqual(codes.shape, (1, 32, 63))

        audio_out = model.decode(codes)
        self.assertEqual(audio_out.shape, (1, 1, 120_960))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/codec/tests/test_s3.py
================================================
import unittest

import mlx.core as mx

from ..models.s3 import S3TokenizerV2
from ..models.s3.utils import log_mel_spectrogram


class TestS3TokenizerV2(unittest.TestCase):
    """Test S3TokenizerV2 model encoding and decoding."""

    def test_s3_tokenizer_v2(self):
        audio = mx.zeros((160_000))
        mel = log_mel_spectrogram(audio)

        model = S3TokenizerV2("speech_tokenizer_v2_25hz")

        mel_batch = mel[None, ...]  # (1, n_mels, T)
        mel_len = mx.array([mel.shape[1]], dtype=mx.int32)

        codes, code_lens = model(mel_batch, mel_len)
        self.assertEqual(codes.shape, (1, 251))

        codes = codes[0, : code_lens[0].item()]
        self.assertEqual(codes.shape, (251,))



================================================
FILE: mlx_audio/codec/tests/test_snac.py
================================================
import unittest

import mlx.core as mx

from ..models.snac import SNAC

config = {
    "sampling_rate": 24000,
    "encoder_dim": 48,
    "encoder_rates": [2, 4, 8, 8],
    "decoder_dim": 1024,
    "decoder_rates": [8, 8, 4, 2],
    "attn_window_size": None,
    "codebook_size": 4096,
    "codebook_dim": 8,
    "vq_strides": [4, 2, 1],
    "noise": True,
    "depthwise": True,
}


class TestSNAC(unittest.TestCase):
    """Test SNAC model encoding and decoding."""

    def test_snac(self):
        audio = mx.zeros((1, 1, 120_000))

        model = SNAC(**config)
        codes = model.encode(audio)
        self.assertEqual(len(codes), 3)
        self.assertEqual(codes[0].shape, (1, 59))
        self.assertEqual(codes[1].shape, (1, 118))
        self.assertEqual(codes[2].shape, (1, 236))

        reconstructed = model.decode(codes).squeeze(-1)
        self.assertEqual(reconstructed.shape, (1, 120_907))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/codec/tests/test_vocos.py
================================================
import unittest

import mlx.core as mx

from ..models.vocos import Vocos
from ..models.vocos.mel import log_mel_spectrogram

config_mel = {
    "feature_extractor": {
        "class_path": "vocos.feature_extractors.MelSpectrogramFeatures",
        "init_args": {
            "sample_rate": 24000,
            "n_fft": 1024,
            "hop_length": 256,
            "n_mels": 100,
        },
    },
    "backbone": {
        "class_path": "vocos.models.VocosBackbone",
        "init_args": {
            "input_channels": 100,
            "dim": 512,
            "intermediate_dim": 1536,
            "num_layers": 8,
        },
    },
    "head": {
        "class_path": "vocos.heads.ISTFTHead",
        "init_args": {"dim": 512, "n_fft": 1024, "hop_length": 256},
    },
}

config_encodec = {
    "feature_extractor": {
        "class_path": "vocos.feature_extractors.EncodecFeatures",
        "init_args": {
            "encodec_model": "encodec_24khz",
            "bandwidths": [1.5, 3.0, 6.0, 12.0, 24.0],
        },
    },
    "backbone": {
        "class_path": "vocos.models.VocosBackbone",
        "init_args": {
            "input_channels": 128,
            "dim": 384,
            "intermediate_dim": 1152,
            "num_layers": 8,
            "adanorm_num_embeddings": 4,
        },
    },
    "head": {
        "class_path": "vocos.heads.ISTFTHead",
        "init_args": {"dim": 384, "n_fft": 1280, "hop_length": 320, "padding": "same"},
    },
}


class TestVocos(unittest.TestCase):
    """Test Vocos model encoding and decoding."""

    def test_vocos_24khz(self):
        audio = mx.zeros((120_000))

        model = Vocos.from_hparams(config_mel)

        # reconstruct from mel spec
        reconstructed_audio = model(audio)
        self.assertEqual(reconstructed_audio.shape, (119552,))

        # decode from mel spec
        mel_spec = log_mel_spectrogram(audio)
        decoded = model.decode(mel_spec)
        self.assertEqual(decoded.shape, (119552,))

        model = Vocos.from_hparams(config_encodec)

        # reconstruct from encodec codes
        bandwidth_id = [3, 3, 3, 3]  # 24kbps
        reconstructed_audio = model(
            audio, bandwidth_id=mx.array(bandwidth_id)[None, ...]
        )
        self.assertEqual(reconstructed_audio.shape, (119680,))

        # decode with encodec codes
        codes = model.get_encodec_codes(audio, bandwidth_id=bandwidth_id)
        decoded = model.decode_from_codes(
            codes, bandwidth_id=mx.array(bandwidth_id)[None, ...]
        )
        self.assertEqual(decoded.shape, (119680,))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/sts/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/sts/voice_pipeline.py
================================================
import argparse
import asyncio
import logging

import mlx.core as mx
import numpy as np
import sounddevice as sd
import webrtcvad
from mlx_lm.generate import generate as generate_text
from mlx_lm.utils import load as load_llm

from mlx_audio.stt.models.whisper import Model as Whisper
from mlx_audio.tts.audio_player import AudioPlayer
from mlx_audio.tts.utils import load_model as load_tts

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class VoicePipeline:
    def __init__(
        self,
        silence_threshold=0.03,
        silence_duration=1.5,
        input_sample_rate=16_000,
        output_sample_rate=24_000,
        streaming_interval=3,
        frame_duration_ms=30,
        vad_mode=3,
        stt_model="mlx-community/whisper-large-v3-turbo",
        llm_model="Qwen/Qwen2.5-0.5B-Instruct-4bit",
        tts_model="mlx-community/csm-1b-fp16",
    ):
        self.silence_threshold = silence_threshold
        self.silence_duration = silence_duration
        self.input_sample_rate = input_sample_rate
        self.output_sample_rate = output_sample_rate
        self.streaming_interval = streaming_interval
        self.frame_duration_ms = frame_duration_ms

        self.stt_model = stt_model
        self.llm_model = llm_model
        self.tts_model = tts_model

        self.vad = webrtcvad.Vad(vad_mode)

        self.input_audio_queue = asyncio.Queue(maxsize=50)
        self.transcription_queue = asyncio.Queue()
        self.output_audio_queue = asyncio.Queue(maxsize=50)

        self.mlx_lock = asyncio.Lock()

    async def init_models(self):
        logger.info(f"Loading text generation model: {self.llm_model}")
        self.llm, self.tokenizer = await asyncio.to_thread(
            lambda: load_llm(self.llm_model)
        )

        logger.info(f"Loading text-to-speech model: {self.tts_model}")
        self.tts = await asyncio.to_thread(lambda: load_tts(self.tts_model))

        logger.info(f"Loading speech-to-text model: {self.stt_model}")
        self.stt = Whisper.from_pretrained(self.stt_model)

    async def start(self):
        self.loop = asyncio.get_running_loop()

        await self.init_models()

        tasks = [
            asyncio.create_task(self._listener()),
            asyncio.create_task(self._response_processor()),
            asyncio.create_task(self._audio_output_processor()),
        ]
        try:
            await asyncio.gather(*tasks)
        finally:
            for t in tasks:
                t.cancel()
            await asyncio.gather(*tasks, return_exceptions=True)

    # speech detection and transcription

    def _is_silent(self, audio_data):
        if isinstance(audio_data, bytes):
            audio_np = np.frombuffer(audio_data, dtype=np.int16)
            audio_np = (
                audio_np.astype(np.float32) / 32768.0
            )  # Normalize if input is bytes
        else:
            audio_np = audio_data

        # Ensure audio_np is float32 for energy calculation.
        audio_np = audio_np.astype(np.float32)

        energy = np.linalg.norm(audio_np) / np.sqrt(audio_np.size)
        return energy < self.silence_threshold

    def _voice_activity_detection(self, frame):
        try:
            return self.vad.is_speech(frame, self.input_sample_rate)
        except ValueError:
            # fall back to energy-based detection
            return not self._is_silent(frame)

    async def _listener(self):
        frame_size = int(self.input_sample_rate * (self.frame_duration_ms / 1000.0))
        stream = sd.InputStream(
            samplerate=self.input_sample_rate,
            blocksize=frame_size,
            channels=1,
            dtype="int16",
            callback=self._sd_callback,
        )
        stream.start()

        logger.info("Listening for voice input...")

        frames = []
        silent_frames = 0
        frames_until_silence = int(
            self.silence_duration * 1000 / self.frame_duration_ms
        )
        speaking_detected = False

        try:
            while True:
                frame = await self.input_audio_queue.get()
                is_speech = self._voice_activity_detection(frame)

                if is_speech:
                    speaking_detected = True
                    silent_frames = 0
                    frames.append(frame)

                    # Cancel the current TTS task
                    if hasattr(self, "current_tts_task") and self.current_tts_task:
                        # Signal the generator loop to stop
                        self.current_tts_cancel.set()

                    # Clear the output audio queue
                    self.loop.call_soon_threadsafe(self.player.flush)
                elif speaking_detected:
                    silent_frames += 1
                    frames.append(frame)

                    if silent_frames > frames_until_silence:
                        # Process the voice input
                        if frames:

                            logger.info("Processing voice input...")
                            await self._process_audio(frames)

                        frames = []
                        speaking_detected = False
                        silent_frames = 0
        except (asyncio.CancelledError, KeyboardInterrupt):
            stream.stop()
            stream.close()
            raise
        finally:
            stream.stop()
            stream.close()

    def _sd_callback(self, indata, frames, _time, status):
        data = indata.reshape(-1).tobytes()

        def _enqueue():
            try:
                self.input_audio_queue.put_nowait(data)
            except asyncio.QueueFull:
                return

        self.loop.call_soon_threadsafe(_enqueue)

    async def _process_audio(self, frames):
        audio = (
            np.frombuffer(b"".join(frames), dtype=np.int16).astype(np.float32) / 32768.0
        )

        async with self.mlx_lock:
            result = await asyncio.to_thread(self.stt.generate, mx.array(audio))
        text = result.text.strip()

        if text:
            logger.info(f"Transcribed: {text}")
            await self.transcription_queue.put(text)

    # response generation

    async def _response_processor(self):
        while True:
            text = await self.transcription_queue.get()
            await self._generate_response(text)
            self.transcription_queue.task_done()

    async def _generate_response(self, text):
        def _get_llm_response(llm, tokenizer, messages, *, verbose=False):
            prompt = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            return generate_text(llm, tokenizer, prompt, verbose=verbose).strip()

        try:
            logger.info("Generating response...")

            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful voice assistant. You always respond with short sentences and never use punctuation like parentheses or colons that wouldn't appear in conversational speech.",
                },
                {"role": "user", "content": text},
            ]
            async with self.mlx_lock:
                response_text = await asyncio.to_thread(
                    _get_llm_response, self.llm, self.tokenizer, messages, verbose=False
                )

            logger.info(f"Generated response: {response_text}")

            if response_text:
                self.current_tts_cancel = asyncio.Event()
                self.current_tts_task = asyncio.create_task(
                    self._speak_response(response_text, self.current_tts_cancel)
                )
        except Exception as e:
            logger.error(f"Generation error: {e}")

    # speech generation

    async def _speak_response(self, text: str, cancel_event: asyncio.Event):
        """
        Speak `text`, yielding PCM chunks into `self.output_audio_queue`.
        Playback can be interrupted at any moment by setting `cancel_event`.
        """
        loop = self.loop

        def _tts_stream(tts, txt, rate, queue, cancel_ev: asyncio.Event):
            # This runs in a worker thread, so we *must* poll a threadâ€‘safe flag.
            for chunk in tts.generate(
                txt,
                sample_rate=rate,
                stream=True,
                streaming_interval=self.streaming_interval,
                verbose=False,
            ):
                if cancel_ev.is_set():  # <-- stop immediately
                    break
                loop.call_soon_threadsafe(queue.put_nowait, chunk.audio)

        try:
            async with self.mlx_lock:
                await asyncio.to_thread(
                    _tts_stream,
                    self.tts,
                    text,
                    self.output_sample_rate,
                    self.output_audio_queue,
                    cancel_event,
                )
        except asyncio.CancelledError:
            # The coroutine itself was cancelled from outside â†’ just exit cleanly.
            pass
        except Exception as exc:
            logger.error("Speech synthesis error: %s", exc)

    async def _audio_output_processor(self):
        self.player = AudioPlayer(sample_rate=self.output_sample_rate)

        try:
            while True:
                audio = await self.output_audio_queue.get()
                self.player.queue_audio(audio)
                self.output_audio_queue.task_done()
        except (asyncio.CancelledError, KeyboardInterrupt):
            self.player.stop()
            raise


async def main():
    parser = argparse.ArgumentParser(description="Voice Pipeline")
    parser.add_argument(
        "--stt_model",
        type=str,
        default="mlx-community/whisper-large-v3-turbo",
        help="STT model",
    )
    parser.add_argument(
        "--tts_model", type=str, default="mlx-community/csm-1b-fp16", help="TTS model"
    )
    parser.add_argument(
        "--llm_model",
        type=str,
        default="mlx-community/Qwen2.5-0.5B-Instruct-4bit",
        help="LLM model",
    )
    parser.add_argument("--vad_mode", type=int, default=3, help="VAD mode")
    parser.add_argument(
        "--silence_duration", type=float, default=1.5, help="Silence duration"
    )
    parser.add_argument(
        "--silence_threshold", type=float, default=0.03, help="Silence threshold"
    )
    parser.add_argument(
        "--streaming_interval", type=int, default=3, help="Streaming interval"
    )
    args = parser.parse_args()

    pipeline = VoicePipeline(
        stt_model=args.stt_model,
        tts_model=args.tts_model,
        llm_model=args.llm_model,
        vad_mode=args.vad_mode,
        silence_duration=args.silence_duration,
        silence_threshold=args.silence_threshold,
        streaming_interval=args.streaming_interval,
    )
    await pipeline.start()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass



================================================
FILE: mlx_audio/sts/tests/test_voice_pipeline.py
================================================
from unittest import mock

import numpy as np
import pytest

from mlx_audio.sts.voice_pipeline import VoicePipeline


class TestVoicePipeline:
    def test_initialization_default_params(self):
        """
        Test that the initialization method initializes the parameters correctly.
        """
        pipeline = VoicePipeline()
        assert pipeline.silence_threshold == 0.03
        assert pipeline.silence_duration == 1.5
        assert pipeline.input_sample_rate == 16_000
        assert pipeline.output_sample_rate == 24_000
        assert pipeline.streaming_interval == 3
        assert pipeline.frame_duration_ms == 30
        assert pipeline.stt_model == "mlx-community/whisper-large-v3-turbo"
        assert pipeline.llm_model == "Qwen/Qwen2.5-0.5B-Instruct-4bit"
        assert pipeline.tts_model == "mlx-community/csm-1b-fp16"

    def test_initialization_custom_params(self):
        """
        Test that the initialization method initializes the parameters correctly.
        """
        pipeline = VoicePipeline(
            silence_threshold=0.05,
            silence_duration=2.0,
            input_sample_rate=8_000,
            output_sample_rate=12_000,
            streaming_interval=5,
            frame_duration_ms=20,
            vad_mode=2,
            stt_model="custom/stt",
            llm_model="custom/llm",
            tts_model="custom/tts",
        )
        assert pipeline.silence_threshold == 0.05
        assert pipeline.silence_duration == 2.0
        assert pipeline.input_sample_rate == 8_000
        assert pipeline.output_sample_rate == 12_000
        assert pipeline.streaming_interval == 5
        assert pipeline.frame_duration_ms == 20
        assert pipeline.stt_model == "custom/stt"
        assert pipeline.llm_model == "custom/llm"
        assert pipeline.tts_model == "custom/tts"

    @mock.patch("mlx_audio.sts.voice_pipeline.load_llm")
    @mock.patch("mlx_audio.sts.voice_pipeline.load_tts")
    @mock.patch("mlx_audio.sts.voice_pipeline.Whisper.from_pretrained")
    async def test_init_models(self, mock_whisper_load, mock_tts_load, mock_llm_load):
        """
        Test that the init_models method initializes the models correctly.
        """
        pipeline = VoicePipeline()

        # Mock the return values of the model loaders
        mock_llm = mock.AsyncMock()
        mock_tokenizer = mock.AsyncMock()
        mock_llm_load.return_value = (mock_llm, mock_tokenizer)

        mock_tts = mock.AsyncMock()
        mock_tts_load.return_value = mock_tts

        mock_stt = mock.AsyncMock()
        mock_whisper_load.return_value = mock_stt

        await pipeline.init_models()

        mock_llm_load.assert_called_once_with(pipeline.llm_model)
        mock_tts_load.assert_called_once_with(pipeline.tts_model)
        mock_whisper_load.assert_called_once_with(pipeline.stt_model)

        assert pipeline.llm is mock_llm
        assert pipeline.tokenizer is mock_tokenizer
        assert pipeline.tts is mock_tts
        assert pipeline.stt is mock_stt

    def test_is_silent_true(self):
        """
        Test that the is_silent method returns True for silent audio frames.
        """
        pipeline = VoicePipeline(silence_threshold=0.1)
        # Create a silent audio frame (very low amplitude)
        silent_audio_data_np = np.random.uniform(-0.01, 0.01, size=480).astype(
            np.float32
        )  # 30ms at 16kHz
        silent_audio_data_bytes = (
            (silent_audio_data_np * 32768.0).astype(np.int16).tobytes()
        )

        assert pipeline._is_silent(silent_audio_data_np) is np.True_
        assert pipeline._is_silent(silent_audio_data_bytes) is np.True_

    def test_is_silent_false(self):
        """
        Test that the is_silent method returns False for non-silent audio frames.
        """
        pipeline = VoicePipeline(silence_threshold=0.001)
        # Create a non-silent audio frame (higher amplitude)
        speech_audio_data_np = np.random.uniform(-2, 2, size=480).astype(np.float32)
        speech_audio_data_bytes = (
            (speech_audio_data_np * 32768.0).astype(np.int16).tobytes()
        )

        assert pipeline._is_silent(speech_audio_data_np) is np.False_
        assert pipeline._is_silent(speech_audio_data_bytes) is np.False_

    @mock.patch("webrtcvad.Vad.is_speech")
    def test_voice_activity_detection_vad_speech(self, mock_is_speech):
        """
        Test that the voice activity detection returns True for speech frames.
        """
        pipeline = VoicePipeline()
        mock_is_speech.return_value = True
        frame = b"\x00\x00" * (16000 * 30 // 1000)  # 30ms of silence at 16kHz, 16-bit
        assert pipeline._voice_activity_detection(frame) is True
        mock_is_speech.assert_called_once_with(frame, pipeline.input_sample_rate)

    @mock.patch("webrtcvad.Vad.is_speech")
    def test_voice_activity_detection_vad_silence(self, mock_is_speech):
        """
        Test that the voice activity detection returns False for silent frames.
        """
        pipeline = VoicePipeline()
        mock_is_speech.return_value = False
        frame = b"\x00\x00" * (16000 * 30 // 1000)
        assert pipeline._voice_activity_detection(frame) is False
        mock_is_speech.assert_called_once_with(frame, pipeline.input_sample_rate)

    @mock.patch("webrtcvad.Vad.is_speech")
    def test_voice_activity_detection_vad_error_fallback_silent(self, mock_is_speech):
        """
        Test that the voice activity detection returns False for silent frames.
        """
        pipeline = VoicePipeline(silence_threshold=0.1)
        mock_is_speech.side_effect = ValueError("VAD error")

        frame_np = np.full(480, 0.001, dtype=np.float32)
        frame_bytes = (frame_np * 32768.0).astype(np.int16).tobytes()

        assert pipeline._voice_activity_detection(frame_bytes) is False
        mock_is_speech.assert_called_once_with(frame_bytes, pipeline.input_sample_rate)

    @mock.patch("webrtcvad.Vad.is_speech")
    def test_voice_activity_detection_vad_error_fallback_speech(self, mock_is_speech):
        pipeline = VoicePipeline(silence_threshold=0.01)
        mock_is_speech.side_effect = ValueError("VAD error")
        frame_np = np.full(480, 0.5, dtype=np.float32)
        frame_bytes = (frame_np * 32768.0).astype(np.int16).tobytes()

        assert pipeline._voice_activity_detection(frame_bytes) is True
        mock_is_speech.assert_called_once_with(frame_bytes, pipeline.input_sample_rate)



================================================
FILE: mlx_audio/stt/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/stt/generate.py
================================================
import argparse
import json
import os
import time
from pathlib import Path
from typing import Optional

import mlx.core as mx

from mlx_audio.stt.utils import load_model


def parse_args():
    parser = argparse.ArgumentParser(
        description="Generate transcriptions from audio files"
    )
    parser.add_argument("--model", type=str, required=True, help="Path to the model")
    parser.add_argument(
        "--audio", type=str, required=True, help="Path to the audio file"
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Path to save the output"
    )
    parser.add_argument(
        "--format",
        type=str,
        default="txt",
        choices=["txt", "srt", "vtt", "json"],
        help="Output format (txt, srt, vtt, or json)",
    )
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    return parser.parse_args()


def format_timestamp(seconds: float) -> str:
    """Convert seconds to HH:MM:SS,mmm format for SRT/VTT"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}".replace(".", ",")


def format_vtt_timestamp(seconds: float) -> str:
    """Convert seconds to HH:MM:SS.mmm format for VTT"""
    return format_timestamp(seconds).replace(",", ".")


def save_as_txt(segments, output_path: str):
    with open(f"{output_path}.txt", "w", encoding="utf-8") as f:
        f.write(segments.text)


def save_as_srt(segments, output_path: str):
    with open(f"{output_path}.srt", "w", encoding="utf-8") as f:
        for i, sentence in enumerate(segments.sentences, 1):
            f.write(f"{i}\n")
            f.write(
                f"{format_timestamp(sentence.start)} --> {format_timestamp(sentence.end)}\n"
            )
            f.write(f"{sentence.text}\n\n")


def save_as_vtt(segments, output_path: str):
    with open(f"{output_path}.vtt", "w", encoding="utf-8") as f:
        f.write("WEBVTT\n\n")
        if hasattr(segments, "sentences"):
            sentences = segments.sentences

            for i, sentence in enumerate(sentences, 1):
                f.write(f"{i}\n")
                f.write(
                    f"{format_vtt_timestamp(sentence.start)} --> {format_vtt_timestamp(sentence.end)}\n"
                )
                f.write(f"{sentence.text}\n\n")
        else:
            sentences = segments.segments
            for i, token in enumerate(sentences, 1):
                f.write(f"{i}\n")
                f.write(
                    f"{format_vtt_timestamp(token['start'])} --> {format_vtt_timestamp(token['end'])}\n"
                )
                f.write(f"{token['text']}\n\n")


def save_as_json(segments, output_path: str):
    if hasattr(segments, "sentences"):
        result = {
            "text": segments.text,
            "sentences": [
                {
                    "text": s.text,
                    "start": s.start,
                    "end": s.end,
                    "duration": s.duration,
                    "tokens": [
                        {
                            "text": t.text,
                            "start": t.start,
                            "end": t.end,
                            "duration": t.duration,
                        }
                        for t in s.tokens
                    ],
                }
                for s in segments.sentences
            ],
        }
    else:
        result = {
            "text": segments.text,
            "segments": [
                {
                    "text": s["text"],
                    "start": s["start"],
                    "end": s["end"],
                    "duration": s["end"] - s["start"],
                }
                for s in segments.segments
            ],
        }

    with open(f"{output_path}.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def generate(
    model_path: str,
    audio_path: str,
    output_path: str,
    format: str = "txt",
    verbose: bool = True,
):
    model = load_model(model_path)
    print(f"\n\033[94mModel:\033[0m {model_path}")
    print(f"\033[94mAudio path:\033[0m {audio_path}")
    print(f"\033[94mOutput path:\033[0m {output_path}")
    print(f"\033[94mFormat:\033[0m {format}")
    mx.reset_peak_memory()
    start_time = time.time()
    segments = model.generate(audio_path)
    end_time = time.time()

    if verbose:
        print("\n\033[94mTranscription:\033[0m")
        print(segments.text)
        print("\n\033[94mSegments:\033[0m")
        if hasattr(segments, "segments"):
            print(segments.segments)
        elif hasattr(segments, "tokens"):
            print(segments.tokens)
        else:
            print(segments)

    print(f"\033[94mProcessing time:\033[0m {end_time - start_time:.2f} seconds")
    print(f"\033[94mPeak memory:\033[0m {mx.get_peak_memory() / 1e9:.2f} GB")

    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    if format == "txt":
        save_as_txt(segments, output_path)
    elif format == "srt":
        save_as_srt(segments, output_path)
    elif format == "vtt":
        save_as_vtt(segments, output_path)
    elif format == "json":
        save_as_json(segments, output_path)

    return segments


if __name__ == "__main__":
    args = parse_args()
    generate(args.model, args.audio, args.output, args.format, args.verbose)



================================================
FILE: mlx_audio/stt/utils.py
================================================
import importlib
import logging
from pathlib import Path
from typing import List, Optional

import mlx.core as mx
import numpy as np
import soundfile as sf
from huggingface_hub import snapshot_download
from scipy import signal

SAMPLE_RATE = 16000

MODEL_REMAPPING = {}
MAX_FILE_SIZE_GB = 5
MODEL_CONVERSION_DTYPES = ["float16", "bfloat16", "float32"]


def resample_audio(audio: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:
    gcd = np.gcd(orig_sr, target_sr)
    up = target_sr // gcd
    down = orig_sr // gcd
    resampled = signal.resample_poly(audio, up, down, padtype="edge")
    return resampled


def load_audio(
    file: str = Optional[str],
    sr: int = SAMPLE_RATE,
    from_stdin=False,
    dtype: mx.Dtype = mx.float32,
):
    """
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """
    audio, sample_rate = sf.read(file, always_2d=True)
    if sample_rate != sr:
        audio = resample_audio(audio, sample_rate, sr)
    return mx.array(audio, dtype=dtype).mean(axis=1)


def get_model_path(path_or_hf_repo: str, revision: Optional[str] = None) -> Path:
    """
    Ensures the model is available locally. If the path does not exist locally,
    it is downloaded from the Hugging Face Hub.

    Args:
        path_or_hf_repo (str): The local path or Hugging Face repository ID of the model.
        revision (str, optional): A revision id which can be a branch name, a tag, or a commit hash.

    Returns:
        Path: The path to the model.
    """
    model_path = Path(path_or_hf_repo)

    if not model_path.exists():
        model_path = Path(
            snapshot_download(
                path_or_hf_repo,
                revision=revision,
                allow_patterns=[
                    "*.json",
                    "*.safetensors",
                    "*.py",
                    "*.model",
                    "*.tiktoken",
                    "*.txt",
                    "*.jsonl",
                    "*.yaml",
                ],
            )
        )

    return model_path


# Get a list of all available model types from the models directory
def get_available_models():
    """
    Get a list of all available TTS model types by scanning the models directory.

    Returns:
        List[str]: A list of available model type names
    """
    models_dir = Path(__file__).parent / "models"
    available_models = []

    if models_dir.exists() and models_dir.is_dir():
        for item in models_dir.iterdir():
            if item.is_dir() and not item.name.startswith("__"):
                available_models.append(item.name)

    return available_models


def get_model_and_args(model_type: str, model_name: List[str]):
    """
    Retrieve the model architecture module based on the model type and name.

    This function attempts to find the appropriate model architecture by:
    1. Checking if the model_type is directly in the MODEL_REMAPPING dictionary
    2. Looking for partial matches in segments of the model_name

    Args:
        model_type (str): The type of model to load (e.g., "outetts").
        model_name (List[str]): List of model name components that might contain
                               remapping information.

    Returns:
        Tuple[module, str]: A tuple containing:
            - The imported architecture module
            - The resolved model_type string after remapping

    Raises:
        ValueError: If the model type is not supported (module import fails).
    """
    # Stage 1: Check if the model type is in the remapping
    model_type = MODEL_REMAPPING.get(model_type, model_type)

    # Stage 2: Check for partial matches in segments of the model name
    models = get_available_models()
    if model_name is not None:
        for part in model_name:
            # First check if the part matches an available model directory name
            if part in models:
                model_type = part

            # Then check if the part is in our custom remapping dictionary
            if part in MODEL_REMAPPING:
                model_type = MODEL_REMAPPING[part]
                break

    try:
        arch = importlib.import_module(f"mlx_audio.stt.models.{model_type}")
    except ImportError:
        msg = f"Model type {model_type} not supported."
        logging.error(msg)
        raise ValueError(msg)

    return arch, model_type


def load_model(model_path: str, lazy: bool = False, strict: bool = True, **kwargs):
    """
    Load and initialize the model from a given path.

    Args:
        model_path (str): The path to load the model from.
        lazy (bool): If False eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``

    Returns:
        nn.Module: The loaded and initialized model.

    Raises:
        FileNotFoundError: If the weight files (.safetensors) are not found.
        ValueError: If the model class or args class are not found or cannot be instantiated.
    """
    model_name = None
    model_type = None
    if isinstance(model_path, str):
        model_name = model_path.lower().split("/")[-1].split("-")
    elif isinstance(model_path, Path):
        index = model_path.parts.index("hub")
        model_name = model_path.parts[index + 1].lower().split("--")[-1].split("-")
    else:
        raise ValueError(f"Invalid model path type: {type(model_path)}")

    model_class, model_type = get_model_and_args(
        model_type=model_type, model_name=model_name
    )
    model = model_class.Model.from_pretrained(model_path)

    if not lazy:
        model.eval()

    return model



================================================
FILE: mlx_audio/stt/models/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/stt/models/parakeet/__init__.py
================================================
from .parakeet import Model



================================================
FILE: mlx_audio/stt/models/parakeet/alignment.py
================================================
from dataclasses import dataclass


@dataclass
class AlignedToken:
    id: int
    text: str
    start: float
    duration: float
    end: float = 0.0  # temporary

    def __post_init__(self) -> None:
        self.end = self.start + self.duration


@dataclass
class AlignedSentence:
    text: str
    tokens: list[AlignedToken]
    start: float = 0.0  # temporary
    end: float = 0.0  # temporary
    duration: float = 0.0  # temporary

    def __post_init__(self) -> None:
        self.tokens = list(sorted(self.tokens, key=lambda x: x.start))
        self.start = self.tokens[0].start
        self.end = self.tokens[-1].end
        self.duration = self.end - self.start


@dataclass
class AlignedResult:
    text: str
    sentences: list[AlignedSentence]

    def __post_init__(self) -> None:
        self.text = self.text.strip()


def tokens_to_sentences(tokens: list[AlignedToken]) -> list[AlignedSentence]:
    sentences = []
    current_tokens = []

    for idx, token in enumerate(tokens):
        current_tokens.append(token)

        # hacky, will fix
        if (
            "!" in token.text
            or "?" in token.text
            or "ã€‚" in token.text
            or "ï¼Ÿ" in token.text
            or "ï¼" in token.text
            or (
                "." in token.text
                and (idx == len(tokens) - 1 or " " in tokens[idx + 1].text)
            )
        ):  # type: ignore
            sentence_text = "".join(t.text for t in current_tokens)
            sentence = AlignedSentence(text=sentence_text, tokens=current_tokens)
            sentences.append(sentence)

            current_tokens = []

    if current_tokens:
        sentence_text = "".join(t.text for t in current_tokens)
        sentence = AlignedSentence(text=sentence_text, tokens=current_tokens)
        sentences.append(sentence)

    return sentences


def sentences_to_result(sentences: list[AlignedSentence]) -> AlignedResult:
    return AlignedResult("".join(sentence.text for sentence in sentences), sentences)


def merge_longest_contiguous(
    a: list[AlignedToken],
    b: list[AlignedToken],
    *,
    overlap_duration: float,
):
    if not a or not b:
        return b if not a else a

    a_end_time = a[-1].end
    b_start_time = b[0].start

    if a_end_time <= b_start_time:
        return a + b

    overlap_a = [token for token in a if token.end > b_start_time - overlap_duration]
    overlap_b = [token for token in b if token.start < a_end_time + overlap_duration]

    enough_pairs = len(overlap_a) // 2

    if len(overlap_a) < 2 or len(overlap_b) < 2:
        cutoff_time = (a_end_time + b_start_time) / 2
        return [t for t in a if t.end <= cutoff_time] + [
            t for t in b if t.start >= cutoff_time
        ]

    best_contiguous = []
    for i in range(len(overlap_a)):
        for j in range(len(overlap_b)):
            if (
                overlap_a[i].id == overlap_b[j].id
                and abs(overlap_a[i].start - overlap_b[j].start) < overlap_duration / 2
            ):
                current = []
                k, l_ = i, j
                while (
                    k < len(overlap_a)
                    and l_ < len(overlap_b)
                    and overlap_a[k].id == overlap_b[l_].id
                    and abs(overlap_a[k].start - overlap_b[l_].start)
                    < overlap_duration / 2
                ):
                    current.append((k, l_))
                    k += 1
                    l_ += 1

                if len(current) > len(best_contiguous):
                    best_contiguous = current

    if len(best_contiguous) >= enough_pairs:
        a_start_idx = len(a) - len(overlap_a)
        lcs_indices_a = [a_start_idx + pair[0] for pair in best_contiguous]
        lcs_indices_b = [pair[1] for pair in best_contiguous]

        result = []
        result.extend(a[: lcs_indices_a[0]])

        for i in range(len(best_contiguous)):
            idx_a = lcs_indices_a[i]
            idx_b = lcs_indices_b[i]

            result.append(a[idx_a])

            if i < len(best_contiguous) - 1:
                next_idx_a = lcs_indices_a[i + 1]
                next_idx_b = lcs_indices_b[i + 1]

                gap_tokens_a = a[idx_a + 1 : next_idx_a]
                gap_tokens_b = b[idx_b + 1 : next_idx_b]

                if len(gap_tokens_b) > len(gap_tokens_a):
                    result.extend(gap_tokens_b)
                else:
                    result.extend(gap_tokens_a)

        result.extend(b[lcs_indices_b[-1] + 1 :])
        return result
    else:
        raise RuntimeError(f"No pairs exceeding {enough_pairs}")


def merge_longest_common_subsequence(
    a: list[AlignedToken],
    b: list[AlignedToken],
    *,
    overlap_duration: float,
):
    if not a or not b:
        return b if not a else a

    a_end_time = a[-1].end
    b_start_time = b[0].start

    if a_end_time <= b_start_time:
        return a + b

    overlap_a = [token for token in a if token.end > b_start_time - overlap_duration]
    overlap_b = [token for token in b if token.start < a_end_time + overlap_duration]

    if len(overlap_a) < 2 or len(overlap_b) < 2:
        cutoff_time = (a_end_time + b_start_time) / 2
        return [t for t in a if t.end <= cutoff_time] + [
            t for t in b if t.start >= cutoff_time
        ]

    dp = [[0 for _ in range(len(overlap_b) + 1)] for _ in range(len(overlap_a) + 1)]

    for i in range(1, len(overlap_a) + 1):
        for j in range(1, len(overlap_b) + 1):
            if (
                overlap_a[i - 1].id == overlap_b[j - 1].id
                and abs(overlap_a[i - 1].start - overlap_b[j - 1].start)
                < overlap_duration / 2
            ):
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

    lcs_pairs = []
    i, j = len(overlap_a), len(overlap_b)

    while i > 0 and j > 0:
        if (
            overlap_a[i - 1].id == overlap_b[j - 1].id
            and abs(overlap_a[i - 1].start - overlap_b[j - 1].start)
            < overlap_duration / 2
        ):
            lcs_pairs.append((i - 1, j - 1))
            i -= 1
            j -= 1
        elif dp[i - 1][j] > dp[i][j - 1]:
            i -= 1
        else:
            j -= 1

    lcs_pairs.reverse()

    if not lcs_pairs:
        cutoff_time = (a_end_time + b_start_time) / 2
        return [t for t in a if t.end <= cutoff_time] + [
            t for t in b if t.start >= cutoff_time
        ]

    a_start_idx = len(a) - len(overlap_a)
    lcs_indices_a = [a_start_idx + pair[0] for pair in lcs_pairs]
    lcs_indices_b = [pair[1] for pair in lcs_pairs]

    result = []

    result.extend(a[: lcs_indices_a[0]])

    for i in range(len(lcs_pairs)):
        idx_a = lcs_indices_a[i]
        idx_b = lcs_indices_b[i]

        result.append(a[idx_a])

        if i < len(lcs_pairs) - 1:
            next_idx_a = lcs_indices_a[i + 1]
            next_idx_b = lcs_indices_b[i + 1]

            gap_tokens_a = a[idx_a + 1 : next_idx_a]
            gap_tokens_b = b[idx_b + 1 : next_idx_b]

            if len(gap_tokens_b) > len(gap_tokens_a):
                result.extend(gap_tokens_b)
            else:
                result.extend(gap_tokens_a)

    result.extend(b[lcs_indices_b[-1] + 1 :])

    return result



================================================
FILE: mlx_audio/stt/models/parakeet/attention.py
================================================
import math

import mlx.core as mx
import mlx.nn as nn


class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        n_head: int,
        n_feat: int,
        bias=True,
    ):
        super().__init__()

        self.n_head = n_head
        self.head_dim = n_feat // n_head
        self.scale = self.head_dim**-0.5

        self.linear_q = nn.Linear(n_feat, n_feat, bias=bias)
        self.linear_k = nn.Linear(n_feat, n_feat, bias=bias)
        self.linear_v = nn.Linear(n_feat, n_feat, bias=bias)
        self.linear_out = nn.Linear(n_feat, n_feat, bias=bias)

    def __call__(
        self,
        q: mx.array,
        k: mx.array,
        v: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)

        batch, q_seq, _ = q.shape
        _, k_seq, _ = k.shape

        q = q.reshape(batch, q_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        k = k.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)

        if cache:
            k, v = cache.update_and_fetch(k, v)

        o = mx.fast.scaled_dot_product_attention(q, k, v, scale=self.scale, mask=mask)
        o = o.transpose(0, 2, 1, 3).reshape(batch, q_seq, self.n_feat)

        return self.linear_out(o)


class RelPositionMultiHeadAttention(MultiHeadAttention):
    def __init__(
        self,
        n_head: int,
        n_feat: int,
        bias: bool = True,
        pos_bias_u: mx.array | None = None,
        pos_bias_v: mx.array | None = None,
    ):
        super().__init__(
            n_head=n_head,
            n_feat=n_feat,
            bias=bias,
        )

        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)

        if pos_bias_u is None:
            self._pos_bias_u_init = mx.zeros((self.n_head, self.head_dim))
        else:
            self._pos_bias_u_init = pos_bias_u

        if pos_bias_v is None:
            self._pos_bias_v_init = mx.zeros((self.n_head, self.head_dim))
        else:
            self._pos_bias_v_init = pos_bias_v

        self.pos_bias_u = self._pos_bias_u_init
        self.pos_bias_v = self._pos_bias_v_init

    def rel_shift(self, x: mx.array) -> mx.array:
        B, H, Tq, pos_len = x.shape
        padding = [(0, 0)] * (x.ndim - 1) + [(1, 0)]

        x = mx.pad(x, padding)
        x = x.reshape(B, H, pos_len + 1, Tq)
        x = x[:, :, 1:, :]
        x = x.reshape(B, H, Tq, pos_len)

        return x

    def __call__(
        self,
        q: mx.array,
        k: mx.array,
        v: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        if pos_emb is None:
            raise ValueError("pos_emb is necessary!")

        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)

        p = self.linear_pos(pos_emb)  # p stands for position

        batch, q_seq, _ = q.shape
        _, k_seq, _ = k.shape
        _, pos_len, _ = p.shape

        q = q.reshape(batch, q_seq, self.n_head, self.head_dim)
        q_u = (q + self.pos_bias_u).transpose(0, 2, 1, 3)
        q_v = (q + self.pos_bias_v).transpose(0, 2, 1, 3)

        k = k.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        p = p.reshape(batch, pos_len, self.n_head, self.head_dim).transpose(0, 2, 1, 3)

        if cache is not None:
            k, v = cache.update_and_fetch(k, v)

        matrix_bd = mx.matmul(q_v, p.swapaxes(-2, -1))
        matrix_bd = self.rel_shift(matrix_bd)
        matrix_bd = matrix_bd[:, :, :, : k.shape[-2]] * self.scale

        if mask is not None:
            mask = mx.expand_dims(mask, 0)
            matrix_bd[mask] = -mx.inf

        o = mx.fast.scaled_dot_product_attention(
            q_u, k, v, scale=self.scale, mask=matrix_bd
        )
        o = o.transpose(0, 2, 1, 3).reshape(batch, q_seq, -1)

        return self.linear_out(o)


class RelPositionalEncoding(nn.Module):
    def __init__(
        self,
        d_model: int,
        max_len: int = 5000,
        scale_input: bool = True,
    ):
        assert d_model % 2 == 0 and max_len > 0
        super().__init__()

        self.d_model = d_model
        self.max_len = max_len
        self.scale = math.sqrt(self.d_model) if scale_input else 1.0
        self.calculate_pe()

    def calculate_pe(self):
        positions = mx.arange(self.max_len - 1, -self.max_len, -1, dtype=mx.int32)
        positions = mx.expand_dims(positions, axis=1).astype(mx.float32)

        div_term = mx.exp(
            mx.arange(0, self.d_model, 2, dtype=mx.float32)
            * -(math.log(10000.0) / self.d_model)
        )
        pe = mx.zeros((2 * self.max_len - 1, self.d_model), dtype=mx.float32)

        pe[:, 0::2] = mx.sin(positions * div_term)
        pe[:, 1::2] = mx.cos(positions * div_term)

        self._pe = mx.expand_dims(pe, axis=0).astype(mx.float32)

        mx.eval(self._pe)

    def __call__(self, x: mx.array, offset: int = 0) -> tuple[mx.array, mx.array]:
        input_len = x.shape[1] + offset

        if input_len > self.max_len:
            self.max_len = input_len + 1
            self.calculate_pe()

        x = x * self.scale

        buffer_len = self._pe.shape[1]
        start_idx = buffer_len // 2 - (input_len - 1)
        end_idx = buffer_len // 2 + (input_len - 1) + 1

        pos_emb = self._pe[:, start_idx:end_idx].astype(x.dtype)

        return x, pos_emb



================================================
FILE: mlx_audio/stt/models/parakeet/audio.py
================================================
from dataclasses import dataclass

import mlx.core as mx

from mlx_audio.utils import (
    STR_TO_WINDOW_FN,
    bartlett,
    blackman,
    hamming,
    hanning,
    mel_filters,
    stft,
)


@dataclass
class PreprocessArgs:
    sample_rate: int
    normalize: str
    window_size: float
    window_stride: float
    window: str
    features: int
    n_fft: int
    dither: float
    pad_to: int = 0
    pad_value: float = 0

    @property
    def win_length(self) -> int:
        return int(self.window_size * self.sample_rate)

    @property
    def hop_length(self) -> int:
        return int(self.window_stride * self.sample_rate)


def log_mel_spectrogram(x: mx.array, args: PreprocessArgs) -> mx.array:
    original_dtype = x.dtype

    if args.pad_to > 0:
        if x.shape[-1] < args.pad_to:
            pad_length = args.pad_to - x.shape[-1]
            x = mx.pad(x, ((0, pad_length),), constant_values=args.pad_value)

    window_fn = STR_TO_WINDOW_FN.get(args.window, None)
    window = window_fn(args.win_length) if window_fn else hanning(args.win_length)

    x = stft(x, args.n_fft, args.hop_length, args.win_length, window)
    x = mx.square(mx.abs(x)).astype(original_dtype)
    filters = mel_filters(
        args.sample_rate, args.n_fft, args.features, norm=args.normalize, mel_scale=None
    )
    x = filters.astype(x.dtype) @ x.T

    x = mx.log(x + 1e-5)

    if args.normalize == "per_feature":
        mean = mx.mean(x, axis=1, keepdims=True)
        std = mx.std(x, axis=1, keepdims=True)
        normalized_mel = (x - mean) / (std + 1e-5)
    else:
        mean = mx.mean(x)
        std = mx.std(x)
        normalized_mel = (x - mean) / (std + 1e-5)

    normalized_mel = normalized_mel.T
    normalized_mel = mx.expand_dims(normalized_mel, axis=0)

    return normalized_mel.astype(original_dtype)



================================================
FILE: mlx_audio/stt/models/parakeet/conformer.py
================================================
import math
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.stt.models.parakeet.attention import (
    MultiHeadAttention,
    RelPositionalEncoding,
    RelPositionMultiHeadAttention,
)


@dataclass
class ConformerArgs:
    feat_in: int  # mel-log
    n_layers: int
    d_model: int
    n_heads: int
    ff_expansion_factor: int
    subsampling_factor: int
    self_attention_model: str
    subsampling: str
    conv_kernel_size: int
    subsampling_conv_channels: int
    pos_emb_max_len: int
    causal_downsampling: bool = False
    use_bias: bool = True
    xscaling: bool = False
    pos_bias_u: mx.array | None = None
    pos_bias_v: mx.array | None = None
    subsampling_conv_chunking_factor: int = 1


class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, use_bias: bool = True):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff, bias=use_bias)
        self.activation = nn.SiLU()
        self.linear2 = nn.Linear(d_ff, d_model, bias=use_bias)

    def __call__(self, x: mx.array) -> mx.array:
        return self.linear2(self.activation(self.linear1(x)))


class Convolution(nn.Module):
    def __init__(self, args: ConformerArgs):
        assert (args.conv_kernel_size - 1) % 2 == 0
        super().__init__()

        self.pointwise_conv1 = nn.Conv1d(
            args.d_model,
            args.d_model * 2,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=args.use_bias,
        )
        self.depthwise_conv = nn.Conv1d(
            args.d_model,
            args.d_model,
            kernel_size=args.conv_kernel_size,
            stride=1,
            padding=(args.conv_kernel_size - 1) // 2,
            groups=args.d_model,
            bias=args.use_bias,
        )
        self.batch_norm = nn.BatchNorm(args.d_model)
        self.activation = nn.SiLU()
        self.pointwise_conv2 = nn.Conv1d(
            args.d_model,
            args.d_model,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=args.use_bias,
        )

    def __call__(self, x: mx.array) -> mx.array:
        # x = x.swapaxes(1, 2)

        x = self.pointwise_conv1(x)
        x = nn.glu(x, axis=2)  # might make it variable later

        x = self.depthwise_conv(x)
        x = self.batch_norm(x)
        x = self.activation(x)
        x = self.pointwise_conv2(x)

        return x


class ConformerBlock(nn.Module):
    def __init__(self, args: ConformerArgs):
        super().__init__()
        ff_hidden_dim = args.d_model * args.ff_expansion_factor

        self.norm_feed_forward1 = nn.LayerNorm(args.d_model)
        self.feed_forward1 = FeedForward(args.d_model, ff_hidden_dim, args.use_bias)

        self.norm_self_att = nn.LayerNorm(args.d_model)
        self.self_attn = (
            RelPositionMultiHeadAttention(
                args.n_heads,
                args.d_model,
                bias=args.use_bias,
                pos_bias_u=args.pos_bias_u,
                pos_bias_v=args.pos_bias_v,
            )
            if args.self_attention_model == "rel_pos"
            else MultiHeadAttention(
                args.n_heads,
                args.d_model,
                bias=True,
            )
        )

        self.norm_conv = nn.LayerNorm(args.d_model)
        self.conv = Convolution(args)

        self.norm_feed_forward2 = nn.LayerNorm(args.d_model)
        self.feed_forward2 = FeedForward(args.d_model, ff_hidden_dim, args.use_bias)

        self.norm_out = nn.LayerNorm(args.d_model)

    def __call__(
        self,
        x: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        x += 0.5 * self.feed_forward1(self.norm_feed_forward1(x))

        x_norm = self.norm_self_att(x)
        x += self.self_attn(
            x_norm, x_norm, x_norm, mask=mask, pos_emb=pos_emb, cache=cache
        )

        x += self.conv(self.norm_conv(x))
        x += 0.5 * self.feed_forward2(self.norm_feed_forward2(x))

        return self.norm_out(x)


class DwStridingSubsampling(nn.Module):
    def __init__(self, args: ConformerArgs):
        super().__init__()

        assert (
            args.subsampling_factor > 0
            and (args.subsampling_factor & (args.subsampling_factor - 1)) == 0
        )
        self.subsampling_conv_chunking_factor = args.subsampling_conv_chunking_factor
        self._conv_channels = args.subsampling_conv_channels
        self._sampling_num = int(math.log(args.subsampling_factor, 2))
        self._stride = 2
        self._kernel_size = 3
        self._padding = (self._kernel_size - 1) // 2

        in_channels = 1
        final_freq_dim = args.feat_in
        for _ in range(self._sampling_num):
            final_freq_dim = (
                math.floor(
                    (final_freq_dim + 2 * self._padding - self._kernel_size)
                    / self._stride
                )
                + 1
            )
            if final_freq_dim < 1:
                raise ValueError("Non-positive final frequency dimension!")

        self.conv = [
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=self._conv_channels,
                kernel_size=self._kernel_size,
                stride=self._stride,
                padding=self._padding,
            ),
            nn.ReLU(),
        ]
        in_channels = self._conv_channels

        for _ in range(self._sampling_num - 1):
            self.conv.append(
                nn.Conv2d(
                    in_channels=in_channels,
                    out_channels=in_channels,
                    kernel_size=self._kernel_size,
                    stride=self._stride,
                    padding=self._padding,
                    groups=in_channels,
                )
            )
            self.conv.append(
                nn.Conv2d(
                    in_channels=in_channels,
                    out_channels=self._conv_channels,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                    groups=1,
                )
            )
            self.conv.append(nn.ReLU())

        self.out = nn.Linear(self._conv_channels * final_freq_dim, args.d_model)

    def conv_forward(self, x: mx.array) -> mx.array:
        x = x.transpose((0, 2, 3, 1))
        for layer in self.conv:
            x = layer(x)
        return x.transpose((0, 3, 1, 2))

    def conv_split_by_batch(self, x: mx.array) -> tuple[mx.array, bool]:
        b = x.shape[0]
        if b == 1:
            return x, False

        if self.subsampling_conv_chunking_factor > 1:
            cf = self.subsampling_conv_chunking_factor
        else:
            x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
            p = math.ceil(math.log(x.size / x_ceil, 2))
            cf: int = 2**p

        new_batch_size = b // cf
        if new_batch_size == 0:
            return x, False

        return (
            mx.concat(
                [self.conv_forward(chunk) for chunk in mx.split(x, new_batch_size, 0)]
            ),
            True,
        )

    def __call__(self, x: mx.array, lengths: mx.array) -> tuple[mx.array, mx.array]:
        for _ in range(self._sampling_num):
            lengths = (
                mx.floor(
                    (lengths + 2 * self._padding - self._kernel_size) / self._stride
                )
                + 1.0
            )
        lengths = lengths.astype(mx.int32)

        x = mx.expand_dims(x, axis=1)

        if self.subsampling_conv_chunking_factor != -1:
            if self.subsampling_conv_chunking_factor == 1:
                x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
                need_to_split = x.size > x_ceil
            else:
                need_to_split = True

            if need_to_split:
                x, success = self.conv_split_by_batch(x)
                if not success:
                    # TODO: Add channel splitting
                    x = self.conv_forward(x)  # try anyways
            else:
                x = self.conv_forward(x)
        else:
            x = self.conv_forward(x)

        x = x.swapaxes(1, 2).reshape(x.shape[0], x.shape[2], -1)
        x = self.out(x)
        return x, lengths


class Conformer(nn.Module):
    def __init__(self, args: ConformerArgs):
        super().__init__()

        if args.self_attention_model == "rel_pos":
            self.pos_enc = RelPositionalEncoding(
                d_model=args.d_model,
                max_len=args.pos_emb_max_len,
                scale_input=args.xscaling,
            )
        else:
            self.pos_enc = None

        if args.subsampling_factor > 1:
            if args.subsampling == "dw_striding" and args.causal_downsampling is False:
                self.pre_encode = DwStridingSubsampling(args)
            else:
                self.pre_encode = nn.Identity()
                raise NotImplementedError(
                    "Other subsampling haven't been implemented yet!"
                )
        else:
            self.pre_encode = nn.Linear(args.feat_in, args.d_model)

        self.layers = [ConformerBlock(args) for _ in range(args.n_layers)]

    def __call__(
        self, x: mx.array, lengths: mx.array | None = None, cache=None
    ) -> tuple[mx.array, mx.array]:
        if lengths is None:
            lengths = mx.full(
                (x.shape[0],),
                x.shape[-2],
                dtype=mx.int64,
            )

        if isinstance(self.pre_encode, DwStridingSubsampling):
            x, out_lengths = self.pre_encode(x, lengths)
        elif isinstance(self.pre_encode, nn.Linear):
            x = self.pre_encode(x)
            out_lengths = lengths
        else:
            raise NotImplementedError("Unimplemented pre-encoding layer type!")

        if cache is None:
            cache = [None] * len(self.layers)

        pos_emb = None
        if self.pos_enc is not None:
            x, pos_emb = self.pos_enc(
                x,
                offset=cache[0].offset if cache[0] is not None else 0,  # type: ignore
            )

        for layer, c in zip(self.layers, cache):
            x = layer(x, pos_emb=pos_emb, cache=c)

        return x, out_lengths



================================================
FILE: mlx_audio/stt/models/parakeet/ctc.py
================================================
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn


@dataclass
class ConvASRDecoderArgs:
    feat_in: int
    num_classes: int
    vocabulary: list[str]


@dataclass
class AuxCTCArgs:
    decoder: ConvASRDecoderArgs


class ConvASRDecoder(nn.Module):
    def __init__(self, args: ConvASRDecoderArgs):
        super().__init__()

        args.num_classes = (
            len(args.vocabulary) if args.num_classes <= 0 else args.num_classes
        ) + 1

        self.decoder_layers = [
            nn.Conv1d(args.feat_in, args.num_classes, kernel_size=1, bias=True)
        ]

        self.temperature = 1.0  # change manually if desired

    def __call__(self, x: mx.array) -> mx.array:
        return nn.log_softmax(self.decoder_layers[0](x) / self.temperature)



================================================
FILE: mlx_audio/stt/models/parakeet/parakeet.py
================================================
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Optional

import mlx.core as mx
import mlx.nn as nn
from dacite import from_dict
from huggingface_hub import hf_hub_download
from mlx.utils import tree_flatten, tree_unflatten

from mlx_audio.stt.models.parakeet import tokenizer
from mlx_audio.stt.models.parakeet.alignment import (
    AlignedResult,
    AlignedToken,
    merge_longest_common_subsequence,
    merge_longest_contiguous,
    sentences_to_result,
    tokens_to_sentences,
)
from mlx_audio.stt.models.parakeet.audio import PreprocessArgs, log_mel_spectrogram
from mlx_audio.stt.models.parakeet.conformer import Conformer, ConformerArgs
from mlx_audio.stt.models.parakeet.ctc import (
    AuxCTCArgs,
    ConvASRDecoder,
    ConvASRDecoderArgs,
)
from mlx_audio.stt.models.parakeet.rnnt import (
    JointArgs,
    JointNetwork,
    PredictArgs,
    PredictNetwork,
)
from mlx_audio.stt.utils import load_audio


@dataclass
class PreprocessArgs:
    sample_rate: int
    normalize: str
    window_size: float
    window_stride: float
    window: str
    features: int
    n_fft: int
    dither: float
    pad_to: int = 0
    pad_value: float = 0

    @property
    def win_length(self) -> int:
        return int(self.window_size * self.sample_rate)

    @property
    def hop_length(self) -> int:
        return int(self.window_stride * self.sample_rate)


@dataclass
class TDTDecodingArgs:
    model_type: str
    durations: list[int]
    greedy: dict | None


@dataclass
class RNNTDecodingArgs:
    greedy: dict | None


@dataclass
class CTCDecodingArgs:
    greedy: dict | None


@dataclass
class ParakeetTDTArgs:
    preprocessor: PreprocessArgs
    encoder: ConformerArgs
    decoder: PredictArgs
    joint: JointArgs
    decoding: TDTDecodingArgs


@dataclass
class ParakeetRNNTArgs:
    preprocessor: PreprocessArgs
    encoder: ConformerArgs
    decoder: PredictArgs
    joint: JointArgs
    decoding: RNNTDecodingArgs


@dataclass
class ParakeetCTCArgs:
    preprocessor: PreprocessArgs
    encoder: ConformerArgs
    decoder: ConvASRDecoderArgs
    decoding: CTCDecodingArgs


@dataclass
class ParakeetTDTCTCArgs(ParakeetTDTArgs):
    aux_ctc: AuxCTCArgs


class Model(nn.Module):
    def __init__(self, preprocess_args: PreprocessArgs):
        super().__init__()

        self.preprocessor_config = preprocess_args

    def decode(self, mel: mx.array) -> list[AlignedResult]:
        """
        Decode mel spectrograms to produce transcriptions with the Parakeet model.
        Handles batches and single input. Uses greedy decoding.
        mel: [batch, sequence, mel_dim] or [sequence, mel_dim]
        """
        raise NotImplementedError

    def generate(
        self,
        path: Path | str,
        *,
        dtype: mx.Dtype = mx.bfloat16,
        chunk_duration: Optional[float] = None,
        overlap_duration: float = 15.0,
        chunk_callback: Optional[Callable] = None,
    ) -> AlignedResult:
        """
        Transcribe an audio file, with optional chunking for long files.

        Args:
            path: Path to the audio file
            dtype: Data type for processing
            chunk_duration: If provided, splits audio into chunks of this length for processing
            overlap_duration: Overlap between chunks (only used when chunking)
            chunk_callback: A function to call back when chunk is processed, called with (current_position, total_position)

        Returns:
            Transcription result with aligned tokens and sentences
        """
        audio_path = Path(path)
        audio_data = load_audio(
            audio_path, self.preprocessor_config.sample_rate, dtype=dtype
        )

        if chunk_duration is None:
            mel = log_mel_spectrogram(audio_data, self.preprocessor_config)

            return self.decode(mel)[0]

        audio_length_seconds = len(audio_data) / self.preprocessor_config.sample_rate

        if audio_length_seconds <= chunk_duration:
            mel = log_mel_spectrogram(audio_data, self.preprocessor_config)

            return self.decode(mel)[0]

        chunk_samples = int(chunk_duration * self.preprocessor_config.sample_rate)
        overlap_samples = int(overlap_duration * self.preprocessor_config.sample_rate)

        all_tokens = []

        for start in range(0, len(audio_data), chunk_samples - overlap_samples):
            end = min(start + chunk_samples, len(audio_data))

            if chunk_callback is not None:
                chunk_callback(end, len(audio_data))

            chunk_audio = audio_data[start:end]
            chunk_mel = log_mel_spectrogram(chunk_audio, self.preprocessor_config)

            chunk_result = self.decode(chunk_mel)[0]

            chunk_offset = start / self.preprocessor_config.sample_rate
            for sentence in chunk_result.sentences:
                for token in sentence.tokens:
                    token.start += chunk_offset
                    token.end = token.start + token.duration

            chunk_tokens = []
            for sentence in chunk_result.sentences:
                chunk_tokens.extend(sentence.tokens)

            if all_tokens:
                try:
                    all_tokens = merge_longest_contiguous(
                        all_tokens, chunk_tokens, overlap_duration=overlap_duration
                    )
                except RuntimeError:
                    all_tokens = merge_longest_common_subsequence(
                        all_tokens, chunk_tokens, overlap_duration=overlap_duration
                    )
            else:
                all_tokens = chunk_tokens

        result = sentences_to_result(tokens_to_sentences(all_tokens))

        # Clear cache after each segment to avoid memory leaks
        mx.clear_cache()

        return result

    @classmethod
    def from_config(cls, config: dict):
        """Loads model from config (randomized weights)"""
        if (
            config.get("target")
            == "nemo.collections.asr.models.rnnt_bpe_models.EncDecRNNTBPEModel"
            and config.get("model_defaults", {}).get("tdt_durations") is not None
        ):
            cfg = from_dict(ParakeetTDTArgs, config)
            model = ParakeetTDT(cfg)
        elif (
            config.get("target")
            == "nemo.collections.asr.models.hybrid_rnnt_ctc_bpe_models.EncDecHybridRNNTCTCBPEModel"
            and config.get("model_defaults", {}).get("tdt_durations") is not None
        ):
            cfg = from_dict(ParakeetTDTCTCArgs, config)
            model = ParakeetTDTCTC(cfg)
        elif (
            config.get("target")
            == "nemo.collections.asr.models.rnnt_bpe_models.EncDecRNNTBPEModel"
            and config.get("model_defaults", {}).get("tdt_durations") is None
        ):
            cfg = from_dict(ParakeetRNNTArgs, config)
            model = ParakeetRNNT(cfg)
        elif (
            config.get("target")
            == "nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE"
        ):
            cfg = from_dict(ParakeetCTCArgs, config)
            model = ParakeetCTC(cfg)
        else:
            raise ValueError("Model is not supported yet!")

        model.eval()  # prevents layernorm not computing correctly on inference!

        return model

    @classmethod
    def from_pretrained(cls, path_or_hf_repo: str, *, dtype: mx.Dtype = mx.bfloat16):
        """Loads model from Hugging Face or local directory"""

        try:
            config = json.load(
                open(hf_hub_download(path_or_hf_repo, "config.json"), "r")
            )
            weight = hf_hub_download(path_or_hf_repo, "model.safetensors")
        except Exception:
            config = json.load(open(Path(path_or_hf_repo) / "config.json", "r"))
            weight = str(Path(path_or_hf_repo) / "model.safetensors")

        model = cls.from_config(config)
        model.load_weights(weight)

        # cast dtype
        curr_weights = dict(tree_flatten(model.parameters()))
        curr_weights = [(k, v.astype(dtype)) for k, v in curr_weights.items()]
        model.update(tree_unflatten(curr_weights))

        return model


class ParakeetTDT(Model):
    def __init__(self, args: ParakeetTDTArgs):
        super().__init__(args.preprocessor)

        assert args.decoding.model_type == "tdt", "Model must be a TDT model"

        self.encoder_config = args.encoder

        self.vocabulary = args.joint.vocabulary
        self.durations = args.decoding.durations
        self.max_symbols: int | None = (
            args.decoding.greedy.get("max_symbols", None)
            if args.decoding.greedy
            else None
        )

        self.encoder = Conformer(args.encoder)
        self.decoder = PredictNetwork(args.decoder)
        self.joint = JointNetwork(args.joint)

    def decode(self, mel: mx.array) -> list[AlignedResult]:
        """
        Generate with skip token logic for the Parakeet model, handling batches and single input. Uses greedy decoding.
        mel: [batch, sequence, mel_dim] or [sequence, mel_dim]
        """
        batch_size: int = mel.shape[0]
        if len(mel.shape) == 2:
            batch_size = 1
            mel = mx.expand_dims(mel, 0)

        batch_features, lengths = self.encoder(mel)
        mx.eval(batch_features, lengths)

        results = []
        for b in range(batch_size):
            features = batch_features[b : b + 1]
            max_length = int(lengths[b])

            last_token = len(
                self.vocabulary
            )  # In TDT, space token is always len(vocab)
            hypothesis = []

            time = 0
            new_symbols = 0
            decoder_hidden = None

            while time < max_length:
                feature = features[:, time : time + 1]

                current_token = (
                    mx.array([[last_token]], dtype=mx.int32)
                    if last_token != len(self.vocabulary)
                    else None
                )
                decoder_output, (hidden, cell) = self.decoder(
                    current_token, decoder_hidden
                )

                # cast
                decoder_output = decoder_output.astype(feature.dtype)
                proposed_decoder_hidden = (
                    hidden.astype(feature.dtype),
                    cell.astype(feature.dtype),
                )

                joint_output = self.joint(feature, decoder_output)

                pred_token = mx.argmax(
                    joint_output[0, 0, :, : len(self.vocabulary) + 1]
                )
                decision = mx.argmax(joint_output[0, 0, :, len(self.vocabulary) + 1 :])

                if pred_token != len(self.vocabulary):
                    hypothesis.append(
                        AlignedToken(
                            int(pred_token),
                            start=time
                            * self.encoder_config.subsampling_factor
                            / self.preprocessor_config.sample_rate
                            * self.preprocessor_config.hop_length,  # hop
                            duration=self.durations[int(decision)]
                            * self.encoder_config.subsampling_factor
                            / self.preprocessor_config.sample_rate
                            * self.preprocessor_config.hop_length,  # hop
                            text=tokenizer.decode([int(pred_token)], self.vocabulary),
                        )
                    )
                    last_token = int(pred_token)
                    decoder_hidden = proposed_decoder_hidden

                time += self.durations[int(decision)]
                new_symbols += 1

                if self.durations[int(decision)] != 0:
                    new_symbols = 0
                else:
                    if self.max_symbols is not None and self.max_symbols <= new_symbols:
                        time += 1
                        new_symbols = 0

            result = sentences_to_result(tokens_to_sentences(hypothesis))
            results.append(result)

        return results


class ParakeetRNNT(Model):
    def __init__(self, args: ParakeetRNNTArgs):
        super().__init__(args.preprocessor)

        self.encoder_config = args.encoder

        self.vocabulary = args.joint.vocabulary
        self.max_symbols: int | None = (
            args.decoding.greedy.get("max_symbols", None)
            if args.decoding.greedy
            else None
        )

        self.encoder = Conformer(args.encoder)
        self.decoder = PredictNetwork(args.decoder)
        self.joint = JointNetwork(args.joint)

    def decode(self, mel: mx.array) -> list[AlignedResult]:
        """
        Generate with skip token logic for the Parakeet model, handling batches and single input. Uses greedy decoding.
        mel: [batch, sequence, mel_dim] or [sequence, mel_dim]
        """
        batch_size: int = mel.shape[0]
        if len(mel.shape) == 2:
            batch_size = 1
            mel = mx.expand_dims(mel, 0)

        batch_features, lengths = self.encoder(mel)
        mx.eval(batch_features, lengths)

        results = []
        for b in range(batch_size):
            features = batch_features[b : b + 1]
            max_length = int(lengths[b])

            last_token = len(self.vocabulary)
            hypothesis = []

            time = 0
            new_symbols = 0
            decoder_hidden = None

            while time < max_length:
                feature = features[:, time : time + 1]

                current_token = (
                    mx.array([[last_token]], dtype=mx.int32)
                    if last_token != len(self.vocabulary)
                    else None
                )
                decoder_output, (hidden, cell) = self.decoder(
                    current_token, decoder_hidden
                )

                # cast
                decoder_output = decoder_output.astype(feature.dtype)
                proposed_decoder_hidden = (
                    hidden.astype(feature.dtype),
                    cell.astype(feature.dtype),
                )

                joint_output = self.joint(feature, decoder_output)

                pred_token = mx.argmax(joint_output)

                if pred_token != len(self.vocabulary):
                    hypothesis.append(
                        AlignedToken(
                            int(pred_token),
                            start=time
                            * self.encoder_config.subsampling_factor
                            / self.preprocessor_config.sample_rate
                            * self.preprocessor_config.hop_length,  # hop
                            duration=1
                            * self.encoder_config.subsampling_factor
                            / self.preprocessor_config.sample_rate
                            * self.preprocessor_config.hop_length,  # hop
                            text=tokenizer.decode([int(pred_token)], self.vocabulary),
                        )
                    )
                    last_token = int(pred_token)
                    decoder_hidden = proposed_decoder_hidden

                    new_symbols += 1
                    if self.max_symbols is not None and self.max_symbols <= new_symbols:
                        time += 1
                        new_symbols = 0
                else:
                    time += 1
                    new_symbols = 0

            result = sentences_to_result(tokens_to_sentences(hypothesis))
            results.append(result)

        return results


class ParakeetCTC(Model):
    def __init__(self, args: ParakeetCTCArgs):
        super().__init__(args.preprocessor)

        self.encoder_config = args.encoder

        self.vocabulary = args.decoder.vocabulary

        self.encoder = Conformer(args.encoder)
        self.decoder = ConvASRDecoder(args.decoder)

    def decode(self, mel: mx.array) -> list[AlignedResult]:
        """
        Generate with CTC decoding for the Parakeet model, handling batches and single input. Uses greedy decoding.
        mel: [batch, sequence, mel_dim] or [sequence, mel_dim]
        """
        batch_size: int = mel.shape[0]
        if len(mel.shape) == 2:
            batch_size = 1
            mel = mx.expand_dims(mel, 0)

        batch_features, lengths = self.encoder(mel)
        logits = self.decoder(batch_features)
        mx.eval(logits, lengths)

        results = []
        for b in range(batch_size):
            features_len = int(lengths[b])
            predictions = logits[b, :features_len]
            best_tokens = mx.argmax(predictions, axis=1)

            hypothesis = []
            token_boundaries = []
            prev_token = -1

            for t, token_id in enumerate(best_tokens):
                token_idx = int(token_id)

                if token_idx == len(self.vocabulary):
                    continue

                if token_idx == prev_token:
                    continue

                if prev_token != -1:
                    token_start_time = (
                        token_boundaries[-1][0]
                        * self.encoder_config.subsampling_factor
                        / self.preprocessor_config.sample_rate
                        * self.preprocessor_config.hop_length
                    )

                    token_end_time = (
                        t
                        * self.encoder_config.subsampling_factor
                        / self.preprocessor_config.sample_rate
                        * self.preprocessor_config.hop_length
                    )

                    token_duration = token_end_time - token_start_time

                    hypothesis.append(
                        AlignedToken(
                            prev_token,
                            start=token_start_time,
                            duration=token_duration,
                            text=tokenizer.decode([prev_token], self.vocabulary),
                        )
                    )

                token_boundaries.append((t, None))
                prev_token = token_idx

            if prev_token != -1:
                last_non_blank = features_len - 1
                for t in range(features_len - 1, token_boundaries[-1][0], -1):
                    if int(best_tokens[t]) != len(self.vocabulary):
                        last_non_blank = t
                        break

                token_start_time = (
                    token_boundaries[-1][0]
                    * self.encoder_config.subsampling_factor
                    / self.preprocessor_config.sample_rate
                    * self.preprocessor_config.hop_length
                )

                token_end_time = (
                    (last_non_blank + 1)
                    * self.encoder_config.subsampling_factor
                    / self.preprocessor_config.sample_rate
                    * self.preprocessor_config.hop_length
                )

                token_duration = token_end_time - token_start_time

                hypothesis.append(
                    AlignedToken(
                        prev_token,
                        start=token_start_time,
                        duration=token_duration,
                        text=tokenizer.decode([prev_token], self.vocabulary),
                    )
                )

            result = sentences_to_result(tokens_to_sentences(hypothesis))
            results.append(result)

        return results


class ParakeetTDTCTC(ParakeetTDT):
    """Has ConvASRDecoder decoder in `.ctc_decoder` but `.generate` uses TDT decoder all the times (Please open an issue if you need CTC decoder use-case!)"""

    def __init__(self, args: ParakeetTDTCTCArgs):
        super().__init__(args)

        self.ctc_decoder = ConvASRDecoder(args.aux_ctc.decoder)



================================================
FILE: mlx_audio/stt/models/parakeet/rnnt.py
================================================
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn


@dataclass
class PredictNetworkArgs:
    pred_hidden: int
    pred_rnn_layers: int
    rnn_hidden_size: int | None = None


@dataclass
class JointNetworkArgs:
    joint_hidden: int
    activation: str
    encoder_hidden: int
    pred_hidden: int


@dataclass
class PredictArgs:
    blank_as_pad: bool
    vocab_size: int
    prednet: PredictNetworkArgs


@dataclass
class JointArgs:
    num_classes: int
    vocabulary: list[str]
    jointnet: JointNetworkArgs
    num_extra_outputs: int = 0


class LSTM(nn.Module):
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        batch_first: bool = True,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.batch_first = batch_first
        self.lstm = [
            nn.LSTM(input_size if i == 0 else hidden_size, hidden_size, bias=bias)
            for i in range(num_layers)
        ]

    def __call__(
        self, x: mx.array, h_c: tuple[mx.array, mx.array] | None = None
    ) -> tuple[mx.array, tuple[mx.array, mx.array]]:
        if self.batch_first:
            x = mx.transpose(x, (1, 0, 2))

        if h_c is None:
            h = [None] * self.num_layers
            c = [None] * self.num_layers
        else:
            h, c = h_c

        outputs = x
        next_h = []
        next_c = []

        for i in range(self.num_layers):
            layer = self.lstm[i]

            all_h_steps, all_c_steps = layer(outputs, hidden=h[i], cell=c[i])
            outputs = all_h_steps
            next_h.append(all_h_steps[-1])
            next_c.append(all_c_steps[-1])

        if self.batch_first:
            outputs = mx.transpose(outputs, (1, 0, 2))

        final_h = mx.stack(next_h, axis=0)
        final_c = mx.stack(next_c, axis=0)

        return outputs, (final_h, final_c)


class PredictNetwork(nn.Module):
    def __init__(self, args: PredictArgs):
        super().__init__()

        self.pred_hidden = args.prednet.pred_hidden

        self.prediction = {
            "embed": nn.Embedding(
                args.vocab_size if not args.blank_as_pad else args.vocab_size + 1,
                args.prednet.pred_hidden,
            ),
            "dec_rnn": LSTM(
                args.prednet.pred_hidden,
                (
                    args.prednet.rnn_hidden_size
                    if args.prednet.rnn_hidden_size
                    else args.prednet.pred_hidden
                ),
                args.prednet.pred_rnn_layers,
            ),
        }

    def __call__(
        self, y: mx.array | None, h_c: tuple[mx.array, mx.array] | None = None
    ) -> tuple[mx.array, tuple[mx.array, mx.array]]:
        if y is not None:
            embedded_y = self.prediction["embed"](y)
        else:
            batch = 1 if h_c is None else h_c[0].shape[1]
            embedded_y = mx.zeros((batch, 1, self.pred_hidden))
        return self.prediction["dec_rnn"](embedded_y, h_c)


class JointNetwork(nn.Module):
    def __init__(self, args: JointArgs):
        super().__init__()
        self._num_classes = args.num_classes + 1 + args.num_extra_outputs

        if args.jointnet.activation not in ["relu", "sigmoid", "tanh"]:
            raise ValueError(
                "Unsupported activation for joint step - please pass one of "
                "[relu, sigmoid, tanh]"
            )

        activation = args.jointnet.activation.lower()

        if activation == "relu":
            activation = nn.ReLU()
        elif activation == "sigmoid":
            activation = nn.Sigmoid()
        else:
            activation = nn.Tanh()

        self.pred = nn.Linear(args.jointnet.pred_hidden, args.jointnet.joint_hidden)
        self.enc = nn.Linear(args.jointnet.encoder_hidden, args.jointnet.joint_hidden)
        self.joint_net = [activation] + [
            nn.Identity(),
            nn.Linear(args.jointnet.joint_hidden, self._num_classes),
        ]

    def __call__(self, enc: mx.array, pred: mx.array) -> mx.array:
        enc = self.enc(enc)
        pred = self.pred(pred)

        x = mx.expand_dims(enc, 2) + mx.expand_dims(pred, 1)

        for layer in self.joint_net:
            x = layer(x)

        return x



================================================
FILE: mlx_audio/stt/models/parakeet/tokenizer.py
================================================
def decode(tokens: list[int], vocabulary: list[str]):
    return "".join([vocabulary[token].replace("â–", " ") for token in tokens])



================================================
FILE: mlx_audio/stt/models/wav2vec/__init__
================================================
[Empty file]


================================================
FILE: mlx_audio/stt/models/wav2vec/feature_extractor.py
================================================
import json
import logging
import os
from collections import UserDict
from dataclasses import dataclass
from enum import Enum
from typing import Any, List, Optional, Union

import mlx.core as mx
import numpy as np

from mlx_audio.tts.utils import get_model_path

logger = logging.getLogger(__name__)


class TensorType(Enum):
    MX = "mx"
    NP = "np"


class BatchFeature(UserDict):
    def __init__(
        self,
        data=None,
        input_values: Any = None,
        attention_mask: Any = None,
        tensor_type: Union[str, TensorType] = TensorType.MX,
        **kwargs,
    ):
        super().__init__()
        if data:
            self.data.update(data)

        _input_values_key = "input_values"
        _attention_mask_key = "attention_mask"

        if input_values is not None:
            # Ensure input_values is a list of items
            if not (
                isinstance(input_values, list)
                and (
                    not input_values
                    or isinstance(input_values[0], (np.ndarray, mx.array, list, tuple))
                )
            ):
                self.data[_input_values_key] = [input_values]
            else:
                self.data[_input_values_key] = input_values

        if attention_mask is not None:
            # Ensure attention_mask is a list of items
            if not (
                isinstance(attention_mask, list)
                and (
                    not attention_mask
                    or isinstance(
                        attention_mask[0],
                        (np.ndarray, mx.array, list, tuple, type(None)),
                    )
                )
            ):
                self.data[_attention_mask_key] = [attention_mask]
            else:
                self.data[_attention_mask_key] = attention_mask

        if isinstance(tensor_type, str):
            self.tensor_type = TensorType(tensor_type)
        else:
            self.tensor_type = tensor_type

        # Update with any other kwargs passed
        self.data.update(kwargs)


class PaddingStrategy(Enum):
    LONGEST = "longest"
    MAX_LENGTH = "max_length"
    DO_NOT_PAD = "do_not_pad"


def load_json(path: os.PathLike) -> dict[str, Any]:
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception as e:
        raise ValueError(f"Error loading JSON file {path}: {e}")


class Wav2Vec2FeatureExtractor:
    r"""
    Constructs a Wav2Vec2 feature extractor.

    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains
    most of the main methods. Users should refer to this superclass for more information regarding those methods.

    Args:
        feature_size (`int`, *optional*, defaults to 1):
            The feature dimension of the extracted features.
        sampling_rate (`int`, *optional*, defaults to 16000):
            The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).
        padding_value (`float`, *optional*, defaults to 0.0):
            The value that is used to fill the padding values.
        do_normalize (`bool`, *optional*, defaults to `True`):
            Whether or not to zero-mean unit-variance normalize the input. Normalizing can help to significantly
            improve the performance for some models, *e.g.*,
            [wav2vec2-lv60](https://huggingface.co/models?search=lv60).
        return_attention_mask (`bool`, *optional*, defaults to `False`):
            Whether or not [`~Wav2Vec2FeatureExtractor.__call__`] should return `attention_mask`.

            <Tip>

            Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
            [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have **not** been trained using
            `attention_mask`. For such models, `input_values` should simply be padded with 0 and no `attention_mask`
            should be passed.

            For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such as
            [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self), `attention_mask` should be
            passed for batched inference.

            </Tip>"""

    model_input_names = ["input_values", "attention_mask"]

    def __init__(
        self,
        feature_size=1,
        sampling_rate=16000,
        padding_value=0.0,
        return_attention_mask=False,
        do_normalize=True,
        **kwargs,
    ):
        self.feature_size = feature_size
        self.sampling_rate = sampling_rate
        self.padding_value = padding_value
        self.padding_side = kwargs.get("padding_side", "right")
        self.return_attention_mask = return_attention_mask
        self.do_normalize = do_normalize

    @staticmethod
    def zero_mean_unit_var_norm(
        input_values: List[np.ndarray],
        attention_mask: List[np.ndarray],
        padding_value: float = 0.0,
    ) -> List[np.ndarray]:
        """
        Every array in the list is normalized to have zero mean and unit variance
        """
        if attention_mask is not None:
            attention_mask = np.array(attention_mask, np.int32)
            normed_input_values = []

            for vector, length in zip(input_values, attention_mask.sum(-1)):
                normed_slice = (vector - vector[:length].mean()) / np.sqrt(
                    vector[:length].var() + 1e-7
                )
                if length < normed_slice.shape[0]:
                    normed_slice[length:] = padding_value

                normed_input_values.append(normed_slice)
        else:
            normed_input_values = [
                (x - x.mean()) / np.sqrt(x.var() + 1e-7) for x in input_values
            ]

        return normed_input_values

    def _truncate(
        self,
        processed_features: Union[dict[str, np.ndarray], BatchFeature],
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        truncation: Optional[bool] = None,
    ):
        """
        Truncate inputs to predefined length or max length in the batch

        Args:
            processed_features(`Union[Dict[str, np.ndarray], BatchFeature]`):
                Dictionary of input values (`np.ndarray[float]`) / input vectors (`List[np.ndarray[float]]`) or batch
                of inputs values (`List[np.ndarray[int]]`) / input vectors (`List[np.ndarray[int]]`)
            max_length (`int`, *optional*):
                maximum length of the returned list and optionally padding length (see below)
            pad_to_multiple_of (`int`, *optional*) :
                Integer if set will pad the sequence to a multiple of the provided value. This is especially useful to
                enable the use of Tensor Core on NVIDIA hardware with compute capability `>= 7.5` (Volta), or on TPUs
                which benefit from having sequence lengths be a multiple of 128.
            truncation (`bool`, *optional*):
                Activates truncation to cut input sequences longer than `max_length` to `max_length`.
        """
        if not truncation:
            return processed_features
        elif truncation and max_length is None:
            raise ValueError(
                "When setting ``truncation=True``, make sure that ``max_length`` is defined."
            )

        required_input = processed_features[self.model_input_names[0]]

        # find `max_length` that fits `pad_to_multiple_of`
        if (
            max_length is not None
            and pad_to_multiple_of is not None
            and (max_length % pad_to_multiple_of != 0)
        ):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_truncated = len(required_input) > max_length

        if needs_to_be_truncated:
            processed_features[self.model_input_names[0]] = processed_features[
                self.model_input_names[0]
            ][:max_length]
            if "attention_mask" in processed_features:
                processed_features["attention_mask"] = processed_features[
                    "attention_mask"
                ][:max_length]

        return processed_features

    def _get_padding_strategies(self, padding=False, max_length=None):
        """
        Find the correct padding strategy
        """

        # Get padding strategy
        if padding is not False:
            if padding is True:
                padding_strategy = (
                    PaddingStrategy.LONGEST
                )  # Default to pad to the longest sequence in the batch
            elif not isinstance(padding, PaddingStrategy):
                padding_strategy = PaddingStrategy(padding)
            elif isinstance(padding, PaddingStrategy):
                padding_strategy = padding
        else:
            padding_strategy = PaddingStrategy.DO_NOT_PAD

        # Set max length if needed
        if max_length is None:
            if padding_strategy == PaddingStrategy.MAX_LENGTH:
                raise ValueError(
                    f"When setting ``padding={PaddingStrategy.MAX_LENGTH}``, make sure that max_length is defined"
                )

        # Test if we have a padding value
        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (
            self.padding_value is None
        ):
            raise ValueError(
                "Asking to pad but the feature_extractor does not have a padding value. Please select a value to use"
                " as `padding_value`. For example: `feature_extractor.padding_value = 0.0`."
            )

        return padding_strategy

    def pad(
        self,
        processed_features: Union[
            BatchFeature,
            list[BatchFeature],
            dict[str, BatchFeature],
            dict[str, list[BatchFeature]],
            list[dict[str, BatchFeature]],
        ],
        padding: Union[bool, str, PaddingStrategy] = True,
        max_length: Optional[int] = None,
        truncation: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, Any]] = None,
    ) -> BatchFeature:
        """
        Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
        max sequence length in the batch.

        Padding side (left/right) padding values are defined at the feature extractor level (with `self.padding_side`,
        `self.padding_value`)

        <Tip>

        If the `processed_features` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
        result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of
        PyTorch tensors, you will lose the specific device of your tensors however.

        </Tip>

        Args:
            processed_features ([`BatchFeature`], list of [`BatchFeature`], `Dict[str, List[float]]`, `Dict[str, List[List[float]]` or `List[Dict[str, List[float]]]`):
                Processed inputs. Can represent one input ([`BatchFeature`] or `Dict[str, List[float]]`) or a batch of
                input values / vectors (list of [`BatchFeature`], *Dict[str, List[List[float]]]* or *List[Dict[str,
                List[float]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader
                collate function.

                Instead of `List[float]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
                see the note above for the return type.
            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):
                Select a strategy to pad the returned sequences (according to the model's padding side and padding
                index) among:

                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
                  sequence if provided).
                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
                  acceptable input length for the model if that argument is not provided.
                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different
                  lengths).
            max_length (`int`, *optional*):
                Maximum length of the returned list and optionally padding length (see above).
            truncation (`bool`):
                Activates truncation to cut input sequences longer than `max_length` to `max_length`.
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value.

                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.
            return_attention_mask (`bool`, *optional*):
                Whether to return the attention mask. If left to the default, will return the attention mask according
                to the specific feature_extractor's default.

                [What are attention masks?](../glossary#attention-mask)
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                If set, will return tensors instead of list of python integers. Acceptable values are:

                - `'mx'`: Return MXNet `mx.ndarray` objects.
                - `'np'`: Return Numpy `np.ndarray` objects.
        """
        # If we have a list of dicts, let's convert it in a dict of lists
        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
        if isinstance(processed_features, (list, tuple)) and isinstance(
            processed_features[0], (dict, BatchFeature)
        ):
            processed_features = {
                key: [example[key] for example in processed_features]
                for key in processed_features[0].keys()
            }

        # The model's main input name, usually `input_values`, has be passed for padding
        if self.model_input_names[0] not in processed_features:
            raise ValueError(
                "You should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature`"
                f" to this method that includes {self.model_input_names[0]}, but you provided"
                f" {list(processed_features.keys())}"
            )

        required_input = processed_features[self.model_input_names[0]]
        return_attention_mask = (
            return_attention_mask
            if return_attention_mask is not None
            else self.return_attention_mask
        )

        if len(required_input) == 0:
            if return_attention_mask:
                processed_features["attention_mask"] = []
            return processed_features

        # If we have PyTorch/TF tensors or lists as inputs, we cast them as Numpy arrays
        # and rebuild them afterwards if no return_tensors is specified
        # Note that we lose the specific device the tensor may be on for PyTorch

        first_element = required_input[0]
        if isinstance(first_element, (list, tuple)):
            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
            index = 0
            while len(required_input[index]) == 0:
                index += 1
            if index < len(required_input):
                first_element = required_input[index][0]

        if return_tensors is None:
            if isinstance(first_element, mx.array):
                return_tensors = "mx"
            elif isinstance(first_element, (int, float, list, tuple, np.ndarray)):
                return_tensors = "np"
            else:
                raise ValueError(
                    f"type of {first_element} unknown: {type(first_element)}. "
                    "Should be one of a python, numpy, pytorch or tensorflow object."
                )

        for key, value in processed_features.items():
            if isinstance(value[0], (int, float)):
                processed_features[key] = np.array(value)
            else:
                processed_features[key] = [np.array(v) for v in value]

        # Convert padding_strategy in PaddingStrategy
        padding_strategy = self._get_padding_strategies(
            padding=padding, max_length=max_length
        )

        required_input = processed_features[self.model_input_names[0]]

        batch_size = len(required_input)
        if not all(len(v) == batch_size for v in processed_features.values()):
            raise ValueError(
                "Some items in the output dictionary have a different batch size than others."
            )

        truncated_inputs = []
        for i in range(batch_size):
            inputs = {k: v[i] for k, v in processed_features.items()}
            # truncation
            inputs_slice = self._truncate(
                inputs,
                max_length=max_length,
                pad_to_multiple_of=pad_to_multiple_of,
                truncation=truncation,
            )
            truncated_inputs.append(inputs_slice)

        if padding_strategy == PaddingStrategy.LONGEST:
            # make sure that `max_length` cannot be longer than the longest truncated length
            max_length = max(
                len(input_slice[self.model_input_names[0]])
                for input_slice in truncated_inputs
            )
            padding_strategy = PaddingStrategy.MAX_LENGTH

        batch_outputs = {}
        for i in range(batch_size):
            # padding
            outputs = self._pad(
                truncated_inputs[i],
                max_length=max_length,
                padding_strategy=padding_strategy,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                if value.dtype is np.dtype(np.float64):
                    value = value.astype(np.float32)
                batch_outputs[key].append(value)

        return BatchFeature(batch_outputs, tensor_type=return_tensors)

    def _pad(
        self,
        processed_features: Union[dict[str, np.ndarray], BatchFeature],
        max_length: Optional[int] = None,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
    ) -> dict:
        """
        Pad inputs (on left/right and up to predefined length or max length in the batch)

        Args:
            processed_features (`Union[Dict[str, np.ndarray], BatchFeature]`):
                Dictionary of input values (`np.ndarray[float]`) / input vectors (`List[np.ndarray[float]]`) or batch
                of inputs values (`List[np.ndarray[int]]`) / input vectors (`List[np.ndarray[int]]`)
            max_length (`int`, *optional*):
                Maximum length of the returned list and optionally padding length (see below)
            padding_strategy (`PaddingStrategy`, *optional*, default to `PaddingStrategy.DO_NOT_PAD`):
                PaddingStrategy to use for padding.

                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch
                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)
                - PaddingStrategy.DO_NOT_PAD: Do not pad
                The feature_extractor padding sides are defined in self.padding_side:

                    - 'left': pads on the left of the sequences
                    - 'right': pads on the right of the sequences
            pad_to_multiple_of (`int`, *optional*):
                Integer if set will pad the sequence to a multiple of the provided value. This is especially useful to
                enable the use of Tensor Core on NVIDIA hardware with compute capability `>= 7.5` (Volta), or on TPUs
                which benefit from having sequence lengths be a multiple of 128.
            return_attention_mask (`bool`, *optional*):
                Set to False to avoid returning attention mask (default: set to model specifics)
        """
        required_input = processed_features[self.model_input_names[0]]

        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if (
            max_length is not None
            and pad_to_multiple_of is not None
            and (max_length % pad_to_multiple_of != 0)
        ):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = (
            padding_strategy != PaddingStrategy.DO_NOT_PAD
            and len(required_input) < max_length
        )

        if return_attention_mask and "attention_mask" not in processed_features:
            processed_features["attention_mask"] = np.ones(
                len(required_input), dtype=np.int32
            )

        if needs_to_be_padded:
            difference = max_length - len(required_input)
            if self.padding_side == "right":
                if return_attention_mask:
                    processed_features["attention_mask"] = np.pad(
                        processed_features["attention_mask"], (0, difference)
                    )
                padding_shape = (
                    ((0, difference), (0, 0))
                    if self.feature_size > 1
                    else (0, difference)
                )
                processed_features[self.model_input_names[0]] = np.pad(
                    required_input,
                    padding_shape,
                    "constant",
                    constant_values=self.padding_value,
                )
            elif self.padding_side == "left":
                if return_attention_mask:
                    processed_features["attention_mask"] = np.pad(
                        processed_features["attention_mask"], (difference, 0)
                    )
                padding_shape = (
                    ((difference, 0), (0, 0))
                    if self.feature_size > 1
                    else (difference, 0)
                )
                processed_features[self.model_input_names[0]] = np.pad(
                    required_input,
                    padding_shape,
                    "constant",
                    constant_values=self.padding_value,
                )
            else:
                raise ValueError("Invalid padding strategy:" + str(self.padding_side))

        return processed_features

    def __call__(
        self,
        raw_speech: Union[np.ndarray, List[float], List[np.ndarray], List[List[float]]],
        padding: Union[bool, str, PaddingStrategy] = False,
        max_length: Optional[int] = None,
        truncation: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, Any]] = None,
        sampling_rate: Optional[int] = None,
        **kwargs,
    ) -> BatchFeature:
        """
        Main method to featurize and prepare for the model one or several sequence(s).

        Args:
            raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`):
                The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
                values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not
                stereo, i.e. single float per timestep.
            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):
                Select a strategy to pad the returned sequences (according to the model's padding side and padding
                index) among:

                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
                  sequence if provided).
                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
                  acceptable input length for the model if that argument is not provided.
                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different
                  lengths).
            max_length (`int`, *optional*):
                Maximum length of the returned list and optionally padding length (see above).
            truncation (`bool`):
                Activates truncation to cut input sequences longer than *max_length* to *max_length*.
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value.

                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.
            return_attention_mask (`bool`, *optional*):
                Whether to return the attention mask. If left to the default, will return the attention mask according
                to the specific feature_extractor's default.

                [What are attention masks?](../glossary#attention-mask)

                <Tip>

                Wav2Vec2 models that have set `config.feat_extract_norm == "group"`, such as
                [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have **not** been trained using
                `attention_mask`. For such models, `input_values` should simply be padded with 0 and no
                `attention_mask` should be passed.

                For Wav2Vec2 models that have set `config.feat_extract_norm == "layer"`, such as
                [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self), `attention_mask` should
                be passed for batched inference.

                </Tip>

            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                If set, will return tensors instead of list of python integers. Acceptable values are:

                - `'mx'`: Return MXNet `mx.ndarray` objects.
                - `'np'`: Return Numpy `np.ndarray` objects.
            sampling_rate (`int`, *optional*):
                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass
                `sampling_rate` at the forward call to prevent silent errors.
            padding_value (`float`, *optional*, defaults to 0.0):
        """

        if sampling_rate is not None:
            if sampling_rate != self.sampling_rate:
                raise ValueError(
                    f"The model corresponding to this feature extractor: {self} was trained using a sampling rate of"
                    f" {self.sampling_rate}. Please make sure that the provided `raw_speech` input was sampled with"
                    f" {self.sampling_rate} and not {sampling_rate}."
                )
        else:
            logger.warning(
                f"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. "
                "Failing to do so can result in silent errors that might be hard to debug."
            )

        is_batched_numpy = (
            isinstance(raw_speech, np.ndarray) and len(raw_speech.shape) > 1
        )
        if is_batched_numpy and len(raw_speech.shape) > 2:
            raise ValueError(
                f"Only mono-channel audio is supported for input to {self}"
            )
        is_batched = is_batched_numpy or (
            isinstance(raw_speech, (list, tuple))
            and (isinstance(raw_speech[0], (np.ndarray, tuple, list)))
        )

        # always return batch
        if not is_batched:
            raw_speech = [raw_speech]

        # convert into correct format for padding
        encoded_inputs = BatchFeature({"input_values": raw_speech})

        padded_inputs = self.pad(
            encoded_inputs,
            padding=padding,
            max_length=max_length,
            truncation=truncation,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        # convert input values to correct format
        input_values = padded_inputs["input_values"]
        if not isinstance(input_values[0], np.ndarray):
            padded_inputs["input_values"] = [
                np.asarray(array, dtype=np.float32) for array in input_values
            ]
        elif (
            not isinstance(input_values, np.ndarray)
            and isinstance(input_values[0], np.ndarray)
            and input_values[0].dtype is np.dtype(np.float64)
        ):
            padded_inputs["input_values"] = [
                array.astype(np.float32) for array in input_values
            ]
        elif isinstance(input_values, np.ndarray) and input_values.dtype is np.dtype(
            np.float64
        ):
            padded_inputs["input_values"] = input_values.astype(np.float32)

        # convert attention_mask to correct format
        attention_mask = padded_inputs.get("attention_mask")
        if attention_mask is not None:
            padded_inputs["attention_mask"] = [
                np.asarray(array, dtype=np.int32) for array in attention_mask
            ]

        # zero-mean and unit-variance normalization
        if self.do_normalize:
            attention_mask = (
                attention_mask
                if self._get_padding_strategies(padding, max_length=max_length)
                is not PaddingStrategy.DO_NOT_PAD
                else None
            )
            padded_inputs["input_values"] = self.zero_mean_unit_var_norm(
                padded_inputs["input_values"],
                attention_mask=attention_mask,
                padding_value=self.padding_value,
            )

        if return_tensors is not None:
            for k, v in padded_inputs.items():
                if return_tensors == "mx":
                    # Convert to numpy array first if it's not already one
                    if isinstance(v, list):
                        v = np.array(v)
                    padded_inputs[k] = mx.array(v)
                elif return_tensors == "np":
                    padded_inputs[k] = np.array(v)
                else:
                    raise ValueError(f"Invalid return_tensors: {return_tensors}")
        return padded_inputs

    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        file_name: str = "preprocessor_config.json",
        revision: str = "main",
        **kwargs,
    ):
        if isinstance(pretrained_model_name_or_path, str):
            path = get_model_path(pretrained_model_name_or_path)
        else:
            path = pretrained_model_name_or_path

        if not (path / file_name).exists():
            raise FileNotFoundError(f"File {file_name} not found in {path}")

        feature_extractor_dict = load_json(path / file_name)

        return cls.from_dict(feature_extractor_dict, **kwargs)

    @classmethod
    def from_dict(
        cls, feature_extractor_dict: dict[str, Any], **kwargs
    ) -> "Wav2Vec2FeatureExtractor":
        """
        Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a Python dictionary of
        parameters.

        Args:
            feature_extractor_dict (`Dict[str, Any]`):
                Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
                retrieved from a pretrained checkpoint by leveraging the
                [`~feature_extraction_utils.FeatureExtractionMixin.to_dict`] method.
            kwargs (`Dict[str, Any]`):
                Additional parameters from which to initialize the feature extractor object.

        Returns:
            [`~feature_extraction_utils.FeatureExtractionMixin`]: The feature extractor object instantiated from those
            parameters.
        """
        return_unused_kwargs = kwargs.pop("return_unused_kwargs", False)

        # Update feature_extractor with kwargs if needed
        to_remove = []
        for key, value in kwargs.items():
            if key in feature_extractor_dict:
                feature_extractor_dict[key] = value
                to_remove.append(key)
        for key in to_remove:
            kwargs.pop(key, None)

        feature_extractor = cls(**feature_extractor_dict)

        logger.info(f"Feature extractor {feature_extractor}")
        if return_unused_kwargs:
            return feature_extractor, kwargs
        else:
            return feature_extractor



================================================
FILE: mlx_audio/stt/models/wav2vec/wav2vec.py
================================================
import inspect
import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import snapshot_download


@dataclass
class BaseModelArgs:
    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


@dataclass
class ModelConfig(BaseModelArgs):
    model_type: str = "wav2vec2"
    vocab_size: int = 32
    hidden_size: int = 768
    num_hidden_layers: int = 12
    num_attention_heads: int = 12
    intermediate_size: int = 3072
    hidden_act: str = "gelu"
    hidden_dropout: float = 0.1
    activation_dropout: float = 0.1
    attention_dropout: float = 0.1
    feat_proj_dropout: float = 0.0
    feat_quantizer_dropout: float = 0.0
    final_dropout: float = 0.1
    layerdrop: float = 0.1
    initializer_range: float = 0.02
    layer_norm_eps: float = 1e-5
    feat_extract_norm: str = "group"
    feat_extract_activation: str = "gelu"
    conv_dim: Tuple[int, ...] = (512, 512, 512, 512, 512, 512, 512)
    conv_stride: Tuple[int, ...] = (5, 2, 2, 2, 2, 2, 2)
    conv_kernel: Tuple[int, ...] = (10, 3, 3, 3, 3, 2, 2)
    conv_bias: bool = False
    num_conv_pos_embeddings: int = 128
    num_conv_pos_embedding_groups: int = 16
    num_feat_extract_layers: int = 7
    do_stable_layer_norm: bool = False
    apply_spec_augment: bool = True
    mask_time_prob: float = 0.05
    mask_time_length: int = 10
    mask_time_min_masks: int = 2
    mask_feature_prob: float = 0.0
    mask_feature_length: int = 10
    mask_feature_min_masks: int = 0
    num_codevectors_per_group: int = 320
    num_codevector_groups: int = 2
    contrastive_logits_temperature: float = 0.1
    num_negatives: int = 100
    codevector_dim: int = 256
    proj_codevector_dim: int = 256
    diversity_loss_weight: float = 0.1
    ctc_loss_reduction: str = "sum"
    ctc_zero_infinity: bool = False
    pad_token_id: int = 0
    bos_token_id: int = 1
    eos_token_id: int = 2


class Wav2Vec2NoLayerNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.activation = nn.GELU()

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2LayerNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.layer_norm = nn.LayerNorm(self.out_conv_dim)
        self.activation = nn.GELU()

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states.swapaxes(-2, -1))

        hidden_states = self.layer_norm(hidden_states)
        hidden_states = hidden_states.swapaxes(-2, -1)

        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2GroupNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.activation = nn.GELU()

        self.layer_norm = nn.GroupNorm(
            num_groups=self.out_conv_dim,
            dims=self.out_conv_dim,
            affine=True,
            pytorch_compatible=True,
        )

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.activation(hidden_states)
        return hidden_states


def normalize_weight(x, except_dim=0):
    if x.ndim != 3:
        raise ValueError("Input tensor must have 3 dimensions")

    axes = tuple(i for i in range(x.ndim) if i != except_dim)
    return mx.sqrt(mx.sum(mx.power(x, 2), axis=axes, keepdims=True))


class WNConv1d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        bias: bool = True,
        groups: int = 1,
    ):
        super().__init__()

        if bias:
            self.bias = mx.zeros((out_channels,))

        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self.stride = stride
        self.groups = groups

        scale = math.sqrt(1 / (in_channels * kernel_size))
        weight_init = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, kernel_size, in_channels // groups),
        )
        self.weight_g = normalize_weight(weight_init, except_dim=1)
        self.weight_v = weight_init / (self.weight_g + 1e-12)

    def _extra_repr(self):
        return (
            f"in_channels={self.weight_v.shape[2]}, out_channels={self.weight_v.shape[0]}, "
            f"kernel_size={self.kernel_size}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"bias={'bias' in self}"
        )

    def __call__(self, x):
        weight = (
            self.weight_g
            * self.weight_v
            / normalize_weight(self.weight_v, except_dim=1)
        )
        y = mx.conv1d(x, weight, self.stride, self.padding, self.dilation, self.groups)
        if "bias" in self:
            y = y + self.bias
        return y


class Wav2Vec2PositionalConvEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.conv = WNConv1d(
            config.hidden_size,
            config.hidden_size,
            kernel_size=config.num_conv_pos_embeddings,
            padding=config.num_conv_pos_embeddings // 2,
            groups=config.num_conv_pos_embedding_groups,
        )

        self.padding = Wav2Vec2SamePadLayer(config.num_conv_pos_embeddings)
        self.activation = nn.GELU()

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = self.padding(hidden_states)
        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2SamePadLayer(nn.Module):
    def __init__(self, num_conv_pos_embeddings):
        super().__init__()
        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0

    def __call__(self, hidden_states):
        if self.num_pad_remove > 0:
            hidden_states = hidden_states[:, : -self.num_pad_remove, :]
        return hidden_states


class Wav2Vec2FeatureEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()

        if config.feat_extract_norm == "group":
            conv_layers = [Wav2Vec2GroupNormConvLayer(config, layer_id=0)] + [
                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1)
                for i in range(config.num_feat_extract_layers - 1)
            ]
        elif config.feat_extract_norm == "layer":
            conv_layers = [
                Wav2Vec2LayerNormConvLayer(config, layer_id=i)
                for i in range(config.num_feat_extract_layers)
            ]
        else:
            raise ValueError(
                f"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']"
            )
        self.conv_layers = conv_layers

    def __call__(self, input_values):
        hidden_states = input_values[:, None]

        for conv_layer in self.conv_layers:
            hidden_states = conv_layer(hidden_states)

        return hidden_states


class Wav2Vec2FeatureProjection(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)
        self.projection = nn.Linear(config.conv_dim[-1], config.hidden_size)
        self.dropout = nn.Dropout(config.feat_proj_dropout)

    def __call__(self, hidden_states):
        norm_hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.projection(norm_hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states, norm_hidden_states


class Wav2Vec2Attention(nn.Module):
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        is_decoder: bool = False,
        bias: bool = True,
        is_causal: bool = False,
        config: Optional[ModelConfig] = None,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        self.config = config

        if (self.head_dim * num_heads) != self.embed_dim:
            raise ValueError(
                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads})."
            )
        self.scaling = self.head_dim**-0.5
        self.is_decoder = is_decoder
        self.is_causal = is_causal

        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

    def _shape(self, tensor: mx.array, seq_len: int, bsz: int):
        return tensor.reshape(bsz, seq_len, self.num_heads, self.head_dim).transpose(
            0, 2, 1, 3
        )

    def __call__(
        self,
        hidden_states: mx.array,
        key_value_states: Optional[Any] = None,
        past_key_value: Optional[Tuple[Any]] = None,
        attention_mask: Optional[Any] = None,
    ) -> Tuple[mx.array, Optional[mx.array], Optional[Tuple[mx.array]]]:
        """Input shape: Batch x Time x Channel"""

        # if key_value_states are provided this layer is used as a cross-attention layer
        # for the decoder
        is_cross_attention = key_value_states is not None

        bsz, tgt_len, _ = hidden_states.shape

        # get query proj
        query_states = self.q_proj(hidden_states) * self.scaling
        # get key, value proj
        # `past_key_value[0].shape[2] == key_value_states.shape[1]`
        # is checking that the `sequence_length` of the `past_key_value` is the same as
        # the provided `key_value_states` to support prefix tuning
        if (
            is_cross_attention
            and past_key_value is not None
            and past_key_value[0].shape[2] == key_value_states.shape[1]
        ):
            # reuse k,v, cross_attentions
            key_states = past_key_value[0]
            value_states = past_key_value[1]
        elif is_cross_attention:
            # cross_attentions
            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
        elif past_key_value is not None:
            # reuse k, v, self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
            key_states = mx.concatenate([past_key_value[0], key_states], axis=2)
            value_states = mx.concatenate([past_key_value[1], value_states], axis=2)
        else:
            # self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

        if self.is_decoder:
            past_key_value = (key_states, value_states)

        query_states = self._shape(query_states, tgt_len, bsz)

        attn_output = mx.fast.scaled_dot_product_attention(
            q=query_states,
            k=key_states,
            v=value_states,
            scale=1.0,
            mask=attention_mask,
        )

        attn_output = attn_output.transpose(0, 2, 1, 3)

        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
        # partitioned across GPUs when using tensor-parallelism.
        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
        attn_output = self.out_proj(attn_output)

        return attn_output, past_key_value


class Wav2Vec2FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.intermediate_dropout = nn.Dropout(config.activation_dropout)

        self.intermediate_dense = nn.Linear(
            config.hidden_size, config.intermediate_size
        )

        self.intermediate_act_fn = nn.GELU()

        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.output_dropout = nn.Dropout(config.hidden_dropout)

    def __call__(self, hidden_states):
        hidden_states = self.intermediate_dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        hidden_states = self.intermediate_dropout(hidden_states)

        hidden_states = self.output_dense(hidden_states)
        hidden_states = self.output_dropout(hidden_states)
        return hidden_states


class Wav2Vec2EncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = Wav2Vec2Attention(
            embed_dim=config.hidden_size,
            num_heads=config.num_attention_heads,
            dropout=config.attention_dropout,
            is_decoder=False,
        )

        self.dropout = nn.Dropout(config.hidden_dropout)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.feed_forward = Wav2Vec2FeedForward(config)
        self.final_layer_norm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )

    def __call__(self, hidden_states, attention_mask=None):
        attn_residual = hidden_states
        hidden_states, _ = self.attention(hidden_states, attention_mask=attention_mask)
        hidden_states = self.dropout(hidden_states)
        hidden_states = attn_residual + hidden_states

        hidden_states = self.layer_norm(hidden_states)
        hidden_states = hidden_states + self.feed_forward(hidden_states)
        hidden_states = self.final_layer_norm(hidden_states)

        outputs = (hidden_states,)

        return outputs


class Wav2Vec2EncoderLayerStableLayerNorm(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = Wav2Vec2Attention(
            embed_dim=config.hidden_size,
            num_heads=config.num_attention_heads,
            dropout=config.attention_dropout,
            is_decoder=False,
        )
        self.dropout = nn.Dropout(config.hidden_dropout)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.feed_forward = Wav2Vec2FeedForward(config)
        self.final_layer_norm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )

    def __call__(
        self,
        hidden_states: mx.array,
        attention_mask: Optional[mx.array] = None,
    ):
        attn_residual = hidden_states
        hidden_states = self.layer_norm(hidden_states)
        hidden_states, _ = self.attention(hidden_states, attention_mask=attention_mask)
        hidden_states = self.dropout(hidden_states)
        hidden_states = attn_residual + hidden_states
        hidden_states = hidden_states + self.feed_forward(
            self.final_layer_norm(hidden_states)
        )

        outputs = (hidden_states,)

        return outputs


class Wav2Vec2Encoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.pos_conv_embed = Wav2Vec2PositionalConvEmbedding(config)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout)
        self.layers = [
            Wav2Vec2EncoderLayer(config) for _ in range(config.num_hidden_layers)
        ]

    def __call__(
        self,
        hidden_states: mx.array,
        attention_mask: Optional[mx.array] = None,
        output_hidden_states: bool = False,
        return_dict: bool = True,
    ):
        all_hidden_states = () if output_hidden_states else None

        if attention_mask is not None:
            # make sure padded tokens output 0
            expand_attention_mask = attention_mask[..., None]
            expand_attention_mask = mx.repeat(
                expand_attention_mask, 1, 1, hidden_states.shape[2]
            )
            hidden_states[~expand_attention_mask] = 0

            # extend attention_mask
            attention_mask = 1.0 - attention_mask[:, None, None, :].astype(
                hidden_states.dtype
            )
            attention_mask = attention_mask * mx.finfo(hidden_states.dtype).min
            attention_mask = attention_mask.expand(
                attention_mask.shape[0],
                1,
                attention_mask.shape[-1],
                attention_mask.shape[-1],
            )

        position_embeddings = self.pos_conv_embed(hidden_states)
        hidden_states = hidden_states + position_embeddings
        hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.dropout(hidden_states)

        for layer in self.layers:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

                layer_outputs = layer(
                    hidden_states,
                    attention_mask=attention_mask,
                )
                hidden_states = layer_outputs[0]

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states] if v is not None)
        return Wav2Vec2BaseModelOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
        )


class Wav2Vec2EncoderStableLayerNorm(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.pos_conv_embed = Wav2Vec2PositionalConvEmbedding(config)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout)
        self.layers = [
            Wav2Vec2EncoderLayerStableLayerNorm(config)
            for _ in range(config.num_hidden_layers)
        ]

    def __call__(
        self,
        hidden_states,
        attention_mask=None,
        output_hidden_states=False,
        return_dict=True,
    ):
        all_hidden_states = () if output_hidden_states else None

        if attention_mask is not None:
            # make sure padded tokens are not attended to
            expand_attention_mask = attention_mask[..., None]
            expand_attention_mask = mx.repeat(
                expand_attention_mask, 1, 1, hidden_states.shape[2]
            )
            hidden_states = hidden_states * expand_attention_mask.astype(
                hidden_states.dtype
            )

            # extend attention_mask
            attention_mask = 1.0 - attention_mask[:, None, None, :].astype(
                hidden_states.dtype
            )
            attention_mask = attention_mask * mx.finfo(hidden_states.dtype).min
            attention_mask = attention_mask.expand(
                attention_mask.shape[0],
                1,
                attention_mask.shape[-1],
                attention_mask.shape[-1],
            )

        position_embeddings = self.pos_conv_embed(hidden_states)
        hidden_states = hidden_states + position_embeddings
        hidden_states = self.dropout(hidden_states)

        for layer in self.layers:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

                layer_outputs = layer(
                    hidden_states,
                    attention_mask=attention_mask,
                )
                hidden_states = layer_outputs[0]

        hidden_states = self.layer_norm(hidden_states)

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states] if v is not None)
        return Wav2Vec2BaseModelOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
        )


@dataclass
class Wav2Vec2BaseModelOutput:
    last_hidden_state: Optional[mx.array] = None
    extract_features: Optional[mx.array] = None
    hidden_states: Optional[Tuple[mx.array, ...]] = None
    attentions: Optional[Tuple[mx.array, ...]] = None


class Wav2Vec2Model(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()

        self.config = config
        self.feature_extractor = Wav2Vec2FeatureEncoder(config)
        self.feature_projection = Wav2Vec2FeatureProjection(config)

        if config.do_stable_layer_norm:
            self.encoder = Wav2Vec2EncoderStableLayerNorm(config)
        else:
            self.encoder = Wav2Vec2Encoder(config)

    def _mask_hidden_states(
        self,
        hidden_states: mx.array,
        mask_time_indices: Optional[mx.array] = None,
        attention_mask: Optional[mx.array] = None,
    ):
        # `config.apply_spec_augment` can set masking to False
        if not getattr(self.config, "apply_spec_augment", True):
            return hidden_states

        return hidden_states

    def __call__(
        self,
        input_values: Optional[mx.array],
        attention_mask: Optional[mx.array] = None,
        output_hidden_states: bool = True,
        return_dict: bool = True,
    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:
        extract_features = self.feature_extractor(input_values)
        extract_features = extract_features.transpose(0, 2, 1)

        if attention_mask is not None:
            # compute reduced attention_mask corresponding to feature vectors
            attention_mask = self._get_feature_vector_attention_mask(
                extract_features.shape[1], attention_mask
            )

        hidden_states, extract_features = self.feature_projection(extract_features)
        hidden_states = self._mask_hidden_states(
            hidden_states,
            attention_mask=attention_mask,
        )

        encoder_outputs = self.encoder(
            hidden_states,
            attention_mask=attention_mask,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = encoder_outputs.last_hidden_state

        if not return_dict:
            return (hidden_states, extract_features) + encoder_outputs[1:]

        return Wav2Vec2BaseModelOutput(
            last_hidden_state=hidden_states,
            extract_features=extract_features,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if k.startswith("wav2vec2."):
                k = k.replace("wav2vec2.", "")
            if k.endswith(".conv.weight"):
                v = v.swapaxes(1, 2)
            if k.endswith(".conv.weight_v") or k.endswith(".conv.weight_g"):
                v = v.swapaxes(1, 2)
            if k.endswith(".parametrizations.weight.original0"):
                k = k.replace(".parametrizations.weight.original0", ".weight_g")
                v = v.swapaxes(1, 2)
            if k.endswith(".parametrizations.weight.original1"):
                k = k.replace(".parametrizations.weight.original1", ".weight_v")
                v = v.swapaxes(1, 2)
            if (
                "lm_head." in k
                or k.startswith("quantizer.")
                or k.startswith("project_")
                or k == "masked_spec_embed"
            ):
                continue

            sanitized_weights[k] = v
        return sanitized_weights

    @classmethod
    def from_pretrained(cls, repo_id: str, **kwargs):
        path = fetch_from_hub(repo_id)

        if path is None:
            raise ValueError(f"Could not find model {path}")

        config_path = path / "config.json"
        model_path = path / "model.safetensors"

        with open(config_path, "r") as f:
            config_dict = json.load(f)
        config = ModelConfig.from_dict(config_dict)
        model = Wav2Vec2Model(config)

        weights = mx.load(model_path.as_posix(), format="safetensors")
        weights = model.sanitize(weights)
        model.load_weights(list(weights.items()))
        mx.eval(model.parameters())

        return model


# fetch model from hub


def fetch_from_hub(model_path: str) -> Path:
    if not Path(model_path).exists():
        model_path = Path(
            snapshot_download(
                repo_id=model_path,
                allow_patterns=["*.safetensors", "*.json"],
            )
        )
    return Path(model_path)



================================================
FILE: mlx_audio/stt/models/whisper/__init__.py
================================================
from .whisper import Model



================================================
FILE: mlx_audio/stt/models/whisper/audio.py
================================================
# Copyright Â© 2023 Apple Inc.

from typing import Union

import mlx.core as mx
import numpy as np

from mlx_audio.stt.utils import load_audio
from mlx_audio.utils import hanning, mel_filters, stft

# hard-coded audio hyperparameters
SAMPLE_RATE = 16000
N_FFT = 400
HOP_LENGTH = 160
CHUNK_LENGTH = 30
N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk
N_FRAMES = N_SAMPLES // HOP_LENGTH  # 3000 frames in a mel spectrogram input

N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2
FRAMES_PER_SECOND = SAMPLE_RATE // HOP_LENGTH  # 10ms per audio frame
TOKENS_PER_SECOND = SAMPLE_RATE // N_SAMPLES_PER_TOKEN  # 20ms per audio token


def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):
    """
    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.
    """
    if array.shape[axis] > length:
        sl = [slice(None)] * array.ndim
        sl[axis] = slice(0, length)
        array = array[tuple(sl)]

    if array.shape[axis] < length:
        pad_widths = [(0, 0)] * array.ndim
        pad_widths[axis] = (0, length - array.shape[axis])
        array = mx.pad(array, pad_widths)

    return array


def log_mel_spectrogram(
    audio: Union[str, np.ndarray],
    n_mels: int = 80,
    padding: int = 0,
):
    """
    Compute the log-Mel spectrogram of

    Parameters
    ----------
    audio: Union[str, np.ndarray, mx.array], shape = (*)
        The path to audio or either a NumPy or mlx array containing the audio waveform in 16 kHz

    n_mels: int
        The number of Mel-frequency filters, only 80 is supported

    padding: int
        Number of zero samples to pad to the right

    Returns
    -------
    mx.array, shape = (80, n_frames)
        An  array that contains the Mel spectrogram
    """
    if isinstance(audio, str):
        audio = load_audio(audio)
    elif not isinstance(audio, mx.array):
        audio = mx.array(audio)

    if padding > 0:
        audio = mx.pad(audio, (0, padding))
    window = hanning(N_FFT)
    freqs = stft(audio, window=window, n_fft=N_FFT, hop_length=HOP_LENGTH)
    magnitudes = freqs[:-1, :].abs().square()

    filters = mel_filters(SAMPLE_RATE, N_FFT, n_mels, norm="slaney", mel_scale=None)
    mel_spec = magnitudes @ filters.T

    log_spec = mx.maximum(mel_spec, 1e-10).log10()
    log_spec = mx.maximum(log_spec, log_spec.max() - 8.0)
    log_spec = (log_spec + 4.0) / 4.0
    return log_spec



================================================
FILE: mlx_audio/stt/models/whisper/decoding.py
================================================
# Copyright Â© 2023 Apple Inc.

import zlib
from dataclasses import dataclass, field, replace
from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union

import mlx.core as mx
import numpy as np
from mlx.utils import tree_map

from .audio import CHUNK_LENGTH
from .tokenizer import Tokenizer, get_tokenizer


def compression_ratio(text) -> float:
    text_bytes = text.encode("utf-8")
    return len(text_bytes) / len(zlib.compress(text_bytes))


def detect_language(
    model: "Whisper", mel: mx.array, tokenizer: Tokenizer = None
) -> Tuple[mx.array, List[dict]]:
    """
    Detect the spoken language in the audio, and return them as list of strings, along with the ids
    of the most probable language tokens and the probability distribution over all language tokens.
    This is performed outside the main decode loop in order to not interfere with kv-caching.

    Returns
    -------
    language_tokens : mx.array, shape = (n_audio,)
        ids of the most probable language tokens, which appears after the startoftranscript token.
    language_probs : List[Dict[str, float]], length = n_audio
        list of dictionaries containing the probability distribution over all languages.
    """
    if tokenizer is None:
        tokenizer = get_tokenizer(
            model.is_multilingual, num_languages=model.num_languages
        )
    if (
        tokenizer.language is None
        or tokenizer.language_token not in tokenizer.sot_sequence
    ):
        raise ValueError(
            "This model doesn't have language tokens so it can't perform lang id"
        )

    single = mel.ndim == 2
    if single:
        mel = mel[None]

    # skip encoder forward pass if already-encoded audio features were given
    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):
        mel = model.encoder(mel)

    # forward pass using a single token, startoftranscript
    n_audio = mel.shape[0]
    x = mx.array([[tokenizer.sot]] * n_audio)  # [n_audio, 1]
    logits = model.logits(x, mel)[:, 0]

    # collect detected languages; suppress all non-language tokens
    mask = mx.full(logits.shape[-1], -mx.inf, dtype=mx.float32)
    mask[list(tokenizer.all_language_tokens)] = 0.0
    logits += mask
    language_tokens = mx.argmax(logits, axis=-1)
    language_token_probs = mx.softmax(logits, axis=-1)
    language_token_probs = np.array(language_token_probs)
    language_probs = [
        {
            c: language_token_probs[i, j].item()
            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)
        }
        for i in range(n_audio)
    ]

    if single:
        language_tokens = language_tokens[0]
        language_probs = language_probs[0]

    return language_tokens, language_probs


@dataclass(frozen=True)
class DecodingOptions:
    # whether to perform X->X "transcribe" or X->English "translate"
    task: str = "transcribe"

    # language that the audio is in; uses detected language if None
    language: Optional[str] = None

    # sampling-related options
    temperature: float = 0.0
    sample_len: Optional[int] = None  # maximum number of tokens to sample
    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0
    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0
    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)

    # "alpha" in Google NMT, or None for length norm, when ranking generations
    # to select which to return among the beams or best-of-N samples
    length_penalty: Optional[float] = None

    # text or tokens to feed as the prompt or the prefix; for more info:
    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051
    prompt: Optional[Union[str, List[int]]] = None  # for the previous context
    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context

    # list of tokens ids (or comma-separated token ids) to suppress
    # "-1" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`
    suppress_tokens: Optional[Union[str, Iterable[int]]] = "-1"
    suppress_blank: bool = True  # this will suppress blank outputs

    # timestamp sampling options
    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only
    max_initial_timestamp: Optional[float] = 1.0

    # implementation details
    fp16: bool = True  # use fp16 for most of the calculation


@dataclass(frozen=True)
class DecodingResult:
    audio_features: mx.array
    language: str
    language_probs: Optional[Dict[str, float]] = None
    tokens: List[int] = field(default_factory=list)
    text: str = ""
    avg_logprob: float = np.nan
    no_speech_prob: float = np.nan
    temperature: float = np.nan
    compression_ratio: float = np.nan


class Inference:
    def __init__(self, model: "Whisper"):
        self.model: "Whisper" = model
        self.kv_cache = None

    def logits(self, tokens: mx.array, audio_features: mx.array) -> mx.array:
        """Perform a forward pass on the decoder and return per-token logits"""
        logits, self.kv_cache, _ = self.model.decoder(
            tokens, audio_features, kv_cache=self.kv_cache
        )
        return logits.astype(mx.float32)

    def rearrange_kv_cache(self, source_indices):
        """Update the key-value cache according to the updated beams"""
        # update the key/value cache to contain the selected sequences
        if source_indices != list(range(len(source_indices))):
            self.kv_cache = tree_map(lambda x: x[source_indices], self.kv_cache)

    def reset(self):
        self.kv_cache = None


class SequenceRanker:
    def rank(
        self, tokens: List[List[mx.array]], sum_logprobs: List[List[float]]
    ) -> List[int]:
        """
        Given a list of groups of samples and their cumulative log probabilities,
        return the indices of the samples in each group to select as the final result
        """
        raise NotImplementedError


class MaximumLikelihoodRanker(SequenceRanker):
    """
    Select the sample with the highest log probabilities, penalized using either
    a simple length normalization or Google NMT paper's length penalty
    """

    def __init__(self, length_penalty: Optional[float]):
        self.length_penalty = length_penalty

    def rank(self, tokens: List[List[List[int]]], sum_logprobs: List[List[float]]):
        def scores(logprobs, lengths):
            result = []
            for logprob, length in zip(logprobs, lengths):
                if self.length_penalty is None:
                    penalty = length
                else:
                    # from the Google NMT paper
                    penalty = ((5 + length) / 6) ** self.length_penalty
                result.append(logprob / penalty)
            return result

        # get the sequence with the highest score
        lengths = [[len(t) for t in s] for s in tokens]
        return [np.argmax(scores(p, l)) for p, l in zip(sum_logprobs, lengths)]


class TokenDecoder:
    def reset(self):
        """Initialize any stateful variables for decoding a new sequence"""

    def update(
        self, tokens: mx.array, logits: mx.array, sum_logprobs: mx.array
    ) -> Tuple[mx.array, bool, mx.array]:
        """Specify how to select the next token, based on the current trace and logits

        Parameters
        ----------
        tokens : mx.array, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        logits : mx.array, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        sum_logprobs : mx.array, shape = (n_batch)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : mx.array, shape = (n_batch, current_sequence_length + 1)
            the tokens, appended with the selected next token

        completed : bool
            True if all sequences has reached the end of text

        sum_logprobs: mx.array, shape = (n_batch)
            updated cumulative log probabilities for each sequence

        """
        raise NotImplementedError

    def finalize(
        self, tokens: mx.array, sum_logprobs: mx.array
    ) -> Tuple[Sequence[Sequence[mx.array]], List[List[float]]]:
        """Finalize search and return the final candidate sequences

        Parameters
        ----------
        tokens : mx.array, shape = (n_audio, n_group, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence

        sum_logprobs : mx.array, shape = (n_audio, n_group)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : Sequence[Sequence[mx.array]], length = n_audio
            sequence of mx.arrays containing candidate token sequences, for each audio input

        sum_logprobs : List[List[float]], length = n_audio
            sequence of cumulative log probabilities corresponding to the above

        """
        raise NotImplementedError


@mx.compile
def categorical(logits, temp):
    return mx.random.categorical(logits / temp)


class GreedyDecoder(TokenDecoder):
    def __init__(self, temperature: float, eot: int):
        self.temperature = temperature
        self.eot = eot

    def update(
        self, tokens: mx.array, logits: mx.array, sum_logprobs: mx.array
    ) -> Tuple[mx.array, bool, mx.array]:
        if self.temperature == 0:
            next_tokens = logits.argmax(axis=-1)
        else:
            next_tokens = categorical(logits, self.temperature)

        logprobs = logits - mx.logsumexp(logits, axis=-1)

        current_logprobs = logprobs[mx.arange(logprobs.shape[0]), next_tokens]
        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)

        eot_mask = tokens[:, -1] == self.eot
        next_tokens = next_tokens * (1 - eot_mask) + self.eot * eot_mask
        tokens = mx.concatenate([tokens, next_tokens[:, None]], axis=-1)

        completed = mx.all(tokens[:, -1] == self.eot)
        return tokens, completed, sum_logprobs

    def finalize(self, tokens: mx.array, sum_logprobs: mx.array):
        # make sure each sequence has at least one EOT token at the end
        tokens = mx.pad(tokens, [(0, 0), (0, 0), (0, 1)], constant_values=self.eot)
        return tokens, sum_logprobs


class LogitFilter:
    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        """Apply any filtering or masking to logits

        Parameters
        ----------
        logits : mx.array, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        tokens : mx.array, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        """
        raise NotImplementedError


class SuppressBlank(LogitFilter):
    def __init__(self, tokenizer: Tokenizer, sample_begin: int, n_vocab: int):
        self.sample_begin = sample_begin
        mask = np.zeros(n_vocab, np.float32)
        mask[tokenizer.encode(" ") + [tokenizer.eot]] = -np.inf
        self.mask = mx.array(mask)

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        if tokens.shape[1] == self.sample_begin:
            return logits + self.mask
        return logits


class SuppressTokens(LogitFilter):
    def __init__(self, suppress_tokens: Sequence[int], n_vocab: int):
        mask = np.zeros(n_vocab, np.float32)
        mask[list(suppress_tokens)] = -np.inf
        self.mask = mx.array(mask)

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        return logits + self.mask


class ApplyTimestampRules(LogitFilter):
    def __init__(
        self,
        tokenizer: Tokenizer,
        sample_begin: int,
        max_initial_timestamp_index: Optional[int],
    ):
        self.tokenizer = tokenizer
        self.sample_begin = sample_begin
        self.max_initial_timestamp_index = max_initial_timestamp_index

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        mask = np.zeros(logits.shape, np.float32)
        # suppress <|notimestamps|> which is handled by without_timestamps
        if self.tokenizer.no_timestamps is not None:
            mask[:, self.tokenizer.no_timestamps] = -np.inf

        ## timestamps have to appear in pairs, except directly before EOT; mask logits accordingly
        tokens = tokens.tolist()
        for k in range(len(tokens)):
            seq = tokens[k][self.sample_begin :]
            last_was_timestamp = (
                len(seq) >= 1 and seq[-1] >= self.tokenizer.timestamp_begin
            )
            penultimate_was_timestamp = (
                len(seq) < 2 or seq[-2] >= self.tokenizer.timestamp_begin
            )

            if last_was_timestamp:
                if penultimate_was_timestamp:  # has to be non-timestamp
                    mask[k, self.tokenizer.timestamp_begin :] = -np.inf
                else:  # cannot be normal text tokens
                    mask[k, : self.tokenizer.eot] = -np.inf

            timestamps = [
                i for i, v in enumerate(seq) if v > self.tokenizer.timestamp_begin
            ]
            if len(timestamps) > 0:
                # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last
                # also force each segment to have a nonzero length, to prevent infinite looping
                last_timestamp = timestamps[-1]
                if not last_timestamp or penultimate_was_timestamp:
                    last_timestamp += 1
                mask[k, self.tokenizer.timestamp_begin : last_timestamp] = -np.inf

        if len(tokens[0]) == self.sample_begin:
            # suppress generating non-timestamp tokens at the beginning
            mask[:, : self.tokenizer.timestamp_begin] = -np.inf

            # apply the `max_initial_timestamp` option
            if self.max_initial_timestamp_index is not None:
                last_allowed = (
                    self.tokenizer.timestamp_begin + self.max_initial_timestamp_index
                )
                mask[:, last_allowed + 1 :] = -np.inf

        # if sum of probability over timestamps is above any other token, sample timestamp
        mask = mx.array(mask)
        logprobs = logits - mx.logsumexp(logits, axis=-1)
        timestamp_logprob = logprobs[:, self.tokenizer.timestamp_begin :].logsumexp(
            axis=-1, keepdims=True
        )
        max_text_token_logprob = logprobs[:, : self.tokenizer.timestamp_begin].max(
            axis=-1, keepdims=True
        )
        mask[:, : self.tokenizer.timestamp_begin] = mx.where(
            timestamp_logprob > max_text_token_logprob,
            -mx.inf,
            mask[:, : self.tokenizer.timestamp_begin],
        )
        return logits + mask


class DecodingTask:
    inference: Inference
    sequence_ranker: SequenceRanker
    decoder: TokenDecoder
    logit_filters: List[LogitFilter]

    def __init__(self, model: "Whisper", options: DecodingOptions):
        self.model = model

        language = options.language or "en"
        tokenizer = get_tokenizer(
            model.is_multilingual,
            num_languages=model.num_languages,
            language=language,
            task=options.task,
        )
        self.tokenizer: Tokenizer = tokenizer
        self.options: DecodingOptions = self._verify_options(options)

        self.n_group: int = options.beam_size or options.best_of or 1
        self.n_ctx: int = model.dims.n_text_ctx
        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2

        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence
        if self.options.without_timestamps:
            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps

        self.initial_tokens: Tuple[int] = self._get_initial_tokens()
        self.sample_begin: int = len(self.initial_tokens)
        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)

        # inference: implements the forward pass through the decoder, including kv caching
        self.inference = Inference(model)

        # sequence ranker: implements how to rank a group of sampled sequences
        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)

        # decoder: implements how to select the next tokens, given the autoregressive distribution
        if options.beam_size is not None:
            raise NotImplementedError("Beam search decoder is not yet implemented")
        else:
            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)

        # logit filters: applies various rules to suppress or penalize certain tokens
        self.logit_filters = []
        if self.options.suppress_blank:
            self.logit_filters.append(
                SuppressBlank(self.tokenizer, self.sample_begin, model.dims.n_vocab)
            )
        if self.options.suppress_tokens:
            self.logit_filters.append(
                SuppressTokens(self._get_suppress_tokens(), model.dims.n_vocab)
            )

        if not options.without_timestamps:
            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds
            max_initial_timestamp_index = None
            if options.max_initial_timestamp:
                max_initial_timestamp_index = round(
                    self.options.max_initial_timestamp / precision
                )
            self.logit_filters.append(
                ApplyTimestampRules(
                    tokenizer, self.sample_begin, max_initial_timestamp_index
                )
            )

    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:
        if options.beam_size is not None and options.best_of is not None:
            raise ValueError("beam_size and best_of can't be given together")
        if options.temperature == 0:
            if options.best_of is not None:
                raise ValueError("best_of with greedy sampling (T=0) is not compatible")
        if options.patience is not None and options.beam_size is None:
            raise ValueError("patience requires beam_size to be given")
        if options.length_penalty is not None and not (
            0 <= options.length_penalty <= 1
        ):
            raise ValueError("length_penalty (alpha) should be a value between 0 and 1")

        return options

    def _get_initial_tokens(self) -> Tuple[int]:
        tokens = list(self.sot_sequence)

        if prefix := self.options.prefix:
            prefix_tokens = (
                self.tokenizer.encode(" " + prefix.strip())
                if isinstance(prefix, str)
                else prefix
            )
            if self.sample_len is not None:
                max_prefix_len = self.n_ctx // 2 - self.sample_len
                prefix_tokens = prefix_tokens[-max_prefix_len:]
            tokens = tokens + prefix_tokens

        if prompt := self.options.prompt:
            prompt_tokens = (
                self.tokenizer.encode(" " + prompt.strip())
                if isinstance(prompt, str)
                else prompt
            )
            tokens = (
                [self.tokenizer.sot_prev]
                + prompt_tokens[-(self.n_ctx // 2 - 1) :]
                + tokens
            )

        return tuple(tokens)

    def _get_suppress_tokens(self) -> Tuple[int]:
        suppress_tokens = self.options.suppress_tokens

        if isinstance(suppress_tokens, str):
            suppress_tokens = [int(t) for t in suppress_tokens.split(",")]

        if -1 in suppress_tokens:
            suppress_tokens = [t for t in suppress_tokens if t >= 0]
            suppress_tokens.extend(self.tokenizer.non_speech_tokens)
        elif suppress_tokens is None or len(suppress_tokens) == 0:
            suppress_tokens = []  # interpret empty string as an empty list
        else:
            assert isinstance(suppress_tokens, list), "suppress_tokens must be a list"

        suppress_tokens.extend(
            [
                self.tokenizer.transcribe,
                self.tokenizer.translate,
                self.tokenizer.sot,
                self.tokenizer.sot_prev,
                self.tokenizer.sot_lm,
            ]
        )
        if self.tokenizer.no_speech is not None:
            # no-speech probability is collected separately
            suppress_tokens.append(self.tokenizer.no_speech)

        return tuple(sorted(set(suppress_tokens)))

    def _get_audio_features(self, mel: mx.array):
        if self.options.fp16:
            mel = mel.astype(mx.float16)

        if mel.shape[-2:] == (
            self.model.dims.n_audio_ctx,
            self.model.dims.n_audio_state,
        ):
            # encoded audio features are given; skip audio encoding
            audio_features = mel
        else:
            audio_features = self.model.encoder(mel)

        if audio_features.dtype != (mx.float16 if self.options.fp16 else mx.float32):
            raise TypeError(
                f"audio_features has an incorrect dtype: {audio_features.dtype}"
            )

        return audio_features

    def _detect_language(self, audio_features: mx.array, tokens: np.array):
        languages = [self.options.language] * audio_features.shape[0]
        lang_probs = None

        if self.options.language is None or self.options.task == "lang_id":
            lang_tokens, lang_probs = self.model.detect_language(
                audio_features, self.tokenizer
            )
            languages = [max(probs, key=probs.get) for probs in lang_probs]
            if self.options.language is None:
                # write language tokens
                tokens[:, self.sot_index + 1] = np.array(lang_tokens)

        return languages, lang_probs

    def _main_loop(self, audio_features: mx.array, tokens: mx.array):
        n_batch = tokens.shape[0]
        sum_logprobs = mx.zeros(n_batch)

        def _step(inputs, audio_features, tokens, sum_logprobs):
            pre_logits = self.inference.logits(inputs, audio_features)

            # consider the logits at the last token only
            logits = pre_logits[:, -1]

            # apply the logit filters, e.g. for suppressing or applying penalty to
            for logit_filter in self.logit_filters:
                logits = logit_filter.apply(logits, tokens)

            # expand the tokens tensor with the selected next tokens
            tokens, completed, sum_logprobs = self.decoder.update(
                tokens, logits, sum_logprobs
            )
            return tokens, completed, sum_logprobs, pre_logits

        tokens, completed, sum_logprobs, pre_logits = _step(
            tokens, audio_features, tokens, sum_logprobs
        )
        if self.tokenizer.no_speech is not None:  # compute no_speech_probs
            probs_at_sot = mx.softmax(pre_logits[:, self.sot_index], axis=-1)
            no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech]
        else:
            no_speech_probs = mx.full(n_batch, mx.nan)
        mx.async_eval(completed, tokens, sum_logprobs, no_speech_probs)

        for i in range(1, self.sample_len):
            inputs = tokens[:, -1:]
            if tokens.shape[-1] > self.n_ctx:
                break
            next_tokens, next_completed, next_sum_logprobs, _ = _step(
                inputs, audio_features, tokens, sum_logprobs
            )
            mx.async_eval(next_completed, next_tokens, next_sum_logprobs)
            if completed:
                break
            tokens = next_tokens
            completed = next_completed
            sum_logprobs = next_sum_logprobs

        return tokens, sum_logprobs, no_speech_probs

    def run(self, mel: mx.array) -> List[DecodingResult]:
        self.inference.reset()
        self.decoder.reset()
        tokenizer: Tokenizer = self.tokenizer
        n_audio: int = mel.shape[0]

        audio_features: mx.array = self._get_audio_features(mel)  # encoder forward pass
        tokens: mx.array = mx.array(self.initial_tokens)
        tokens = mx.broadcast_to(tokens, (n_audio, len(self.initial_tokens)))

        # detect language if requested, overwriting the language token
        languages, language_probs = self._detect_language(audio_features, tokens)
        if self.options.task == "lang_id":
            return [
                DecodingResult(
                    audio_features=features, language=language, language_probs=probs
                )
                for features, language, probs in zip(
                    audio_features, languages, language_probs
                )
            ]

        # repeat tokens by the group size, for beam search or best-of-n sampling
        if self.n_group > 1:
            tokens = tokens[:, None, :]
            tokens = mx.broadcast_to(
                tokens, [n_audio, self.n_group, len(self.initial_tokens)]
            )
            tokens = tokens.reshape(
                tokens, (n_audio * self.n_group, len(self.initial_tokens))
            )

        # call the main sampling loop
        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)

        # reshape the tensors to have (n_audio, n_group) as the first two dimensions
        audio_features = audio_features[:: self.n_group]
        no_speech_probs = no_speech_probs[:: self.n_group]
        assert audio_features.shape[0] == len(no_speech_probs) == n_audio

        tokens = tokens.reshape(n_audio, self.n_group, -1)
        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)

        # get the final candidates for each group, and slice between the first sampled token and EOT
        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)
        tokens = tokens[..., self.sample_begin :]

        # eval and convert to list
        mx.eval(tokens, sum_logprobs, no_speech_probs)
        tokens = tokens.tolist()
        sum_logprobs = sum_logprobs.tolist()
        no_speech_probs = no_speech_probs.tolist()
        tokens = [[t[: t.index(tokenizer.eot)] for t in s] for s in tokens]

        # select the top-ranked sample in each group
        selected = self.sequence_ranker.rank(tokens, sum_logprobs)
        tokens: List[List[int]] = [t[i] for i, t in zip(selected, tokens)]
        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]

        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]
        avg_logprobs: List[float] = [
            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)
        ]

        fields = (
            texts,
            languages,
            tokens,
            audio_features,
            avg_logprobs,
            no_speech_probs,
        )
        if len(set(map(len, fields))) != 1:
            raise RuntimeError(f"inconsistent result lengths: {list(map(len, fields))}")

        return [
            DecodingResult(
                audio_features=features,
                language=language,
                tokens=tokens,
                text=text,
                avg_logprob=avg_logprob,
                no_speech_prob=no_speech_prob,
                temperature=self.options.temperature,
                compression_ratio=compression_ratio(text),
            )
            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(
                *fields
            )
        ]


def decode(
    model: "Whisper",
    mel: mx.array,
    options: DecodingOptions = DecodingOptions(),
    **kwargs,
) -> Union[DecodingResult, List[DecodingResult]]:
    """
    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).

    Parameters
    ----------
    model: Whisper
        the Whisper model instance

    mel: mx.array, shape = (80, 3000) or (*, 80, 3000)
        An array containing the Mel spectrogram(s)

    options: DecodingOptions
        A dataclass that contains all necessary options for decoding 30-second segments

    Returns
    -------
    result: Union[DecodingResult, List[DecodingResult]]
        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)
    """
    if single := mel.ndim == 2:
        mel = mel[None]

    if kwargs:
        options = replace(options, **kwargs)

    result = DecodingTask(model, options).run(mel)
    return result[0] if single else result



================================================
FILE: mlx_audio/stt/models/whisper/timing.py
================================================
# Copyright Â© 2023 Apple Inc.

import itertools
from dataclasses import dataclass
from typing import TYPE_CHECKING, List

import mlx.core as mx
import numba
import numpy as np
from scipy import signal

from .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND
from .tokenizer import Tokenizer

if TYPE_CHECKING:
    from .model import Whisper


def median_filter(x: np.ndarray, filter_width: int):
    """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    pad_width = filter_width // 2
    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    if (ndim := x.ndim) <= 2:
        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D
        x = x[None, None, :]

    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), "`filter_width` should be an odd number"

    x = np.pad(x, ((0, 0), (0, 0), (pad_width, pad_width)), mode="reflect")

    # todo: more efficient version in mlx
    result = signal.medfilt(x.astype(np.float32), kernel_size=(1, 1, filter_width))[
        ..., pad_width:-pad_width
    ]

    if ndim <= 2:
        result = result[0, 0]

    return result


@numba.jit(nopython=True)
def backtrace(trace: np.ndarray):
    i = trace.shape[0] - 1
    j = trace.shape[1] - 1
    trace[0, :] = 2
    trace[:, 0] = 1

    result = []
    while i > 0 or j > 0:
        result.append((i - 1, j - 1))

        if trace[i, j] == 0:
            i -= 1
            j -= 1
        elif trace[i, j] == 1:
            i -= 1
        elif trace[i, j] == 2:
            j -= 1
        else:
            raise ValueError("Unexpected trace[i, j]")

    result = np.array(result)
    return result[::-1, :].T


@numba.jit(nopython=True, parallel=True)
def dtw_cpu(x: np.ndarray):
    N, M = x.shape
    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf
    trace = -np.ones((N + 1, M + 1), dtype=np.float32)

    cost[0, 0] = 0
    for j in range(1, M + 1):
        for i in range(1, N + 1):
            c0 = cost[i - 1, j - 1]
            c1 = cost[i - 1, j]
            c2 = cost[i, j - 1]

            if c0 < c1 and c0 < c2:
                c, t = c0, 0
            elif c1 < c0 and c1 < c2:
                c, t = c1, 1
            else:
                c, t = c2, 2

            cost[i, j] = x[i - 1, j - 1] + c
            trace[i, j] = t

    return backtrace(trace)


def dtw(x: np.ndarray) -> np.ndarray:
    # todo: more efficient version in mlx
    return dtw_cpu(x)


@dataclass
class WordTiming:
    word: str
    tokens: List[int]
    start: float
    end: float
    probability: float


def find_alignment(
    model: "Whisper",
    tokenizer: Tokenizer,
    text_tokens: List[int],
    mel: mx.array,
    num_frames: int,
    *,
    medfilt_width: int = 7,
    qk_scale: float = 1.0,
) -> List[WordTiming]:
    if len(text_tokens) == 0:
        return []

    tokens = mx.array(
        [
            *tokenizer.sot_sequence,
            tokenizer.no_timestamps,
            *text_tokens,
            tokenizer.eot,
        ]
    )

    logits, cross_qk = model.forward_with_cross_qk(mel[None, :], tokens[None, :])
    # consider only the logits associated with predicting text
    sampled_logits = logits[0][len(tokenizer.sot_sequence) : -2, : tokenizer.eot]
    token_probs = mx.softmax(sampled_logits, precise=True, axis=-1)
    text_token_probs = mx.take_along_axis(
        token_probs, mx.array(text_tokens)[:, None], axis=1
    ).squeeze(1)
    text_token_probs = np.array(text_token_probs)

    # heads * tokens * frames
    weights = mx.stack(
        [cross_qk[_l][0, _h] for _l, _h in model.alignment_heads.tolist()]
    )
    weights = weights[:, :, : num_frames // 2]
    weights = mx.softmax(weights * qk_scale, axis=-1, precise=True)
    weights = weights.astype(mx.float32)
    mean = mx.mean(weights, axis=-2, keepdims=True)
    std = mx.var(weights, axis=-2, keepdims=True, ddof=0).sqrt()
    weights = (weights - mean) / std
    weights = median_filter(np.array(weights), medfilt_width)

    matrix = weights.mean(axis=0)
    matrix = matrix[len(tokenizer.sot_sequence) : -1]
    text_indices, time_indices = dtw(-matrix)

    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])
    if len(word_tokens) <= 1:
        # return on eot only
        # >>> np.pad([], (1, 0))
        # array([0.])
        # This results in crashes when we lookup jump_times with float, like
        # IndexError: arrays used as indices must be of integer (or boolean) type
        return []
    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))

    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)
    jump_times = time_indices[jumps] / TOKENS_PER_SECOND
    start_times = jump_times[word_boundaries[:-1]]
    end_times = jump_times[word_boundaries[1:]]
    word_probabilities = [
        np.mean(text_token_probs[i:j])
        for i, j in zip(word_boundaries[:-1], word_boundaries[1:])
    ]

    return [
        WordTiming(word, tokens, start, end, probability)
        for word, tokens, start, end, probability in zip(
            words, word_tokens, start_times, end_times, word_probabilities
        )
    ]


def merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):
    # merge prepended punctuations
    i = len(alignment) - 2
    j = len(alignment) - 1
    while i >= 0:
        previous = alignment[i]
        following = alignment[j]
        if previous.word.startswith(" ") and previous.word.strip() in prepended:
            # prepend it to the following word
            following.word = previous.word + following.word
            following.tokens = previous.tokens + following.tokens
            previous.word = ""
            previous.tokens = []
        else:
            j = i
        i -= 1

    # merge appended punctuations
    i = 0
    j = 1
    while j < len(alignment):
        previous = alignment[i]
        following = alignment[j]
        if not previous.word.endswith(" ") and following.word in appended:
            # append it to the previous word
            previous.word = previous.word + following.word
            previous.tokens = previous.tokens + following.tokens
            following.word = ""
            following.tokens = []
        else:
            i = j
        j += 1


def add_word_timestamps(
    *,
    segments: List[dict],
    model: "Whisper",
    tokenizer: Tokenizer,
    mel: mx.array,
    num_frames: int,
    prepend_punctuations: str = "\"'â€œÂ¿([{-",
    append_punctuations: str = "\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼šâ€)]}ã€",
    last_speech_timestamp: float,
    **kwargs,
):
    if len(segments) == 0:
        return

    text_tokens_per_segment = [
        [token for token in segment["tokens"] if token < tokenizer.eot]
        for segment in segments
    ]

    text_tokens = list(itertools.chain.from_iterable(text_tokens_per_segment))
    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **kwargs)
    word_durations = np.array([t.end - t.start for t in alignment])
    word_durations = word_durations[word_durations.nonzero()]
    median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0
    median_duration = min(0.7, float(median_duration))
    max_duration = median_duration * 2

    # hack: truncate long words at sentence boundaries.
    # a better segmentation algorithm based on VAD should be able to replace this.
    if len(word_durations) > 0:
        sentence_end_marks = ".ã€‚!ï¼?ï¼Ÿ"
        # ensure words at sentence boundaries are not longer than twice the median word duration.
        for i in range(1, len(alignment)):
            if alignment[i].end - alignment[i].start > max_duration:
                if alignment[i].word in sentence_end_marks:
                    alignment[i].end = alignment[i].start + max_duration
                elif alignment[i - 1].word in sentence_end_marks:
                    alignment[i].start = alignment[i].end - max_duration

    merge_punctuations(alignment, prepend_punctuations, append_punctuations)

    time_offset = segments[0]["seek"] * HOP_LENGTH / SAMPLE_RATE
    word_index = 0

    for segment, text_tokens in zip(segments, text_tokens_per_segment):
        saved_tokens = 0
        words = []

        while word_index < len(alignment) and saved_tokens < len(text_tokens):
            timing = alignment[word_index]

            if timing.word:
                words.append(
                    dict(
                        word=timing.word,
                        start=round(time_offset + timing.start, 2),
                        end=round(time_offset + timing.end, 2),
                        probability=float(timing.probability),
                    )
                )

            saved_tokens += len(timing.tokens)
            word_index += 1

        # hack: truncate long words at segment boundaries.
        # a better segmentation algorithm based on VAD should be able to replace this.
        if len(words) > 0:
            # ensure the first and second word after a pause is not longer than
            # twice the median word duration.
            if words[0]["end"] - last_speech_timestamp > median_duration * 4 and (
                words[0]["end"] - words[0]["start"] > max_duration
                or (
                    len(words) > 1
                    and words[1]["end"] - words[0]["start"] > max_duration * 2
                )
            ):
                if (
                    len(words) > 1
                    and words[1]["end"] - words[1]["start"] > max_duration
                ):
                    boundary = max(words[1]["end"] / 2, words[1]["end"] - max_duration)
                    words[0]["end"] = words[1]["start"] = boundary
                words[0]["start"] = max(0, words[0]["end"] - max_duration)

            # prefer the segment-level start timestamp if the first word is too long.
            if (
                segment["start"] < words[0]["end"]
                and segment["start"] - 0.5 > words[0]["start"]
            ):
                words[0]["start"] = max(
                    0, min(words[0]["end"] - median_duration, segment["start"])
                )
            else:
                segment["start"] = words[0]["start"]

            # prefer the segment-level end timestamp if the last word is too long.
            if (
                segment["end"] > words[-1]["start"]
                and segment["end"] + 0.5 < words[-1]["end"]
            ):
                words[-1]["end"] = max(
                    words[-1]["start"] + median_duration, segment["end"]
                )
            else:
                segment["end"] = words[-1]["end"]

            last_speech_timestamp = segment["end"]

        segment["words"] = words



================================================
FILE: mlx_audio/stt/models/whisper/tokenizer.py
================================================
# Copyright Â© 2023 Apple Inc.

import base64
import os
import string
from dataclasses import dataclass, field
from functools import cached_property, lru_cache
from typing import Dict, List, Optional, Tuple

import tiktoken

LANGUAGES = {
    "en": "english",
    "zh": "chinese",
    "de": "german",
    "es": "spanish",
    "ru": "russian",
    "ko": "korean",
    "fr": "french",
    "ja": "japanese",
    "pt": "portuguese",
    "tr": "turkish",
    "pl": "polish",
    "ca": "catalan",
    "nl": "dutch",
    "ar": "arabic",
    "sv": "swedish",
    "it": "italian",
    "id": "indonesian",
    "hi": "hindi",
    "fi": "finnish",
    "vi": "vietnamese",
    "he": "hebrew",
    "uk": "ukrainian",
    "el": "greek",
    "ms": "malay",
    "cs": "czech",
    "ro": "romanian",
    "da": "danish",
    "hu": "hungarian",
    "ta": "tamil",
    "no": "norwegian",
    "th": "thai",
    "ur": "urdu",
    "hr": "croatian",
    "bg": "bulgarian",
    "lt": "lithuanian",
    "la": "latin",
    "mi": "maori",
    "ml": "malayalam",
    "cy": "welsh",
    "sk": "slovak",
    "te": "telugu",
    "fa": "persian",
    "lv": "latvian",
    "bn": "bengali",
    "sr": "serbian",
    "az": "azerbaijani",
    "sl": "slovenian",
    "kn": "kannada",
    "et": "estonian",
    "mk": "macedonian",
    "br": "breton",
    "eu": "basque",
    "is": "icelandic",
    "hy": "armenian",
    "ne": "nepali",
    "mn": "mongolian",
    "bs": "bosnian",
    "kk": "kazakh",
    "sq": "albanian",
    "sw": "swahili",
    "gl": "galician",
    "mr": "marathi",
    "pa": "punjabi",
    "si": "sinhala",
    "km": "khmer",
    "sn": "shona",
    "yo": "yoruba",
    "so": "somali",
    "af": "afrikaans",
    "oc": "occitan",
    "ka": "georgian",
    "be": "belarusian",
    "tg": "tajik",
    "sd": "sindhi",
    "gu": "gujarati",
    "am": "amharic",
    "yi": "yiddish",
    "lo": "lao",
    "uz": "uzbek",
    "fo": "faroese",
    "ht": "haitian creole",
    "ps": "pashto",
    "tk": "turkmen",
    "nn": "nynorsk",
    "mt": "maltese",
    "sa": "sanskrit",
    "lb": "luxembourgish",
    "my": "myanmar",
    "bo": "tibetan",
    "tl": "tagalog",
    "mg": "malagasy",
    "as": "assamese",
    "tt": "tatar",
    "haw": "hawaiian",
    "ln": "lingala",
    "ha": "hausa",
    "ba": "bashkir",
    "jw": "javanese",
    "su": "sundanese",
    "yue": "cantonese",
}

# language code lookup by name, with a few language aliases
TO_LANGUAGE_CODE = {
    **{language: code for code, language in LANGUAGES.items()},
    "burmese": "my",
    "valencian": "ca",
    "flemish": "nl",
    "haitian": "ht",
    "letzeburgesch": "lb",
    "pushto": "ps",
    "panjabi": "pa",
    "moldavian": "ro",
    "moldovan": "ro",
    "sinhalese": "si",
    "castilian": "es",
    "mandarin": "zh",
}


@dataclass
class Tokenizer:
    """A thin wrapper around `tiktoken` providing quick access to special tokens"""

    encoding: tiktoken.Encoding
    num_languages: int
    language: Optional[str] = None
    task: Optional[str] = None
    sot_sequence: Tuple[int] = ()
    special_tokens: Dict[str, int] = field(default_factory=dict)

    def __post_init__(self):
        for special in self.encoding.special_tokens_set:
            special_token = self.encoding.encode_single_token(special)
            self.special_tokens[special] = special_token

        sot: int = self.special_tokens["<|startoftranscript|>"]
        translate: int = self.special_tokens["<|translate|>"]
        transcribe: int = self.special_tokens["<|transcribe|>"]

        langs = tuple(LANGUAGES.keys())[: self.num_languages]
        sot_sequence = [sot]
        if self.language is not None:
            sot_sequence.append(sot + 1 + langs.index(self.language))
        if self.task is not None:
            task_token: int = transcribe if self.task == "transcribe" else translate
            sot_sequence.append(task_token)

        self.sot_sequence = tuple(sot_sequence)

    def encode(self, text, **kwargs):
        return self.encoding.encode(text, **kwargs)

    def decode(self, token_ids: List[int], **kwargs) -> str:
        token_ids = [t for t in token_ids if t < self.timestamp_begin]
        return self.encoding.decode(token_ids, **kwargs)

    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:
        """
        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.
        This method decodes given tokens with timestamps tokens annotated, e.g. "<|1.08|>".
        """
        return self.encoding.decode(token_ids, **kwargs)

    @cached_property
    def eot(self) -> int:
        return self.encoding.eot_token

    @cached_property
    def transcribe(self) -> int:
        return self.special_tokens["<|transcribe|>"]

    @cached_property
    def translate(self) -> int:
        return self.special_tokens["<|translate|>"]

    @cached_property
    def sot(self) -> int:
        return self.special_tokens["<|startoftranscript|>"]

    @cached_property
    def sot_lm(self) -> int:
        return self.special_tokens["<|startoflm|>"]

    @cached_property
    def sot_prev(self) -> int:
        return self.special_tokens["<|startofprev|>"]

    @cached_property
    def no_speech(self) -> int:
        return self.special_tokens["<|nospeech|>"]

    @cached_property
    def no_timestamps(self) -> int:
        return self.special_tokens["<|notimestamps|>"]

    @cached_property
    def timestamp_begin(self) -> int:
        return self.special_tokens["<|0.00|>"]

    @cached_property
    def language_token(self) -> int:
        """Returns the token id corresponding to the value of the `language` field"""
        if self.language is None:
            raise ValueError("This tokenizer does not have language token configured")

        return self.to_language_token(self.language)

    def to_language_token(self, language):
        if token := self.special_tokens.get(f"<|{language}|>", None):
            return token

        raise KeyError(f"Language {language} not found in tokenizer.")

    @cached_property
    def all_language_tokens(self) -> Tuple[int]:
        result = []
        for token, token_id in self.special_tokens.items():
            if token.strip("<|>") in LANGUAGES:
                result.append(token_id)
        return tuple(result)[: self.num_languages]

    @cached_property
    def all_language_codes(self) -> Tuple[str]:
        return tuple(self.decode([_l]).strip("<|>") for _l in self.all_language_tokens)

    @cached_property
    def sot_sequence_including_notimestamps(self) -> Tuple[int]:
        return tuple(list(self.sot_sequence) + [self.no_timestamps])

    @cached_property
    def non_speech_tokens(self) -> Tuple[int]:
        """
        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech
        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.

        - â™ªâ™ªâ™ª
        - ( SPEAKING FOREIGN LANGUAGE )
        - [DAVID] Hey there,

        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.
        """
        symbols = list('"#()*+/:;<=>@[\\]^_`{|}~ã€Œã€ã€Žã€')
        symbols += (
            "<< >> <<< >>> -- --- -( -[ (' (\" (( )) ((( ))) [[ ]] {{ }} â™ªâ™ª â™ªâ™ªâ™ª".split()
        )

        # symbols that may be a single token or multiple tokens depending on the tokenizer.
        # In case they're multiple tokens, suppress the first token, which is safe because:
        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress
        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.
        miscellaneous = set("â™©â™ªâ™«â™¬â™­â™®â™¯")
        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)

        # allow hyphens "-" and single quotes "'" between words, but not at the beginning of a word
        result = {self.encoding.encode(" -")[0], self.encoding.encode(" '")[0]}
        for symbol in symbols + list(miscellaneous):
            for tokens in [
                self.encoding.encode(symbol),
                self.encoding.encode(" " + symbol),
            ]:
                if len(tokens) == 1 or symbol in miscellaneous:
                    result.add(tokens[0])

        return tuple(sorted(result))

    def split_to_word_tokens(self, tokens: List[int]):
        if self.language in {"zh", "ja", "th", "lo", "my", "yue"}:
            # These languages don't typically use spaces, so it is difficult to split words
            # without morpheme analysis. Here, we instead split words at any
            # position where the tokens are decoded as valid unicode points
            return self.split_tokens_on_unicode(tokens)

        return self.split_tokens_on_spaces(tokens)

    def split_tokens_on_unicode(self, tokens: List[int]):
        decoded_full = self.decode_with_timestamps(tokens)
        replacement_char = "\ufffd"

        words = []
        word_tokens = []
        current_tokens = []
        unicode_offset = 0

        for token in tokens:
            current_tokens.append(token)
            decoded = self.decode_with_timestamps(current_tokens)

            if (
                replacement_char not in decoded
                or decoded_full[unicode_offset + decoded.index(replacement_char)]
                == replacement_char
            ):
                words.append(decoded)
                word_tokens.append(current_tokens)
                current_tokens = []
                unicode_offset += len(decoded)

        return words, word_tokens

    def split_tokens_on_spaces(self, tokens: List[int]):
        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)
        words = []
        word_tokens = []

        for subword, subword_tokens in zip(subwords, subword_tokens_list):
            special = subword_tokens[0] >= self.eot
            with_space = subword.startswith(" ")
            punctuation = subword.strip() in string.punctuation
            if special or with_space or punctuation or len(words) == 0:
                words.append(subword)
                word_tokens.append(subword_tokens)
            else:
                words[-1] = words[-1] + subword
                word_tokens[-1].extend(subword_tokens)

        return words, word_tokens


@lru_cache(maxsize=None)
def get_encoding(name: str = "gpt2", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), "assets", f"{name}.tiktoken")
    with open(vocab_path) as fid:
        ranks = {
            base64.b64decode(token): int(rank)
            for token, rank in (line.split() for line in fid if line)
        }
    n_vocab = len(ranks)
    special_tokens = {}

    specials = [
        "<|endoftext|>",
        "<|startoftranscript|>",
        *[f"<|{lang}|>" for lang in list(LANGUAGES.keys())[:num_languages]],
        "<|translate|>",
        "<|transcribe|>",
        "<|startoflm|>",
        "<|startofprev|>",
        "<|nospeech|>",
        "<|notimestamps|>",
        *[f"<|{i * 0.02:.2f}|>" for i in range(1501)],
    ]

    for token in specials:
        special_tokens[token] = n_vocab
        n_vocab += 1

    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab,
        pat_str=r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )


@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal["transcribe", "translate", None]
) -> Tokenizer:
    if language is not None:
        language = language.lower()
        if language not in LANGUAGES:
            if language in TO_LANGUAGE_CODE:
                language = TO_LANGUAGE_CODE[language]
            else:
                raise ValueError(f"Unsupported language: {language}")

    if multilingual:
        encoding_name = "multilingual"
        language = language or "en"
        task = task or "transcribe"
    else:
        encoding_name = "gpt2"
        language = None
        task = None

    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )



================================================
FILE: mlx_audio/stt/models/whisper/whisper.py
================================================
# Copyright Â© 2023 Apple Inc.

import base64
import gzip
import json
import math
import sys
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import tqdm
from huggingface_hub import snapshot_download
from mlx.utils import tree_unflatten

from .audio import (
    FRAMES_PER_SECOND,
    HOP_LENGTH,
    N_FRAMES,
    N_SAMPLES,
    SAMPLE_RATE,
    log_mel_spectrogram,
    pad_or_trim,
)
from .decoding import DecodingOptions, DecodingResult
from .decoding import decode as decode_function
from .decoding import detect_language as detect_language_function
from .timing import add_word_timestamps
from .tokenizer import LANGUAGES, get_tokenizer


def _format_timestamp(seconds: float):
    assert seconds >= 0, "non-negative timestamp expected"
    milliseconds = round(seconds * 1000.0)

    hours = milliseconds // 3_600_000
    milliseconds -= hours * 3_600_000

    minutes = milliseconds // 60_000
    milliseconds -= minutes * 60_000

    seconds = milliseconds // 1_000
    milliseconds -= seconds * 1_000

    hours_marker = f"{hours:02d}:" if hours > 0 else ""
    return f"{hours_marker}{minutes:02d}:{seconds:02d}.{milliseconds:03d}"


def _get_end(segments: List[dict]) -> Optional[float]:
    return next(
        (w["end"] for s in reversed(segments) for w in reversed(s["words"])),
        segments[-1]["end"] if segments else None,
    )


@dataclass
class STTOutput:
    text: str
    segments: List[dict] = None
    language: str = None


@dataclass
class ModelDimensions:
    n_mels: int
    n_audio_ctx: int
    n_audio_state: int
    n_audio_head: int
    n_audio_layer: int
    n_vocab: int
    n_text_ctx: int
    n_text_state: int
    n_text_head: int
    n_text_layer: int


def sinusoids(length, channels, max_timescale=10000):
    """Returns sinusoids for positional embedding"""
    assert channels % 2 == 0
    log_timescale_increment = math.log(max_timescale) / (channels // 2 - 1)
    inv_timescales = mx.exp(-log_timescale_increment * mx.arange(channels // 2))
    scaled_time = mx.arange(length)[:, None] * inv_timescales[None, :]
    return mx.concatenate([mx.sin(scaled_time), mx.cos(scaled_time)], axis=1)


class MultiHeadAttention(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()
        self.n_head = n_head
        self.query = nn.Linear(n_state, n_state)
        self.key = nn.Linear(n_state, n_state, bias=False)
        self.value = nn.Linear(n_state, n_state)
        self.out = nn.Linear(n_state, n_state)

    def __call__(
        self,
        x,
        xa=None,
        mask=None,
        kv_cache=None,
    ):
        q = self.query(x)

        if xa is None:
            k = self.key(x)
            v = self.value(x)
            if kv_cache is not None:
                k = mx.concatenate([kv_cache[0], k], axis=1)
                v = mx.concatenate([kv_cache[1], v], axis=1)
        elif kv_cache is None:
            k = self.key(xa)
            v = self.value(xa)
        else:
            k, v = kv_cache

        wv, qk = self.qkv_attention(q, k, v, mask)
        return self.out(wv), (k, v), qk

    def qkv_attention(self, q, k, v, mask=None):
        n_batch, n_ctx, n_state = q.shape
        scale = (n_state // self.n_head) ** -0.25
        q = q.reshape(*q.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3) * scale
        k = k.reshape(*k.shape[:2], self.n_head, -1).transpose(0, 2, 3, 1) * scale
        v = v.reshape(*v.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3)

        qk = q @ k
        if mask is not None:
            qk = qk + mask[:n_ctx, :n_ctx]

        w = mx.softmax(qk, axis=-1, precise=True)
        out = (w @ v).transpose(0, 2, 1, 3)
        out = out.reshape(n_batch, n_ctx, n_state)
        return out, qk


class ResidualAttentionBlock(nn.Module):
    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):
        super().__init__()

        self.attn = MultiHeadAttention(n_state, n_head)
        self.attn_ln = nn.LayerNorm(n_state)

        self.cross_attn = (
            MultiHeadAttention(n_state, n_head) if cross_attention else None
        )
        self.cross_attn_ln = nn.LayerNorm(n_state) if cross_attention else None

        n_mlp = n_state * 4
        self.mlp1 = nn.Linear(n_state, n_mlp)
        self.mlp2 = nn.Linear(n_mlp, n_state)
        self.mlp_ln = nn.LayerNorm(n_state)

    def __call__(self, x, xa=None, mask=None, kv_cache=None):
        kv, cross_kv = kv_cache if kv_cache else (None, None)
        y, kv, _ = self.attn(self.attn_ln(x), mask=mask, kv_cache=kv)
        x += y
        cross_qk = None
        if self.cross_attn:
            y, cross_kv, cross_qk = self.cross_attn(
                self.cross_attn_ln(x), xa, kv_cache=cross_kv
            )
            x += y
        x = x + self.mlp2(nn.gelu(self.mlp1(self.mlp_ln(x))))
        return x, (kv, cross_kv), cross_qk


class AudioEncoder(nn.Module):
    def __init__(
        self,
        n_mels: int,
        n_ctx: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        dtype: mx.Dtype = mx.float16,
    ):
        super().__init__()
        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)
        self._positional_embedding = sinusoids(n_ctx, n_state).astype(dtype)

        self.blocks = [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]
        self.ln_post = nn.LayerNorm(n_state)

    def __call__(self, x):
        x = nn.gelu(self.conv1(x))
        x = nn.gelu(self.conv2(x))
        assert x.shape[1:] == self._positional_embedding.shape, "incorrect audio shape"
        x = x + self._positional_embedding

        for block in self.blocks:
            x, _, _ = block(x)

        x = self.ln_post(x)
        return x


class TextDecoder(nn.Module):
    def __init__(
        self,
        n_vocab: int,
        n_ctx: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        dtype: mx.Dtype = mx.float16,
    ):
        super().__init__()

        self.token_embedding = nn.Embedding(n_vocab, n_state)
        self.positional_embedding = mx.zeros((n_ctx, n_state))

        self.blocks = [
            ResidualAttentionBlock(n_state, n_head, cross_attention=True)
            for _ in range(n_layer)
        ]
        self.ln = nn.LayerNorm(n_state)
        self._mask = nn.MultiHeadAttention.create_additive_causal_mask(n_ctx).astype(
            dtype
        )

    def __call__(self, x, xa, kv_cache=None):
        """
        x : mx.array, shape = (batch_size, <= n_ctx)
            the text tokens
        xa : mx.array, shape = (batch_size, n_audio_ctx, n_audio_state)
            the encoded audio features to be attended on
        """
        offset = kv_cache[0][0][0].shape[1] if kv_cache else 0
        x = (
            self.token_embedding(x)
            + self.positional_embedding[offset : offset + x.shape[-1]]
        )

        if kv_cache is None:
            kv_cache = [None] * len(self.blocks)
        cross_qk = [None] * len(self.blocks)
        for e, block in enumerate(self.blocks):
            x, kv_cache[e], cross_qk[e] = block(
                x, xa, mask=self._mask, kv_cache=kv_cache[e]
            )

        x = self.ln(x)
        return self.token_embedding.as_linear(x), kv_cache, cross_qk


class Model(nn.Module):
    def __init__(self, dims: ModelDimensions, dtype: mx.Dtype = mx.float16):
        super().__init__()
        self.dims = dims
        self.dtype = dtype
        self.encoder = AudioEncoder(
            self.dims.n_mels,
            self.dims.n_audio_ctx,
            self.dims.n_audio_state,
            self.dims.n_audio_head,
            self.dims.n_audio_layer,
            dtype,
        )
        self.decoder = TextDecoder(
            self.dims.n_vocab,
            self.dims.n_text_ctx,
            self.dims.n_text_state,
            self.dims.n_text_head,
            self.dims.n_text_layer,
            dtype,
        )
        # use the last half among the decoder layers for time alignment by default;
        # to use a specific set of heads, see `set_alignment_heads()` below.
        all_heads = np.zeros(
            (self.dims.n_text_layer, self.dims.n_text_head), dtype=bool
        )
        all_heads[self.dims.n_text_layer // 2 :] = True
        self.alignment_heads = mx.array(np.asarray(all_heads.nonzero()).T)

    def set_alignment_heads(self, dump: Union[bytes, np.ndarray]):
        if isinstance(dump, np.ndarray):
            self.alignment_heads = mx.array(dump)
        elif isinstance(dump, bytes):
            array = np.frombuffer(
                gzip.decompress(base64.b85decode(dump)), dtype=bool
            ).copy()
            mask = array.reshape(self.dims.n_text_layer, self.dims.n_text_head)
            self.alignment_heads = mx.array(np.asarray(mask.nonzero()).T)
        else:
            raise ValueError(
                f"Invalid type for `dump`: {type(dump)}. Expected a np.ndarray or base85-encoded bytes containing"
                " alignment_head information"
            )

    def embed_audio(self, mel):
        return self.encoder(mel)

    def logits(self, tokens, audio_features):
        return self.decoder(tokens, audio_features)[0]

    def forward_with_cross_qk(self, mel, tokens):
        logits, _, cross_qk = self.decoder(tokens, self.encoder(mel))
        return logits, cross_qk

    def __call__(self, mel, tokens):
        return self.decoder(tokens, self.encoder(mel))[0]

    @property
    def is_multilingual(self):
        return self.dims.n_vocab >= 51865

    @property
    def num_languages(self):
        return self.dims.n_vocab - 51765 - int(self.is_multilingual)

    detect_language = detect_language_function
    decode = decode_function

    @classmethod
    def from_pretrained(
        cls,
        path_or_hf_repo: str = "mlx-community/whisper-tiny",
        dtype: mx.Dtype = mx.float16,
    ) -> "Whisper":
        model_path = Path(path_or_hf_repo)
        if not model_path.exists():
            model_path = Path(snapshot_download(repo_id=path_or_hf_repo))

        with open(str(model_path / "config.json"), "r") as f:
            config = json.loads(f.read())
            config.pop("model_type", None)
            quantization = config.pop("quantization", None)

        model_args = ModelDimensions(**config)

        wf = model_path / "weights.safetensors"
        if not wf.exists():
            wf = model_path / "weights.npz"
        weights = mx.load(str(wf))

        model = Model(model_args, dtype)

        if quantization is not None:
            class_predicate = (
                lambda p, m: isinstance(m, (nn.Linear, nn.Embedding))
                and f"{p}.scales" in weights
            )
            nn.quantize(model, **quantization, class_predicate=class_predicate)

        weights = tree_unflatten(list(weights.items()))
        model.update(weights)
        mx.eval(model.parameters())
        return model

    def generate(
        self,
        audio: Union[str, np.ndarray, mx.array],
        *,
        verbose: Optional[bool] = None,
        temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
        compression_ratio_threshold: Optional[float] = 2.4,
        logprob_threshold: Optional[float] = -1.0,
        no_speech_threshold: Optional[float] = 0.6,
        condition_on_previous_text: bool = True,
        initial_prompt: Optional[str] = None,
        word_timestamps: bool = False,
        prepend_punctuations: str = "\"'â€œÂ¿([{-",
        append_punctuations: str = "\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼šâ€)]}ã€",
        clip_timestamps: Union[str, List[float]] = "0",
        hallucination_silence_threshold: Optional[float] = None,
        **decode_options,
    ):
        """
        Transcribe an audio file using Whisper

        Parameters
        ----------
        audio: Union[str, np.ndarray, mx.array]
            The path to the audio file to open, or the audio waveform

        verbose: bool
            Whether to display the text being decoded to the console. If True, displays all the details,
            If False, displays minimal details. If None, does not display anything

        temperature: Union[float, Tuple[float, ...]]
            Temperature for sampling. It can be a tuple of temperatures, which will be successively used
            upon failures according to either `compression_ratio_threshold` or `logprob_threshold`.

        compression_ratio_threshold: float
            If the gzip compression ratio is above this value, treat as failed

        logprob_threshold: float
            If the average log probability over sampled tokens is below this value, treat as failed

        no_speech_threshold: float
            If the no_speech probability is higher than this value AND the average log probability
            over sampled tokens is below `logprob_threshold`, consider the segment as silent

        condition_on_previous_text: bool
            if True, the previous output of the model is provided as a prompt for the next window;
            disabling may make the text inconsistent across windows, but the model becomes less prone to
            getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.

        word_timestamps: bool
            Extract word-level timestamps using the cross-attention pattern and dynamic time warping,
            and include the timestamps for each word in each segment.

        prepend_punctuations: str
            If word_timestamps is True, merge these punctuation symbols with the next word

        append_punctuations: str
            If word_timestamps is True, merge these punctuation symbols with the previous word

        initial_prompt: Optional[str]
            Optional text to provide as a prompt for the first window. This can be used to provide, or
            "prompt-engineer" a context for transcription, e.g. custom vocabularies or proper nouns
            to make it more likely to predict those word correctly.

        decode_options: dict
            Keyword arguments to construct `DecodingOptions` instances

        clip_timestamps: Union[str, List[float]]
            Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.
            The last end timestamp defaults to the end of the file.

        hallucination_silence_threshold: Optional[float]
            When word_timestamps is True, skip silent periods longer than this threshold (in seconds)
            when a possible hallucination is detected

        Returns
        -------
        A dictionary containing the resulting text ("text") and segment-level details ("segments"), and
        the spoken language ("language"), which is detected when `decode_options["language"]` is None.
        """

        # Pad 30-seconds of silence to the input audio, for slicing
        mel = log_mel_spectrogram(audio, n_mels=self.dims.n_mels, padding=N_SAMPLES)
        content_frames = mel.shape[-2] - N_FRAMES
        content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)

        if verbose:
            system_encoding = sys.getdefaultencoding()
            if system_encoding != "utf-8":
                make_safe = lambda x: x.encode(
                    system_encoding, errors="replace"
                ).decode(system_encoding)
            else:
                make_safe = lambda x: x

        if decode_options.get("language", None) is None:
            if not self.is_multilingual:
                decode_options["language"] = "en"
            else:
                if verbose:
                    print(
                        "Detecting language using up to the first 30 seconds. "
                        "Use the `language` decoding option to specify the language"
                    )
                mel_segment = pad_or_trim(mel, N_FRAMES, axis=-2).astype(self.dtype)
                _, probs = self.detect_language(mel_segment)
                decode_options["language"] = max(probs, key=probs.get)
                if verbose is not None:
                    print(
                        f"Detected language: {LANGUAGES[decode_options['language']].title()}"
                    )

        language: str = decode_options["language"]
        task: str = decode_options.get("task", "transcribe")
        tokenizer = get_tokenizer(
            self.is_multilingual,
            num_languages=self.num_languages,
            language=language,
            task=task,
        )

        if isinstance(clip_timestamps, str):
            clip_timestamps = [
                float(ts)
                for ts in (clip_timestamps.split(",") if clip_timestamps else [])
            ]
        seek_points: List[int] = [
            round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps
        ]
        if len(seek_points) == 0:
            seek_points.append(0)
        if len(seek_points) % 2 == 1:
            seek_points.append(content_frames)
        else:
            seek_points[-1] = min(content_frames, seek_points[-1])
        seek_clips: List[Tuple[int, int]] = list(
            zip(seek_points[::2], seek_points[1::2])
        )

        punctuation = "\"'â€œÂ¿([{-\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼šâ€)]}ã€"

        if word_timestamps and task == "translate":
            warnings.warn("Word-level timestamps on translations may not be reliable.")

        def decode_with_fallback(segment: mx.array) -> DecodingResult:
            temperatures = (
                [temperature] if isinstance(temperature, (int, float)) else temperature
            )
            decode_result = None

            for t in temperatures:
                kwargs = {**decode_options}
                if t > 0:
                    # disable beam_size and patience when t > 0
                    kwargs.pop("beam_size", None)
                    kwargs.pop("patience", None)
                else:
                    # disable best_of when t == 0
                    kwargs.pop("best_of", None)

                options = DecodingOptions(**kwargs, temperature=t)
                decode_result = self.decode(segment, options)

                needs_fallback = False
                if (
                    compression_ratio_threshold is not None
                    and decode_result.compression_ratio > compression_ratio_threshold
                ):
                    needs_fallback = True  # too repetitive
                if (
                    logprob_threshold is not None
                    and decode_result.avg_logprob < logprob_threshold
                ):
                    needs_fallback = True  # average log probability is too low
                if (
                    no_speech_threshold is not None
                    and decode_result.no_speech_prob > no_speech_threshold
                ):
                    needs_fallback = False  # silence
                if not needs_fallback:
                    break

            return decode_result

        clip_idx = 0
        seek = seek_clips[clip_idx][0]
        input_stride = (
            N_FRAMES // self.dims.n_audio_ctx
        )  # mel frames per output token: 2
        time_precision = (
            input_stride * HOP_LENGTH / SAMPLE_RATE
        )  # time per output token: 0.02 (seconds)
        all_tokens = []
        all_segments = []
        prompt_reset_since = 0

        if initial_prompt is not None:
            initial_prompt_tokens = tokenizer.encode(" " + initial_prompt.strip())
            all_tokens.extend(initial_prompt_tokens)
        else:
            initial_prompt_tokens = []

        def new_segment(
            *, start: float, end: float, tokens: mx.array, result: DecodingResult
        ):
            tokens = tokens.tolist()
            text_tokens = [token for token in tokens if token < tokenizer.eot]
            return {
                "seek": seek,
                "start": start,
                "end": end,
                "text": tokenizer.decode(text_tokens),
                "tokens": tokens,
                "temperature": result.temperature,
                "avg_logprob": result.avg_logprob,
                "compression_ratio": result.compression_ratio,
                "no_speech_prob": result.no_speech_prob,
            }

        # show the progress bar when verbose is False (if True, transcribed text will be printed)
        with tqdm.tqdm(
            total=content_frames, unit="frames", disable=verbose is not False
        ) as pbar:
            last_speech_timestamp = 0.0
            for seek_clip_start, seek_clip_end in seek_clips:
                while seek < seek_clip_end:
                    time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)
                    window_end_time = float(
                        (seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE
                    )
                    segment_size = min(
                        N_FRAMES, content_frames - seek, seek_clip_end - seek
                    )
                    mel_segment = mel[seek : seek + segment_size]
                    segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE
                    mel_segment = pad_or_trim(mel_segment, N_FRAMES, axis=-2).astype(
                        self.dtype
                    )

                    decode_options["prompt"] = all_tokens[prompt_reset_since:]
                    result: DecodingResult = decode_with_fallback(mel_segment)

                    tokens = np.array(result.tokens)

                    if no_speech_threshold is not None:
                        # no voice activity check
                        should_skip = result.no_speech_prob > no_speech_threshold
                        if (
                            logprob_threshold is not None
                            and result.avg_logprob > logprob_threshold
                        ):
                            # don't skip if the logprob is high enough, despite the no_speech_prob
                            should_skip = False

                        if should_skip:
                            seek += segment_size  # fast-forward to the next segment boundary
                            continue

                    previous_seek = seek
                    current_segments = []

                    # anomalous words are very long/short/improbable
                    def word_anomaly_score(word: dict) -> float:
                        probability = word.get("probability", 0.0)
                        duration = word["end"] - word["start"]
                        score = 0.0
                        if probability < 0.15:
                            score += 1.0
                        if duration < 0.133:
                            score += (0.133 - duration) * 15
                        if duration > 2.0:
                            score += duration - 2.0
                        return score

                    def is_segment_anomaly(segment: Optional[dict]) -> bool:
                        if segment is None or not segment["words"]:
                            return False
                        words = [
                            w for w in segment["words"] if w["word"] not in punctuation
                        ]
                        words = words[:8]
                        score = sum(word_anomaly_score(w) for w in words)
                        return score >= 3 or score + 0.01 >= len(words)

                    def next_words_segment(segments: List[dict]) -> Optional[dict]:
                        return next((s for s in segments if s["words"]), None)

                    timestamp_tokens = tokens >= tokenizer.timestamp_begin
                    single_timestamp_ending = timestamp_tokens[-2:].tolist() == [
                        False,
                        True,
                    ]

                    consecutive = np.where(
                        np.logical_and(timestamp_tokens[:-1], timestamp_tokens[1:])
                    )[0]
                    consecutive += 1
                    if len(consecutive) > 0:
                        # if the output contains two consecutive timestamp tokens
                        slices = consecutive.tolist()
                        if single_timestamp_ending:
                            slices.append(len(tokens))

                        last_slice = 0
                        for current_slice in slices:
                            sliced_tokens = tokens[last_slice:current_slice]
                            start_timestamp_pos = (
                                sliced_tokens[0].item() - tokenizer.timestamp_begin
                            )
                            end_timestamp_pos = (
                                sliced_tokens[-1].item() - tokenizer.timestamp_begin
                            )
                            current_segments.append(
                                new_segment(
                                    start=time_offset
                                    + start_timestamp_pos * time_precision,
                                    end=time_offset
                                    + end_timestamp_pos * time_precision,
                                    tokens=sliced_tokens,
                                    result=result,
                                )
                            )
                            last_slice = current_slice

                        if single_timestamp_ending:
                            # single timestamp at the end means no speech after the last timestamp.
                            seek += segment_size
                        else:
                            # otherwise, ignore the unfinished segment and seek to the last timestamp
                            last_timestamp_pos = (
                                tokens[last_slice - 1].item()
                                - tokenizer.timestamp_begin
                            )
                            seek += last_timestamp_pos * input_stride
                    else:
                        duration = segment_duration
                        timestamps = tokens[timestamp_tokens.nonzero()[0]]
                        if (
                            len(timestamps) > 0
                            and timestamps[-1].item() != tokenizer.timestamp_begin
                        ):
                            # no consecutive timestamps but it has a timestamp; use the last one.
                            last_timestamp_pos = (
                                timestamps[-1].item() - tokenizer.timestamp_begin
                            )
                            duration = last_timestamp_pos * time_precision

                        current_segments.append(
                            new_segment(
                                start=time_offset,
                                end=time_offset + duration,
                                tokens=tokens,
                                result=result,
                            )
                        )
                        seek += segment_size

                    if word_timestamps:
                        add_word_timestamps(
                            segments=current_segments,
                            model=self,
                            tokenizer=tokenizer,
                            mel=mel_segment,
                            num_frames=segment_size,
                            prepend_punctuations=prepend_punctuations,
                            append_punctuations=append_punctuations,
                            last_speech_timestamp=last_speech_timestamp,
                        )

                        if not single_timestamp_ending:
                            last_word_end = _get_end(current_segments)
                            if (
                                last_word_end is not None
                                and last_word_end > time_offset
                            ):
                                seek = round(last_word_end * FRAMES_PER_SECOND)

                        # skip silence before possible hallucinations
                        if hallucination_silence_threshold is not None:
                            threshold = hallucination_silence_threshold
                            if not single_timestamp_ending:
                                last_word_end = _get_end(current_segments)
                                if (
                                    last_word_end is not None
                                    and last_word_end > time_offset
                                ):
                                    remaining_duration = window_end_time - last_word_end
                                    if remaining_duration > threshold:
                                        seek = round(last_word_end * FRAMES_PER_SECOND)
                                    else:
                                        seek = previous_seek + segment_size

                            # if first segment might be a hallucination, skip leading silence
                            first_segment = next_words_segment(current_segments)
                            if first_segment is not None and is_segment_anomaly(
                                first_segment
                            ):
                                gap = first_segment["start"] - time_offset
                                if gap > threshold:
                                    seek = previous_seek + round(
                                        gap * FRAMES_PER_SECOND
                                    )
                                    continue

                            # skip silence before any possible hallucination that is surrounded
                            # by silence or more hallucinations
                            hal_last_end = last_speech_timestamp
                            for si in range(len(current_segments)):
                                segment = current_segments[si]
                                if not segment["words"]:
                                    continue
                                if is_segment_anomaly(segment):
                                    next_segment = next_words_segment(
                                        current_segments[si + 1 :]
                                    )
                                    if next_segment is not None:
                                        hal_next_start = next_segment["words"][0][
                                            "start"
                                        ]
                                    else:
                                        hal_next_start = time_offset + segment_duration
                                    silence_before = (
                                        segment["start"] - hal_last_end > threshold
                                        or segment["start"] < threshold
                                        or segment["start"] - time_offset < 2.0
                                    )
                                    silence_after = (
                                        hal_next_start - segment["end"] > threshold
                                        or is_segment_anomaly(next_segment)
                                        or window_end_time - segment["end"] < 2.0
                                    )
                                    if silence_before and silence_after:
                                        seek = round(
                                            max(time_offset + 1, segment["start"])
                                            * FRAMES_PER_SECOND
                                        )
                                        if (
                                            content_duration - segment["end"]
                                            < threshold
                                        ):
                                            seek = content_frames
                                        current_segments[si:] = []
                                        break
                                hal_last_end = segment["end"]

                        last_word_end = _get_end(current_segments)
                        if last_word_end is not None:
                            last_speech_timestamp = last_word_end

                    if verbose:
                        for segment in current_segments:
                            start, end, text = (
                                segment["start"],
                                segment["end"],
                                segment["text"],
                            )
                            line = f"[{_format_timestamp(start)} --> {_format_timestamp(end)}] {text}"
                            print(make_safe(line))

                    # if a segment is instantaneous or does not contain text, clear it
                    for i, segment in enumerate(current_segments):
                        if (
                            segment["start"] == segment["end"]
                            or segment["text"].strip() == ""
                        ):
                            segment["text"] = ""
                            segment["tokens"] = []
                            segment["words"] = []

                    all_segments.extend(
                        [
                            {"id": i, **segment}
                            for i, segment in enumerate(
                                current_segments, start=len(all_segments)
                            )
                        ]
                    )
                    all_tokens.extend(
                        [
                            token
                            for segment in current_segments
                            for token in segment["tokens"]
                        ]
                    )

                    if not condition_on_previous_text or result.temperature > 0.5:
                        # do not feed the prompt tokens if a high temperature was used
                        prompt_reset_since = len(all_tokens)

                    # update progress bar
                    pbar.update(min(content_frames, seek) - previous_seek)

        # Clear cache after each segment to avoid memory leaks
        mx.clear_cache()

        return STTOutput(
            text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),
            segments=all_segments,
            language=language,
        )



================================================
FILE: mlx_audio/stt/models/whisper/writers.py
================================================
# Copyright Â© 2024 Apple Inc.

import json
import pathlib
import re
from typing import Callable, List, Optional, TextIO


def format_timestamp(
    seconds: float, always_include_hours: bool = False, decimal_marker: str = "."
):
    assert seconds >= 0, "non-negative timestamp expected"
    milliseconds = round(seconds * 1000.0)

    hours = milliseconds // 3_600_000
    milliseconds -= hours * 3_600_000

    minutes = milliseconds // 60_000
    milliseconds -= minutes * 60_000

    seconds = milliseconds // 1_000
    milliseconds -= seconds * 1_000

    hours_marker = f"{hours:02d}:" if always_include_hours or hours > 0 else ""
    return (
        f"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}"
    )


def get_start(segments: List[dict]) -> Optional[float]:
    return next(
        (w["start"] for s in segments for w in s["words"]),
        segments[0]["start"] if segments else None,
    )


class ResultWriter:
    extension: str

    def __init__(self, output_dir: str):
        self.output_dir = output_dir

    def __call__(
        self, result: dict, output_name: str, options: Optional[dict] = None, **kwargs
    ):
        output_path = (pathlib.Path(self.output_dir) / output_name).with_suffix(
            f".{self.extension}"
        )

        with output_path.open("wt", encoding="utf-8") as f:
            self.write_result(result, file=f, options=options, **kwargs)

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        raise NotImplementedError


class WriteTXT(ResultWriter):
    extension: str = "txt"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        for segment in result["segments"]:
            print(segment["text"].strip(), file=file, flush=True)


class SubtitlesWriter(ResultWriter):
    always_include_hours: bool
    decimal_marker: str

    def iterate_result(
        self,
        result: dict,
        options: Optional[dict] = None,
        *,
        max_line_width: Optional[int] = None,
        max_line_count: Optional[int] = None,
        highlight_words: bool = False,
        max_words_per_line: Optional[int] = None,
    ):
        options = options or {}
        max_line_width = max_line_width or options.get("max_line_width")
        max_line_count = max_line_count or options.get("max_line_count")
        highlight_words = highlight_words or options.get("highlight_words", False)
        max_words_per_line = max_words_per_line or options.get("max_words_per_line")
        preserve_segments = max_line_count is None or max_line_width is None
        max_line_width = max_line_width or 1000
        max_words_per_line = max_words_per_line or 1000

        def iterate_subtitles():
            line_len = 0
            line_count = 1
            # the next subtitle to yield (a list of word timings with whitespace)
            subtitle: List[dict] = []
            last: float = get_start(result["segments"]) or 0.0
            for segment in result["segments"]:
                chunk_index = 0
                words_count = max_words_per_line
                while chunk_index < len(segment["words"]):
                    remaining_words = len(segment["words"]) - chunk_index
                    if max_words_per_line > len(segment["words"]) - chunk_index:
                        words_count = remaining_words
                    for i, original_timing in enumerate(
                        segment["words"][chunk_index : chunk_index + words_count]
                    ):
                        timing = original_timing.copy()
                        long_pause = (
                            not preserve_segments and timing["start"] - last > 3.0
                        )
                        has_room = line_len + len(timing["word"]) <= max_line_width
                        seg_break = i == 0 and len(subtitle) > 0 and preserve_segments
                        if (
                            line_len > 0
                            and has_room
                            and not long_pause
                            and not seg_break
                        ):
                            # line continuation
                            line_len += len(timing["word"])
                        else:
                            # new line
                            timing["word"] = timing["word"].strip()
                            if (
                                len(subtitle) > 0
                                and max_line_count is not None
                                and (long_pause or line_count >= max_line_count)
                                or seg_break
                            ):
                                # subtitle break
                                yield subtitle
                                subtitle = []
                                line_count = 1
                            elif line_len > 0:
                                # line break
                                line_count += 1
                                timing["word"] = "\n" + timing["word"]
                            line_len = len(timing["word"].strip())
                        subtitle.append(timing)
                        last = timing["start"]
                    chunk_index += max_words_per_line
            if len(subtitle) > 0:
                yield subtitle

        if len(result["segments"]) > 0 and "words" in result["segments"][0]:
            for subtitle in iterate_subtitles():
                subtitle_start = self.format_timestamp(subtitle[0]["start"])
                subtitle_end = self.format_timestamp(subtitle[-1]["end"])
                subtitle_text = "".join([word["word"] for word in subtitle])
                if highlight_words:
                    last = subtitle_start
                    all_words = [timing["word"] for timing in subtitle]
                    for i, this_word in enumerate(subtitle):
                        start = self.format_timestamp(this_word["start"])
                        end = self.format_timestamp(this_word["end"])
                        if last != start:
                            yield last, start, subtitle_text

                        yield start, end, "".join(
                            [
                                (
                                    re.sub(r"^(\s*)(.*)$", r"\1<u>\2</u>", word)
                                    if j == i
                                    else word
                                )
                                for j, word in enumerate(all_words)
                            ]
                        )
                        last = end
                else:
                    yield subtitle_start, subtitle_end, subtitle_text
        else:
            for segment in result["segments"]:
                segment_start = self.format_timestamp(segment["start"])
                segment_end = self.format_timestamp(segment["end"])
                segment_text = segment["text"].strip().replace("-->", "->")
                yield segment_start, segment_end, segment_text

    def format_timestamp(self, seconds: float):
        return format_timestamp(
            seconds=seconds,
            always_include_hours=self.always_include_hours,
            decimal_marker=self.decimal_marker,
        )


class WriteVTT(SubtitlesWriter):
    extension: str = "vtt"
    always_include_hours: bool = False
    decimal_marker: str = "."

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        print("WEBVTT\n", file=file)
        for start, end, text in self.iterate_result(result, options, **kwargs):
            print(f"{start} --> {end}\n{text}\n", file=file, flush=True)


class WriteSRT(SubtitlesWriter):
    extension: str = "srt"
    always_include_hours: bool = True
    decimal_marker: str = ","

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        for i, (start, end, text) in enumerate(
            self.iterate_result(result, options, **kwargs), start=1
        ):
            print(f"{i}\n{start} --> {end}\n{text}\n", file=file, flush=True)


class WriteTSV(ResultWriter):
    """
    Write a transcript to a file in TSV (tab-separated values) format containing lines like:
    <start time in integer milliseconds>\t<end time in integer milliseconds>\t<transcript text>

    Using integer milliseconds as start and end times means there's no chance of interference from
    an environment setting a language encoding that causes the decimal in a floating point number
    to appear as a comma; also is faster and more efficient to parse & store, e.g., in C++.
    """

    extension: str = "tsv"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        print("start", "end", "text", sep="\t", file=file)
        for segment in result["segments"]:
            print(round(1000 * segment["start"]), file=file, end="\t")
            print(round(1000 * segment["end"]), file=file, end="\t")
            print(segment["text"].strip().replace("\t", " "), file=file, flush=True)


class WriteJSON(ResultWriter):
    extension: str = "json"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        json.dump(result, file, ensure_ascii=False)


def get_writer(
    output_format: str, output_dir: str
) -> Callable[[dict, TextIO, dict], None]:
    writers = {
        "txt": WriteTXT,
        "vtt": WriteVTT,
        "srt": WriteSRT,
        "tsv": WriteTSV,
        "json": WriteJSON,
    }

    if output_format == "all":
        all_writers = [writer(output_dir) for writer in writers.values()]

        def write_all(
            result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
        ):
            for writer in all_writers:
                writer(result, file, options, **kwargs)

        return write_all

    return writers[output_format](output_dir)



================================================
FILE: mlx_audio/stt/tests/test_models.py
================================================
import json
import unittest
from pathlib import Path
from unittest.mock import ANY, MagicMock, PropertyMock, patch

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from mlx_audio.stt.models.parakeet.parakeet import ParakeetTDT
from mlx_audio.stt.models.whisper.audio import (
    HOP_LENGTH,
    N_FRAMES,
    N_SAMPLES,
    SAMPLE_RATE,
)
from mlx_audio.stt.models.whisper.decoding import DecodingOptions, DecodingResult
from mlx_audio.stt.models.whisper.whisper import Model, ModelDimensions, STTOutput


class TestWhisperModel(unittest.TestCase):
    def setUp(self):
        self.dims = ModelDimensions(
            n_mels=80,
            n_audio_ctx=1500,
            n_audio_state=384,
            n_audio_head=6,
            n_audio_layer=4,
            n_vocab=51864,
            n_text_ctx=448,
            n_text_state=384,
            n_text_head=6,
            n_text_layer=4,
        )
        self.model_mock = MagicMock(spec=Model, name="MockModelInstance")

        self.model_mock.dims = self.dims
        self.model_mock.dtype = mx.float32

        type(self.model_mock).is_multilingual = PropertyMock(return_value=False)
        type(self.model_mock).num_languages = PropertyMock(return_value=0)

    @patch("mlx_audio.stt.models.whisper.whisper.Path")
    @patch("mlx_audio.stt.models.whisper.whisper.snapshot_download")
    @patch("mlx_audio.stt.models.whisper.whisper.mx.load")
    @patch("mlx_audio.stt.models.whisper.whisper.json.loads")
    @patch("builtins.open", new_callable=MagicMock)
    def test_from_pretrained(
        self,
        mock_open,
        mock_json_loads_in_whisper,
        mock_mx_load,
        mock_snapshot_download,
        mock_pathlib_path,
    ):

        mock_snapshot_download.return_value = "dummy_path"

        mock_paths_registry = {}

        def path_constructor_side_effect(path_str_arg):
            if path_str_arg in mock_paths_registry:
                return mock_paths_registry[path_str_arg]
            new_mock_path = MagicMock(spec=Path)
            new_mock_path.__str__.return_value = str(path_str_arg)
            if str(path_str_arg) == "dummy_path/weights.safetensors":
                new_mock_path.exists.return_value = True
            elif str(path_str_arg) == "dummy_path":
                new_mock_path.exists.return_value = True
            else:
                new_mock_path.exists.return_value = False

            def mock_truediv(other_segment):
                concatenated_path_str = f"{str(path_str_arg)}/{other_segment}"
                return path_constructor_side_effect(concatenated_path_str)

            new_mock_path.__truediv__.side_effect = mock_truediv
            new_mock_path.__rtruediv__ = mock_truediv
            mock_paths_registry[path_str_arg] = new_mock_path
            return new_mock_path

        mock_pathlib_path.side_effect = path_constructor_side_effect

        dummy_config = {
            "n_mels": 80,
            "n_audio_ctx": 1500,
            "n_audio_state": 384,
            "n_audio_head": 6,
            "n_audio_layer": 4,
            "n_vocab": 51865,
            "n_text_ctx": 448,
            "n_text_state": 384,
            "n_text_head": 6,
            "n_text_layer": 4,
        }
        mock_open.return_value.__enter__.return_value.read.return_value = json.dumps(
            dummy_config
        )
        mock_json_loads_in_whisper.return_value = dummy_config
        dummy_weights = {
            "encoder.conv1.weight": mx.random.normal((384, 80, 3)),
            "encoder.conv1.bias": mx.random.normal((384,)),
        }
        mock_mx_load.return_value = dummy_weights

        model_instance = Model.from_pretrained(
            path_or_hf_repo="mlx-community/whisper-tiny", dtype=mx.float32
        )

        self.assertIsInstance(model_instance, Model)
        self.assertEqual(model_instance.dims.n_mels, dummy_config["n_mels"])
        mock_snapshot_download.assert_called_once_with(
            repo_id="mlx-community/whisper-tiny"
        )
        mock_open.assert_called_once_with("dummy_path/config.json", "r")
        mock_mx_load.assert_called_once_with("dummy_path/weights.safetensors")

    @patch("mlx_audio.stt.models.whisper.whisper.pad_or_trim")
    @patch("mlx_audio.stt.models.whisper.whisper.tqdm.tqdm")
    @patch("mlx_audio.stt.models.whisper.whisper.get_tokenizer")
    @patch("mlx_audio.stt.models.whisper.whisper.log_mel_spectrogram")
    def test_generate_simple_case(
        self,
        mock_log_mel,
        mock_get_tokenizer,
        mock_tqdm_tqdm,
        mock_pad_or_trim,
    ):
        """Test model.generate for a simple case with one segment."""

        mock_mel_data = mx.zeros((N_FRAMES + 100, self.dims.n_mels), dtype=mx.float32)
        mock_log_mel.return_value = mock_mel_data

        EOT_TOKEN_ID = 50257
        TIMESTAMP_BEGIN_ID = 50364
        mock_tokenizer_inst = MagicMock(
            name="mock_tokenizer_instance_for_test",
            eot=EOT_TOKEN_ID,
            timestamp_begin=TIMESTAMP_BEGIN_ID,
        )

        def actual_decode_side_effect(tokens_to_decode):
            text_parts = []
            for token_val in tokens_to_decode:
                t = int(token_val)
                if t == 100:
                    text_parts.append("hello")
                elif t == 200:
                    text_parts.append("world")
                elif t == EOT_TOKEN_ID:
                    break
            return " ".join(text_parts) if text_parts else ""

        mock_tokenizer_inst.decode.side_effect = actual_decode_side_effect
        mock_tokenizer_inst.encode.return_value = []
        mock_get_tokenizer.return_value = mock_tokenizer_inst

        decoded_tokens_list = [100, 200, EOT_TOKEN_ID]
        mock_decoding_result = DecodingResult(
            tokens=mx.array(decoded_tokens_list),
            temperature=0.0,
            avg_logprob=-0.25,
            compression_ratio=1.2,
            no_speech_prob=0.05,
            audio_features=mx.zeros((1, self.dims.n_mels), dtype=mx.float32),
            language="en",
        )

        mock_pbar = MagicMock()
        mock_pbar.update = MagicMock()
        mock_tqdm_constructor = MagicMock()
        mock_tqdm_constructor.return_value.__enter__.return_value = mock_pbar
        mock_tqdm_constructor.return_value.__exit__ = MagicMock()
        mock_tqdm_tqdm.side_effect = mock_tqdm_constructor

        def pad_or_trim_side_effect(array, length, axis):
            return mx.zeros((length, array.shape[1]), dtype=array.dtype)

        mock_pad_or_trim.side_effect = pad_or_trim_side_effect

        dummy_audio_input = np.zeros(SAMPLE_RATE * 1, dtype=np.float32)

        real_model_for_test = Model(self.dims, dtype=mx.float32)

        # Patch this specific instance's 'decode' method
        with patch.object(
            real_model_for_test, "decode", return_value=mock_decoding_result
        ) as mock_instance_decode:
            output = real_model_for_test.generate(
                dummy_audio_input,
                language="en",
                word_timestamps=False,
                temperature=0.0,
                fp16=False,
            )

            mock_instance_decode.assert_called_once()
            args_decode_call, _ = mock_instance_decode.call_args
            self.assertEqual(
                args_decode_call[0].shape, (N_FRAMES, self.dims.n_mels)
            )  # mel_segment
            self.assertIsInstance(args_decode_call[1], DecodingOptions)
            self.assertEqual(args_decode_call[1].language, "en")
            self.assertEqual(args_decode_call[1].fp16, False)

        self.assertIsInstance(output, STTOutput)
        self.assertEqual(output.language, "en")
        expected_text_output = "hello world"
        self.assertEqual(output.text, expected_text_output)  #

        self.assertIsInstance(output.segments, list)
        self.assertEqual(len(output.segments), 1, "Should produce one segment")
        segment = output.segments[0]
        self.assertEqual(segment["text"], expected_text_output)
        self.assertEqual(segment["tokens"], decoded_tokens_list)

        self.assertEqual(segment["seek"], 0)
        self.assertAlmostEqual(segment["start"], 0.0)
        self.assertAlmostEqual(segment["end"], 1.0)
        self.assertEqual(segment["temperature"], mock_decoding_result.temperature)
        self.assertAlmostEqual(segment["avg_logprob"], mock_decoding_result.avg_logprob)
        self.assertAlmostEqual(
            segment["compression_ratio"], mock_decoding_result.compression_ratio
        )
        self.assertAlmostEqual(
            segment["no_speech_prob"], mock_decoding_result.no_speech_prob
        )

        mock_log_mel.assert_called_once_with(
            ANY, n_mels=self.dims.n_mels, padding=N_SAMPLES
        )
        np.testing.assert_array_equal(mock_log_mel.call_args[0][0], dummy_audio_input)
        mock_get_tokenizer.assert_called_once_with(
            real_model_for_test.is_multilingual,  # Reads from the instance
            num_languages=real_model_for_test.num_languages,  # Reads from the instance
            language="en",
            task="transcribe",
        )
        mock_pad_or_trim.assert_called_once()
        args_pad_call, _ = mock_pad_or_trim.call_args
        self.assertEqual(args_pad_call[0].shape, (100, self.dims.n_mels))
        self.assertEqual(args_pad_call[1], N_FRAMES)


class TestParakeetModel(unittest.TestCase):

    @patch("mlx.nn.Module.load_weights")
    @patch("mlx_audio.stt.models.parakeet.parakeet.hf_hub_download")
    @patch("mlx_audio.stt.models.parakeet.parakeet.json.load")
    @patch("mlx_audio.stt.models.parakeet.parakeet.open", new_callable=MagicMock)
    @patch("mlx.core.load")
    def test_parakeet_tdt_from_pretrained(
        self,
        mock_mlx_core_load,
        mock_parakeet_module_open,
        mock_parakeet_json_load,
        mock_hf_hub_download,
        mock_module_load_weights,
    ):
        """Test ParakeetTDT.from_pretrained method."""

        dummy_repo_id = "dummy/parakeet-tdt-model"
        dummy_config_path = "dummy_path/config.json"
        dummy_weights_path = "dummy_path/model.safetensors"

        # Configure hf_hub_download
        def hf_hub_download_side_effect(repo_id_arg, filename_arg):
            if repo_id_arg == dummy_repo_id and filename_arg == "config.json":
                return dummy_config_path
            if repo_id_arg == dummy_repo_id and filename_arg == "model.safetensors":
                return dummy_weights_path
            raise ValueError(
                f"Unexpected hf_hub_download call: {repo_id_arg}, {filename_arg}"
            )

        mock_hf_hub_download.side_effect = hf_hub_download_side_effect

        # Dummy config content
        dummy_vocabulary = [" ", "a", "b", "c"]
        dummy_config_dict = {
            "target": "nemo.collections.asr.models.rnnt_bpe_models.EncDecRNNTBPEModel",
            "model_defaults": {"tdt_durations": [0, 1, 2, 3]},
            "preprocessor": {
                "sample_rate": 16000,
                "normalize": "per_feature",
                "window_size": 0.02,
                "window_stride": 0.01,
                "window": "hann",
                "features": 80,
                "n_fft": 512,
                "dither": 1e-05,
                "pad_to": 0,
                "pad_value": 0.0,
            },
            "encoder": {
                "feat_in": 80,
                "n_layers": 17,
                "d_model": 512,
                "conv_dim": 512,
                "n_heads": 8,
                "self_attention_model": "rel_pos",
                "subsampling": "dw_striding",
                "causal_downsampling": False,
                "pos_emb_max_len": 5000,
                "ff_expansion_factor": 4,
                "subsampling_factor": 4,
                "subsampling_conv_channels": 512,
                "dropout_rate": 0.1,
                "attention_dropout_rate": 0.1,
                "conv_dropout_rate": 0.1,
                "conv_kernel_size": 31,
                "causal_depthwise_conv": False,
            },
            "decoder": {
                "blank_as_pad": True,
                "vocab_size": len(dummy_vocabulary),
                "input_dim": 512,
                "hidden_dim": 512,
                "output_dim": 1024,
                "num_layers": 1,
                "dropout_rate": 0.1,
                "prednet": {
                    "input_dim": 512,
                    "pred_hidden": 512,
                    "output_dim": 1024,
                    "pred_rnn_layers": 1,
                    "dropout_rate": 0.1,
                },
            },
            "joint": {
                "input_dim_encoder": 512,
                "input_dim_decoder": 1024,
                "num_classes": len(dummy_vocabulary) + 1,
                "joint_dropout_rate": 0.1,
                "vocabulary": dummy_vocabulary,
                "jointnet": {
                    "encoder_hidden": 512,
                    "pred_hidden": 1024,
                    "joint_hidden": 512,
                    "activation": "relu",
                },
            },
            "decoding": {
                "model_type": "tdt",
                "durations": [0, 1, 2, 3],
                "greedy": {"max_symbols": 10},
            },
        }

        # Configure mocks for config loading
        mock_file_object_for_context_manager = (
            MagicMock()
        )  # This is what __enter__ would return
        mock_parakeet_module_open.return_value.__enter__.return_value = (
            mock_file_object_for_context_manager
        )
        # If open is used not as a context manager, its direct return value is the file handle
        # json.load will be called with mock_parakeet_module_open.return_value

        mock_parakeet_json_load.return_value = dummy_config_dict

        mock_mlx_core_load.return_value = {"some.valid.path.if.needed": mx.array([0.0])}

        model = ParakeetTDT.from_pretrained(dummy_repo_id, dtype=mx.float32)

        self.assertIsInstance(model, ParakeetTDT)

        mock_hf_hub_download.assert_any_call(dummy_repo_id, "config.json")
        mock_hf_hub_download.assert_any_call(dummy_repo_id, "model.safetensors")

        self.assertEqual(model.preprocessor_config.sample_rate, 16000)
        self.assertEqual(model.preprocessor_config.features, 80)
        self.assertEqual(
            model.encoder_config.d_model, 512
        )  # d_model is correct for ConformerArgs
        self.assertEqual(model.vocabulary, dummy_vocabulary)
        self.assertEqual(model.durations, [0, 1, 2, 3])


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/tts/__init__.py
================================================




================================================
FILE: mlx_audio/tts/audio_player.html
================================================
<title>3D Orb Audio Visualization</title>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            background-color: #000;
            font-family: Arial, sans-serif;
        }
        #controls {
            position: fixed;
            top: 20px;
            left: 20px;
            z-index: 10;
            color: white;
            background-color: rgba(0, 0, 0, 0.5);
            padding: 15px;
            border-radius: 10px;
            max-width: 350px;
            max-height: 80vh;
            overflow-y: auto;
        }
        button {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 8px 16px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
        button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #stopBtn {
            background-color: #f44336;
        }
        #status {
            margin-top: 10px;
        }
        .form-group {
            margin-bottom: 15px;
        }
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        input[type="text"], select, textarea {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-sizing: border-box;
            background-color: rgba(255, 255, 255, 0.9);
        }
        textarea {
            height: 80px;
            resize: vertical;
        }
        .tab-container {
            margin-top: 15px;
        }
        .tab {
            overflow: hidden;
            border-bottom: 1px solid #ccc;
            margin-bottom: 10px;
        }
        .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 8px 16px;
            transition: 0.3s;
            color: #ddd;
        }
        .tab button:hover {
            background-color: rgba(255, 255, 255, 0.1);
        }
        .tab button.active {
            background-color: rgba(255, 255, 255, 0.2);
            color: white;
        }
        .tabcontent {
            display: none;
            padding: 10px 0;
        }
        .error {
            color: #ff6b6b;
            font-weight: bold;
            margin-top: 10px;
        }
        /* Improved slider styling */
        .slider-container {
            margin-top: 10px;
            position: relative;
        }
        .slider-labels {
            display: flex;
            justify-content: space-between;
            margin-bottom: 5px;
            font-size: 12px;
            color: #ddd;
        }
        input[type="range"] {
            width: 100%;
            margin: 0;
            background: transparent;
            -webkit-appearance: none;
        }
        input[type="range"]:focus {
            outline: none;
        }
        input[type="range"]::-webkit-slider-runnable-track {
            width: 100%;
            height: 6px;
            cursor: pointer;
            background: rgba(255, 255, 255, 0.3);
            border-radius: 3px;
        }
        input[type="range"]::-webkit-slider-thumb {
            height: 16px;
            width: 16px;
            border-radius: 50%;
            background: #4CAF50;
            cursor: pointer;
            -webkit-appearance: none;
            margin-top: -5px;
        }
        input[type="range"]::-moz-range-track {
            width: 100%;
            height: 6px;
            cursor: pointer;
            background: rgba(255, 255, 255, 0.3);
            border-radius: 3px;
        }
        input[type="range"]::-moz-range-thumb {
            height: 16px;
            width: 16px;
            border-radius: 50%;
            background: #4CAF50;
            cursor: pointer;
        }
        .speed-value-display {
            position: relative;
            text-align: center;
            font-weight: bold;
            color: white;
            margin-top: 5px;
            font-size: 14px;
        }
        /* Add tab styling if not already present */
        .tab {
            overflow: hidden;
            background-color: #333;
            border-radius: 5px 5px 0 0;
        }

        .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            color: white;
        }

        .tab button:hover {
            background-color: #555;
        }

        .tab button.active {
            background-color: #4CAF50;
        }

        .tabcontent {
            display: none;
            padding: 20px;
            border-top: none;
        }

        /* WebRTC specific styles */
        #streamStatus {
            margin-top: 10px;
            color: #4CAF50;
            font-weight: bold;
        }

        #startStreamBtn {
            background-color: #4CAF50;
        }

        #startStreamBtn.streaming {
            background-color: #f44336;
        }
    </style>
</head>
<body>

    <div id="controls">
        <h1>MLX-Audio Player</h1>

        <div class="tab">
            <button class="tablinks active" onclick="openTab(event, 'textToSpeech')">Text to Speech</button>
            <button class="tablinks" onclick="openTab(event, 'fileUpload')">File Upload</button>
            <button class="tablinks" onclick="openTab(event, 'speechToSpeech')">Speech to Speech</button>
        </div>

        <div id="textToSpeech" class="tabcontent" style="display: block;">
            <div class="form-group">
                <label for="text">Text to convert:</label>
                <textarea id="text" placeholder="Enter text here..."></textarea>
            </div>

            <div class="form-group">
                <label for="voice">Voice:</label>
                <select id="voice">
                    <option value="af_bella">AF Bella</option>
                    <option value="af_heart">AF Heart</option>
                    <option value="af_nicole">AF Nicole</option>
                    <option value="af_nova">AF Nova</option>
                    <option value="af_sarah">AF Sarah</option>
                    <option value="af_sky">AF Sky</option>
                    <option value="am_adam">AM Adam</option>
                    <option value="am_michael">AM Michael</option>
                    <option value="bf_emma">BF Emma</option>
                    <option value="bf_isabella">BF Isabella</option>
                    <option value="bm_george">BM George</option>
                    <option value="bm_lewis">BM Lewis</option>
                  </select>
            </div>

            <div class="form-group">
                <label for="model">Model:</label>
                <select id="model">
                    <option value="mlx-community/Kokoro-82M-4bit">Kokoro 82M 4bit</option>
                    <option value="mlx-community/Kokoro-82M-6bit">Kokoro 82M 6bit</option>
                    <option value="mlx-community/Kokoro-82M-8bit">Kokoro 82M 8bit</option>
                    <option value="mlx-community/Kokoro-82M-bf16">Kokoro 82M bf16</option>
                </select>
            </div>

            <div class="form-group">
                <label for="speed">Speech Speed:</label>
                <div class="slider-container">
                    <div class="slider-labels">
                        <span>Slower</span>
                        <span>Normal</span>
                        <span>Faster</span>
                    </div>
                    <input type="range" id="speed" min="0.5" max="2.0" step="0.1" value="1.0">
                    <div class="speed-value-display"><span id="speed-value">1.0</span>x</div>
                </div>
            </div>

            <button id="generateBtn">Generate Speech</button>
            <button id="openFolderBtn" style="background-color: #2196F3;">Open Output Folder</button>
            <div id="ttsError" class="error" style="display: none;"></div>
            <div id="ttsStatus" style="margin-top: 10px; max-width: 350px;"></div>
        </div>

        <div id="fileUpload" class="tabcontent">
            <input type="file" id="audioUpload" accept="audio/*">
            <div style="margin-top: 10px;">
                <button id="playBtn" disabled>Play</button>
                <button id="stopBtn" disabled>Stop</button>
            </div>
            <div id="status">Upload an audio file to begin visualization</div>
        </div>

        <!-- Speech to Speech Tab (New) -->
        <div id="speechToSpeech" class="tabcontent">
            <h3>Real-time Speech Conversion</h3>

            <div>
                <label for="s2sVoice">Voice:</label>
                <select id="s2sVoice" class="form-control">
                    <option value="af_bella">AF Bella</option>
                    <option value="af_heart">AF Heart</option>
                    <option value="af_nicole">AF Nicole</option>
                    <option value="af_nova">AF Nova</option>
                    <option value="af_sarah">AF Sarah</option>
                    <option value="af_sky">AF Sky</option>
                    <option value="am_adam">AM Adam</option>
                    <option value="am_michael">AM Michael</option>
                    <option value="bf_emma">BF Emma</option>
                    <option value="bf_isabella">BF Isabella</option>
                    <option value="bm_george">BM George</option>
                    <option value="bm_lewis">BM Lewis</option>
                </select>
            </div>

            <div>
                <label for="s2sModel">Model:</label>
                <select id="s2sModel" class="form-control">
                    <option value="kokoro_82m_4bit">Kokoro 82M 4bit</option>
                </select>
            </div>

            <div>
                <label for="s2sSpeed">Speech Speed:</label>
                <div class="speed-control">
                    <span>Slower</span>
                    <input type="range" id="s2sSpeed" min="0.5" max="2.0" step="0.1" value="1.0">
                    <span>Faster</span>
                </div>
                <div id="s2sSpeedValue">1.0x</div>
            </div>

            <button id="startStreamBtn">Start Stream</button>
            <div id="streamStatus"></div>
        </div>
        <audio id="audioElement" autoplay style="display: none;"></audio>
    </div>

    <!-- Load Three.js from CDN -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>

    <script>
        // DOM elements
        const audioUpload = document.getElementById('audioUpload');
        const playBtn = document.getElementById('playBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusElement = document.getElementById('status');

        // TTS elements
        const textInput = document.getElementById('text');
        const voiceSelect = document.getElementById('voice');
        const modelSelect = document.getElementById('model');
        const speedInput = document.getElementById('speed');
        const speedValue = document.getElementById('speed-value');
        const generateBtn = document.getElementById('generateBtn');
        const openFolderBtn = document.getElementById('openFolderBtn');
        const ttsErrorElement = document.getElementById('ttsError');
        const ttsStatusElement = document.getElementById('ttsStatus');

        // Audio variables
        let audioContext;
        let analyser;
        let dataArray;
        let audioElement = document.getElementById('audioElement');
        let audioSource;

        // Three.js setup
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setClearColor(0x000000);
        document.body.appendChild(renderer.domElement);

        // Add orbit controls
        const controls = new THREE.OrbitControls(camera, renderer.domElement);
        controls.enableDamping = true;
        controls.dampingFactor = 0.05;

        // Position camera
        camera.position.set(0, 0, 100);
        camera.lookAt(0, 0, 0);

        // Add lights
        const ambientLight = new THREE.AmbientLight(0x404040);
        scene.add(ambientLight);

        const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
        directionalLight.position.set(1, 1, 1);
        scene.add(directionalLight);

        const pointLight = new THREE.PointLight(0xffffff, 1, 100);
        pointLight.position.set(0, 0, 0);
        scene.add(pointLight);

        // Create orb mesh
        const sphereGeometry = new THREE.IcosahedronGeometry(30, 4); // Higher detail icosahedron
        const sphereMaterial = new THREE.MeshPhongMaterial({
            color: 0x0088ff,
            emissive: 0x222222,
            shininess: 30,
            wireframe: false,
            flatShading: true
        });
        const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
        scene.add(sphere);

        // Store original vertex positions
        const originalVertices = [];
        for (let i = 0; i < sphereGeometry.attributes.position.count; i++) {
            originalVertices.push(
                new THREE.Vector3(
                    sphereGeometry.attributes.position.getX(i),
                    sphereGeometry.attributes.position.getY(i),
                    sphereGeometry.attributes.position.getZ(i)
                )
            );
        }

        // Create a glow effect
        const glowGeometry = new THREE.SphereGeometry(32, 32, 32);
        const glowMaterial = new THREE.MeshBasicMaterial({
            color: 0x0088ff,
            transparent: true,
            opacity: 0.15,
            side: THREE.BackSide
        });
        const glowMesh = new THREE.Mesh(glowGeometry, glowMaterial);
        scene.add(glowMesh);

        // Define rotation speed variables
        let rotationSpeedY = 0.002;
        let rotationSpeedX = 0.001;
        let isGenerating = false;

        // Tab functionality
        function openTab(evt, tabName) {
            // Hide all tabcontent
            const tabcontent = document.getElementsByClassName("tabcontent");
            for (let i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
            }

            // Remove active class from all tablinks
            const tablinks = document.getElementsByClassName("tablinks");
            for (let i = 0; i < tablinks.length; i++) {
                tablinks[i].className = tablinks[i].className.replace(" active", "");
            }

            // Show the current tab and add active class to the button
            document.getElementById(tabName).style.display = "block";
            evt.currentTarget.className += " active";
            if (tabName != "speechToSpeech") {
                closeWebRTCStream();
            }
        }

        // Speed slider update
        speedInput.addEventListener('input', function() {
            speedValue.textContent = this.value;
        });

        // Generate speech button handler
        generateBtn.addEventListener('click', function() {
            const text = textInput.value;
            const voice = voiceSelect.value;
            const model = modelSelect.value;
            const speed = speedInput.value;

            if (!text.trim()) {
                showTtsError('Please enter some text');
                return;
            }

            // Hide previous error
            ttsErrorElement.style.display = 'none';
            ttsStatusElement.textContent = 'Generating speech...';

            // Increase rotation speed to indicate processing
            isGenerating = true;
            rotationSpeedY = 0.01;
            rotationSpeedX = 0.005;

            // Create form data
            const formData = new FormData();
            formData.append('text', text);
            formData.append('voice', voice);
            formData.append('model', model);
            formData.append('speed', speed);

            // Send request to server
            fetch('/tts', {
                method: 'POST',
                body: formData
            })
            .then(response => {
                if (!response.ok) {
                    return response.json().then(data => {
                        throw new Error(data.error || 'Failed to generate speech');
                    });
                }
                return response.json();
            })
            .then(data => {
                ttsStatusElement.textContent = 'Speech generated successfully!';

                // Reset rotation speed
                isGenerating = false;
                rotationSpeedY = 0.002;
                rotationSpeedX = 0.001;

                // Clean up previous audio resources
                if (audioElement) {
                    audioElement.pause();
                    audioElement.removeAttribute('src');
                }

                if (audioSource) {
                    audioSource.disconnect();
                    audioSource = null; // Reset the variable explicitly
                }
                // Recreate audioElement to avoid potential issues with re-using the same element
                // This helps ensure createMediaElementSource works consistently.
                const oldAudioElement = document.getElementById('audioElement');
                if (oldAudioElement) {
                    oldAudioElement.remove();
                }
                audioElement = document.createElement('audio');
                audioElement.id = 'audioElement';
                audioElement.autoplay = false; // Don't autoplay immediately
                audioElement.style.display = 'none';
                document.getElementById('controls').appendChild(audioElement);

                // Set audio source with absolute path
                audioElement.src = `/audio/${data.filename}`;
                audioElement.loop = false;

                // Enable play button
                playBtn.disabled = false;
                stopBtn.disabled = true;

                // Add ended event listener
                audioElement.addEventListener('ended', function() {
                    statusElement.textContent = "Audio finished playing.";
                    playBtn.disabled = false;
                    stopBtn.disabled = true;
                    resetSphere();
                });

                // Auto-play the generated audio
                playAudio(); // playAudio will now create a fresh connection
            })
            .catch(error => {
                showTtsError(error.message);

                // Reset rotation speed on error too
                isGenerating = false;
                rotationSpeedY = 0.002;
                rotationSpeedX = 0.001;
            });
        });

        // Open output folder button handler
        openFolderBtn.addEventListener('click', function() {
            fetch('/open_output_folder', {
                method: 'POST'
            })
            .then(response => {
                if (!response.ok) {
                    return response.json().then(data => {
                        throw new Error(data.error || 'Failed to open output folder');
                    });
                }
                return response.json();
            })
            .then(data => {
                ttsStatusElement.textContent = `Opened output folder: ${data.path}`;
            })
            .catch(error => {
                showTtsError(error.message);
            });
        });

        function showTtsError(message) {
            ttsErrorElement.textContent = message;
            ttsErrorElement.style.display = 'block';
        }

        // Function to play audio (reused for both upload and TTS)
        function playAudio() {
            if (!audioElement || !audioElement.src) {
                statusElement.textContent = "No audio available to play.";
                return;
            }

            statusElement.textContent = "Playing audio...";

            // Initialize audio context if needed
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }

            // Create analyser if needed
            if (!analyser) {
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
            }

            // Connect audio element to analyser if not already connected
            if (!audioSource) {
                try {
                    audioSource = audioContext.createMediaElementSource(audioElement);
                    audioSource.connect(analyser);
                    analyser.connect(audioContext.destination);
                } catch (error) {
                    console.error("Error connecting audio source:", error);
                    statusElement.textContent = "Error setting up audio visualization. Try refreshing the page.";

                    // Still try to play the audio even if visualization fails
                    audioElement.play().catch(playError => {
                        statusElement.textContent = "Error playing audio: " + playError.message;
                    });
                    return;
                }
            }

            // Play audio
            audioElement.play().then(() => {
                playBtn.disabled = true;
                stopBtn.disabled = false;
            }).catch(error => {
                statusElement.textContent = "Error playing audio: " + error.message;
            });
        }

        // Handle audio upload
        audioUpload.addEventListener('change', function(e) {
            const file = e.target.files[0];
            if (!file) return;

            statusElement.textContent = "Audio file loaded. Press Play to start.";

            // Clean up previous audio resources
            if (audioElement) {
                audioElement.pause();
                audioElement.removeAttribute('src');
            }

            if (audioSource) {
                audioSource.disconnect();
                audioSource = null; // Reset the variable explicitly
            }

            // Recreate audioElement to avoid potential issues with re-using the same element
            // This helps ensure createMediaElementSource works consistently.
            const oldAudioElement = document.getElementById('audioElement');
            if (oldAudioElement) {
                oldAudioElement.remove();
            }
            audioElement = document.createElement('audio');
            audioElement.id = 'audioElement';
            audioElement.autoplay = false; // Don't autoplay immediately
            audioElement.style.display = 'none';
            document.getElementById('controls').appendChild(audioElement);

            audioElement.src = URL.createObjectURL(file);
            audioElement.loop = false;

            // Enable play button
            playBtn.disabled = false;
            stopBtn.disabled = true;

            // Add ended event listener
            audioElement.addEventListener('ended', function() {
                statusElement.textContent = "Audio finished playing.";
                playBtn.disabled = false;
                stopBtn.disabled = true;
                resetSphere();
            });
        });

        // Play button handler
        playBtn.addEventListener('click', function() {
            playAudio();
        });

        // Stop button handler
        stopBtn.addEventListener('click', function() {
            if (audioElement) {
                audioElement.pause();
                audioElement.currentTime = 0;
                statusElement.textContent = "Audio stopped. Press Play to restart.";
                playBtn.disabled = false;
                stopBtn.disabled = true;

                // Reset sphere to original state
                resetSphere();
            }
        });

        // Reset sphere to original state
        function resetSphere() {
            const positionAttribute = sphereGeometry.attributes.position;

            for (let i = 0; i < positionAttribute.count; i++) {
                const originalVertex = originalVertices[i];
                positionAttribute.setXYZ(i, originalVertex.x, originalVertex.y, originalVertex.z);
            }

            positionAttribute.needsUpdate = true;
            sphereGeometry.computeVertexNormals();

            // Reset colors
            sphere.material.color.set(0x0088ff);
            sphere.material.emissive.set(0x222222);
            glowMesh.material.color.set(0x0088ff);
        }

        // Animation loop
        function  animate() {
            requestAnimationFrame(animate);

            // Update controls
            controls.update();

            // Get current time for pulsating effect
            const time = performance.now() * 0.001; // Convert to seconds

            // Rotate sphere with current speed
            sphere.rotation.y += rotationSpeedY;
            sphere.rotation.x += rotationSpeedX;
            glowMesh.rotation.copy(sphere.rotation);

            // Update visualization if audio is playing
            if (analyser && dataArray && !audioElement.paused) {
                analyser.getByteFrequencyData(dataArray);

                // Calculate average frequency values for different ranges
                const bassAvg = getAverageFrequency(dataArray, 0, 5);
                const midAvg = getAverageFrequency(dataArray, 6, 20);
                const trebleAvg = getAverageFrequency(dataArray, 21, 40);

                // Calculate base pulsating factor (same as when no audio is playing)
                const pulseFactor = Math.sin(time * 1.5) * 0.03 + 1; // Subtle pulsation (Â±3%)

                // Update sphere vertices based on frequency data
                const positionAttribute = sphereGeometry.attributes.position;

                for (let i = 0; i < positionAttribute.count; i++) {
                    const originalVertex = originalVertices[i];

                    // Calculate normalized distance from center (0-1)
                    const vertexLength = originalVertex.length();

                    // Get frequency value based on vertex position
                    let frequencyFactor;

                    // Use different frequency ranges based on vertex position
                    if (Math.abs(originalVertex.y) > vertexLength * 0.7) {
                        // Top/bottom vertices - use treble
                        frequencyFactor = trebleAvg / 255;
                    } else if (Math.abs(originalVertex.x) > vertexLength * 0.7) {
                        // Left/right vertices - use mids
                        frequencyFactor = midAvg / 255;
                    } else {
                        // Other vertices - use bass
                        frequencyFactor = bassAvg / 255;
                    }

                    // Scale vertex based on both pulsation and frequency
                    // First apply the pulsating effect, then add audio reactivity
                    const scaleFactor = pulseFactor * (1 + frequencyFactor * 0.5);

                    positionAttribute.setXYZ(
                        i,
                        originalVertex.x * scaleFactor,
                        originalVertex.y * scaleFactor,
                        originalVertex.z * scaleFactor
                    );
                }

                positionAttribute.needsUpdate = true;
                sphereGeometry.computeVertexNormals();

                // Update colors based on frequency
                const hue = (bassAvg / 255) * 0.3;
                const saturation = 0.8;
                const lightness = 0.4 + (midAvg / 255) * 0.2;

                sphere.material.color.setHSL(hue, saturation, lightness);
                sphere.material.emissive.setHSL(hue, saturation, lightness * 0.5);

                // Update glow with both pulsation and audio reactivity
                glowMesh.material.color.setHSL(hue, saturation, lightness);
                const glowPulseFactor = 1 + Math.sin(time * 1.2) * 0.04;
                glowMesh.scale.set(
                    glowPulseFactor * (1 + (bassAvg / 255) * 0.1),
                    glowPulseFactor * (1 + (bassAvg / 255) * 0.1),
                    glowPulseFactor * (1 + (bassAvg / 255) * 0.1)
                );

                // Update point light with both pulsation and audio reactivity
                const lightPulseFactor = 0.5 + Math.sin(time * 1.8) * 0.2;
                pointLight.intensity = lightPulseFactor + (bassAvg / 255) * 1.5;
                pointLight.color.setHSL(hue, saturation, lightness);
            } else {
                // Apply subtle pulsating effect when no audio is playing
                const pulseFactor = Math.sin(time * 1.5) * 0.03 + 1; // Subtle pulsation (Â±3%)

                // Update sphere vertices for pulsating effect
                const positionAttribute = sphereGeometry.attributes.position;

                for (let i = 0; i < positionAttribute.count; i++) {
                    const originalVertex = originalVertices[i];

                    positionAttribute.setXYZ(
                        i,
                        originalVertex.x * pulseFactor,
                        originalVertex.y * pulseFactor,
                        originalVertex.z * pulseFactor
                    );
                }

                positionAttribute.needsUpdate = true;
                sphereGeometry.computeVertexNormals();

                // Subtle color pulsation
                const hue = 0.6; // Blue hue
                const saturation = 0.8;
                const lightness = 0.4 + Math.sin(time * 2) * 0.05; // Subtle brightness pulsation

                sphere.material.color.setHSL(hue, saturation, lightness);
                sphere.material.emissive.setHSL(hue, saturation, lightness * 0.5);

                // Update glow with subtle pulsation
                glowMesh.material.color.setHSL(hue, saturation, lightness);
                glowMesh.scale.set(
                    1 + Math.sin(time * 1.2) * 0.04, // Slightly different frequency for interesting effect
                    1 + Math.sin(time * 1.2) * 0.04,
                    1 + Math.sin(time * 1.2) * 0.04
                );

                // Subtle point light pulsation
                pointLight.intensity = 0.5 + Math.sin(time * 1.8) * 0.2;
                pointLight.color.setHSL(hue, saturation, lightness);
            }

            renderer.render(scene, camera);
        }

        // Helper function to get average frequency in a range
        function getAverageFrequency(dataArray, startIndex, endIndex) {
            let sum = 0;
            for (let i = startIndex; i <= endIndex; i++) {
                sum += dataArray[i];
            }
            return sum / (endIndex - startIndex + 1);
        }

        // Handle window resize
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // Start animation loop
        animate();

        // WebRTC variables
        let webrtcConnection = null;
        let webrtcId = null;
        let dataChannel = null;
        let analyzer = null;

        // Speech to Speech functionality
        document.getElementById('s2sSpeed').addEventListener('input', function() {
            document.getElementById('s2sSpeedValue').textContent = this.value + 'x';
        });

        document.getElementById('startStreamBtn').addEventListener('click', function() {
            if (!webrtcConnection) {
                startWebRTCStream();
                this.textContent = "Connecting...";
                this.disabled = true;
            } else {
                closeWebRTCStream();
                this.textContent = "Start Stream";
                this.classList.remove('streaming');
            }
        });

        async function startWebRTCStream() {
            try {
                const streamStatus = document.getElementById('streamStatus');
                streamStatus.textContent = "Initializing connection...";

                // Get user media for microphone
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

                // Create RTCPeerConnection
                webrtcConnection = new RTCPeerConnection({});

                // Add local stream tracks to connection
                stream.getAudioTracks().forEach(track => {
                    webrtcConnection.addTrack(track, stream);
                });

                // Set up data channel
                dataChannel = webrtcConnection.createDataChannel("text");
                dataChannel.onopen = handleDataChannelOpen;
                dataChannel.onmessage = handleDataChannelMessage;
                dataChannel.onclose = () => {
                    console.log("Data channel closed");
                };

                // Handle ICE candidate events
                webrtcConnection.onicecandidate = event => {
                    if (event.candidate) {
                        // We'll handle ICE candidates in the offer
                    }
                };

                // Handle track events for receiving audio
                webrtcConnection.ontrack = event => {
                    const remoteStream = new MediaStream();
                    event.streams[0].getTracks().forEach(track => {
                        remoteStream.addTrack(track);
                    });

                    if (!audioContext) {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }

                    if (audioSource) {
                        audioSource.disconnect();
                    }

                    audioSource = audioContext.createMediaStreamSource(remoteStream);

                    if (!analyser) {
                        analyser = audioContext.createAnalyser();
                        analyser.fftSize = 256;
                        dataArray = new Uint8Array(analyser.frequencyBinCount);
                    }

                    audioSource.connect(analyser);
                    analyser.connect(audioContext.destination);

                    audioElement.srcObject = remoteStream;
                    audioElement.play();
                };

                // Create offer
                const offer = await webrtcConnection.createOffer({
                    offerToReceiveAudio: true,
                    offerToReceiveVideo: false
                });

                await webrtcConnection.setLocalDescription(offer);

                // Wait for ICE gathering to complete
                await new Promise(resolve => {
                    if (webrtcConnection.iceGatheringState === 'complete') {
                        resolve();
                    } else {
                        webrtcConnection.addEventListener('icegatheringstatechange', () => {
                            if (webrtcConnection.iceGatheringState === 'complete') {
                                resolve();
                            }
                        });
                    }
                });

                // Send offer to server
                const response = await fetch('/webrtc/offer', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        sdp: webrtcConnection.localDescription.sdp,
                        type: webrtcConnection.localDescription.type,
                        webrtc_id: generateWebRTCId(),
                    }),
                });

                const responseData = await response.json();

                if (responseData.status === 'failed') {
                    throw new Error(responseData.meta?.error || 'Connection failed');
                }

                // Set remote description
                await webrtcConnection.setRemoteDescription(new RTCSessionDescription({
                    type: 'answer',
                    sdp: responseData.sdp,
                }));

                streamStatus.textContent = "Connecting...";

            } catch (error) {
                console.error('Error starting WebRTC stream:', error);
                document.getElementById('streamStatus').textContent = "Error: " + error.message;
                document.getElementById('startStreamBtn').disabled = false;
                document.getElementById('startStreamBtn').textContent = "Start Stream";
                closeWebRTCStream();
            }
        }

        function handleDataChannelOpen() {
            console.log("Data channel opened!");
            const streamStatus = document.getElementById('streamStatus');
            streamStatus.textContent = "Connected! Speak into your microphone.";

            const startStreamBtn = document.getElementById('startStreamBtn');
            startStreamBtn.disabled = false;
            startStreamBtn.textContent = "Close Stream";
            startStreamBtn.classList.add('streaming');

            // Send initial configuration
            sendInputConfiguration();
        }

        function handleDataChannelMessage(event) {
            try {
                const message = JSON.parse(event.data);
                console.log("Received message:", message);

                if (message.type === 'send_input') {
                    // Server is requesting updated input parameters
                    sendInputConfiguration();
                } else if (message.type === 'error') {
                    document.getElementById('streamStatus').textContent = "Error: " + message.data;
                } else if (message.type === 'log') {
                    // Handle different log types
                    if (message.data === 'pause_detected') {
                        document.getElementById('streamStatus').textContent = "Pause detected, processing...";
                    } else if (message.data === 'response_starting') {
                        document.getElementById('streamStatus').textContent = "Generating response...";
                    } else if (message.data === 'started_talking') {
                        document.getElementById('streamStatus').textContent = "Speaking...";
                    }
                }
            } catch (e) {
                console.error("Error parsing message:", e);
            }
        }

        function sendInputConfiguration() {
            if (!dataChannel || dataChannel.readyState !== 'open') return;

            // Get current configuration values
            const voice = document.getElementById('s2sVoice').value;
            const model = document.getElementById('s2sModel').value;
            const speed = parseFloat(document.getElementById('s2sSpeed').value);

            // Send POST request to the specified endpoint
            fetch('/speech_to_speech_input', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    voice: voice,
                    model: model,
                    speed: speed,
                    webrtc_id: webrtcId
                })
            }).catch(error => {
                console.error('Error sending configuration:', error);
            });
        }

        function closeWebRTCStream() {
            if (webrtcConnection) {
                webrtcConnection.close();
                webrtcConnection = null;
            }

            if (dataChannel) {
                dataChannel.close();
                dataChannel = null;
            }
            dataChannel = null;
            analyzer = null;

            document.getElementById('streamStatus').textContent = "";
            webrtcId = null;
            audioContext = null;
            analyser = null;
            audioSource = null;
        }

        function generateWebRTCId() {
            webrtcId = 'webrtc-' + Date.now() + '-' + Math.floor(Math.random() * 1000000);
            return webrtcId;
        }

        // Initialize tabs
        const defaultTab = document.getElementsByClassName("tablinks")[0];
        defaultTab.click();
    </script>
</body>
</html>


================================================
FILE: mlx_audio/tts/audio_player.py
================================================
import time
from collections import deque
from threading import Event, Lock

import numpy as np
import sounddevice as sd


class AudioPlayer:
    min_buffer_seconds = 1.5  # with respect to real-time, not the sample rate
    measure_window = 0.25
    ema_alpha = 0.25

    def __init__(self, sample_rate=24_000, buffer_size=2048):
        self.sample_rate = sample_rate
        self.buffer_size = buffer_size

        self.audio_buffer = deque()
        self.buffer_lock = Lock()
        self.stream: sd.OutputStream | None = None
        self.playing = False
        self.drain_event = Event()

        self.window_sample_count = 0
        self.window_start = time.perf_counter()
        self.arrival_rate = sample_rate  # assume real-time to start

    def callback(self, outdata, frames, time, status):
        outdata.fill(0)  # initialize the frame with silence
        filled = 0

        with self.buffer_lock:
            while filled < frames and self.audio_buffer:
                buf = self.audio_buffer[0]
                to_copy = min(frames - filled, len(buf))
                outdata[filled : filled + to_copy, 0] = buf[:to_copy]
                filled += to_copy

                if to_copy == len(buf):
                    self.audio_buffer.popleft()
                else:
                    self.audio_buffer[0] = buf[to_copy:]

            if not self.audio_buffer and filled < frames:
                self.drain_event.set()
                self.playing = False
                raise sd.CallbackStop()

    def start_stream(self):
        print("Starting audio stream...")
        self.stream = sd.OutputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=self.callback,
            blocksize=self.buffer_size,
        )
        self.stream.start()
        self.playing = True
        self.drain_event.clear()

    def stop_stream(self):
        try:
            if self.stream:
                self.stream.stop()
                self.stream.close()
        finally:
            self.stream = None
            self.playing = False

    def buffered_samples(self) -> int:
        return sum(map(len, self.audio_buffer))

    def queue_audio(self, samples):
        if not len(samples):
            return

        now = time.perf_counter()

        # arrival-rate statistics
        self.window_sample_count += len(samples)
        if now - self.window_start >= self.measure_window:
            inst_rate = self.window_sample_count / (now - self.window_start)
            self.arrival_rate = (
                inst_rate
                if self.arrival_rate is None
                else self.ema_alpha * inst_rate
                + (1 - self.ema_alpha) * self.arrival_rate
            )
            self.window_sample_count = 0
            self.window_start = now

        with self.buffer_lock:
            self.audio_buffer.append(np.asarray(samples))

        # start playback only when we have enough buffered audio
        needed = int(self.arrival_rate * self.min_buffer_seconds)
        if not self.playing and self.buffered_samples() >= needed:
            self.start_stream()

    def wait_for_drain(self):
        return self.drain_event.wait()

    def stop(self):
        if self.playing:
            self.wait_for_drain()
            sd.sleep(100)

            self.stop_stream()
            self.playing = False

    def flush(self):
        """Discard everything and stop playback immediately."""
        if not self.playing:
            return

        with self.buffer_lock:
            self.audio_buffer.clear()
        self.stop_stream()
        self.playing = False
        self.drain_event.set()



================================================
FILE: mlx_audio/tts/convert.py
================================================
# Copyright Â© 2023-2024 Prince Canuma

import argparse

from .utils import convert

QUANT_RECIPES = ["mixed_2_6", "mixed_3_4", "mixed_3_6", "mixed_4_6"]


def configure_parser() -> argparse.ArgumentParser:
    """
    Configures and returns the argument parser for the script.

    Returns:
        argparse.ArgumentParser: Configured argument parser.
    """
    parser = argparse.ArgumentParser(
        description="Convert Hugging Face model to MLX format"
    )

    parser.add_argument("--hf-path", type=str, help="Path to the Hugging Face model.")
    parser.add_argument(
        "--mlx-path", type=str, default="mlx_model", help="Path to save the MLX model."
    )
    parser.add_argument(
        "-q", "--quantize", help="Generate a quantized model.", action="store_true"
    )
    parser.add_argument(
        "--q-group-size", help="Group size for quantization.", type=int, default=64
    )
    parser.add_argument(
        "--q-bits", help="Bits per weight for quantization.", type=int, default=4
    )
    parser.add_argument(
        "--quant-predicate",
        help=f"Mixed-bit quantization recipe.",
        choices=QUANT_RECIPES,
        type=str,
        required=False,
    )
    parser.add_argument(
        "--dtype",
        help="Type to save the parameters, ignored if -q is given.",
        type=str,
        choices=["float16", "bfloat16", "float32"],
        default="float16",
    )
    parser.add_argument(
        "--upload-repo",
        help="The Hugging Face repo to upload the model to.",
        type=str,
        default=None,
    )
    parser.add_argument(
        "-d",
        "--dequantize",
        help="Dequantize a quantized model.",
        action="store_true",
        default=False,
    )
    return parser


def main():
    parser = configure_parser()
    args = parser.parse_args()
    convert(**vars(args))


if __name__ == "__main__":
    main()



================================================
FILE: mlx_audio/tts/generate.py
================================================
import argparse
import os
import random
import sys
from typing import Optional, Tuple

import mlx.core as mx
import numpy as np
import soundfile as sf
from numpy.lib.stride_tricks import sliding_window_view
from scipy.signal import resample

from .audio_player import AudioPlayer
from .utils import load_model


def load_audio(
    audio_path: str,
    sample_rate: int = 24000,
    length: int = None,
    volume_normalize: bool = False,
    segment_duration: int = None,
) -> mx.array:
    samples, orig_sample_rate = sf.read(audio_path)
    shape = samples.shape

    # Collapse multi channel as mono
    if len(shape) > 1:
        samples = samples.sum(axis=1)
        # Divide summed samples by channel count.
        samples = samples / shape[1]
    if sample_rate != orig_sample_rate:
        print(f"Resampling from {orig_sample_rate} to {sample_rate}")
        duration = samples.shape[0] / orig_sample_rate
        num_samples = int(duration * sample_rate)
        samples = resample(samples, num_samples)

    if segment_duration is not None:
        seg_length = int(sample_rate * segment_duration)
        samples = random_select_audio_segment(samples, seg_length)

    # Audio volume normalize
    if volume_normalize:
        samples = audio_volume_normalize(samples)

    if length is not None:
        assert abs(samples.shape[0] - length) < 1000
        if samples.shape[0] > length:
            samples = samples[:length]
        else:
            samples = np.pad(samples, (0, int(length - samples.shape[0])))

    audio = mx.array(samples, dtype=mx.float32)

    return audio


def audio_volume_normalize(audio: np.ndarray, coeff: float = 0.2) -> np.ndarray:
    """
    Normalize the volume of an audio signal.

    Parameters:
        audio (numpy array): Input audio signal array.
        coeff (float): Target coefficient for normalization, default is 0.2.

    Returns:
        numpy array: The volume-normalized audio signal.
    """
    # Sort the absolute values of the audio signal
    temp = np.sort(np.abs(audio))

    # If the maximum value is less than 0.1, scale the array to have a maximum of 0.1
    if temp[-1] < 0.1:
        scaling_factor = max(
            temp[-1], 1e-3
        )  # Prevent division by zero with a small constant
        audio = audio / scaling_factor * 0.1

    # Filter out values less than 0.01 from temp
    temp = temp[temp > 0.01]
    L = temp.shape[0]  # Length of the filtered array

    # If there are fewer than or equal to 10 significant values, return the audio without further processing
    if L <= 10:
        return audio

    # Compute the average of the top 10% to 1% of values in temp
    volume = np.mean(temp[int(0.9 * L) : int(0.99 * L)])

    # Normalize the audio to the target coefficient level, clamping the scale factor between 0.1 and 10
    audio = audio * np.clip(coeff / volume, a_min=0.1, a_max=10)

    # Ensure the maximum absolute value in the audio does not exceed 1
    max_value = np.max(np.abs(audio))
    if max_value > 1:
        audio = audio / max_value

    return audio


def random_select_audio_segment(audio: np.ndarray, length: int) -> np.ndarray:
    """get an audio segment given the length

    Args:
        audio (np.ndarray):
        length (int): audio length = sampling_rate * duration
    """
    if audio.shape[0] < length:
        audio = np.pad(audio, (0, int(length - audio.shape[0])))
    start_index = random.randint(0, audio.shape[0] - length)
    end_index = int(start_index + length)

    return audio[start_index:end_index]


def detect_speech_boundaries(
    wav: np.ndarray,
    sample_rate: int,
    window_duration: float = 0.1,
    energy_threshold: float = 0.01,
    margin_factor: int = 2,
) -> Tuple[int, int]:
    """Detect the start and end points of speech in an audio signal using RMS energy.

    Args:
        wav: Input audio signal array with values in [-1, 1]
        sample_rate: Audio sample rate in Hz
        window_duration: Duration of detection window in seconds
        energy_threshold: RMS energy threshold for speech detection
        margin_factor: Factor to determine extra margin around detected boundaries

    Returns:
        tuple: (start_index, end_index) of speech segment

    Raises:
        ValueError: If the audio contains only silence
    """
    window_size = int(window_duration * sample_rate)
    margin = margin_factor * window_size
    step_size = window_size // 10

    # Create sliding windows using stride tricks to avoid loops
    windows = sliding_window_view(wav, window_size)[::step_size]

    # Calculate RMS energy for each window
    energy = np.sqrt(np.mean(windows**2, axis=1))
    speech_mask = energy >= energy_threshold

    if not np.any(speech_mask):
        raise ValueError("No speech detected in audio (only silence)")

    start = max(0, np.argmax(speech_mask) * step_size - margin)
    end = min(
        len(wav),
        (len(speech_mask) - 1 - np.argmax(speech_mask[::-1])) * step_size + margin,
    )

    return start, end


def remove_silence_on_both_ends(
    wav: np.ndarray,
    sample_rate: int,
    window_duration: float = 0.1,
    volume_threshold: float = 0.01,
) -> np.ndarray:
    """Remove silence from both ends of an audio signal.

    Args:
        wav: Input audio signal array
        sample_rate: Audio sample rate in Hz
        window_duration: Duration of detection window in seconds
        volume_threshold: Amplitude threshold for silence detection

    Returns:
        np.ndarray: Audio signal with silence removed from both ends

    Raises:
        ValueError: If the audio contains only silence
    """
    start, end = detect_speech_boundaries(
        wav, sample_rate, window_duration, volume_threshold
    )
    return wav[start:end]


def hertz_to_mel(pitch: float) -> float:
    """
    Converts a frequency from the Hertz scale to the Mel scale.

    Parameters:
    - pitch: float or ndarray
        Frequency in Hertz.

    Returns:
    - mel: float or ndarray
        Frequency in Mel scale.
    """
    mel = 2595 * np.log10(1 + pitch / 700)
    return mel


def generate_audio(
    text: str,
    model_path: str = "prince-canuma/Kokoro-82M",
    max_tokens: int = 1200,
    voice: str = "af_heart",
    speed: float = 1.0,
    lang_code: str = "a",
    ref_audio: Optional[str] = None,
    ref_text: Optional[str] = None,
    stt_model: str = "mlx-community/whisper-large-v3-turbo",
    file_prefix: str = "audio",
    audio_format: str = "wav",
    join_audio: bool = False,
    play: bool = False,
    verbose: bool = True,
    temperature: float = 0.7,
    stream: bool = False,
    streaming_interval: float = 2.0,
    **kwargs,
) -> None:
    """
    Generates audio from text using a specified TTS model.

    Parameters:
    - text (str): The input text to be converted to speech.
    - model (str): The TTS model to use.
    - voice (str): The voice style to use.
    - temperature (float): The temperature for the model.
    - speed (float): Playback speed multiplier.
    - lang_code (str): The language code.
    - ref_audio (mx.array): Reference audio you would like to clone the voice from.
    - ref_text (str): Caption for reference audio.
    - stt_model (str): A mlx whisper model to use to transcribe.
    - file_prefix (str): The output file path without extension.
    - audio_format (str): Output audio format (e.g., "wav", "flac").
    - join_audio (bool): Whether to join multiple audio files into one.
    - play (bool): Whether to play the generated audio.
    - verbose (bool): Whether to print status messages.
    Returns:
    - None: The function writes the generated audio to a file.
    """
    try:
        play = play or stream

        # Load model
        model = load_model(model_path=model_path)

        # Load reference audio for voice matching if specified
        if ref_audio:
            if not os.path.exists(ref_audio):
                raise FileNotFoundError(f"Reference audio file not found: {ref_audio}")

            normalize = False
            if hasattr(model, "model_type") and model.model_type() == "spark":
                normalize = True

            ref_audio = load_audio(
                ref_audio, sample_rate=model.sample_rate, volume_normalize=normalize
            )
            if not ref_text:
                print("Ref_text not found. Transcribing ref_audio...")
                from mlx_audio.stt.models.whisper import Model as Whisper

                stt_model = Whisper.from_pretrained(path_or_hf_repo=stt_model)
                ref_text = stt_model.generate(ref_audio).text
                print("Ref_text", ref_text)

                # clear memory
                del stt_model
                mx.clear_cache()

        # Load AudioPlayer
        player = AudioPlayer(sample_rate=model.sample_rate) if play else None

        print(
            f"\n\033[94mModel:\033[0m {model_path}\n"
            f"\033[94mText:\033[0m {text}\n"
            f"\033[94mVoice:\033[0m {voice}\n"
            f"\033[94mSpeed:\033[0m {speed}x\n"
            f"\033[94mLanguage:\033[0m {lang_code}"
        )

        results = model.generate(
            text=text,
            voice=voice,
            speed=speed,
            lang_code=lang_code,
            ref_audio=ref_audio,
            ref_text=ref_text,
            temperature=temperature,
            max_tokens=max_tokens,
            verbose=verbose,
            stream=stream,
            streaming_interval=streaming_interval,
            **kwargs,
        )

        audio_list = []
        file_name = f"{file_prefix}.{audio_format}"
        for i, result in enumerate(results):
            if play:
                player.queue_audio(result.audio)

            if join_audio:
                audio_list.append(result.audio)
            elif not stream:
                file_name = f"{file_prefix}_{i:03d}.{audio_format}"
                sf.write(file_name, result.audio, result.sample_rate)
                print(f"âœ… Audio successfully generated and saving as: {file_name}")

            if verbose:

                print("==========")
                print(f"Duration:              {result.audio_duration}")
                print(
                    f"Samples/sec:           {result.audio_samples['samples-per-sec']:.1f}"
                )
                print(
                    f"Prompt:                {result.token_count} tokens, {result.prompt['tokens-per-sec']:.1f} tokens-per-sec"
                )
                print(
                    f"Audio:                 {result.audio_samples['samples']} samples, {result.audio_samples['samples-per-sec']:.1f} samples-per-sec"
                )
                print(f"Real-time factor:      {result.real_time_factor:.2f}x")
                print(f"Processing time:       {result.processing_time_seconds:.2f}s")
                print(f"Peak memory usage:     {result.peak_memory_usage:.2f}GB")

        if join_audio and not stream:
            if verbose:
                print(f"Joining {len(audio_list)} audio files")
            audio = mx.concatenate(audio_list, axis=0)
            sf.write(
                f"{file_prefix}.{audio_format}",
                audio,
                model.sample_rate,
            )
            if verbose:
                print(f"âœ… Audio successfully generated and saving as: {file_name}")

        if play:
            player.wait_for_drain()
            player.stop()

    except ImportError as e:
        print(f"Import error: {e}")
        print(
            "This might be due to incorrect Python path. Check your project structure."
        )
    except Exception as e:
        print(f"Error loading model: {e}")
        import traceback

        traceback.print_exc()


def parse_args():
    parser = argparse.ArgumentParser(description="Generate audio from text using TTS.")
    parser.add_argument(
        "--model",
        type=str,
        default="mlx-community/Kokoro-82M-bf16",
        help="Path or repo id of the model",
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=1200,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--text",
        type=str,
        default=None,
        help="Text to generate (leave blank to input via stdin)",
    )
    parser.add_argument("--voice", type=str, default=None, help="Voice name")
    parser.add_argument("--speed", type=float, default=1.0, help="Speed of the audio")
    parser.add_argument(
        "--gender", type=str, default="male", help="Gender of the voice [male, female]"
    )
    parser.add_argument("--pitch", type=float, default=1.0, help="Pitch of the voice")
    parser.add_argument("--lang_code", type=str, default="a", help="Language code")
    parser.add_argument(
        "--file_prefix", type=str, default="audio", help="Output file name prefix"
    )
    parser.add_argument("--verbose", action="store_true", help="Print verbose output")
    parser.add_argument(
        "--join_audio", action="store_true", help="Join all audio files into one"
    )
    parser.add_argument("--play", action="store_true", help="Play the output audio")
    parser.add_argument(
        "--audio_format", type=str, default="wav", help="Output audio format"
    )
    parser.add_argument(
        "--ref_audio", type=str, default=None, help="Path to reference audio"
    )
    parser.add_argument(
        "--ref_text", type=str, default=None, help="Caption for reference audio"
    )
    parser.add_argument(
        "--stt_model",
        type=str,
        default="mlx-community/whisper-large-v3-turbo",
        help="STT model to use to transcribe reference audio",
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help="Temperature for the model"
    )
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p for the model")
    parser.add_argument("--top_k", type=int, default=50, help="Top-k for the model")
    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=1.1,
        help="Repetition penalty for the model",
    )
    parser.add_argument(
        "--stream",
        action="store_true",
        help="Stream the audio as segments instead of saving to a file",
    )
    parser.add_argument(
        "--streaming_interval",
        type=float,
        default=2.0,
        help="The time interval in seconds for streaming segments",
    )

    args = parser.parse_args()

    if args.text is None:
        if not sys.stdin.isatty():
            args.text = sys.stdin.read().strip()
        else:
            print("Please enter the text to generate:")
            args.text = input("> ").strip()

    return args


def main():
    args = parse_args()
    generate_audio(model_path=args.model, **vars(args))


if __name__ == "__main__":
    main()



================================================
FILE: mlx_audio/tts/utils.py
================================================
import glob
import importlib
import json
import logging
import shutil
from pathlib import Path
from textwrap import dedent
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import snapshot_download
from mlx.utils import tree_flatten
from mlx_lm.convert import mixed_quant_predicate_builder
from mlx_lm.utils import dequantize_model, quantize_model, save_config, save_model
from transformers import AutoConfig

MODEL_REMAPPING = {"outetts": "outetts", "spark": "spark", "sam": "sesame"}
MAX_FILE_SIZE_GB = 5
MODEL_CONVERSION_DTYPES = ["float16", "bfloat16", "float32"]


def get_model_path(path_or_hf_repo: str, revision: Optional[str] = None) -> Path:
    """
    Ensures the model is available locally. If the path does not exist locally,
    it is downloaded from the Hugging Face Hub.

    Args:
        path_or_hf_repo (str): The local path or Hugging Face repository ID of the model.
        revision (str, optional): A revision id which can be a branch name, a tag, or a commit hash.

    Returns:
        Path: The path to the model.
    """
    model_path = Path(path_or_hf_repo)

    if not model_path.exists():
        model_path = Path(
            snapshot_download(
                path_or_hf_repo,
                revision=revision,
                allow_patterns=[
                    "*.json",
                    "*.safetensors",
                    "*.py",
                    "*.model",
                    "*.tiktoken",
                    "*.txt",
                    "*.jsonl",
                    "*.yaml",
                ],
            )
        )

    return model_path


# Get a list of all available model types from the models directory
def get_available_models():
    """
    Get a list of all available TTS model types by scanning the models directory.

    Returns:
        List[str]: A list of available model type names
    """
    models_dir = Path(__file__).parent / "models"
    available_models = []

    if models_dir.exists() and models_dir.is_dir():
        for item in models_dir.iterdir():
            if item.is_dir() and not item.name.startswith("__"):
                available_models.append(item.name)

    return available_models


def get_model_and_args(model_type: str, model_name: List[str]):
    """
    Retrieve the model architecture module based on the model type and name.

    This function attempts to find the appropriate model architecture by:
    1. Checking if the model_type is directly in the MODEL_REMAPPING dictionary
    2. Looking for partial matches in segments of the model_name

    Args:
        model_type (str): The type of model to load (e.g., "outetts").
        model_name (List[str]): List of model name components that might contain
                               remapping information.

    Returns:
        Tuple[module, str]: A tuple containing:
            - The imported architecture module
            - The resolved model_type string after remapping

    Raises:
        ValueError: If the model type is not supported (module import fails).
    """
    # Stage 1: Check if the model type is in the remapping
    model_type = MODEL_REMAPPING.get(model_type, model_type)

    # Stage 2: Check for partial matches in segments of the model name
    models = get_available_models()
    if model_name is not None:
        for part in model_name:
            # First check if the part matches an available model directory name
            if part in models:
                model_type = part

            # Then check if the part is in our custom remapping dictionary
            if part in MODEL_REMAPPING:
                model_type = MODEL_REMAPPING[part]
                break

    try:
        arch = importlib.import_module(f"mlx_audio.tts.models.{model_type}")
    except ImportError:
        msg = f"Model type {model_type} not supported."
        logging.error(msg)
        raise ValueError(msg)

    return arch, model_type


def load_config(model_path: Union[str, Path], **kwargs) -> dict:
    """Load model configuration from a path or Hugging Face repo.

    Args:
        model_path: Local path or Hugging Face repo ID to load config from
        **kwargs: Additional keyword arguments to pass to the config loader

    Returns:
        dict: Model configuration

    Raises:
        FileNotFoundError: If config.json is not found at the path
    """
    if isinstance(model_path, str):
        model_path = get_model_path(model_path)

    try:
        return AutoConfig.from_pretrained(model_path, **kwargs).to_dict()
    except ValueError:
        try:
            with open(model_path / "config.json", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError as exc:
            raise FileNotFoundError(f"Config not found at {model_path}") from exc


def load_model(
    model_path: Path, lazy: bool = False, strict: bool = True, **kwargs
) -> nn.Module:
    """
    Load and initialize the model from a given path.

    Args:
        model_path (Path): The path to load the model from.
        lazy (bool): If False eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``

    Returns:
        nn.Module: The loaded and initialized model.

    Raises:
        FileNotFoundError: If the weight files (.safetensors) are not found.
        ValueError: If the model class or args class are not found or cannot be instantiated.
    """
    model_name = None
    if isinstance(model_path, str):
        model_name = model_path.lower().split("/")[-1].split("-")
        model_path = get_model_path(model_path)
    elif isinstance(model_path, Path):
        index = model_path.parts.index("hub")
        model_name = model_path.parts[index + 1].lower().split("--")[-1].split("-")
    else:
        raise ValueError(f"Invalid model path type: {type(model_path)}")

    config = load_config(model_path, **kwargs)
    config["tokenizer_name"] = model_path

    # Determine model_type from config or model_name
    model_type = config.get("model_type", None)
    if model_type is None:
        model_type = model_name[0].lower() if model_name is not None else None

    quantization = config.get("quantization", None)

    weight_files = glob.glob(str(model_path / "*.safetensors"))
    if not weight_files:
        # Check in LLM directory if no safetensors found in the main directory
        # For Spark model
        weight_files = glob.glob(str(model_path / "LLM" / "*.safetensors"))

    if not weight_files:
        logging.error(f"No safetensors found in {model_path}")
        message = f"""
No safetensors found in {model_path}
Create safetensors using the following code:
```
from transformers import AutoModelForCausalLM, AutoProcessor

model_id= "<huggingface_model_id>"
model = AutoModelForCausalLM.from_pretrained(model_id)
processor = AutoProcessor.from_pretrained(model_id)

model.save_pretrained("<local_dir>")
processor.save_pretrained("<local_dir>")
```
Then use the <local_dir> as the --hf-path in the convert script.
```
python -m mlx_audio.tts.convert --hf-path <local_dir> --mlx-path <mlx_dir>
```
        """
        raise FileNotFoundError(message)

    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf))

    model_class, model_type = get_model_and_args(
        model_type=model_type, model_name=model_name
    )

    # Get model config from model class if it exists, otherwise use the config
    model_config = (
        model_class.ModelConfig.from_dict(config)
        if hasattr(model_class, "ModelConfig")
        else config
    )

    if model_config is not None and hasattr(model_config, "model_path"):
        # For Spark model
        model_config.model_path = model_path

    model = model_class.Model(model_config)
    quantization = config.get("quantization", None)
    if quantization is None:
        weights = model.sanitize(weights)

    if (quantization := config.get("quantization", None)) is not None:

        def get_class_predicate(p, m):
            # Handle custom per layer quantizations
            if p in config["quantization"]:
                return config["quantization"][p]
            if not hasattr(m, "to_quantized"):
                return False
            # Skip layers not divisible by 64
            if hasattr(m, "weight") and m.weight.size % 64 != 0:
                return False
            # Handle legacy models which may not have everything quantized
            return f"{p}.scales" in weights

        nn.quantize(
            model,
            group_size=quantization["group_size"],
            bits=quantization["bits"],
            class_predicate=get_class_predicate,
        )

    model.load_weights(list(weights.items()), strict=strict)

    if not lazy:
        mx.eval(model.parameters())

    model.eval()
    return model


def fetch_from_hub(
    model_path: Path, lazy: bool = False, **kwargs
) -> Tuple[nn.Module, dict]:
    model = load_model(model_path, lazy, **kwargs)
    config = load_config(model_path, **kwargs)
    return model, config


def upload_to_hub(path: str, upload_repo: str, hf_path: str):
    """
    Uploads the model to Hugging Face hub.

    Args:
        path (str): Local path to the model.
        upload_repo (str): Name of the HF repo to upload to.
        hf_path (str): Path to the original Hugging Face model.
    """
    import os

    from huggingface_hub import HfApi, ModelCard, logging

    from ..version import __version__

    card = ModelCard.load(hf_path)
    card.data.tags = ["mlx"] if card.data.tags is None else card.data.tags + ["mlx"]
    card.text = dedent(
        f"""
        # {upload_repo}
        This model was converted to MLX format from [`{hf_path}`](https://huggingface.co/{hf_path}) using mlx-audio version **{__version__}**.
        Refer to the [original model card](https://huggingface.co/{hf_path}) for more details on the model.
        ## Use with mlx

        ```bash
        pip install -U mlx-audio
        ```

        ```bash
        python -m mlx_audio.tts.generate --model {upload_repo} --text "Describe this image."
        ```
        """
    )
    card.save(os.path.join(path, "README.md"))

    logging.set_verbosity_info()

    api = HfApi()
    api.create_repo(repo_id=upload_repo, exist_ok=True)
    api.upload_folder(
        folder_path=path,
        repo_id=upload_repo,
        repo_type="model",
    )
    print(f"Upload successful, go to https://huggingface.co/{upload_repo} for details.")


def convert(
    hf_path: str,
    mlx_path: str = "mlx_model",
    quantize: bool = False,
    q_group_size: int = 64,
    q_bits: int = 4,
    dtype: str = None,
    upload_repo: str = None,
    revision: Optional[str] = None,
    dequantize: bool = False,
    trust_remote_code: bool = True,
    quant_predicate: Optional[str] = None,
):
    print("[INFO] Loading")
    model_path = get_model_path(hf_path, revision=revision)
    model, config = fetch_from_hub(
        model_path, lazy=True, trust_remote_code=trust_remote_code
    )

    if isinstance(quant_predicate, str):
        quant_predicate = mixed_quant_predicate_builder(quant_predicate, model)

    # Get model-specific quantization predicate if available
    model_quant_predicate = getattr(
        model, "model_quant_predicate", lambda p, m, config: True
    )

    # Define base quantization requirements
    def base_quant_requirements(p, m, config):
        return (
            hasattr(m, "weight")
            and m.weight.shape[-1] % 64 == 0  # Skip layers not divisible by 64
            and hasattr(m, "to_quantized")
            and model_quant_predicate(p, m, config)
        )

    # Combine with user-provided predicate if available
    if quant_predicate is None:
        quant_predicate = base_quant_requirements
    else:
        original_predicate = quant_predicate
        quant_predicate = lambda p, m, config: (
            base_quant_requirements(p, m, config) and original_predicate(p, m, config)
        )

    weights = dict(tree_flatten(model.parameters()))

    if dtype is None:
        dtype = config.get("torch_dtype", None)
    if dtype in MODEL_CONVERSION_DTYPES:
        print("[INFO] Using dtype:", dtype)
        dtype = getattr(mx, dtype)
        weights = {k: v.astype(dtype) for k, v in weights.items()}

    if quantize and dequantize:
        raise ValueError("Choose either quantize or dequantize, not both.")

    if quantize:
        print("[INFO] Quantizing")
        model.load_weights(list(weights.items()))
        weights, config = quantize_model(
            model, config, q_group_size, q_bits, quant_predicate=quant_predicate
        )

    if dequantize:
        print("[INFO] Dequantizing")
        model = dequantize_model(model)
        weights = dict(tree_flatten(model.parameters()))

    if isinstance(mlx_path, str):
        mlx_path = Path(mlx_path)

    # Ensure the destination directory for MLX model exists before copying files
    mlx_path.mkdir(parents=True, exist_ok=True)

    # Copy Python and JSON files from the model path to the MLX path
    for pattern in ["*.py", "*.json", "*.wav", "*.pt", "*.safetensors", "*.yaml"]:
        files = glob.glob(str(model_path / pattern))
        for file in files:
            shutil.copy(file, mlx_path)

        # Check files in subdirectories up to two levels deep
        subdir_files = glob.glob(str(model_path / "**" / pattern), recursive=True)
        for file in subdir_files:
            rel_path = Path(file).relative_to(model_path)
            # Create subdirectories if they don't exist
            dest_dir = mlx_path / rel_path.parent
            dest_dir.mkdir(parents=True, exist_ok=True)
            shutil.copy(file, dest_dir)

    save_model(mlx_path, model, donate_model=True)

    save_config(config, config_path=mlx_path / "config.json")

    if upload_repo is not None:
        upload_to_hub(mlx_path, upload_repo, hf_path)



================================================
FILE: mlx_audio/tts/models/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/models/base.py
================================================
import inspect
from dataclasses import dataclass

import mlx.core as mx
import numpy as np


@dataclass
class BaseModelArgs:
    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


def check_array_shape(arr):
    shape = arr.shape

    # Check if the shape has 4 dimensions
    if len(shape) != 3:
        return False

    out_channels, kH, KW = shape

    # Check if out_channels is the largest, and kH and KW are the same
    if (out_channels >= kH) and (out_channels >= KW) and (kH == KW):
        return True
    else:
        return False


def adjust_speed(audio_array, speed_factor):
    """
    Adjust the speed of the audio by resampling
    speed_factor > 1: faster
    speed_factor < 1: slower
    """
    # Ensure we're working with MLX arrays
    if not isinstance(audio_array, mx.array):
        audio_array = mx.array(audio_array)

    # Calculate new length
    old_length = audio_array.shape[0]
    new_length = int(old_length / speed_factor)

    # Create new time points
    old_indices = mx.arange(old_length)
    new_indices = mx.linspace(0, old_length - 1, new_length)

    # Resample using linear interpolation
    # Since mx doesn't have interp, we'll implement it directly
    indices_floor = mx.floor(new_indices).astype(mx.int32)
    indices_ceil = mx.minimum(indices_floor + 1, old_length - 1)
    weights_ceil = new_indices - indices_floor
    weights_floor = 1.0 - weights_ceil

    # Perform the interpolation
    result = (
        weights_floor.reshape(-1, 1) * audio_array[indices_floor]
        + weights_ceil.reshape(-1, 1) * audio_array[indices_ceil]
    )

    return result


@dataclass
class GenerationResult:
    audio: mx.array
    samples: int
    sample_rate: int
    segment_idx: int
    token_count: int
    audio_samples: int
    audio_duration: str
    real_time_factor: float
    prompt: dict
    audio_samples: dict
    processing_time_seconds: float
    peak_memory_usage: float



================================================
FILE: mlx_audio/tts/models/interpolate.py
================================================
from typing import List, Optional, Tuple, Union

import mlx.core as mx


def interpolate(
    input: mx.array,
    size: Optional[Union[int, Tuple[int, ...], List[int]]] = None,
    scale_factor: Optional[Union[float, List[float], Tuple[float, ...]]] = None,
    mode: str = "nearest",
    align_corners: Optional[bool] = None,
) -> mx.array:
    """Interpolate array with correct shape handling.

    Args:
        input (mx.array): Input tensor [N, C, ...] where ... represents spatial dimensions
        size (int or tuple): Output size
        scale_factor (float or tuple): Multiplier for spatial size
        mode (str): 'nearest' or 'linear'
        align_corners (bool): If True, align corners of input and output tensors
    """
    ndim = input.ndim
    if ndim < 3:
        raise ValueError(f"Expected at least 3D input (N, C, D1), got {ndim}D")

    spatial_dims = ndim - 2

    # Handle size and scale_factor
    if size is not None and scale_factor is not None:
        raise ValueError("Only one of size or scale_factor should be defined")
    elif size is None and scale_factor is None:
        raise ValueError("One of size or scale_factor must be defined")

    # Convert single values to tuples
    if size is not None and not isinstance(size, (list, tuple)):
        size = [size] * spatial_dims
    if scale_factor is not None and not isinstance(scale_factor, (list, tuple)):
        scale_factor = [scale_factor] * spatial_dims

    # Calculate output size from scale factor if needed
    if size is None:
        size = []
        for i in range(spatial_dims):
            # Use ceiling instead of floor to match PyTorch behavior
            curr_size = max(1, int(mx.ceil(input.shape[i + 2] * scale_factor[i])))
            size.append(curr_size)

    # Handle 1D case (N, C, W)
    if spatial_dims == 1:
        return interpolate1d(input, size[0], mode, align_corners)
    else:
        raise ValueError(
            f"Only 1D interpolation currently supported, got {spatial_dims}D"
        )


def interpolate1d(
    input: mx.array,
    size: int,
    mode: str = "linear",
    align_corners: Optional[bool] = None,
) -> mx.array:
    """1D interpolation implementation."""
    batch_size, channels, in_width = input.shape

    # Handle edge cases
    if size < 1:
        size = 1
    if in_width < 1:
        in_width = 1

    if mode == "nearest":
        if size == 1:
            indices = mx.array([0])
        else:
            scale = in_width / size
            indices = mx.floor(mx.arange(size) * scale).astype(mx.int32)
            indices = mx.clip(indices, 0, in_width - 1)
        return input[:, :, indices]

    # Linear interpolation
    if align_corners and size > 1:
        x = mx.arange(size) * ((in_width - 1) / (size - 1))
    else:
        if size == 1:
            x = mx.array([0.0])
        else:
            x = mx.arange(size) * (in_width / size)
            if not align_corners:
                x = x + 0.5 * (in_width / size) - 0.5

    # Handle the case where input width is 1
    if in_width == 1:
        output = mx.broadcast_to(input, (batch_size, channels, size))
        return output

    x_low = mx.floor(x).astype(mx.int32)
    x_high = mx.minimum(x_low + 1, in_width - 1)
    x_frac = x - x_low

    # Pre-compute indices to avoid repeated computation
    y_low = input[:, :, x_low]
    y_high = input[:, :, x_high]

    # Vectorized interpolation
    output = y_low * (1 - x_frac)[None, None, :] + y_high * x_frac[None, None, :]

    return output



================================================
FILE: mlx_audio/tts/models/bark/__init__.py
================================================
from .bark import Model, ModelConfig
from .pipeline import Pipeline

__all__ = ["Model", "Pipeline", "ModelConfig"]



================================================
FILE: mlx_audio/tts/models/bark/bark.py
================================================
import argparse
import glob
import math
import time
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import tqdm
from mlx.utils import tree_map, tree_unflatten
from mlx_lm.models.base import create_causal_mask
from scipy.io.wavfile import write as write_wav
from transformers import BertTokenizer

from ..base import BaseModelArgs, GenerationResult
from .pipeline import Pipeline

mx.random.seed(42)

TEXT_ENCODING_OFFSET = 10_048
SEMANTIC_PAD_TOKEN = 10_000
TEXT_PAD_TOKEN = 129595
SEMANTIC_INFER_TOKEN = 129_599

CONTEXT_WINDOW_SIZE = 1024

SEMANTIC_RATE_HZ = 49.9
SEMANTIC_VOCAB_SIZE = 10_000

CODEBOOK_SIZE = 1024
N_COARSE_CODEBOOKS = 2
N_FINE_CODEBOOKS = 8
COARSE_RATE_HZ = 75
COARSE_SEMANTIC_PAD_TOKEN = 12_048
COARSE_INFER_TOKEN = 12_050
SAMPLE_RATE = 24_000


def filter_dataclass_fields(data_dict, dataclass_type):
    """Filter a dictionary to only include keys that are fields in the dataclass."""
    valid_fields = {f.name for f in dataclass_type.__dataclass_fields__.values()}
    return {k: v for k, v in data_dict.items() if k in valid_fields}


@dataclass
class SemanticConfig(BaseModelArgs):
    bad_words_ids: list[list[int]] = None
    block_size: int = 1024
    input_vocab_size: int = 129600
    output_vocab_size: int = 129600
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    bias: bool = False
    model_type: str = "semantic"
    dropout: float = 0.0
    architectures: list[str] = None


@dataclass
class CoarseAcousticsConfig(BaseModelArgs):
    block_size: int = 1024
    input_vocab_size: int = 12096
    output_vocab_size: int = 12096
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    bias: bool = False
    model_type: str = "coarse_acoustics"
    dropout: float = 0.0


@dataclass
class FineAcousticsConfig(BaseModelArgs):
    block_size: int = 1024
    input_vocab_size: int = 1056
    output_vocab_size: int = 1056
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    bias: bool = False
    model_type: str = "fine_acoustics"
    n_codes_total: int = 8
    n_codes_given: int = 1
    dropout: float = 0.0


@dataclass
class CodecConfig(BaseModelArgs):
    model_type: str = "codec"
    sample_rate: int = 24000
    target_bandwidth: float = 6.0


@dataclass
class ModelConfig(BaseModelArgs):
    semantic_config: SemanticConfig
    coarse_acoustics_config: CoarseAcousticsConfig
    fine_acoustics_config: FineAcousticsConfig
    codec_config: CodecConfig
    block_size: int = 1024
    input_vocab_size: int = 10_048
    output_vocab_size: int = 10_048
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = False
    n_codes_total: Optional[int] = None
    n_codes_given: Optional[int] = None
    model_size: str = "base"
    model_type: str = "bark"
    initializer_range: float = 0.02
    codec_path: str = "mlx-community/encodec-24khz-float32"
    sample_rate: int = 24000


class LayerNorm(nn.Module):
    def __init__(self, dims: int, eps: float = 1e-5, bias: bool = True):
        super().__init__()
        self.bias = mx.zeros((dims,)) if bias else None
        self.weight = mx.ones((dims,))
        self.dims = dims
        self.eps = eps

    def __call__(self, x):
        mean = mx.mean(x, axis=-1, keepdims=True)
        var = mx.var(x, axis=-1, keepdims=True)
        x = (x - mean) * mx.rsqrt(var + self.eps)
        if self.bias is not None:
            x = x * self.weight + self.bias
        else:
            x = x * self.weight
        return x


class CausalSelfAttention(nn.Module):
    def __init__(
        self, args: Union[SemanticConfig, CoarseAcousticsConfig, FineAcousticsConfig]
    ):
        super().__init__()
        self.att_proj = nn.Linear(args.n_embd, 3 * args.n_embd, bias=args.bias)
        self.out_proj = nn.Linear(args.n_embd, args.n_embd, bias=args.bias)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.n_head = args.n_head
        self.n_embd = args.n_embd
        self.dropout = args.dropout
        self.bias = (
            mx.tril(mx.ones([args.block_size, args.block_size]))
            .reshape(1, 1, args.block_size, args.block_size)
            .astype(mx.float32)
        )

    def __call__(self, x, past_kv=None, use_cache=False):
        B, T, C = x.shape
        query, key, value = mx.split(self.att_proj(x), 3, axis=2)
        key = key.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)
        query = query.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)
        value = value.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)
        if past_kv is not None:
            past_key, past_value = past_kv
            key = mx.concatenate([past_key, key], axis=-2)
            value = mx.concatenate([past_value, value], axis=-2)

        FULL_T = key.shape[-2]
        if use_cache is True:
            present = (key, value)
        else:
            present = None

        y = mx.fast.scaled_dot_product_attention(
            query,
            key,
            value,
            scale=1.0 / math.sqrt(key.shape[3]),
            mask=self.bias[:, :, FULL_T - T : FULL_T, :FULL_T],
        )
        y = self.attn_dropout(y)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)
        y = self.resid_dropout(self.out_proj(y))
        return (y, present)


class NonCausalSelfAttention(nn.Module):
    def __init__(
        self, args: Union[SemanticConfig, CoarseAcousticsConfig, FineAcousticsConfig]
    ):
        super().__init__()
        self.att_proj = nn.Linear(args.n_embd, 3 * args.n_embd, bias=args.bias)
        self.out_proj = nn.Linear(args.n_embd, args.n_embd, bias=args.bias)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.n_head = args.n_head
        self.n_embd = args.n_embd
        self.dropout = args.dropout

    def __call__(self, x):
        B, T, C = x.shape
        query, key, value = mx.split(self.att_proj(x), 3, axis=2)
        key = key.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)
        query = query.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)
        value = value.reshape(B, T, self.n_head, C // self.n_head).transpose(0, 2, 1, 3)

        y = mx.fast.scaled_dot_product_attention(
            query, key, value, scale=1.0 / math.sqrt(key.shape[3])
        )
        y = self.attn_dropout(y)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)
        y = self.resid_dropout(self.out_proj(y))
        return y


class MLP(nn.Module):
    def __init__(
        self, args: Union[SemanticConfig, CoarseAcousticsConfig, FineAcousticsConfig]
    ):
        super().__init__()

        self.in_proj = nn.Linear(args.n_embd, 4 * args.n_embd, bias=False)
        self.out_proj = nn.Linear(4 * args.n_embd, args.n_embd, bias=False)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(args.dropout)

    def __call__(self, x: mx.array) -> mx.array:
        x = self.in_proj(x)
        x = self.gelu(x)
        x = self.out_proj(x)
        x = self.dropout(x)
        return x


class Block(nn.Module):
    def __init__(
        self, args: Union[SemanticConfig, CoarseAcousticsConfig], layer_idx: int = 0
    ):
        super().__init__()
        self.args = args
        self.layernorm_1 = LayerNorm(args.n_embd, bias=False)
        self.attn = CausalSelfAttention(args)
        self.layernorm_2 = LayerNorm(args.n_embd, bias=False)
        self.mlp = MLP(args)
        self.layer_idx = layer_idx

    def __call__(self, x: mx.array, past_kv=None, use_cache=False):
        attn_output, prev_kvs = self.attn(
            self.layernorm_1(x), past_kv=past_kv, use_cache=use_cache
        )
        x = x + attn_output
        x = x + self.mlp(self.layernorm_2(x))
        return (x, prev_kvs)


class FineBlock(nn.Module):
    def __init__(self, args: FineAcousticsConfig):
        super().__init__()
        self.args = args
        self.layernorm_1 = nn.LayerNorm(args.n_embd)
        self.attn = NonCausalSelfAttention(args)
        self.layernorm_2 = nn.LayerNorm(args.n_embd)
        self.mlp = MLP(args)

    def __call__(self, x: mx.array):
        x = x + self.attn(self.layernorm_1(x))
        x = x + self.mlp(self.layernorm_2(x))
        return x


class GPT(nn.Module):
    def __init__(self, args: Union[SemanticConfig, CoarseAcousticsConfig]):
        super().__init__()
        self.args = args
        self.input_embeds_layer = nn.Embedding(args.input_vocab_size, args.n_embd)
        self.position_embeds_layer = nn.Embedding(args.block_size, args.n_embd)
        self.drop = nn.Dropout(args.dropout)
        self.layers = [Block(args=args) for _ in range(args.n_layer)]
        self.layernorm_final = LayerNorm(args.n_embd, bias=False)
        self.lm_head = nn.Linear(args.n_embd, args.output_vocab_size, bias=False)

    def __call__(
        self,
        x: mx.array,
        merge_context: bool = False,
        past_kv: mx.array = None,
        position_ids: mx.array = None,
        use_cache: bool = False,
    ) -> mx.array:
        b, t = x.shape

        if past_kv is not None:
            assert t == 1
            tok_emb = self.input_embeds_layer(x)
        else:
            if merge_context:
                assert x.shape[1] >= 256 + 256 + 1
                t = x.shape[1] - 256
                tok_emb = mx.concatenate(
                    [
                        self.input_embeds_layer(x[:, :256])
                        + self.input_embeds_layer(x[:, 256 : 256 + 256]),
                        self.input_embeds_layer(x[:, 256 + 256 :]),
                    ],
                    axis=1,
                )
            else:
                tok_emb = self.input_embeds_layer(x)

        # past length
        if past_kv is None:
            past_length = 0
            past_kv = tuple([None] * len(self.layers))
        else:
            past_length = past_kv[0][0].shape[-2]

        if position_ids is None:
            position_ids = mx.arange(past_length, t + past_length)
            position_ids = position_ids.reshape(1, -1)  # shape (1, t)

        pos_emb = self.position_embeds_layer(
            position_ids
        )  # position embeddings of shape (1, t, n_embd)
        x = self.drop(tok_emb + pos_emb)

        new_kv = () if use_cache else None

        for i, (block, past_layer_kv) in enumerate(zip(self.layers, past_kv)):
            x, kv = block(x, past_kv=past_layer_kv, use_cache=use_cache)

            if use_cache:
                new_kv = new_kv + (kv,)

        x = self.layernorm_final(x)

        logits = self.lm_head(
            x[:, -1:, :]
        )  # note: using list [-1] to preserve the time dim

        return (logits, new_kv)


class FineGPT(nn.Module):
    def __init__(self, args: FineAcousticsConfig):
        super().__init__()
        self.args = args
        self.n_codes_total = args.n_codes_total
        self.input_embeds_layers = [
            nn.Embedding(args.input_vocab_size, args.n_embd)
            for _ in range(args.n_codes_total)
        ]
        self.position_embeds_layer = nn.Embedding(args.block_size, args.n_embd)
        self.drop = nn.Dropout(args.dropout)
        self.layers = [FineBlock(args=args) for _ in range(args.n_layer)]
        self.layernorm_final = nn.LayerNorm(args.n_embd)

        self.lm_heads = [
            nn.Linear(args.n_embd, args.output_vocab_size, bias=False)
            for _ in range(args.n_codes_given, args.n_codes_total)
        ]
        for i in range(self.n_codes_total - args.n_codes_given):
            self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight

    def __call__(self, pred_idx: mx.array, idx: mx.array) -> mx.array:
        b, t, codes = idx.shape
        assert (
            t <= self.args.block_size
        ), f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
        assert pred_idx > 0, "cannot predict 0th codebook"
        assert codes == self.n_codes_total, (b, t, codes)
        pos = mx.arange(0, t).astype(mx.int64).reshape(1, t)  # shape (1, t)
        tok_embs = [
            self.input_embeds_layers[i](idx[:, :, i].astype(mx.int32)).reshape(
                b, t, -1, 1
            )
            for i in range(self.n_codes_total)
        ]  # token embeddings of shape (b, t, n_embd)
        tok_emb = mx.concatenate(tok_embs, axis=-1)
        pos_emb = self.position_embeds_layer(
            pos
        )  # position embeddings of shape (1, t, n_embd)
        x = tok_emb[:, :, :, : pred_idx + 1].sum(axis=-1)
        x = self.drop(x + pos_emb)
        for block in self.layers:
            x = block(x)
        x = self.layernorm_final(x)

        logits = self.lm_heads[pred_idx - self.args.n_codes_given](x)
        return logits


class Model(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config

        # Convert config dictionaries to proper configuration objects if needed
        if isinstance(config.semantic_config, dict):
            filtered_config = filter_dataclass_fields(
                config.semantic_config, SemanticConfig
            )
            semantic_config = SemanticConfig(**filtered_config)
        else:
            semantic_config = config.semantic_config

        if isinstance(config.coarse_acoustics_config, dict):
            filtered_config = filter_dataclass_fields(
                config.coarse_acoustics_config, CoarseAcousticsConfig
            )
            coarse_config = CoarseAcousticsConfig(**filtered_config)
        else:
            coarse_config = config.coarse_acoustics_config

        if isinstance(config.fine_acoustics_config, dict):
            filtered_config = filter_dataclass_fields(
                config.fine_acoustics_config, FineAcousticsConfig
            )
            fine_config = FineAcousticsConfig(**filtered_config)
        else:
            fine_config = config.fine_acoustics_config

        self.semantic = GPT(semantic_config)
        self.fine_acoustics = FineGPT(fine_config)
        self.coarse_acoustics = GPT(coarse_config)

        self.tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

    def sanitize(self, weights):

        sanitized_weights = {}
        for key, value in weights.items():
            # there's no _orig_mod.transformer
            if "_orig_mod.transformer." in key:
                key = key.replace("_orig_mod.transformer.", "")
            # transformer block mapping
            if "h" in key:
                layer_count = 24 if self.config.model_size == "large" else 12
                for i in range(layer_count):
                    prefix = f"h.{i}."
                    key = key.replace(prefix, f"layers.{i}.")

            # lm_head
            if "lm_head" in key:
                key = key.replace("_orig_mod.", "")

            if "codec" in key:
                pass
            else:
                sanitized_weights[key] = value

        return sanitized_weights

    @property
    def sample_rate(self):
        return self.config.sample_rate

    def generate(self, text: str, voice: str = None, **kwargs):
        pipeline = Pipeline(
            model=self,
            tokenizer=self.tokenizer,
            config=self.config,
        )

        # Track overall generation time
        start_time = time.time()

        for segment_idx, (audio, tokens) in enumerate(
            pipeline(text, voice=voice, use_kv_caching=True, **kwargs)
        ):
            # Track per-segment generation time
            segment_time = time.time() - start_time

            samples = audio.shape[0] if audio is not None else 0
            assert samples > 0, "No audio generated"

            # Calculate token count
            token_count = len(tokens) if tokens is not None else 0

            # Calculate audio duration in seconds
            sample_rate = self.config.sample_rate
            audio_duration_seconds = samples / sample_rate * audio.shape[1]

            # Calculate milliseconds per sample
            ms_per_sample = (
                1000 / sample_rate
            )  # This gives 0.0417 ms per sample at 24kHz

            # Calculate real-time factor (RTF)
            rtf = (
                segment_time / audio_duration_seconds
                if audio_duration_seconds > 0
                else 0
            )

            # Format duration as HH:MM:SS.mmm
            duration_mins = int(audio_duration_seconds // 60)
            duration_secs = int(audio_duration_seconds % 60)
            duration_ms = int((audio_duration_seconds % 1) * 1000)
            duration_hours = int(audio_duration_seconds // 3600)
            duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

            yield GenerationResult(
                audio=audio[0],
                samples=samples,
                sample_rate=sample_rate,
                segment_idx=segment_idx,
                token_count=token_count,
                audio_duration=duration_str,
                real_time_factor=round(rtf, 2),
                prompt={
                    "tokens": token_count,
                    "tokens-per-sec": (
                        round(token_count / segment_time, 2) if segment_time > 0 else 0
                    ),
                },
                audio_samples={
                    "samples": samples,
                    "samples-per-sec": (
                        round(samples / segment_time, 2) if segment_time > 0 else 0
                    ),
                },
                processing_time_seconds=segment_time,
                peak_memory_usage=mx.get_peak_memory() / 1e9,
            )

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/bark/isftnet.py
================================================
import mlx.core as mx
import mlx.nn as nn


# Loads to torch Encodec model
def codec_decode(codec: nn.Module, fine_tokens: mx.array):
    arr = fine_tokens.astype(mx.int32)[None]
    emb = codec.quantizer.decode(arr)
    out = codec.decoder(emb).astype(mx.float32)
    audio_arr = mx.squeeze(out, -1)
    del arr, emb, out
    return audio_arr



================================================
FILE: mlx_audio/tts/models/bark/pipeline.py
================================================
import math
import os
from dataclasses import dataclass
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import tqdm

from mlx_audio.codec.models.encodec.encodec import Encodec

from ..base import adjust_speed
from .isftnet import codec_decode

TEXT_ENCODING_OFFSET = 10_048
SEMANTIC_PAD_TOKEN = 10_000
TEXT_PAD_TOKEN = 129595
SEMANTIC_INFER_TOKEN = 129_599

CONTEXT_WINDOW_SIZE = 1024

SEMANTIC_RATE_HZ = 49.9
SEMANTIC_VOCAB_SIZE = 10_000

CODEBOOK_SIZE = 1024
N_COARSE_CODEBOOKS = 2
N_FINE_CODEBOOKS = 8
COARSE_RATE_HZ = 75
COARSE_SEMANTIC_PAD_TOKEN = 12_048
COARSE_INFER_TOKEN = 12_050
SAMPLE_RATE = 24_000

CUR_PATH = os.path.dirname(os.path.abspath(__file__))


SUPPORTED_LANGS = [
    ("English", "en"),
    ("German", "de"),
    ("Spanish", "es"),
    ("French", "fr"),
    ("Hindi", "hi"),
    ("Italian", "it"),
    ("Japanese", "ja"),
    ("Korean", "ko"),
    ("Polish", "pl"),
    ("Portuguese", "pt"),
    ("Russian", "ru"),
    ("Turkish", "tr"),
    ("Chinese", "zh"),
]

ALLOWED_PROMPTS = {"announcer"}
for _, lang in SUPPORTED_LANGS:
    for prefix in ("", f"v2{os.path.sep}"):
        for n in range(10):
            ALLOWED_PROMPTS.add(f"{prefix}{lang}_speaker_{n}")


@dataclass
class Result:
    audio: mx.array
    tokens: mx.array

    ### MARK: BEGIN BACKWARD COMPAT ###
    def __iter__(self):
        yield self.audio
        yield self.tokens

    def __getitem__(self, index):
        return [self.audio, self.tokens][index]

    def __len__(self):
        return 2


def _load_voice_prompt(voice_prompt_input):
    if isinstance(voice_prompt_input, str) and voice_prompt_input.endswith(".npz"):
        voice_prompt = np.load(voice_prompt_input)
    elif isinstance(voice_prompt_input, str):
        # make sure this works on non-ubuntu
        voice_prompt_input = os.path.join(*voice_prompt_input.split("/"))
        if voice_prompt_input not in ALLOWED_PROMPTS:
            raise ValueError("voice prompt not found")

        path = f"{voice_prompt_input}.npz"

        # TODO: Get the path from the Hugging Face cache directory
        # TODO: If not found, download the voice from Hugging Face
        # TODO: If still not found, raise an error

        if not os.path.exists(path):
            raise ValueError("voice prompt not found")
        voice_prompt = np.load(path)
    elif isinstance(voice_prompt_input, dict):
        assert "semantic_prompt" in voice_prompt_input
        assert "coarse_prompt" in voice_prompt_input
        assert "fine_prompt" in voice_prompt_input
        voice_prompt = voice_prompt_input
    else:
        raise ValueError("voice prompt format unrecognized")
    return voice_prompt


def _flatten_codebooks(arr, offset_size=CODEBOOK_SIZE):
    assert len(arr.shape) == 2
    if offset_size is not None:
        for n in range(1, arr.shape[0]):
            arr[n, :] += offset_size * n
    # MLX doesn't have ravel with order parameter, so we transpose and reshape
    # to achieve the same effect as numpy's ravel('F')
    flat_arr = arr.transpose().reshape(-1)
    return flat_arr


class Pipeline:
    def __init__(self, model: nn.Module, tokenizer: any, config: any):
        self.model = model
        self.tokenizer = tokenizer
        self.codec_model, _ = Encodec.from_pretrained(config.codec_path)

    def generate_text_semantic(
        self,
        text: str,
        voice: str = "announcer",
        temperature: float = 0.7,
        use_kv_caching: bool = False,
        allow_early_stop: bool = True,
        **kwargs,
    ):
        """Generate semantic tokens from text."""
        verbose = kwargs.get("verbose", False)
        if verbose:
            print("Generating semantic tokens...")
        if voice is not None:
            voice_prompt = _load_voice_prompt(voice)
            semantic_history = mx.array(voice_prompt["semantic_prompt"])
            assert (
                isinstance(semantic_history, mx.array)
                and len(semantic_history.shape) == 1
                and len(semantic_history) > 0
                and semantic_history.min() >= 0
                and semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1
            )
        else:
            semantic_history = None

        encoded_text = (
            mx.array(self.tokenizer.encode(text, add_special_tokens=False))
            + TEXT_ENCODING_OFFSET
        )
        if len(encoded_text) > 256:
            p = round((len(encoded_text) - 256) / len(encoded_text) * 100, 1)
            encoded_text = encoded_text[:256]
        encoded_text = mx.pad(
            encoded_text,
            (0, 256 - len(encoded_text)),
            constant_values=TEXT_PAD_TOKEN,
        )
        if semantic_history is not None:
            semantic_history = semantic_history.astype(mx.int64)
            # lop off if history is too long, pad if needed
            semantic_history = semantic_history[-256:]
            semantic_history = mx.pad(
                semantic_history,
                (0, 256 - len(semantic_history)),
                constant_values=SEMANTIC_PAD_TOKEN,
                mode="constant",
            )
        else:
            semantic_history = mx.array([SEMANTIC_PAD_TOKEN] * 256)

        x = (
            mx.concatenate(
                [encoded_text, semantic_history, mx.array([SEMANTIC_INFER_TOKEN])]
            )
            .reshape(1, -1)
            .astype(mx.int64)
        )
        n_tot_steps = 768
        kv_cache = None
        for i in tqdm.tqdm(range(n_tot_steps), disable=not verbose):
            if use_kv_caching and kv_cache is not None:
                x_input = x[:, -1:]
            else:
                x_input = x
            logits, kv_cache = self.model.semantic(
                x_input, merge_context=True, use_cache=use_kv_caching, past_kv=kv_cache
            )
            relevant_logits = logits[0, 0, :SEMANTIC_VOCAB_SIZE]
            if allow_early_stop:
                # Early stop
                relevant_logits = mx.concatenate(
                    [relevant_logits, logits[0, 0, SEMANTIC_PAD_TOKEN].reshape(1)],
                    axis=-1,
                )
            next_token = mx.random.categorical(
                relevant_logits * 1 / (temperature), num_samples=1
            ).astype(mx.int32)

            if next_token == SEMANTIC_VOCAB_SIZE:
                print(f"Early stop at step {i} with token {next_token.tolist()}")
                break
            x = mx.concatenate([x, next_token.reshape(1, -1)], axis=1)
            if i == n_tot_steps - 1:
                break
        out = x.squeeze()[256 + 256 + 1 :]
        return out, encoded_text

    def generate_coarse(
        self,
        x_semantic: mx.array,
        voice: str = "announcer",
        temperature: float = 0.7,
        max_coarse_history: int = 60,  # min 60 (faster), max 630 (more context)
        sliding_window_len: int = 60,
        use_kv_caching: bool = False,
        **kwargs,
    ):
        """Generate coarse tokens from semantic tokens."""
        verbose = kwargs.get("verbose", False)
        if verbose:
            print("Generating coarse tokens...")
        semantic_to_coarse_ratio = (
            COARSE_RATE_HZ / SEMANTIC_RATE_HZ * N_COARSE_CODEBOOKS
        )
        max_semantic_history = int(
            math.floor(max_coarse_history / semantic_to_coarse_ratio)
        )
        if voice is not None:
            voice_prompt = _load_voice_prompt(voice)
            x_semantic_history = mx.array(voice_prompt["semantic_prompt"])
            x_coarse_history = mx.array(voice_prompt["coarse_prompt"])
            assert (
                isinstance(x_semantic_history, mx.array)
                and len(x_semantic_history.shape) == 1
                and len(x_semantic_history) > 0
                and x_semantic_history.min() >= 0
                and x_semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1
                and isinstance(x_coarse_history, mx.array)
                and len(x_coarse_history.shape) == 2
                and x_coarse_history.shape[0] == N_COARSE_CODEBOOKS
                and x_coarse_history.shape[-1] >= 0
                and x_coarse_history.min() >= 0
                and x_coarse_history.max() <= CODEBOOK_SIZE - 1
                and (
                    round(x_coarse_history.shape[-1] / len(x_semantic_history), 1)
                    == round(semantic_to_coarse_ratio / N_COARSE_CODEBOOKS, 1)
                )
            )
            x_coarse_history = (
                _flatten_codebooks(x_coarse_history) + SEMANTIC_VOCAB_SIZE
            )
            # trim histories correctly
            n_semantic_hist_provided = min(
                max_semantic_history,
                len(x_semantic_history) - len(x_semantic_history) % 2,
                int(math.floor(len(x_coarse_history) / semantic_to_coarse_ratio)),
            )
            n_coarse_hist_provided = int(
                round(n_semantic_hist_provided * semantic_to_coarse_ratio)
            )
            x_semantic_history = x_semantic_history[-n_semantic_hist_provided:].astype(
                mx.int32
            )
            x_coarse_history = x_coarse_history[-n_coarse_hist_provided:].astype(
                mx.int32
            )
            # TODO: bit of a hack for time alignment (sounds better)
            x_coarse_history = x_coarse_history[:-2]
        else:
            x_semantic_history = mx.array([], dtype=mx.int32)
            x_coarse_history = mx.array([], dtype=mx.int32)

        n_steps = int(
            round(
                math.floor(
                    len(x_semantic) * semantic_to_coarse_ratio / N_COARSE_CODEBOOKS
                )
                * N_COARSE_CODEBOOKS
            )
        )
        x_semantic = mx.concatenate([x_semantic_history, x_semantic]).astype(mx.int32)
        x_coarse = x_coarse_history.astype(mx.int32)
        base_semantic_idx = len(x_semantic_history)
        # Inference
        x_semantic_in = x_semantic.reshape(1, -1)
        x_coarse_in = x_coarse.reshape(1, -1)
        n_window_steps = int(round(n_steps / sliding_window_len))
        n_step = 0
        for _ in tqdm.tqdm(
            range(n_window_steps), total=n_window_steps, disable=not verbose
        ):
            semantic_idx = base_semantic_idx + int(
                round(n_step / semantic_to_coarse_ratio)
            )
            x_in = x_semantic_in[:, max(0, semantic_idx - max_semantic_history) :]
            x_in = x_in[:, :256]
            x_in = mx.pad(
                x_in,
                ((0, 0), (0, 256 - x_in.shape[-1])),
                constant_values=COARSE_SEMANTIC_PAD_TOKEN,
            )
            x_in = mx.concatenate(
                [
                    x_in,
                    mx.array([COARSE_INFER_TOKEN]).reshape(1, -1),
                    x_coarse_in[:, -max_coarse_history:],
                ],
                axis=1,
            )
            kv_cache = None
            for _ in range(sliding_window_len):
                if n_step >= n_steps:
                    continue
                is_major_step = n_step % N_COARSE_CODEBOOKS == 0
                x_input = (
                    x_in[:, -1:] if use_kv_caching and kv_cache is not None else x_in
                )
                logits, kv_cache = self.model.coarse_acoustics(
                    x_input, use_cache=use_kv_caching, past_kv=kv_cache
                )
                logit_start_idx = (
                    SEMANTIC_VOCAB_SIZE + (1 - int(is_major_step)) * CODEBOOK_SIZE
                )
                logit_end_idx = (
                    SEMANTIC_VOCAB_SIZE + (2 - int(is_major_step)) * CODEBOOK_SIZE
                )
                logit_end_idx = min(logit_end_idx, logits.shape[-1])
                relevant_logits = logits[0, 0, logit_start_idx:logit_end_idx]
                item_next = mx.random.categorical(
                    relevant_logits * (1 / temperature), num_samples=1
                ).astype(mx.int32)

                item_next += logit_start_idx
                x_coarse_in = mx.concatenate(
                    [x_coarse_in, item_next.reshape(1, 1)], axis=1
                )
                x_in = mx.concatenate([x_in, item_next.reshape(1, 1)], axis=1)
                n_step += 1

        gen_coarse_arr = x_coarse_in[0, len(x_coarse_history) :]
        gen_coarse_audio_arr = (
            gen_coarse_arr.reshape(-1, N_COARSE_CODEBOOKS).T - SEMANTIC_VOCAB_SIZE
        )
        for n in range(1, N_COARSE_CODEBOOKS):
            gen_coarse_audio_arr[n, :] -= n * CODEBOOK_SIZE

        return gen_coarse_audio_arr

    def generate_fine(
        self,
        x_coarse_gen: mx.array,
        temperature: float = 0.7,
        **kwargs,
    ):
        verbose = kwargs.get("verbose", False)
        """Generate fine tokens from coarse tokens."""
        if verbose:
            print("Generating fine tokens...")
        x_fine_history = None
        n_coarse = x_coarse_gen.shape[0]
        in_arr = mx.concatenate(
            [
                x_coarse_gen,
                mx.zeros((N_FINE_CODEBOOKS - n_coarse, x_coarse_gen.shape[1]))
                + CODEBOOK_SIZE,  # padding
            ],
            axis=0,
        )
        n_history = 0
        n_remove_from_end = 0
        # need to pad if too short (since non-causal model)
        if in_arr.shape[1] < 1024:
            n_remove_from_end = 1024 - in_arr.shape[1]
            in_arr = mx.concatenate(
                [
                    in_arr,
                    mx.zeros((N_FINE_CODEBOOKS, n_remove_from_end)) + CODEBOOK_SIZE,
                ],
                axis=1,
            )
        # Inference
        n_loops = (
            max(0, int(math.ceil((x_coarse_gen.shape[1] - (1024 - n_history)) / 512)))
            + 1
        )
        in_arr = in_arr.T
        for n in tqdm.tqdm(range(n_loops), disable=not verbose):
            start_idx = mx.min(mx.array([n * 512, in_arr.shape[0] - 1024])).item()
            start_fill_idx = mx.min(
                mx.array([n_history + n * 512, in_arr.shape[0] - 512])
            ).item()
            rel_start_fill_idx = start_fill_idx - start_idx
            in_buffer = in_arr[start_idx : start_idx + 1024, :][None]
            for nn in range(n_coarse, N_FINE_CODEBOOKS):
                logits = self.model.fine_acoustics(nn, in_buffer)
                if temperature is None:
                    relevant_logits = logits[0, rel_start_fill_idx:, :CODEBOOK_SIZE]
                    codebook_preds = mx.argmax(relevant_logits, -1)
                else:
                    relevant_logits = logits[0, :, :CODEBOOK_SIZE] / temperature
                    codebook_preds = (
                        mx.random.categorical(
                            relevant_logits[rel_start_fill_idx:1024], num_samples=1
                        )
                        .reshape(-1)
                        .astype(mx.int32)
                    )
                in_buffer[0, rel_start_fill_idx:, nn] = codebook_preds
            for nn in range(n_coarse, N_FINE_CODEBOOKS):
                in_arr[
                    start_fill_idx : start_fill_idx + (1024 - rel_start_fill_idx), nn
                ] = in_buffer[0, rel_start_fill_idx:, nn]
        gen_fine_arr = in_arr.squeeze().T
        gen_fine_arr = gen_fine_arr[:, n_history:]
        if n_remove_from_end > 0:
            gen_fine_arr = gen_fine_arr[:, :-n_remove_from_end]
        assert gen_fine_arr.shape[-1] == x_coarse_gen.shape[-1]
        return gen_fine_arr

    def __call__(
        self,
        text: str,
        voice: str = None,
        temperature: float = 0.7,
        speed: float = 1.0,
        use_kv_caching: bool = False,
        **kwargs,
    ):
        semantic_tokens, tokens = self.generate_text_semantic(
            text, voice, temperature, use_kv_caching, **kwargs
        )
        coarse_tokens = self.generate_coarse(
            semantic_tokens, voice, temperature, use_kv_caching, **kwargs
        )
        fine_tokens = self.generate_fine(coarse_tokens, temperature, **kwargs)
        # TODO: adjust speed
        # audio_arr = adjust_speed(fine_tokens, speed)
        audio_arr = codec_decode(self.codec_model, fine_tokens)

        yield Result(audio=audio_arr, tokens=tokens)



================================================
FILE: mlx_audio/tts/models/dia/__init__.py
================================================
from .dia import Model



================================================
FILE: mlx_audio/tts/models/dia/audio.py
================================================
import typing as tp

import mlx.core as mx

from .config import DataConfig


def build_delay_indices(
    B: int, T: int, C: int, delay_pattern: tp.List[int]
) -> tp.Tuple[mx.array, mx.array]:
    """
    Precompute (t_idx_BxTxC, indices_BTCx3) so that out[t, c] = in[t - delay[c], c].
    Negative t_idx => BOS; t_idx >= T => PAD.
    """
    delay_arr = mx.array(delay_pattern, dtype=mx.int32)

    t_idx_BxT = mx.broadcast_to(
        mx.arange(T, dtype=mx.int32)[None, :],
        [B, T],
    )
    t_idx_BxTx1 = mx.expand_dims(t_idx_BxT, -1)
    t_idx_BxTxC = t_idx_BxTx1 - mx.reshape(delay_arr, (1, 1, C))

    b_idx_BxTxC = mx.broadcast_to(
        mx.reshape(mx.arange(B, dtype=mx.int32), (B, 1, 1)),
        [B, T, C],
    )
    c_idx_BxTxC = mx.broadcast_to(
        mx.reshape(mx.arange(C, dtype=mx.int32), (1, 1, C)),
        [B, T, C],
    )

    # We must clamp time indices to [0..T-1] so gather_nd equivalent won't fail
    t_clamped_BxTxC = mx.clip(t_idx_BxTxC, 0, T - 1)

    indices_BTCx3 = mx.stack(
        [
            mx.reshape(b_idx_BxTxC, (-1,)),
            mx.reshape(t_clamped_BxTxC, (-1,)),
            mx.reshape(c_idx_BxTxC, (-1,)),
        ],
        axis=1,
    ).astype(mx.int32)

    return t_idx_BxTxC, indices_BTCx3


def apply_audio_delay(
    audio_BxTxC: mx.array,
    pad_value: int,
    bos_value: int,
    precomp: tp.Tuple[mx.array, mx.array],
) -> mx.array:
    """
    Applies the delay pattern to batched audio tokens using precomputed indices,
    inserting BOS where t_idx < 0 and PAD where t_idx >= T.

    Args:
        audio_BxTxC: [B, T, C] int16 audio tokens (or int32/float)
        pad_value: the padding token
        bos_value: the BOS token
        precomp:  (t_idx_BxTxC, indices_BTCx3) from build_delay_indices

    Returns:
        result_BxTxC: [B, T, C] delayed audio tokens
    """
    t_idx_BxTxC, indices_BTCx3 = precomp

    def gather_nd(array, indices):
        gathered = []
        for idx in range(indices.shape[0]):
            b, t, c = indices[idx, 0], indices[idx, 1], indices[idx, 2]
            gathered.append(array[b, t, c])
        return mx.array(gathered)

    # Apply gather
    gathered_flat = gather_nd(audio_BxTxC, indices_BTCx3)
    gathered_BxTxC = mx.reshape(gathered_flat, audio_BxTxC.shape)

    # Create masks
    mask_bos = t_idx_BxTxC < 0  # => place bos_value
    mask_pad = t_idx_BxTxC >= audio_BxTxC.shape[1]  # => place pad_value

    # Create scalar values
    bos_tensor = mx.full(1, bos_value, dtype=audio_BxTxC.dtype)
    pad_tensor = mx.full(1, pad_value, dtype=audio_BxTxC.dtype)

    # Apply masks (if mask_bos, BOS; else if mask_pad, PAD; else original gather)
    result_BxTxC = mx.where(
        mask_bos, bos_tensor, mx.where(mask_pad, pad_tensor, gathered_BxTxC)
    )

    return result_BxTxC


def audio_to_codebook(
    model,
    input_values,
    data_config: DataConfig,
    padding_mask=None,
    sample_rate=44100,
):
    """
    Encodes the input audio waveform into discrete codes.

    Args:
        model: The model to use for encoding.
        input_values (`mx.array` of shape `(batch_size, channels, sequence_length)`):
            Float values of the input audio waveform.
        padding_mask (`mx.array` of shape `(batch_size, channels, sequence_length)`):
            Padding mask used to pad the `input_values`.
        sample_rate (`int`, *optional*) :
            Signal sampling_rate

    Returns:
        A list of frames containing the discrete encoded codes for the input audio waveform, along with rescaling
        factors for each chunk when `normalize` is True. Each frames is a tuple `(codebook, scale)`, with
        `codebook` of shape `[batch_size, num_codebooks, frames]`.
        Scale is not used here.
    """
    audio_data = model.preprocess(input_values, sample_rate)

    if padding_mask is None:
        padding_mask = mx.ones_like(input_values).astype(mx.bool_)

    _, encoded_frame, _, _, _ = model.encode(audio_data, n_quantizers=None)  # 1, C, T
    seq_length = encoded_frame.shape[2]

    t_idx_BxTxC, indices_BTCx3 = build_delay_indices(
        B=1,
        T=seq_length,
        C=data_config.channels,
        delay_pattern=data_config.delay_pattern,
    )

    encoded_frame = apply_audio_delay(
        audio_BxTxC=mx.transpose(encoded_frame, (0, 2, 1)),  # 1, T, C
        pad_value=data_config.audio_pad_value,
        bos_value=data_config.audio_bos_value,
        precomp=(t_idx_BxTxC, indices_BTCx3),
    )

    return encoded_frame


def build_revert_indices(
    B: int, T: int, C: int, delay_pattern: tp.List[int]
) -> tp.Tuple[mx.array, mx.array]:
    """
    Precompute indices for the revert operation using MLX.

    Returns:
        A tuple (t_idx_BxTxC, indices_BTCx3) where:
            - t_idx_BxTxC is a tensor of shape [B, T, C] computed as time indices plus the delay.
            - indices_BTCx3 is a tensor of shape [B*T*C, 3] used for gathering, computed from:
                batch indices, clamped time indices, and channel indices.
    """
    delay_arr = mx.array(delay_pattern, dtype=mx.int32)

    t_idx_BT1 = mx.broadcast_to(mx.expand_dims(mx.arange(T), 0), [B, T])
    t_idx_BT1 = mx.expand_dims(t_idx_BT1, -1)

    t_idx_BxTxC = mx.minimum(
        t_idx_BT1 + mx.reshape(delay_arr, (1, 1, C)),
        mx.array(T - 1, dtype=mx.int32),
    )
    b_idx_BxTxC = mx.broadcast_to(mx.reshape(mx.arange(B), (B, 1, 1)), [B, T, C])
    c_idx_BxTxC = mx.broadcast_to(mx.reshape(mx.arange(C), (1, 1, C)), [B, T, C])

    indices_BTCx3 = mx.stack(
        [
            mx.reshape(b_idx_BxTxC, (-1,)),
            mx.reshape(t_idx_BxTxC, (-1,)),
            mx.reshape(c_idx_BxTxC, (-1,)),
        ],
        axis=1,
    ).astype(mx.int32)

    return t_idx_BxTxC, indices_BTCx3


def revert_audio_delay(
    audio_BxTxC: mx.array,
    pad_value: int,
    precomp: tp.Tuple[mx.array, mx.array],
    T: int,
) -> mx.array:
    """
    Reverts a delay pattern from batched audio tokens using precomputed indices (MLX version).

    Args:
        audio_BxTxC: Input delayed audio tensor
        pad_value: Padding value for out-of-bounds indices
        precomp: Precomputed revert indices tuple containing:
            - t_idx_BxTxC: Time offset indices tensor
            - indices_BTCx3: Gather indices tensor for original audio
        T: Original sequence length before padding

    Returns:
        Reverted audio tensor with same shape as input
    """
    t_idx_BxTxC, indices_BTCx3 = precomp

    def gather_nd(array, indices):
        gathered = []
        for idx in range(indices.shape[0]):
            b, t, c = indices[idx, 0], indices[idx, 1], indices[idx, 2]
            gathered.append(array[b, t, c])
        return mx.array(gathered)

    gathered_flat = gather_nd(audio_BxTxC, indices_BTCx3)
    gathered_BxTxC = mx.reshape(gathered_flat, audio_BxTxC.shape)
    pad_tensor = mx.full(1, pad_value, dtype=audio_BxTxC.dtype)
    T_tensor = mx.array(T)

    result_BxTxC = mx.where(t_idx_BxTxC >= T_tensor, pad_tensor, gathered_BxTxC)

    return result_BxTxC


def decode(
    model,
    audio_codes,
):
    """
    Decodes the given frames into an output audio waveform
    """

    if len(audio_codes) != 1:
        raise ValueError(f"Expected one frame, got {len(audio_codes)}")

    try:
        audio_values = model.quantizer.from_codes(audio_codes)
        audio_values = model.decode(audio_values[0])

        return audio_values
    except Exception as e:
        print(f"Error in decode method: {str(e)}")
        raise


def codebook_to_audio(
    generated_codes: mx.array, model, delay_pattern, B=1, T=2600, C=9
):
    """Process a single codebook file to generate audio"""
    # Remove BOS token
    generated_codes = generated_codes[:, 1:]

    if generated_codes.shape[1] > T:
        generated_codes = generated_codes[:, :T]

    seq_length = generated_codes.shape[1]

    # Build revert indices
    t_idx_BxTxC, indices_BTCx3 = build_revert_indices(
        B=B, T=seq_length, C=C, delay_pattern=delay_pattern
    )

    # Transpose and add batch dimension
    audio_BxTxC = mx.expand_dims(mx.transpose(generated_codes, (1, 0)), 0)
    reverted_codebook = revert_audio_delay(
        audio_BxTxC=audio_BxTxC,
        pad_value=0,
        precomp=(t_idx_BxTxC, indices_BTCx3),
        T=seq_length,
    )
    reverted_codebook = reverted_codebook[:, :-30, :]

    codebook = mx.transpose(reverted_codebook, (0, 2, 1))

    min_valid_index = 0
    max_valid_index = 1023
    invalid_mask = (codebook < min_valid_index) | (codebook > max_valid_index)

    num_invalid = mx.sum(invalid_mask).item()
    if num_invalid > 0:
        print(
            f"Warning: Clamping {num_invalid} indices outside range [{min_valid_index}, {max_valid_index}] to 0."
        )

    # Set invalid values to 0
    zeros = mx.zeros_like(codebook)
    codebook = mx.where(invalid_mask, zeros, codebook)

    audio_array = decode(model, codebook)

    return audio_array



================================================
FILE: mlx_audio/tts/models/dia/config.py
================================================
"""Configuration management module for the Dia model.

This module provides comprehensive configuration management for the Dia model,
utilizing dataclasses for validation. It defines configurations for data processing,
model architecture (encoder and decoder), and training settings.

Key components:
- DataConfig: Parameters for data loading and preprocessing.
- EncoderConfig: Architecture details for the encoder module.
- DecoderConfig: Architecture details for the decoder module.
- ModelConfig: Combined model architecture settings.
- TrainingConfig: Training hyperparameters and settings.
- DiaConfig: Master configuration combining all components.
"""

import json
import os
from dataclasses import dataclass, field
from typing import List, Optional, Tuple


@dataclass(frozen=True)
class DataConfig:
    """Configuration for data loading and preprocessing.

    Attributes:
        text_length: Maximum length of text sequences (must be multiple of 128).
        audio_length: Maximum length of audio sequences (must be multiple of 128).
        channels: Number of audio channels.
        text_pad_value: Value used for padding text sequences.
        audio_eos_value: Value representing the end of audio sequences.
        audio_bos_value: Value representing the beginning of audio sequences.
        audio_pad_value: Value used for padding audio sequences.
        delay_pattern: List of delay values for each audio channel.
    """

    text_length: int
    audio_length: int
    channels: int = 9
    text_pad_value: int = 0
    audio_eos_value: int = 1024
    audio_pad_value: int = 1025
    audio_bos_value: int = 1026
    delay_pattern: List[int] = field(
        default_factory=lambda: [0, 8, 9, 10, 11, 12, 13, 14, 15]
    )

    def __post_init__(self):
        # Ensure text_length and audio_length are multiples of 128
        object.__setattr__(self, "text_length", (self.text_length + 127) // 128 * 128)
        object.__setattr__(self, "audio_length", (self.audio_length + 127) // 128 * 128)

    def __hash__(self) -> int:
        """Generate a hash based on all fields of the config."""
        return hash(
            (
                self.text_length,
                self.audio_length,
                self.channels,
                self.text_pad_value,
                self.audio_pad_value,
                self.audio_bos_value,
                self.audio_eos_value,
                tuple(self.delay_pattern),
            )
        )


@dataclass(frozen=True)
class EncoderConfig:
    """Configuration for the encoder component of the Dia model.

    Attributes:
        n_layer: Number of transformer layers.
        n_embd: Embedding dimension.
        n_hidden: Hidden dimension size in the MLP layers.
        n_head: Number of attention heads.
        head_dim: Dimension per attention head.
        mlp_activations: List of activation functions for the MLP layers.
        use_pre_norm: Whether to use pre-normalization (LayerNorm before attention/MLP).
    """

    n_layer: int
    n_embd: int
    n_hidden: int
    n_head: int
    head_dim: int
    mlp_activations: List[str] = field(default_factory=lambda: ["silu", "linear"])
    use_pre_norm: bool = False


@dataclass(frozen=True)
class DecoderConfig:
    """Configuration for the decoder component of the Dia model.

    Attributes:
        n_layer: Number of transformer layers.
        n_embd: Embedding dimension.
        n_hidden: Hidden dimension size in the MLP layers.
        gqa_query_heads: Number of query heads for grouped-query self-attention.
        kv_heads: Number of key/value heads for grouped-query self-attention.
        gqa_head_dim: Dimension per query head for grouped-query self-attention.
        cross_query_heads: Number of query heads for cross-attention.
        cross_head_dim: Dimension per cross-attention head.
        mlp_activations: List of activation functions for the MLP layers.
        use_pre_norm: Whether to use pre-normalization.
    """

    n_layer: int
    n_embd: int
    n_hidden: int
    gqa_query_heads: int
    kv_heads: int
    gqa_head_dim: int
    cross_query_heads: int
    cross_head_dim: int
    mlp_activations: List[str] = field(default_factory=lambda: ["silu", "linear"])
    use_pre_norm: bool = False


@dataclass(frozen=True)
class ModelConfig:
    """Main configuration container for the Dia model architecture.

    Attributes:
        encoder: Configuration for the encoder component.
        decoder: Configuration for the decoder component.
        src_vocab_size: Size of the source (text) vocabulary.
        tgt_vocab_size: Size of the target (audio code) vocabulary.
        dropout: Dropout probability applied within the model.
        normalization_layer_epsilon: Epsilon value for normalization layers (e.g., LayerNorm).
        weight_dtype: Data type for model weights (e.g., "float32", "bfloat16").
        rope_min_timescale: Minimum timescale for Rotary Positional Embeddings (RoPE).
        rope_max_timescale: Maximum timescale for Rotary Positional Embeddings (RoPE).
    """

    encoder: EncoderConfig
    decoder: DecoderConfig
    src_vocab_size: int = 128
    tgt_vocab_size: int = 1028
    dropout: float = 0.0
    normalization_layer_epsilon: float = 1.0e-5
    weight_dtype: str = "float32"
    rope_min_timescale: int = 1
    rope_max_timescale: int = 10_000
    sample_rate: int = 44100


@dataclass(frozen=True)
class TrainingConfig:
    """Training process configuration and hyperparameters.

    Note: This configuration currently only includes precision settings.
    Other training parameters (like batch size, learning rate, optimizer settings)
    are assumed to be handled externally.

    Attributes:
        dtype: Data type for activations during training (e.g., "bfloat16", "float32").
        logits_dot_in_fp32: Whether to compute the final logits dot product in fp32 for stability.
    """

    dtype: str = "bfloat16"
    logits_dot_in_fp32: bool = False


@dataclass(frozen=True)
class DiaConfig:
    """Master configuration for the Dia model.

    Combines all sub-configurations into a single validated object.

    Attributes:
        version: Configuration version string.
        model: Model architecture configuration.
        training: Training process configuration (precision settings).
        data: Data loading and processing configuration.
    """

    model: ModelConfig
    training: TrainingConfig
    data: DataConfig
    version: str = "1.0"

    def save(self, path: str) -> None:
        """Save the current configuration instance to a JSON file.

        Ensures the parent directory exists and the file has a .json extension.

        Args:
            path: The target file path to save the configuration.

        Raises:
            ValueError: If the path is not a file with a .json extension.
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        config_dict = {
            "version": self.version,
            "model": {
                "encoder": vars(self.model.encoder),
                "decoder": vars(self.model.decoder),
                "src_vocab_size": self.model.src_vocab_size,
                "tgt_vocab_size": self.model.tgt_vocab_size,
                "dropout": self.model.dropout,
                "normalization_layer_epsilon": self.model.normalization_layer_epsilon,
                "weight_dtype": self.model.weight_dtype,
                "rope_min_timescale": self.model.rope_min_timescale,
                "rope_max_timescale": self.model.rope_max_timescale,
                "sample_rate": self.model.sample_rate,
            },
            "training": vars(self.training),
            "data": vars(self.data),
        }
        with open(path, "w") as f:
            json.dump(config_dict, f, indent=2)

    @classmethod
    def load_dict(cls, config: dict) -> Optional["DiaConfig"]:
        try:
            model_config = ModelConfig(
                encoder=EncoderConfig(**config["model"]["encoder"]),
                decoder=DecoderConfig(**config["model"]["decoder"]),
                **{
                    k: v
                    for k, v in config["model"].items()
                    if k not in ["encoder", "decoder"]
                },
            )
            return cls(
                version=config.get("version", "1.0"),
                model=model_config,
                training=TrainingConfig(**config["training"]),
                data=DataConfig(**config["data"]),
            )
        except (KeyError, TypeError):
            return None

    @classmethod
    def load(cls, path: str) -> Optional["DiaConfig"]:
        """Load and validate a Dia configuration from a JSON file.

        Args:
            path: The path to the configuration file.

        Returns:
            A validated DiaConfig instance if the file exists and is valid,
            otherwise None if the file is not found.

        Raises:
            ValueError: If the JSON content fails validation against the DiaConfig schema.
        """
        try:
            with open(path, "r") as f:
                config = json.load(f)
            return cls.load_dict(config)
        except FileNotFoundError:
            return None



================================================
FILE: mlx_audio/tts/models/dia/dia.py
================================================
import re
import time
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from huggingface_hub import hf_hub_download
from mlx_lm.sample_utils import make_sampler
from tqdm import trange

from mlx_audio.codec.models import DAC

from ..base import GenerationResult
from .audio import audio_to_codebook, codebook_to_audio
from .config import DiaConfig
from .layers import DiaModel, KVCache


def _sample_next_token(
    logits_BCxV: mx.array,
    temperature: float,
    sampler: callable,
) -> mx.array:
    if temperature == 0.0:
        return mx.argmax(logits_BCxV, axis=-1)

    sampled = sampler(logits_BCxV)
    return sampled


class Model(nn.Module):
    def __init__(self, config: dict):
        """Initializes the Dia model.

        Args:
            config: The configuration object for the model.

        Raises:
            RuntimeError: If there is an error loading the DAC model.
        """
        super().__init__()
        self.config = DiaConfig.load_dict(config)
        self.model = DiaModel(self.config)
        self.dac_model = DAC.from_pretrained("mlx-community/descript-audio-codec-44khz")

    @classmethod
    def from_local(cls, config_path: str, checkpoint_path: str) -> "Dia":
        """Loads the Dia model from local configuration and checkpoint files.

        Args:
            config_path: Path to the configuration JSON file.
            checkpoint_path: Path to the model checkpoint (.pth) file.

        Returns:
            An instance of the Dia model loaded with weights and set to eval mode.

        Raises:
            FileNotFoundError: If the config or checkpoint file is not found.
            RuntimeError: If there is an error loading the checkpoint.
        """
        config = DiaConfig.load(config_path)
        if config is None:
            raise FileNotFoundError(f"Config file not found at {config_path}")

        dia = cls(config)

        try:
            weights = mx.load(checkpoint_path)
            dia.model.load_weights(list(weights.items()))
        except FileNotFoundError:
            raise FileNotFoundError(f"Checkpoint file not found at {checkpoint_path}")
        except Exception as e:
            raise RuntimeError(
                f"Error loading checkpoint from {checkpoint_path}"
            ) from e

        dia.dac_model = DAC.from_pretrained("mlx-community/descript-audio-codec-44khz")

        return dia

    @classmethod
    def from_pretrained(cls, model_name: str = "mlx-community/Dia-1.6B") -> "Dia":
        """Loads the Dia model from a Hugging Face Hub repository.

        Downloads the configuration and checkpoint files from the specified
        repository ID and then loads the model.

        Args:
            model_name: The Hugging Face Hub repository ID (e.g., "NariLabs/Dia-1.6B").

        Returns:
            An instance of the Dia model loaded with weights and set to eval mode.

        Raises:
            FileNotFoundError: If config or checkpoint download/loading fails.
            RuntimeError: If there is an error loading the checkpoint.
        """
        config_path = hf_hub_download(repo_id=model_name, filename="config.json")
        checkpoint_path = hf_hub_download(
            repo_id=model_name, filename="model.safetensors"
        )
        return cls.from_local(config_path, checkpoint_path)

    def load_weights(self, weights, strict: bool = True):
        self.model.load_weights(weights, strict=strict)

    def sanitize(self, weights):
        return weights

    def parameters(self):
        return self.model.parameters()

    def eval(self):
        self.model.eval()

    @property
    def sample_rate(self):
        return self.config.model.sample_rate

    def _create_attn_mask(
        self,
        q_padding_mask_1d: mx.array,
        k_padding_mask_1d: mx.array,
        is_causal: bool = False,
    ) -> mx.array:
        """
        Creates the attention mask (self or cross) mimicking JAX segment ID logic.
        """
        B1, Tq = q_padding_mask_1d.shape
        B2, Tk = k_padding_mask_1d.shape
        assert B1 == B2, "Query and key batch dimensions must match"

        p_mask_q = mx.expand_dims(q_padding_mask_1d, 2)  # Shape [B, Tq, 1]
        p_mask_k = mx.expand_dims(k_padding_mask_1d, 1)  # Shape [B, 1, Tk]

        # Condition A: Non-padding query attends to non-padding key
        non_pad_attends_non_pad = mx.logical_and(
            p_mask_q, p_mask_k
        )  # Shape [B, Tq, Tk]

        # Condition B: Padding query attends to padding key
        pad_attends_pad = mx.logical_and(
            mx.logical_not(p_mask_q), mx.logical_not(p_mask_k)
        )  # Shape [B, Tq, Tk]

        # Combine: True if padding status is compatible (both non-pad OR both pad)
        # This implementation follows Jax TPU splash attention kernel
        mask = mx.logical_or(
            non_pad_attends_non_pad, pad_attends_pad
        )  # Shape [B, Tq, Tk]

        if is_causal:
            # Ensure causality for self-attention (Tq == Tk)
            assert (
                Tq == Tk
            ), "Causal mask requires query and key sequence lengths to be equal"
            # Standard lower-triangular causal mask (True means allow)
            causal_mask_2d = mx.tril(
                mx.ones((Tq, Tk), dtype=mx.bool_)
            )  # Shape [Tq, Tk]
            causal_mask = mx.logical_and(mask, causal_mask_2d)  # Shape [B, Tq, Tk]
            return mx.expand_dims(
                causal_mask, 1
            )  # Shape [B, 1, Tq, Tk] for broadcasting across heads
        else:
            # For cross-attention or non-causal self-attention
            return mx.expand_dims(
                mask, 1
            )  # Shape [B, 1, Tq, Tk] for broadcasting across heads

    def _prepare_text_input(
        self, text: str
    ) -> tuple[mx.array, mx.array, mx.array, mx.array]:
        """Encodes text prompt, pads, and creates attention mask and positions."""
        text_pad_value = self.config.data.text_pad_value
        max_len = self.config.data.text_length

        byte_text = text.encode("utf-8")
        replaced_bytes = byte_text.replace(b"[S1]", b"\x01").replace(b"[S2]", b"\x02")
        text_tokens = list(replaced_bytes)

        current_len = len(text_tokens)
        padding_needed = max_len - current_len
        if padding_needed <= 0:
            text_tokens = text_tokens[:max_len]
            padded_text_np = np.array(text_tokens, dtype=np.uint8)
        else:
            padded_text_np = np.pad(
                text_tokens,
                (0, padding_needed),
                mode="constant",
                constant_values=text_pad_value,
            ).astype(np.uint8)

        src_tokens = mx.array(padded_text_np, dtype=mx.int32)
        src_tokens = mx.expand_dims(src_tokens, 0)  # [1, S]
        src_positions = mx.expand_dims(mx.arange(max_len, dtype=mx.int32), 0)  # [1, S]

        src_padding_mask = src_tokens != text_pad_value  # [1, S]

        enc_self_attn_mask = self._create_attn_mask(
            src_padding_mask, src_padding_mask, is_causal=False
        )  # [1, S, S]

        return src_tokens, src_positions, src_padding_mask, enc_self_attn_mask

    def _split_turns(self, text: str) -> List[str]:
        """
        Splits a conversation text into segments each containing a maximum of two [S1]/[S2] chunks.
        """
        pattern = re.compile(
            r"\[S1\]\s*(.*?)\s*\[S2\]\s*(.*?)(?=(?:\[S1\])|$)", re.DOTALL
        )
        segments = []
        for s1_chunk, s2_chunk in pattern.findall(text):
            segments.append(f"[S1] {s1_chunk.strip()} [S2] {s2_chunk.strip()}")

        if len(segments) > 1:
            merged_segments = []
            for i in range(0, len(segments), 2):
                if i + 1 < len(segments):
                    merged_segments.append(f"{segments[i]} {segments[i + 1]}")
                else:
                    merged_segments.append(segments[i])
            segments = merged_segments

        return segments

    def generate(
        self,
        text,
        voice: Optional[str] = None,
        temperature: float = 1.3,
        top_p: float = 0.95,
        split_pattern: str = "\n",
        max_tokens: int | None = None,
        verbose: bool = False,
        ref_audio: Optional[mx.array] = None,
        ref_text: Optional[str] = None,
        **kwargs,
    ):
        prompt = text.replace("\\n", "\n").replace("\\t", "\t")
        prompts = prompt.split(split_pattern)

        segments = []
        for p in prompts:
            if "[S1]" in p and "[S2]" in p:
                segments.extend(self._split_turns(p))
            else:
                segments.append(p)

        for segment_index, segment in enumerate(segments):
            time_start = time.perf_counter()

            audio, token_count = self._generate(
                segment,
                max_tokens=max_tokens,
                ref_audio=ref_audio,
                ref_text=ref_text,
            )

            time_end = time.perf_counter()

            samples = audio.shape[0] if audio is not None else 0
            assert samples > 0, "No audio generated"

            sample_rate = self.config.model.sample_rate
            audio_duration_seconds = samples / sample_rate

            elapsed_time = time_end - time_start
            rtf = (
                elapsed_time / audio_duration_seconds
                if audio_duration_seconds > 0
                else 0
            )

            duration_mins = int(audio_duration_seconds // 60)
            duration_secs = int(audio_duration_seconds % 60)
            duration_ms = int((audio_duration_seconds % 1) * 1000)
            duration_hours = int(audio_duration_seconds // 3600)
            duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

            yield GenerationResult(
                audio=audio,
                samples=samples,
                sample_rate=sample_rate,
                segment_idx=segment_index,
                token_count=token_count,
                audio_duration=duration_str,
                real_time_factor=rtf,
                prompt={
                    "tokens": token_count,
                    "tokens-per-sec": (
                        round(token_count / elapsed_time, 2) if elapsed_time > 0 else 0
                    ),
                },
                audio_samples={
                    "samples": samples,
                    "samples-per-sec": (
                        round(samples / elapsed_time, 2) if elapsed_time > 0 else 0
                    ),
                },
                processing_time_seconds=time_end - time_start,
                peak_memory_usage=mx.get_peak_memory() / 1e9,
            )

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()

    def _generate(
        self,
        text: str,
        max_tokens: Optional[int] = None,
        cfg_scale: float = 3.0,
        temperature: float = 1.3,
        top_p: float = 0.95,
        use_cfg_filter: bool = True,
        cfg_filter_top_k: int = 35,
        ref_audio: Optional[mx.array] = None,
        ref_text: Optional[str] = None,
    ) -> np.ndarray:
        """
        Generates audio from a text prompt (and optional audio prompt) using the Dia model.

        Returns:
            A numpy array of generated audio samples.
        """
        num_channels = self.config.data.channels
        audio_bos_value = int(self.config.data.audio_bos_value)
        audio_eos_value = int(self.config.data.audio_eos_value)
        audio_pad_value = int(self.config.data.audio_pad_value)
        delay_pattern = self.config.data.delay_pattern
        max_tokens = self.config.data.audio_length if max_tokens is None else max_tokens
        delay_tensor = mx.array(delay_pattern, dtype=mx.int32)
        max_delay_pattern = max(delay_pattern)

        if ref_text is not None:
            text = ref_text.strip() + " " + text

        (
            cond_src_BxS,
            cond_src_positions_BxS,
            cond_src_padding_mask_BxS,
            cond_enc_self_attn_mask_Bx1xSxS,
        ) = self._prepare_text_input(text)

        unc_src_BxS = mx.zeros_like(cond_src_BxS)
        src_BxS = mx.concatenate([unc_src_BxS, cond_src_BxS], axis=0)
        src_positions_BxS = mx.concatenate(
            [cond_src_positions_BxS, cond_src_positions_BxS], axis=0
        )
        src_padding_mask_BxS = mx.concatenate(
            [cond_src_padding_mask_BxS, cond_src_padding_mask_BxS], axis=0
        )
        enc_self_attn_mask_Bx1xSxS = mx.concatenate(
            [cond_enc_self_attn_mask_Bx1xSxS, cond_enc_self_attn_mask_Bx1xSxS], axis=0
        )

        # 2. Encoder Pass
        encoder_out = self.model.encoder(
            x_ids=src_BxS,
            src_positions=src_positions_BxS,
            deterministic=True,
            attn_mask=enc_self_attn_mask_Bx1xSxS,
        )  # Shape: (B, S, E)

        # 3. Prepare Decoder Inputs
        # 3-1. Allocate KV Cache (Static)
        decoder_cross_attention_cache: list[KVCache] = (
            self.model.decoder.precompute_cross_attention_kv(
                max_tokens, encoder_out, src_positions_BxS
            )
        )

        decoder_self_attention_cache: list[KVCache] = []
        for _ in range(self.model.decoder.num_layers):
            decoder_self_attention_cache.append(
                KVCache(
                    self.config.model.decoder.gqa_query_heads,
                    max_tokens,
                    self.config.model.decoder.gqa_head_dim,
                )
            )

        # 3-2. Initialize Decoder Inputs
        generated_BxTxC = mx.full(
            (2, 1, num_channels),
            vals=audio_bos_value,
            dtype=mx.int32,
        )

        current_step = 0
        prompt_len_inc_bos = 1  # Start with BOS length

        # 3-3. Load Audio Prompt (if provided)
        if ref_audio is not None:
            audio_prompt = mx.array(ref_audio)[None, None, ...]  # 1, C, T

            audio_prompt_codebook = audio_to_codebook(
                self.dac_model, audio_prompt, data_config=self.config.data
            )
            audio_prompt_codebook = mx.concatenate(
                [audio_prompt_codebook, audio_prompt_codebook], axis=0
            )
            generated_BxTxC = mx.concatenate(
                [generated_BxTxC, audio_prompt_codebook], axis=1
            )

            prefill_len = generated_BxTxC.shape[1]
            prompt_len_inc_bos = prefill_len
            prefill_tgt_pos = mx.broadcast_to(
                mx.expand_dims(mx.arange(prefill_len), 0), (2, prefill_len)
            )
            prefill_tgt_padding_mask = mx.any(
                generated_BxTxC != audio_pad_value, axis=2
            )

            prefill_self_attn_mask = self._create_attn_mask(
                prefill_tgt_padding_mask,
                prefill_tgt_padding_mask,
                is_causal=True,
            )
            prefill_cross_attn_mask = self._create_attn_mask(
                prefill_tgt_padding_mask,
                src_padding_mask_BxS,
                is_causal=False,
            )

            _ = self.model.decoder(
                tgt_ids_BxTxC=generated_BxTxC,
                encoder_out=encoder_out,
                tgt_positions=prefill_tgt_pos,
                src_positions=src_positions_BxS,
                deterministic=True,
                self_attn_mask=prefill_self_attn_mask,
                cross_attn_mask=prefill_cross_attn_mask,
                self_attention_cache=decoder_self_attention_cache,
                cross_attention_cache=decoder_cross_attention_cache,
            )

            current_step = prefill_len - 1

        # 4. Autoregressive Generation Loop
        eos_detected_channel_0 = False
        eos_countdown = -1
        extra_steps_after_eos = 30

        # Make generated_BxTxC a fixed size tensor
        # Length is either 1 + max tokens or 1 + prompt len + max tokens
        padding = mx.full(
            (2, max_tokens, num_channels),
            vals=-1,
            dtype=mx.int32,
        )
        generated_BxTxC = mx.concatenate([generated_BxTxC, padding], axis=1)

        decode_step = self.model.decoder.decode_step

        tgt_padding_mask = mx.any(
            mx.expand_dims(generated_BxTxC[:, -1, :], 1) != audio_pad_value, axis=2
        )  # [B, 1]

        # Generated tokens are never PAD, so we use fixed mask
        decoder_cross_attn_mask = self._create_attn_mask(
            tgt_padding_mask,  # Query mask [B, 1]
            src_padding_mask_BxS,  # Key mask [B, S]
            is_causal=False,
        )  # [B, 1, 1, S]

        top_k = -1
        if use_cfg_filter and cfg_filter_top_k is not None:
            top_k = cfg_filter_top_k
        sampler = make_sampler(temperature, top_p, top_k=top_k)

        for step in trange(current_step, current_step + max_tokens):
            tgt_ids_Bx1xC = mx.expand_dims(generated_BxTxC[:, step, :], 1)
            tgt_pos_Bx1 = mx.full(
                (2, 1),
                vals=step,
                dtype=mx.int32,
            )

            logits_Bx1xCxV = decode_step(
                tgt_ids_Bx1xC=tgt_ids_Bx1xC,
                tgt_pos_Bx1=tgt_pos_Bx1,
                encoder_out=encoder_out,
                self_attn_mask=None,
                cross_attn_mask=decoder_cross_attn_mask,
                self_attention_cache=decoder_self_attention_cache,
                cross_attention_cache=decoder_cross_attention_cache,
            )

            V = self.config.model.tgt_vocab_size
            logits_last_BxCxV = logits_Bx1xCxV[:, -1, :, :]  # B, C, V
            uncond_logits_CxV = logits_last_BxCxV[0, :, :]
            cond_logits_CxV = logits_last_BxCxV[1, :, :]

            cfg_logits_CxV = cond_logits_CxV + cfg_scale * (
                cond_logits_CxV - uncond_logits_CxV
            )

            logits_CxV = mx.reshape(cfg_logits_CxV, (-1, V))  # C, V

            # Create a mask for setting tokens beyond 1025 to -inf
            inf_mask = mx.full(logits_CxV.shape, -float("inf"), dtype=logits_CxV.dtype)
            keep_mask = mx.concatenate(
                [
                    mx.ones((logits_CxV.shape[0], 1025)),
                    mx.zeros((logits_CxV.shape[0], logits_CxV.shape[1] - 1025)),
                ],
                axis=1,
            )
            logits_CxV = mx.where(keep_mask == 1, logits_CxV, inf_mask)

            # Sample next token
            pred_C = _sample_next_token(
                logits_CxV,
                temperature=temperature,
                sampler=sampler,
            )

            generation_step_index = step - current_step
            if ref_audio is None:
                pred_C = mx.where(
                    generation_step_index >= delay_tensor,
                    pred_C,
                    mx.full(pred_C.shape, audio_bos_value, dtype=pred_C.dtype),
                )

            # Update generated tokens for next step
            pred_C_expanded = mx.broadcast_to(
                mx.expand_dims(pred_C, 0), (2, num_channels)
            )

            # Split the tensor into parts: before the update, the update itself, and after the update
            before_update = generated_BxTxC[:, : step + 1, :]
            new_token = mx.expand_dims(
                pred_C_expanded, 1
            )  # Shape: (2, 1, num_channels)
            after_update = generated_BxTxC[:, step + 2 :, :]
            generated_BxTxC = mx.concatenate(
                [before_update, new_token, after_update], axis=1
            )

            if not eos_detected_channel_0 and pred_C[0] == audio_eos_value:
                print(f"EOS detected at step {step} for channel 0")
                eos_detected_channel_0 = True
                eos_countdown = extra_steps_after_eos

            if eos_countdown > 0:
                step_after_eos = max_delay_pattern - eos_countdown
                for i, d in enumerate(delay_pattern):
                    if step_after_eos == d:
                        # Update EOS token
                        # Create new array with updated value at position i in the current sequence
                        eos_values = mx.zeros((2, num_channels), dtype=mx.int32)
                        eos_values = eos_values.at[:, i].add(audio_eos_value)
                        # Replace the values at step+1
                        generated_BxTxC = generated_BxTxC.astype(mx.int32)
                        generated_BxTxC = generated_BxTxC.at[:, step + 1, :].add(
                            eos_values
                        )
                    elif step_after_eos > d:
                        # Update PAD token
                        # Create new array with updated value at position i in the current sequence
                        pad_values = mx.zeros((2, num_channels), dtype=mx.int32)
                        pad_values = pad_values.at[:, i].add(audio_pad_value)
                        # Replace the values at step+1
                        generated_BxTxC = generated_BxTxC.astype(mx.int32)
                        generated_BxTxC = generated_BxTxC.at[:, step + 1, :].add(
                            pad_values
                        )

                eos_countdown -= 1
                if eos_countdown == 0:
                    break

            generation_step_index = step - current_step + 1

        output_codes = generated_BxTxC[:, prompt_len_inc_bos : step + 1, :]
        generated_codes = output_codes[0]

        audio = codebook_to_audio(
            generated_codes.transpose(1, 0),
            self.dac_model,
            delay_pattern,
            B=1,
            T=max_tokens,
            C=num_channels,
        )
        return audio.squeeze(), generation_step_index



================================================
FILE: mlx_audio/tts/models/dia/layers.py
================================================
from typing import Any, List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from einops.array_api import repeat

from .config import DiaConfig


def _normalize_axes(axes: tuple[int, ...], ndim: int) -> tuple[int, ...]:
    return tuple(ax if ax >= 0 else ndim + ax for ax in axes)


def _str_to_dtype(dtype_str: str):
    # Allow None for default behavior
    if dtype_str is None or dtype_str.lower() == "none":
        return None
    if dtype_str == "float32":
        return mx.float32
    elif dtype_str == "float16":
        return mx.float16
    elif dtype_str == "bfloat16":
        return mx.bfloat16
    else:
        raise ValueError(f"Unsupported dtype string: {dtype_str}")


class DenseGeneral(nn.Module):
    def __init__(
        self,
        in_shapes: Tuple[int, ...],
        out_features: Tuple[int, ...],
        axis: Tuple[int, ...] = (-1,),
        dtype: Optional[mx.Dtype] = None,
        weight_dtype: Optional[mx.Dtype] = None,
    ):
        super().__init__()
        self.in_shapes = in_shapes
        self.out_features = out_features
        self.axis = axis
        self.dtype = dtype
        self.kernel_shape = self.in_shapes + self.out_features

        weight_type = weight_dtype if weight_dtype is not None else dtype
        self.weight = mx.zeros(self.kernel_shape, dtype=weight_type)

    def __call__(self, inputs: mx.array) -> mx.array:
        norm_axis = _normalize_axes(self.axis, inputs.ndim)
        kernel_contract_axes = tuple(range(len(norm_axis)))

        output = mx.tensordot(
            inputs,
            self.weight,
            axes=(norm_axis, kernel_contract_axes),
        )

        if self.dtype is not None and output.dtype != self.dtype:
            output = output.astype(self.dtype)

        return output


def get_activation_fn(activation_string: str) -> nn.Module:
    if activation_string == "gelu":
        return nn.GELU()
    elif activation_string == "relu":
        return nn.ReLU()
    elif activation_string == "silu" or activation_string == "swish":
        return nn.SiLU()
    elif activation_string == "linear":
        return nn.Identity()
    else:
        raise ValueError(f"Unsupported activation function: {activation_string}")


class MlpBlock(nn.Module):
    def __init__(
        self,
        config: DiaConfig,
        embed_dim: int,
        intermediate_dim: int,
        dropout_rate: float,
        activations: List[str] = ["silu", "linear"],
        use_pre_norm: bool = False,
    ):
        super().__init__()
        self.use_pre_norm = use_pre_norm
        num_activations = len(activations)

        compute_dtype = _str_to_dtype(config.training.dtype)
        weight_dtype = _str_to_dtype(config.model.weight_dtype)
        self.dtype = compute_dtype

        if use_pre_norm:
            self.pre_norm = nn.RMSNorm(
                embed_dim,
                eps=config.model.normalization_layer_epsilon,
            )

        self.wi_fused = DenseGeneral(
            in_shapes=(embed_dim,),
            out_features=(
                num_activations,
                intermediate_dim,
            ),
            axis=(-1,),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )

        self.activation_fn_0 = get_activation_fn(activations[0])  # silu
        self.activation_fn_1 = get_activation_fn(activations[1])  # linear

        self.dropout = nn.Dropout(dropout_rate)

        self.wo = DenseGeneral(
            in_shapes=(intermediate_dim,),
            out_features=(embed_dim,),
            axis=(-1,),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )

    def __call__(self, x: mx.array, deterministic: bool = False) -> mx.array:
        if self.use_pre_norm and hasattr(self, "pre_norm"):
            x = self.pre_norm(x)

        fused_x = self.wi_fused(x)

        gate_input = fused_x[..., 0, :]
        up_input = fused_x[..., 1, :]

        gate = self.activation_fn_0(gate_input)
        up = self.activation_fn_1(up_input)
        hidden = mx.multiply(gate, up)

        if self.dtype is not None and self.dtype != hidden.dtype:
            hidden = hidden.astype(self.dtype)

        if not deterministic:
            hidden = self.dropout(hidden)

        output = self.wo(hidden)
        return output


class RotaryEmbedding(nn.Module):
    def __init__(
        self,
        embedding_dims: int,
        min_timescale: int = 1,
        max_timescale: int = 10000,
        dtype: mx.Dtype = mx.float32,
    ):
        super().__init__()
        if embedding_dims % 2 != 0:
            raise ValueError("Embedding dim must be even for RoPE.")
        self.embedding_dims = embedding_dims
        self.min_timescale = min_timescale
        self.max_timescale = max_timescale
        self.dtype = dtype
        half_embedding_dim = embedding_dims // 2
        fraction = (2.0 * mx.arange(half_embedding_dim)) / embedding_dims

        self._timescale = (
            self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction
        )

    def __call__(self, inputs: mx.array, position: mx.array):
        """Applies RoPE."""
        position = mx.expand_dims(mx.expand_dims(position, -1), -1)

        sinusoid_inp = position / self._timescale

        sin = mx.sin(sinusoid_inp).astype(inputs.dtype)
        cos = mx.cos(sinusoid_inp).astype(inputs.dtype)

        first_half = inputs[..., : self.embedding_dims // 2]
        second_half = inputs[..., self.embedding_dims // 2 :]

        first_part = first_half * cos - second_half * sin
        second_part = second_half * cos + first_half * sin

        return mx.concatenate([first_part, second_part], axis=-1)


class KVCache:
    def __init__(self, num_heads, max_len, head_dim, k=None, v=None):
        self.k = mx.zeros((2, num_heads, max_len, head_dim)) if k is None else k
        self.v = mx.zeros((2, num_heads, max_len, head_dim)) if v is None else v
        self.current_idx = 0
        self.max_len = max_len

    def update_and_fetch(self, k, v):
        assert self.current_idx < self.max_len
        self.k[:, :, self.current_idx : self.current_idx + 1, :] = k
        self.v[:, :, self.current_idx : self.current_idx + 1, :] = v
        self.current_idx += 1
        return self.k[:, :, : self.current_idx, :], self.v[:, :, : self.current_idx, :]

    def prefill_kv(self, k, v):
        prefill_len = k.shape[2]
        assert prefill_len <= self.max_len
        self.k[:, :, :prefill_len, :] = k
        self.v[:, :, :prefill_len, :] = v
        self.current_idx = prefill_len


class Attention(nn.Module):
    def __init__(
        self,
        config: DiaConfig,
        q_embed_dim: int,
        kv_embed_dim: int,
        num_query_heads: int,
        num_kv_heads: int,
        head_dim: int,
        dropout_rate: float,
        is_cross_attn: bool = False,
        out_embed_dim: Optional[int] = None,
    ):
        super().__init__()
        self.num_query_heads = num_query_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = head_dim
        self.is_cross_attn = is_cross_attn
        self.dropout_rate = dropout_rate

        compute_dtype = _str_to_dtype(config.training.dtype)
        weight_dtype = _str_to_dtype(config.model.weight_dtype)

        self.output_dim = out_embed_dim if out_embed_dim is not None else q_embed_dim
        self.projected_query_dim = num_query_heads * head_dim

        if num_query_heads % num_kv_heads != 0:
            raise ValueError(
                f"num_query_heads ({num_query_heads}) must be divisible by num_kv_heads ({num_kv_heads})"
            )

        self.num_gqa_groups = num_query_heads // num_kv_heads

        # --- Projection Layers using DenseGeneral ---
        self.q_proj = DenseGeneral(
            in_shapes=(q_embed_dim,),
            out_features=(num_query_heads, head_dim),
            axis=(-1,),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )
        self.k_proj = DenseGeneral(
            in_shapes=(kv_embed_dim,),
            out_features=(num_kv_heads, head_dim),
            axis=(-1,),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )
        self.v_proj = DenseGeneral(
            in_shapes=(kv_embed_dim,),
            out_features=(num_kv_heads, head_dim),
            axis=(-1,),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )
        self.o_proj = DenseGeneral(
            in_shapes=(num_query_heads, head_dim),
            out_features=(self.output_dim,),
            axis=(-2, -1),
            dtype=compute_dtype,
            weight_dtype=weight_dtype,
        )

        # --- Rotary Embedding ---
        self.rotary_emb = RotaryEmbedding(
            embedding_dims=self.head_dim,
            min_timescale=config.model.rope_min_timescale,
            max_timescale=config.model.rope_max_timescale,
            dtype=compute_dtype,
        )

    def __call__(
        self,
        Xq: mx.array,  # (B, T, D) T = 1 in AR generation
        Xkv: mx.array,  # (B, S, E) S = 1 in AR generation
        q_positions: mx.array,  # (B, T)
        kv_positions: Optional[mx.array] = None,  # (B, S)
        deterministic: bool = True,
        attn_mask: Optional[
            mx.array
        ] = None,  # None in Decoder Self Attention, Valid mask in Others
        cache: Optional[KVCache] = None,  # None in Encoder, KVCache in Decoder
        prefill: bool = False,  # True only when prefilling KV Cache
    ) -> Tuple[mx.array, Optional[Tuple[mx.array, mx.array]]]:
        """
        Performs attention calculation with optional KV caching.

        Args:
            Xq: Query tensor (B, T, D). T=1 during single-step decoding.
            Xkv: Key/Value source tensor (B, S, E). S=1 during single-step decoding for self-attn.
            q_positions: Positions for queries (B, T).
            kv_positions: Positions for keys/values (B, S). If None, uses q_positions.
            deterministic: If True, disable dropout.
            attn_mask: Attention mask.
            cache: KVCache.
            prefill: If True, use prefill mode.

        Returns:
            A tuple containing:
            - output: The attention output tensor (B, T, output_dim).
            - present_kv: The K/V state to be cached for the next step ((B, N, S_new, H), (B, N, S_new, H)).
              For self-attn, S_new = S_past + S. For cross-attn, S_new = S_kv.
        """
        if kv_positions is None:
            kv_positions = q_positions
        original_dtype = Xq.dtype

        Xq_BxTxNxH = self.q_proj(Xq)
        Xq_BxTxNxH = self.rotary_emb(Xq_BxTxNxH, position=q_positions)
        Xq_BxNxTxH = mx.transpose(Xq_BxTxNxH, (0, 2, 1, 3))

        # Input values into attention calculation
        attn_k = None
        attn_v = None

        # Decoder Cross Attention
        if self.is_cross_attn:
            # Directly use cache (no need to check index)
            attn_k, attn_v = cache.k, cache.v
            if (
                attn_k.shape[1] != self.num_query_heads
                or attn_v.shape[1] != self.num_query_heads
            ):
                raise ValueError(
                    f"Cross-attention cache head dimension ({attn_k.shape[1]}) "
                    f"does not match num_query_heads ({self.num_query_heads}). "
                    "Cache should be pre-repeated for GQA."
                )
        # Self Attention
        else:
            Xk_BxSxKxH = self.k_proj(Xkv)  # (B, S, K, H)
            Xv_BxSxKxH = self.v_proj(Xkv)  # (B, S, K, H)
            Xk_BxSxKxH = self.rotary_emb(
                Xk_BxSxKxH, position=kv_positions
            )  # (B, S, K, H)

            Xk_BxKxSxH = mx.transpose(Xk_BxSxKxH, (0, 2, 1, 3))  # (B, K, S, H)
            Xv_BxKxSxH = mx.transpose(Xv_BxSxKxH, (0, 2, 1, 3))  # (B, K, S, H)
            # S=1 for Decode Step

            if self.num_gqa_groups > 1:
                Xk_BxNxSxH = repeat(
                    Xk_BxKxSxH, "b k s h -> b (k g) s h", g=self.num_gqa_groups
                )
                Xv_BxNxSxH = repeat(
                    Xv_BxKxSxH, "b k s h -> b (k g) s h", g=self.num_gqa_groups
                )
            else:
                Xk_BxNxSxH = Xk_BxKxSxH
                Xv_BxNxSxH = Xv_BxKxSxH

            # Encoder Self Attention
            if cache is None:
                attn_k = Xk_BxNxSxH
                attn_v = Xv_BxNxSxH
            # Decoder Self Attention
            else:
                # In prefill mode, we fill in cache until prefill length
                if prefill:
                    attn_k, attn_v = Xk_BxNxSxH, Xv_BxNxSxH
                    cache.prefill_kv(attn_k, attn_v)
                # In decode step, we add current K/V to cache step by step
                else:
                    attn_k, attn_v = cache.update_and_fetch(Xk_BxNxSxH, Xv_BxNxSxH)

        # Attention Calculation
        attn_scores = mx.matmul(Xq_BxNxTxH, attn_k.swapaxes(2, 3))

        # Apply Scaling
        scale_factor = 1.0
        attn_scores = attn_scores * scale_factor

        # Apply Attention Mask
        if attn_mask is not None:
            # Add large negative value where mask is False/0
            attn_scores = mx.where(
                attn_mask, attn_scores, -1e9
            )  # Using -1e9 for numerical stability

        attn_weights = mx.softmax(attn_scores, axis=-1)
        attn_output = mx.matmul(attn_weights, attn_v)

        attn_output = mx.transpose(attn_output, (0, 2, 1, 3))  # (B, T, N, H)
        output = self.o_proj(attn_output)

        if output.dtype != original_dtype:
            output = output.astype(original_dtype)

        return output


class EncoderLayer(nn.Module):
    def __init__(self, config: DiaConfig):
        super().__init__()
        self.config = config
        model_config = config.model
        enc_config = config.model.encoder
        embed_dim = enc_config.n_embd

        self.pre_sa_norm = nn.RMSNorm(
            embed_dim,
            eps=model_config.normalization_layer_epsilon,
        )

        self.self_attention = Attention(
            config=config,
            q_embed_dim=embed_dim,
            kv_embed_dim=embed_dim,
            num_query_heads=enc_config.n_head,
            num_kv_heads=enc_config.n_head,
            head_dim=enc_config.head_dim,
            dropout_rate=model_config.dropout,
            is_cross_attn=False,
            out_embed_dim=embed_dim,
        )

        self.post_sa_norm = nn.RMSNorm(
            embed_dim,
            eps=model_config.normalization_layer_epsilon,
        )

        self.mlp = MlpBlock(
            config=config,
            embed_dim=embed_dim,
            intermediate_dim=enc_config.n_hidden,
            activations=enc_config.mlp_activations,
            dropout_rate=model_config.dropout,
            use_pre_norm=enc_config.use_pre_norm,
        )

        self.dropout = nn.Dropout(model_config.dropout)

    def __call__(
        self,
        x: mx.array,
        src_positions: Optional[mx.array] = None,
        deterministic: bool = True,
        attn_mask: Optional[mx.array] = None,
    ) -> mx.array:
        residual = x
        x_norm = self.pre_sa_norm(x)

        sa_out = self.self_attention(
            Xq=x_norm,
            Xkv=x_norm,
            q_positions=src_positions,
            kv_positions=src_positions,
            deterministic=deterministic,
            attn_mask=attn_mask,
        )
        x = residual + sa_out

        residual = x
        x_norm = self.post_sa_norm(x)
        mlp_out = self.mlp(x_norm, deterministic=deterministic)
        x = residual + mlp_out

        if not deterministic:
            x = self.dropout(x)

        return x


class Encoder(nn.Module):
    def __init__(self, config: DiaConfig):
        super().__init__()
        self.config = config
        model_config = config.model
        enc_config = config.model.encoder

        self.embedding = nn.Embedding(
            model_config.src_vocab_size,
            enc_config.n_embd,
        )
        self.dropout = nn.Dropout(model_config.dropout)
        self.layers = [EncoderLayer(config=config) for _ in range(enc_config.n_layer)]
        self.norm = nn.RMSNorm(
            enc_config.n_embd,
            eps=model_config.normalization_layer_epsilon,
        )

    def __call__(
        self,
        x_ids: mx.array,
        src_positions: Optional[mx.array] = None,
        deterministic: bool = True,
        attn_mask: Optional[mx.array] = None,
    ) -> mx.array:
        x = self.embedding(x_ids)

        if not deterministic:
            x = self.dropout(x)

        for layer_index, layer in enumerate(self.layers):
            x = layer(
                x,
                src_positions=src_positions,
                deterministic=deterministic,
                attn_mask=attn_mask,
            )

        x = self.norm(x)

        if not deterministic:
            x = self.dropout(x)

        return x


class DecoderLayer(nn.Module):
    def __init__(self, config: DiaConfig):
        super().__init__()
        self.config = config
        model_config = config.model
        dec_config = config.model.decoder
        enc_config = config.model.encoder
        dec_embed_dim = dec_config.n_embd
        enc_embed_dim = enc_config.n_embd

        # Norms
        self.pre_sa_norm = nn.RMSNorm(
            dec_embed_dim,
            eps=model_config.normalization_layer_epsilon,
        )
        self.pre_ca_norm = nn.RMSNorm(
            dec_embed_dim,
            eps=model_config.normalization_layer_epsilon,
        )
        self.pre_mlp_norm = nn.RMSNorm(
            dec_embed_dim,
            eps=model_config.normalization_layer_epsilon,
        )

        # Self-Attention (GQA) with Causal Masking
        self.self_attention = Attention(
            config=config,
            q_embed_dim=dec_embed_dim,
            kv_embed_dim=dec_embed_dim,
            num_query_heads=dec_config.gqa_query_heads,
            num_kv_heads=dec_config.kv_heads,
            head_dim=dec_config.gqa_head_dim,
            dropout_rate=model_config.dropout,
            is_cross_attn=False,
            out_embed_dim=dec_embed_dim,
        )

        # Cross-Attention (MHA)
        self.cross_attention = Attention(
            config=config,
            q_embed_dim=dec_embed_dim,
            kv_embed_dim=enc_embed_dim,  # Note kv_embed_dim
            num_query_heads=dec_config.cross_query_heads,
            num_kv_heads=dec_config.cross_query_heads,
            head_dim=dec_config.cross_head_dim,
            dropout_rate=model_config.dropout,
            is_cross_attn=True,
            out_embed_dim=dec_embed_dim,
        )

        # MLP
        self.mlp = MlpBlock(
            config=config,
            embed_dim=dec_embed_dim,
            intermediate_dim=dec_config.n_hidden,
            activations=dec_config.mlp_activations,
            dropout_rate=model_config.dropout,
            use_pre_norm=dec_config.use_pre_norm,
        )

    def __call__(
        self,
        x: mx.array,
        encoder_out: mx.array,
        tgt_positions: mx.array,
        src_positions: Optional[mx.array],
        deterministic: bool,
        self_attn_mask: mx.array,
        cross_attn_mask: mx.array,
        self_attn_cache: KVCache,
        cross_attn_cache: KVCache,
        prefill: bool = False,
    ) -> Tuple[mx.array, Tuple[mx.array, mx.array]]:
        # 1. Self-Attention
        residual = x
        x_norm = self.pre_sa_norm(x)

        sa_out = self.self_attention(
            Xq=x_norm,  # (2, 1, D)
            Xkv=x_norm,  # (2, 1, D)
            q_positions=tgt_positions,  # (2, 1)
            kv_positions=tgt_positions,  # (2, 1)
            deterministic=deterministic,
            attn_mask=self_attn_mask,  # (2, 1, 1, S_max)
            cache=self_attn_cache,
            prefill=prefill,
        )
        x = residual + sa_out

        # 2. Cross-Attention
        residual = x
        x_norm = self.pre_ca_norm(x)
        ca_out = self.cross_attention(
            Xq=x_norm,
            Xkv=encoder_out,
            q_positions=tgt_positions,
            kv_positions=src_positions,
            deterministic=deterministic,
            attn_mask=cross_attn_mask,
            cache=cross_attn_cache,
        )
        x = residual + ca_out

        # 3. MLP
        residual = x
        x_norm = self.pre_mlp_norm(x)
        mlp_out = self.mlp(x_norm, deterministic=deterministic)
        x = residual + mlp_out

        return x


class Decoder(nn.Module):
    def __init__(self, config: DiaConfig):
        super().__init__()
        self.config = config
        model_config = config.model
        dec_config = config.model.decoder
        train_config = config.training
        data_config = config.data
        weight_dtype = _str_to_dtype(config.model.weight_dtype)
        self.num_channels = data_config.channels
        self.num_layers = dec_config.n_layer

        self.embeddings = [
            nn.Embedding(model_config.tgt_vocab_size, dec_config.n_embd)
            for _ in range(self.num_channels)
        ]
        self.dropout = nn.Dropout(model_config.dropout)
        self.layers = [DecoderLayer(config=config) for _ in range(self.num_layers)]
        self.norm = nn.RMSNorm(
            dec_config.n_embd,
            eps=model_config.normalization_layer_epsilon,
        )

        # Final Logits Projection using DenseGeneral
        self.logits_dense = DenseGeneral(
            in_shapes=(dec_config.n_embd,),
            out_features=(self.num_channels, model_config.tgt_vocab_size),
            axis=(-1,),
            dtype=mx.float32,
            weight_dtype=weight_dtype,
        )
        self.logits_in_fp32 = train_config.logits_dot_in_fp32

    def precompute_cross_attention_kv(
        self,
        max_len: int,
        encoder_out: mx.array,  # (B, S, E)
        src_positions: Optional[mx.array],  # (B, S)
    ) -> List[KVCache]:
        """
        Computes the Key and Value tensors for cross-attention for each layer from the encoder output.
        """
        per_layer_kv_cache: List[KVCache] = []

        for layer in self.layers:
            cross_attn_module = layer.cross_attention
            k_proj = cross_attn_module.k_proj(encoder_out)
            v_proj = cross_attn_module.v_proj(encoder_out)

            k_proj = cross_attn_module.rotary_emb(k_proj, position=src_positions)
            k = mx.transpose(k_proj, (0, 2, 1, 3))  # equivalent to transpose(1, 2)
            v = mx.transpose(v_proj, (0, 2, 1, 3))  # equivalent to transpose(1, 2)

            # Create KVCache without device parameter
            per_layer_kv_cache.append(
                KVCache(
                    cross_attn_module.num_kv_heads,
                    max_len,
                    cross_attn_module.head_dim,
                    k=k,
                    v=v,
                )
            )

        return per_layer_kv_cache

    def decode_step(
        self,
        tgt_ids_Bx1xC: mx.array,  # [B, 1, C]
        tgt_pos_Bx1: mx.array,  # [B, 1]
        encoder_out: mx.array,  # [B, S, E]
        self_attn_mask: Any,  # None
        cross_attn_mask: mx.array,  # [B, 1, 1, S]
        self_attention_cache: List[KVCache],
        cross_attention_cache: List[KVCache],
    ) -> mx.array:
        """
        Performs a single decoding step, managing KV caches layer by layer.

        Returns:
            A tuple containing:
            - logits_Bx1xCV: The final output logits for the current step (B, 1, C*V), cast to float32.
            - new_cache: The updated KV cache for the next decoding step.
        """
        assert (
            self_attn_mask is None
        ), "Self-attention mask should be None, kept for pattern"

        x = None
        for i in range(self.num_channels):
            channel_tokens = tgt_ids_Bx1xC[..., i]
            channel_embed = self.embeddings[i](channel_tokens)
            x = channel_embed if x is None else x + channel_embed

        for i, layer in enumerate(self.layers):
            self_cache = self_attention_cache[i]
            cross_cache = cross_attention_cache[i]
            x = layer(
                x,  # (2, 1, D)
                encoder_out,  # (2, S, E)
                src_positions=None,  # CA KV is already computed
                tgt_positions=tgt_pos_Bx1,  # (2, 1)
                deterministic=True,
                self_attn_mask=None,
                cross_attn_mask=cross_attn_mask,
                self_attn_cache=self_cache,
                cross_attn_cache=cross_cache,
            )

        x = self.norm(x)
        logits_Bx1xCxV = self.logits_dense(x)

        # Convert to float32 if needed
        if logits_Bx1xCxV.dtype != mx.float32:
            logits_Bx1xCxV = logits_Bx1xCxV.astype(mx.float32)

        return logits_Bx1xCxV

    def __call__(
        self,
        tgt_ids_BxTxC: mx.array,
        encoder_out: mx.array,
        tgt_positions: mx.array,
        src_positions: mx.array,
        deterministic: bool,
        self_attn_mask: mx.array,
        cross_attn_mask: mx.array,
        self_attention_cache: List[KVCache],
        cross_attention_cache: List[KVCache],
    ) -> mx.array:
        """
        Forward pass for the Decoder stack, managing KV caches.

        Args:
            tgt_ids_BxTxC: Target token IDs (B, T, C).
            encoder_out: Output from the encoder (B, S, E).
            tgt_positions: Positions for target sequence (B, T).
            src_positions: Positions for source sequence (B, S).
            deterministic: Disable dropout if True.
            self_attn_mask: Mask for self-attention.
            cross_attn_mask: Mask for cross-attention.
            self_attention_cache: List containing the self-attention KV cache for each layer.
            cross_attention_cache: List containing the cross-attention KV cache for each layer.

        Returns:
            logits: The final output logits (B, T, C * V), cast to float32.
        """
        _, _, num_channels_in = tgt_ids_BxTxC.shape
        assert num_channels_in == self.num_channels, "Input channels mismatch"

        # Embeddings
        x = None
        for i in range(self.num_channels):
            channel_tokens = tgt_ids_BxTxC[..., i]
            channel_embed = self.embeddings[i](channel_tokens)
            x = channel_embed if x is None else x + channel_embed

        # Apply dropout if not deterministic
        if not deterministic:
            x = self.dropout(x)

        # Process through each decoder layer
        for i, layer in enumerate(self.layers):
            x = layer(
                x,
                encoder_out,
                tgt_positions=tgt_positions,
                src_positions=src_positions,
                deterministic=deterministic,
                self_attn_mask=self_attn_mask,
                cross_attn_mask=cross_attn_mask,
                self_attn_cache=self_attention_cache[i],
                cross_attn_cache=cross_attention_cache[i],
                prefill=True,
            )

        # Final Norm
        x = self.norm(x)
        logits_BxTxCxV = self.logits_dense(x)

        # Convert to float32 if needed
        if logits_BxTxCxV.dtype != mx.float32:
            logits_BxTxCxV = logits_BxTxCxV.astype(mx.float32)

        return logits_BxTxCxV


class DiaModel(nn.Module):
    def __init__(self, config: DiaConfig):
        super().__init__()
        self.config = config
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)

    def __call__(
        self,
        src_BxS: mx.array,
        tgt_BxTxC: mx.array,
        src_positions: Optional[mx.array] = None,
        tgt_positions: Optional[mx.array] = None,
        enc_self_attn_mask: Optional[mx.array] = None,
        dec_self_attn_mask: Optional[mx.array] = None,
        dec_cross_attn_mask: Optional[mx.array] = None,
        enable_dropout: bool = True,
    ):
        deterministic = not enable_dropout

        # --- Encoder Pass ---
        encoder_out = self.encoder(
            x_ids=src_BxS,
            src_positions=src_positions,
            deterministic=deterministic,
            attn_mask=enc_self_attn_mask,
        )

        # --- Decoder Pass ---
        max_len = self.config.model.max_sequence_length

        self_attention_cache = []

        for layer in self.decoder.layers:
            self_attn_module = layer.self_attention
            self_attention_cache.append(
                KVCache(
                    self_attn_module.num_query_heads, max_len, self_attn_module.head_dim
                )
            )

        logits = self.decoder(
            tgt_ids_BxTxC=tgt_BxTxC,
            encoder_out=encoder_out,
            tgt_positions=tgt_positions,
            src_positions=src_positions,
            deterministic=deterministic,
            self_attn_mask=dec_self_attn_mask,
            cross_attn_mask=dec_cross_attn_mask,
            self_attention_cache=self_attention_cache,
            cross_attention_cache=None,
        )

        return logits



================================================
FILE: mlx_audio/tts/models/indextts/__init__.py
================================================
from mlx_audio.tts.models.indextts.indextts import Model, ModelArgs

__all__ = ["Model", "ModelArgs"]



================================================
FILE: mlx_audio/tts/models/indextts/attention.py
================================================
import math
from typing import Optional

import mlx.core as mx
import mlx.nn as nn


class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        n_head: int,
        n_feat: int,
        bias=True,
        head_dim: Optional[int] = None,
    ):
        super().__init__()

        self.n_head = n_head
        self.head_dim = n_feat // n_head if not head_dim else head_dim
        self.scale = self.head_dim**-0.5

        self.linear_q = nn.Linear(n_feat, self.head_dim * self.n_head, bias=bias)
        self.linear_k = nn.Linear(n_feat, self.head_dim * self.n_head, bias=bias)
        self.linear_v = nn.Linear(n_feat, self.head_dim * self.n_head, bias=bias)
        self.linear_out = nn.Linear(self.head_dim * self.n_head, n_feat, bias=bias)

    def __call__(
        self,
        q: mx.array,
        k: mx.array,
        v: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)

        batch, q_seq, _ = q.shape
        _, k_seq, _ = k.shape

        q = q.reshape(batch, q_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        k = k.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)

        if cache:
            k, v = cache.update_and_fetch(k, v)

        o = mx.fast.scaled_dot_product_attention(q, k, v, scale=self.scale, mask=mask)
        o = o.transpose(0, 2, 1, 3).reshape(batch, q_seq, -1)

        return self.linear_out(o)


class RelPositionMultiHeadAttention(MultiHeadAttention):
    def __init__(
        self,
        n_head: int,
        n_feat: int,
        bias: bool = True,
        head_dim: Optional[int] = None,
        pos_bias_u: mx.array | None = None,
        pos_bias_v: mx.array | None = None,
    ):
        super().__init__(n_head=n_head, n_feat=n_feat, bias=bias, head_dim=head_dim)

        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)

        if pos_bias_u is None:
            self._pos_bias_u_init = mx.zeros((self.n_head, self.head_dim))
        else:
            self._pos_bias_u_init = pos_bias_u

        if pos_bias_v is None:
            self._pos_bias_v_init = mx.zeros((self.n_head, self.head_dim))
        else:
            self._pos_bias_v_init = pos_bias_v

        self.pos_bias_u = self._pos_bias_u_init
        self.pos_bias_v = self._pos_bias_v_init

    def __call__(
        self,
        q: mx.array,
        k: mx.array,
        v: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        if pos_emb is None:
            raise ValueError("pos_emb is necessary!")

        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)

        p = self.linear_pos(pos_emb)  # p stands for position

        batch, q_seq, _ = q.shape
        _, k_seq, _ = k.shape
        _, pos_len, _ = p.shape

        q = q.reshape(batch, q_seq, self.n_head, self.head_dim)
        q_u = (q + self.pos_bias_u).transpose(0, 2, 1, 3)
        q_v = (q + self.pos_bias_v).transpose(0, 2, 1, 3)

        k = k.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(batch, k_seq, self.n_head, self.head_dim).transpose(0, 2, 1, 3)
        p = p.reshape(batch, pos_len, self.n_head, self.head_dim).transpose(0, 2, 1, 3)

        if cache is not None:
            k, v = cache.update_and_fetch(k, v)

        matrix_bd = mx.matmul(q_v, p.swapaxes(-2, -1))
        matrix_bd = matrix_bd * self.scale

        if mask is not None:
            mask = mx.expand_dims(mask, 0)
            matrix_bd[mask] = -mx.inf

        o = mx.fast.scaled_dot_product_attention(
            q_u, k, v, scale=self.scale, mask=matrix_bd
        )
        o = o.transpose(0, 2, 1, 3).reshape(batch, q_seq, -1)

        return self.linear_out(o)


class RelPositionalEncoding(nn.Module):
    def __init__(
        self,
        d_model: int,
        max_len: int = 5000,
        scale_input: bool = True,
    ):
        assert d_model % 2 == 0 and max_len > 0
        super().__init__()

        self.d_model = d_model
        self.max_len = max_len
        self.scale = math.sqrt(self.d_model) if scale_input else 1.0
        self.calculate_pe()

    def calculate_pe(self):
        positions = mx.arange(0, self.max_len, 1, dtype=mx.int32)
        positions = mx.expand_dims(positions, axis=1).astype(mx.float32)

        div_term = mx.exp(
            mx.arange(0, self.d_model, 2, dtype=mx.float32)
            * -(math.log(10000.0) / self.d_model)
        )
        pe = mx.zeros((self.max_len, self.d_model), dtype=mx.float32)

        pe[:, 0::2] = mx.sin(positions * div_term)
        pe[:, 1::2] = mx.cos(positions * div_term)

        self._pe = mx.expand_dims(pe, axis=0).astype(mx.float32)

        mx.eval(self._pe)

    def __call__(self, x: mx.array, offset: int = 0) -> tuple[mx.array, mx.array]:
        input_len = x.shape[1] + offset

        if input_len > self.max_len:
            self.max_len = input_len + 1
            self.calculate_pe()

        x = x * self.scale

        pos_emb = self._pe[:, offset : offset + x.shape[1]].astype(x.dtype)

        return x, pos_emb


class LearnedPositionEncoding(nn.Module):
    def __init__(self, seq_len: int, model_dim: int):
        super().__init__()

        self.emb = nn.Embedding(seq_len, model_dim)

    def __call__(self, x: mx.array, offset: int = 0):
        return self.emb(mx.arange(offset, offset + x.shape[1]))



================================================
FILE: mlx_audio/tts/models/indextts/bigvgan.py
================================================
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten

from mlx_audio.codec.models.bigvgan.bigvgan import BigVGAN, BigVGANConfig
from mlx_audio.codec.models.bigvgan.conv import WNConv1d
from mlx_audio.tts.models.indextts.ecapa_tdnn.ecapa_tdnn import ECPATDNN, ECPATDNNArgs


@dataclass
class BigVGANConditioningConfig(BigVGANConfig):
    gpt_dim: int = 1
    speaker_embedding_dim: int = 1
    cond_d_vector_in_each_upsampling_layer: bool = True


class BigVGANConditioning(BigVGAN):
    def __init__(self, config: BigVGANConditioningConfig):
        super().__init__(config)

        self.conv_pre = WNConv1d(
            config.gpt_dim, config.upsample_initial_channel, 7, 1, 3
        )

        self.cond_in_each_up_layer = config.cond_d_vector_in_each_upsampling_layer

        self.speaker_encoder = ECPATDNN(
            ECPATDNNArgs(config.num_mels, lin_neurons=config.speaker_embedding_dim)
        )
        self.cond_layer = nn.Conv1d(
            config.speaker_embedding_dim, config.upsample_initial_channel, 1
        )

        if config.cond_d_vector_in_each_upsampling_layer:
            self.conds = [
                nn.Conv1d(
                    config.speaker_embedding_dim,
                    config.upsample_initial_channel // (2 ** (i + 1)),
                    1,
                )
                for i in range(len(self.ups))
            ]
        else:
            self.conds = []

    def __call__(
        self, x: mx.array, mel_refer: mx.array
    ) -> mx.array:  # (batch, num_mels, seq)
        x = x.transpose(0, 2, 1)
        mel_refer = mel_refer.transpose(0, 2, 1)

        speaker_embedding = self.speaker_encoder(mel_refer)

        x = self.conv_pre(x)
        x += self.cond_layer(speaker_embedding)

        for step in range(self.num_upsamples):
            for idx in range(len(self.ups[step])):
                x = self.ups[step][idx](x)

            if self.cond_in_each_up_layer:
                x += self.conds[step](speaker_embedding)

            xs = self.resblocks[step * self.num_kernels](x)
            for idx in range(1, self.num_kernels):
                xs += self.resblocks[step * self.num_kernels + idx](x)

            x = xs / self.num_kernels

        x = self.activation_post(x)
        x = self.conv_post(x)

        if self.use_tanh_at_final:
            x = mx.tanh(x)
        else:
            x = mx.clip(x, -1.0, 1.0)

        return x.transpose(0, 2, 1)

    def sanitize(self, weights: dict[str, mx.array]):
        new_weights = {}

        curr_weights = dict(tree_flatten(self.parameters()))

        for key, value in weights.items():
            if "num_batches_tracked" in key:
                continue

            key = (
                key.replace("norm.norm", "norm")
                .replace("conv.conv", "conv")
                .replace("conv1.conv", "conv1")
                .replace("conv2.conv", "conv2")
                .replace("fc.conv", "fc")
                .replace("asp_bn.norm", "asp_bn")
            )

            if (
                "conv" in key
                or "cond_layer" in key
                or "lowpass.filter" in key
                or "upsample.filter" in key
                or "conds" in key
                or "fc" in key
            ):
                if value.ndim == 3:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(0, 2, 1)
                elif value.ndim == 4:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(0, 2, 3, 1)

            if "ups." in key:
                if value.ndim == 3:
                    if value.shape != curr_weights[key].shape:
                        value = value.transpose(1, 2, 0)

            new_weights[key] = value

        del curr_weights

        return new_weights



================================================
FILE: mlx_audio/tts/models/indextts/conformer.py
================================================
from dataclasses import dataclass
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.indextts.attention import (
    MultiHeadAttention,
    RelPositionalEncoding,
    RelPositionMultiHeadAttention,
)


@dataclass
class ConformerArgs:
    input_size: int = 100
    output_size: int = 256
    num_blocks: int = 6
    linear_units: int = 2048
    attention_heads: int = 4
    pos_enc_layer_type: str = "rel_pos"
    input_layer: str = "conv2d"
    cnn_module_kernel: int = 15
    pos_emb_max_len: int = 2048
    causal_downsampling: bool = False
    use_bias: bool = True
    xscaling: bool = True
    macaron_style: bool = False
    pos_bias_u: mx.array | None = None
    pos_bias_v: mx.array | None = None
    perceiver_mult: int = 2


class FeedForward(nn.Module):
    def __init__(self, dim: int, d_ff: int, use_bias: bool = True):
        super().__init__()
        self.w_1 = nn.Linear(dim, d_ff, bias=use_bias)
        self.activation = nn.SiLU()
        self.w_2 = nn.Linear(d_ff, dim, bias=use_bias)

    def __call__(self, x: mx.array) -> mx.array:
        return self.w_2(self.activation(self.w_1(x)))


class Convolution(nn.Module):
    def __init__(self, args: ConformerArgs):
        assert (args.cnn_module_kernel - 1) % 2 == 0
        super().__init__()

        self.pointwise_conv1 = nn.Conv1d(
            args.output_size,
            args.output_size * 2,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=args.use_bias,
        )
        self.depthwise_conv = nn.Conv1d(
            args.output_size,
            args.output_size,
            kernel_size=args.cnn_module_kernel,
            stride=1,
            padding=(args.cnn_module_kernel - 1) // 2,
            groups=args.output_size,
            bias=args.use_bias,
        )
        self.norm = nn.LayerNorm(args.output_size)
        self.activation = nn.SiLU()
        self.pointwise_conv2 = nn.Conv1d(
            args.output_size,
            args.output_size,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=args.use_bias,
        )

    def __call__(self, x: mx.array) -> mx.array:
        x = self.pointwise_conv1(x)
        x = nn.glu(x, axis=2)

        x = self.depthwise_conv(x)
        x = self.norm(x)
        x = self.activation(x)
        x = self.pointwise_conv2(x)

        return x


class ConformerBlock(nn.Module):
    def __init__(self, args: ConformerArgs):
        super().__init__()
        self.macaron_style = args.macaron_style
        self.ff_scale = 0.5 if self.macaron_style else 1
        if args.macaron_style:
            self.norm_ff_macaron = nn.LayerNorm(args.output_size)
            self.feed_forward_macaron = FeedForward(
                args.output_size, args.linear_units, args.use_bias
            )

        self.norm_mha = nn.LayerNorm(args.output_size)
        self.self_attn = (
            RelPositionMultiHeadAttention(
                args.attention_heads,
                args.output_size,
                bias=args.use_bias,
                pos_bias_u=args.pos_bias_u,
                pos_bias_v=args.pos_bias_v,
            )
            if args.pos_enc_layer_type == "rel_pos"
            else MultiHeadAttention(
                args.attention_heads,
                args.output_size,
                bias=True,
            )
        )

        self.norm_conv = nn.LayerNorm(args.output_size)
        self.conv_module = Convolution(args)

        self.norm_ff = nn.LayerNorm(args.output_size)
        self.feed_forward = FeedForward(
            args.output_size, args.linear_units, args.use_bias
        )

        self.norm_final = nn.LayerNorm(args.output_size)

    def __call__(
        self,
        x: mx.array,
        pos_emb: mx.array | None = None,
        mask: mx.array | None = None,
        cache=None,
    ) -> mx.array:
        if self.macaron_style:
            x += self.ff_scale * self.feed_forward_macaron(self.norm_ff_macaron(x))

        x_norm = self.norm_mha(x)
        x += self.self_attn(
            x_norm, x_norm, x_norm, mask=mask, pos_emb=pos_emb, cache=cache
        )

        x += self.conv_module(self.norm_conv(x))
        x += self.ff_scale * self.feed_forward(self.norm_ff(x))

        return self.norm_final(x)


class Conv2dSubsampling(nn.Module):
    CONV_LAYERS = {
        "conv2d2": [(3, 2)],
        "conv2d3": [(5, 3)],
        "conv2d4": [(3, 2), (3, 2)],
        "conv2d6": [(3, 2), (5, 3)],
        "conv2d8": [(3, 2), (3, 2), (3, 2)],
    }
    CONV_MASKS = {
        "conv2d2": [slice(2, None, 2)],
        "conv2d3": [slice(None, -2, 3)],
        "conv2d4": [slice(2, None, 2), slice(2, None, 2)],
        "conv2d6": [slice(2, None, 2), slice(4, None, 3)],
        "conv2d8": [slice(2, None, 2), slice(2, None, 2), slice(2, None, 2)],
    }

    def __init__(self, args: ConformerArgs):
        super().__init__()
        conv_layers = self.CONV_LAYERS[args.input_layer]

        self.mask_patterns = self.CONV_MASKS[args.input_layer]
        self.conv = []
        self.subsampling_rate = 0

        in_channels = 1
        out_freq = args.input_size
        for kernel_size, stride in conv_layers:
            self.conv.append(
                nn.Conv2d(
                    in_channels,
                    args.output_size,
                    kernel_size=kernel_size,
                    stride=stride,
                )
            )
            self.conv.append(nn.ReLU())

            in_channels = args.output_size
            out_freq = (out_freq - kernel_size + stride) // stride
            self.subsampling_rate *= stride

        self.out = [nn.Linear(args.output_size * out_freq, args.output_size)]

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None):
        x = x[:, :, :, None]

        for layer in self.conv:
            x = layer(x)

        x = x.swapaxes(2, 3).reshape(*x.shape[:2], -1)

        for layer in self.out:
            x = layer(x)

        if mask is not None:
            for pattern in self.mask_patterns:
                mask = mask[pattern]

        return x, mask


class Conformer(nn.Module):
    def __init__(self, args: ConformerArgs):
        super().__init__()

        if args.pos_enc_layer_type == "rel_pos":
            self.pos_enc = RelPositionalEncoding(
                d_model=args.output_size,
                max_len=args.pos_emb_max_len,
                scale_input=args.xscaling,
            )
        else:
            self.pos_enc = None

        self.embed = Conv2dSubsampling(args)
        self.encoders = [ConformerBlock(args) for _ in range(args.num_blocks)]
        self.after_norm = nn.LayerNorm(args.output_size, eps=1e-5)

    def __call__(
        self, x: mx.array, mask: Optional[mx.array] = None, cache=None
    ) -> mx.array:
        x, mask = self.embed(x, mask)

        if cache is None:
            cache = [None] * len(self.encoders)

        pos_emb = None
        if self.pos_enc is not None:
            x, pos_emb = self.pos_enc(
                x,
                offset=cache[0].offset if cache[0] is not None else 0,  # type: ignore
            )

        for layer, c in zip(self.encoders, cache):
            x = layer(x, pos_emb=pos_emb, cache=c, mask=mask)

        x = self.after_norm(x)

        return x



================================================
FILE: mlx_audio/tts/models/indextts/gpt2.py
================================================
import mlx.core as mx
import mlx.nn as nn
from mlx_lm.models.base import create_attention_mask
from mlx_lm.models.gpt2 import ModelArgs, TransformerBlock


class GPT2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_embd = args.n_embd
        self.n_positions = args.n_positions
        self.vocab_size = args.vocab_size
        self.n_layer = args.n_layer
        self.layer_norm_epsilon = args.layer_norm_epsilon
        assert self.vocab_size > 0
        self.wte = nn.Embedding(self.vocab_size, self.n_embd)
        self.wpe = nn.Embedding(self.n_positions, self.n_embd)
        self.h = [TransformerBlock(args=args) for _ in range(self.n_layer)]
        self.ln_f = nn.LayerNorm(self.n_embd, eps=self.layer_norm_epsilon)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,  # type: ignore
        cache=None,
    ):
        hidden_states = self.wte(inputs)

        if mask is None:
            mask = create_attention_mask(hidden_states, cache)

        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            hidden_states = layer(hidden_states, mask, cache=c)

        return self.ln_f(hidden_states)



================================================
FILE: mlx_audio/tts/models/indextts/indextts.py
================================================
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List, Optional

import dacite
import huggingface_hub
import mlx.core as mx
import mlx.nn as nn
import sentencepiece as spm
import tqdm
from mlx_lm.models.cache import KVCache
from mlx_lm.models.gpt2 import ModelArgs as GPT2Args
from mlx_lm.sample_utils import make_sampler

from mlx_audio.tts.models.base import GenerationResult
from mlx_audio.tts.models.indextts import normalize
from mlx_audio.tts.models.indextts.attention import LearnedPositionEncoding
from mlx_audio.tts.models.indextts.bigvgan import (
    BigVGANConditioning,
    BigVGANConditioningConfig,
)
from mlx_audio.tts.models.indextts.conformer import Conformer, ConformerArgs
from mlx_audio.tts.models.indextts.gpt2 import GPT2Model
from mlx_audio.tts.models.indextts.mel import log_mel_spectrogram
from mlx_audio.tts.models.indextts.perceiver import PerceiverResampler


@dataclass
class GPTConfig:
    model_dim: int
    heads: int
    layers: int
    max_mel_tokens: int
    max_text_tokens: int

    # special tokens
    number_text_tokens: int
    number_mel_codes: int
    start_mel_token: int
    stop_mel_token: int
    start_text_token: int
    stop_text_token: int

    # conditioner
    use_mel_codes_as_input: bool
    mel_length_compression: int
    condition_type: str
    condition_module: ConformerArgs
    max_conditioning_inputs: int = 1
    condition_num_latent: int = 32


@dataclass
class ModelArgs:
    bigvgan: BigVGANConditioningConfig
    gpt: GPTConfig
    tokenizer_name: str | Path
    sample_rate: int = 24000


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        if isinstance(args, dict):
            args = dacite.from_dict(ModelArgs, args)

        if not args.gpt.use_mel_codes_as_input:
            raise NotImplementedError(
                "use_mel_codes_as_input=false is not supported. Please open a new issue in mlx-audio to get this model supported."
            )
        if args.gpt.condition_type != "conformer_perceiver":
            raise NotImplementedError(
                f"condition_type={args.gpt.condition_type} is not supported. Please open a new issue in mlx-audio to get this model supported."
            )

        self.args = args
        self.sample_rate = args.sample_rate

        try:
            self.tokenizer = spm.SentencePieceProcessor(
                model_file=huggingface_hub.hf_hub_download(  # type: ignore
                    str(args.tokenizer_name), "tokenizer.model"
                )
            )
        except Exception:
            self.tokenizer = spm.SentencePieceProcessor(
                model_file=str(  # type: ignore
                    (Path(args.tokenizer_name) / "tokenizer.model").resolve()
                )
            )

        self.bigvgan = BigVGANConditioning(args.bigvgan)

        self.text_embedding = nn.Embedding(
            args.gpt.number_text_tokens + 1, args.gpt.model_dim
        )
        self.mel_embedding = nn.Embedding(args.gpt.number_mel_codes, args.gpt.model_dim)
        self.mel_pos_embedding = LearnedPositionEncoding(
            args.gpt.max_mel_tokens + 2 + args.gpt.max_conditioning_inputs,
            args.gpt.model_dim,
        )
        self.text_pos_embedding = LearnedPositionEncoding(
            args.gpt.max_text_tokens + 2, args.gpt.model_dim
        )

        self.text_head = nn.Linear(args.gpt.model_dim, args.gpt.number_text_tokens + 1)
        self.mel_head = nn.Linear(args.gpt.model_dim, args.gpt.number_mel_codes)

        self.conditioning_encoder = Conformer(args.gpt.condition_module)
        self.perceiver_encoder = PerceiverResampler(
            args.gpt.model_dim,
            n_dim_context=args.gpt.condition_module.output_size,
            n_ff_mult=args.gpt.condition_module.perceiver_mult,
            n_heads=args.gpt.condition_module.attention_heads,
            n_latents=args.gpt.condition_num_latent,
        )
        self.gpt = GPT2Model(
            GPT2Args(
                "gpt2",
                1,
                args.gpt.model_dim,
                args.gpt.heads,
                args.gpt.layers,
                1,
                1e-5,
                1,
            )
        )

        self.final_norm = nn.LayerNorm(args.gpt.model_dim)

        # patching
        self.gpt.wpe = nn.Identity()  # type: ignore
        self.gpt.wte = nn.Identity()  # type: ignore

    def sanitize(self, weights: dict[str, mx.array]):
        already_sanitized = all(
            ("num_batches_tracked" not in key) for key in weights.keys()
        )
        if already_sanitized:
            return weights

        bigvgan_prefixes = [
            "ups.",
            "speaker_encoder.",
            "resblocks.",
            "conv_pre.",
            "conv_post.",
            "conds.",
            "cond_layer.",
            "activation_post.",
        ]

        gpt_weights = {
            k: v
            for k, v in weights.items()
            if not any(k.startswith(prefix) for prefix in bigvgan_prefixes)
        }
        bigvgan_weights = {
            k: v
            for k, v in weights.items()
            if any(k.startswith(prefix) for prefix in bigvgan_prefixes)
        }

        new_gpt_weights = {}

        for key, value in gpt_weights.items():
            if "pos_enc" in key:
                continue  # it should calculate self

            if "conv" in key:
                if value.ndim == 3:
                    value = value.transpose(0, 2, 1)
                elif value.ndim == 4:
                    value = value.transpose(0, 2, 3, 1)

            if "perceiver_encoder.norm.gamma" in key:
                key = "perceiver_encoder.norm.weight"

            new_gpt_weights[key] = value

        for i in range(self.args.gpt.layers):
            if f"gpt.h.{i}.attn.bias" in new_gpt_weights:
                del new_gpt_weights[f"gpt.h.{i}.attn.bias"]
            if f"gpt.h.{i}.attn.c_attn.weight" in new_gpt_weights:
                new_gpt_weights[f"gpt.h.{i}.attn.c_attn.weight"] = new_gpt_weights[
                    f"gpt.h.{i}.attn.c_attn.weight"
                ].transpose(1, 0)
            if f"gpt.h.{i}.attn.c_proj.weight" in new_gpt_weights:
                new_gpt_weights[f"gpt.h.{i}.attn.c_proj.weight"] = new_gpt_weights[
                    f"gpt.h.{i}.attn.c_proj.weight"
                ].transpose(1, 0)
            if f"gpt.h.{i}.mlp.c_fc.weight" in new_gpt_weights:
                new_gpt_weights[f"gpt.h.{i}.mlp.c_fc.weight"] = new_gpt_weights[
                    f"gpt.h.{i}.mlp.c_fc.weight"
                ].transpose(1, 0)
            if f"gpt.h.{i}.mlp.c_proj.weight" in new_gpt_weights:
                new_gpt_weights[f"gpt.h.{i}.mlp.c_proj.weight"] = new_gpt_weights[
                    f"gpt.h.{i}.mlp.c_proj.weight"
                ].transpose(1, 0)

        for i in range(2):  # hard coded in original impl
            if f"perceiver_encoder.layers.{i}.0.to_q.weight" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.0.linear_q.weight"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_q.weight"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_q.weight"]
            if f"perceiver_encoder.layers.{i}.0.to_kv.weight" in new_gpt_weights:
                (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.0.linear_k.weight"],
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.0.linear_v.weight"],
                ) = mx.split(
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_kv.weight"],
                    2,
                    axis=0,
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_kv.weight"]
            if f"perceiver_encoder.layers.{i}.0.to_out.weight" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.0.linear_out.weight"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_out.weight"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.0.to_out.weight"]

            if f"perceiver_encoder.layers.{i}.1.0.weight" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.1.w_1.weight"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.1.0.weight"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.1.0.weight"]
            if f"perceiver_encoder.layers.{i}.1.2.weight" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.1.w_2.weight"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.1.2.weight"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.1.2.weight"]
            if f"perceiver_encoder.layers.{i}.1.0.bias" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.1.w_1.bias"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.1.0.bias"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.1.0.bias"]
            if f"perceiver_encoder.layers.{i}.1.2.bias" in new_gpt_weights:
                new_gpt_weights[f"perceiver_encoder.layers.{i}.1.w_2.bias"] = (
                    new_gpt_weights[f"perceiver_encoder.layers.{i}.1.2.bias"]
                )
                del new_gpt_weights[f"perceiver_encoder.layers.{i}.1.2.bias"]

        new_bigvgan_weight = {
            "bigvgan." + k: v for k, v in self.bigvgan.sanitize(bigvgan_weights).items()
        }

        return {**new_gpt_weights, **new_bigvgan_weight}

    def get_conditioning(self, mel: mx.array) -> mx.array:  # (b, c, t)
        latent = self.conditioning_encoder(mel)
        return self.perceiver_encoder(latent)

    def prepare_input_embedding(
        self,
        prompts: List[str],
        ref_audio: Optional[mx.array],
        ref_mel: Optional[mx.array] = None,
    ) -> mx.array:
        if ref_audio is not None:
            ref_mel = log_mel_spectrogram(ref_audio)

        if ref_mel is None:
            raise ValueError("Must provide one of ref_audio or ref_mel")

        conditioning = self.get_conditioning(ref_mel)
        # for case with multiple batch, and single ref_audio
        conditioning = mx.repeat(conditioning, len(prompts), axis=0)

        tokenized = [
            self.tokenizer.encode(
                normalize.tokenize_by_CJK_char(normalize.normalize(prompt))
            )
            for prompt in prompts
        ]  # type: ignore

        longest = max((len(tokens) for tokens in tokenized)) + 3

        embedding = mx.zeros(
            (len(tokenized), longest + conditioning.shape[1], self.args.gpt.model_dim)
        )

        for idx, tokens in enumerate(tokenized):
            # append tokens
            tokens.insert(0, self.args.gpt.start_text_token)
            tokens.append(self.args.gpt.stop_text_token)
            tokens.append(self.args.gpt.start_mel_token)
            length = len(tokens)

            tokens = mx.array(tokens)[None, :]

            text_embedding = self.text_embedding(tokens) + self.text_pos_embedding(
                tokens
            )
            embedding[idx : idx + 1, longest - length :, :] = mx.concat(
                [conditioning, text_embedding], axis=1
            )

        return embedding

    def generate_result(
        self,
        audio: mx.array,
        start_time: float,
        token_count: int,
        **kwargs,
    ) -> GenerationResult:
        audio = audio.squeeze(0).squeeze(0)

        samples = audio.shape[0] if audio is not None else 0
        assert samples > 0, "No audio generated"

        sample_rate = self.sample_rate
        audio_duration_seconds = samples / sample_rate

        elapsed_time = time.perf_counter() - start_time
        rtf = audio_duration_seconds / elapsed_time

        duration_mins = int(audio_duration_seconds // 60)
        duration_secs = int(audio_duration_seconds % 60)
        duration_ms = int((audio_duration_seconds % 1) * 1000)
        duration_hours = int(audio_duration_seconds // 3600)
        duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

        return GenerationResult(
            audio=audio,
            samples=samples,
            sample_rate=sample_rate,
            segment_idx=0,
            token_count=token_count,
            audio_duration=duration_str,
            real_time_factor=rtf,
            prompt={
                "tokens": token_count,
                "tokens-per-sec": (
                    round(token_count / elapsed_time, 2) if elapsed_time > 0 else 0
                ),
            },
            audio_samples={
                "samples": samples,
                "samples-per-sec": (
                    round(samples / elapsed_time, 2) if elapsed_time > 0 else 0
                ),
            },  # type: ignore
            processing_time_seconds=elapsed_time,
            peak_memory_usage=mx.get_peak_memory() / 1e9,
        )

    def generate(
        self,
        text: str,
        ref_audio: Optional[mx.array],
        ref_mel: Optional[mx.array] = None,
        verbose: bool = False,
        max_tokens: int = 5000,
        sampler: Optional[Callable[..., mx.array]] = None,
        **kwargs,
    ):
        if ref_audio is not None:
            ref_mel = log_mel_spectrogram(ref_audio)

        if ref_mel is None:
            raise ValueError("Must provide one of ref_audio or ref_mel")

        time_start = time.perf_counter()

        embedding = self.prepare_input_embedding([text], None, ref_mel)

        cache = [KVCache() for _ in range(self.args.gpt.layers)]
        sampler = sampler or make_sampler(temp=0.8, top_k=30)

        inputs = embedding
        generated_tokens = []
        latent_states = []

        mel_position = 0

        for _ in range(max_tokens) if not verbose else tqdm.trange(max_tokens):
            hidden_states = self.gpt(inputs, cache=cache)

            hidden_states = self.final_norm(hidden_states)

            latent_states.append(hidden_states[:, -1:, :])
            mel_logits = self.mel_head(hidden_states[:, -1:, :])

            next_token = sampler(mel_logits)

            if next_token.item() == self.args.gpt.stop_mel_token:
                break

            generated_tokens.append(next_token.item())

            mel_emb = self.mel_embedding(next_token) + self.mel_pos_embedding(
                next_token, embedding.shape[1] + mel_position
            )

            inputs = mel_emb
            mel_position += 1

        latent_states = mx.concat(latent_states, axis=-2)

        audio = self.bigvgan(
            latent_states.transpose(0, 2, 1),
            ref_mel.transpose(0, 2, 1),
        )

        yield self.generate_result(audio, time_start, latent_states.shape[1])

        mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/indextts/mel.py
================================================
import mlx.core as mx

from mlx_audio.utils import mel_filters, stft


def log_mel_spectrogram(
    audio: mx.array,
    sample_rate: int = 24_000,
    n_mels: int = 100,
    n_fft: int = 1024,
    hop_length: int = 256,
    padding: int = 0,
):
    if not isinstance(audio, mx.array):
        audio = mx.array(audio)

    if padding > 0:
        audio = mx.pad(audio, (0, padding))

    freqs = stft(
        audio,
        window="hann",
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=n_fft,
    )
    magnitudes = freqs.abs()
    filters = mel_filters(
        sample_rate=sample_rate,
        n_fft=n_fft,
        n_mels=n_mels,
        norm=None,
        mel_scale="htk",
    )
    mel_spec = magnitudes @ filters.T
    log_spec = mx.maximum(mel_spec, 1e-5).log()
    return mx.expand_dims(log_spec, axis=0)



================================================
FILE: mlx_audio/tts/models/indextts/normalize.py
================================================
import re
from typing import Dict, List, Tuple

CHAR_MAP = {
    "ï¼š": ",",
    "ï¼›": ",",
    ";": ",",
    "ï¼Œ": ",",
    "ã€‚": ".",
    "ï¼": "!",
    "ï¼Ÿ": "?",
    "\n": " ",
    "Â·": "-",
    "ã€": ",",
    "...": "â€¦",
    ",,,": "â€¦",
    "ï¼Œï¼Œï¼Œ": "â€¦",
    "â€¦â€¦": "â€¦",
    """: "'", """: "'",
    '"': "'",
    "'": "'",
    "ï¼ˆ": "'",
    "ï¼‰": "'",
    "(": "'",
    ")": "'",
    "ã€Š": "'",
    "ã€‹": "'",
    "ã€": "'",
    "ã€‘": "'",
    "[": "'",
    "]": "'",
    "â€”": "-",
    "ï½ž": "-",
    "~": "-",
    "ã€Œ": "'",
    "ã€": "'",
    ":": ",",
}

ZH_CHAR_MAP = {"$": ".", **CHAR_MAP}

PINYIN_PATTERN = r"(?<![a-z])((?:[bpmfdtnlgkhjqxzcsryw]|[zcs]h)?(?:[aeiouÃ¼v]|[ae]i|u[aio]|ao|ou|i[aue]|[uÃ¼v]e|[uvÃ¼]ang?|uai|[aeiuv]n|[aeio]ng|ia[no]|i[ao]ng)|ng|er)([1-5])"
NAME_PATTERN = r"[\u4e00-\u9fff]+(?:[-Â·â€”][\u4e00-\u9fff]+){1,2}"
CONTRACTION_PATTERN = r"(what|where|who|which|how|t?here|it|s?he|that|this)'s"
EMAIL_PATTERN = r"^[a-zA-Z0-9]+@[a-zA-Z0-9]+\.[a-zA-Z]+$"


def is_email(text: str) -> bool:
    return bool(re.match(EMAIL_PATTERN, text))


def has_chinese(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text))


def has_alpha(text: str) -> bool:
    return bool(re.search(r"[a-zA-Z]", text))


def has_pinyin(text: str) -> bool:
    return bool(re.search(PINYIN_PATTERN, text, re.IGNORECASE))


def use_chinese(text: str) -> bool:
    return (
        has_chinese(text) or not has_alpha(text) or is_email(text) or has_pinyin(text)
    )


def replace_chars(text: str, char_map: Dict[str, str]) -> str:
    pattern = re.compile("|".join(re.escape(p) for p in char_map.keys()))
    return pattern.sub(lambda x: char_map[x.group()], text)


def extract_all_digits(text):
    return "".join(filter(str.isdigit, text))


def expand_contractions(text: str) -> str:
    return re.sub(CONTRACTION_PATTERN, r"\1 is", text, flags=re.IGNORECASE)


def correct_pinyin(pinyin: str) -> str:
    if pinyin[0] not in "jqxJQX":
        return pinyin
    return re.sub(
        r"([jqx])[uÃ¼](n|e|an)*(\d)", r"\g<1>v\g<2>\g<3>", pinyin, flags=re.IGNORECASE
    ).upper()


def extract_patterns(text: str, pattern: str) -> List[str]:
    matches = re.findall(re.compile(pattern, re.IGNORECASE), text)
    return list(set("".join(m) for m in matches))


def create_placeholders(items: List[str], prefix: str) -> Dict[str, str]:
    return {item: f"<{prefix}_{chr(ord('a') + i)}>" for i, item in enumerate(items)}


def apply_placeholders(text: str, placeholders: Dict[str, str]) -> str:
    result = text
    for original, placeholder in placeholders.items():
        result = result.replace(original, placeholder)
    return result


def restore_placeholders(
    text: str, placeholders: Dict[str, str], transform_fn=None
) -> str:
    result = text
    for original, placeholder in placeholders.items():
        replacement = transform_fn(original) if transform_fn else original
        result = result.replace(placeholder, replacement)
    return result


def save_and_replace(
    text: str, pattern: str, prefix: str
) -> Tuple[str, Dict[str, str]]:
    items = extract_patterns(text, pattern)
    if not items:
        return text, {}
    placeholders = create_placeholders(items, prefix)
    return apply_placeholders(text, placeholders), placeholders


# number normalizers
def number_to_words(n: int):
    ones = ["", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine"]
    teens = [
        "ten",
        "eleven",
        "twelve",
        "thirteen",
        "fourteen",
        "fifteen",
        "sixteen",
        "seventeen",
        "eighteen",
        "nineteen",
    ]
    tens = [
        "",
        "",
        "twenty",
        "thirty",
        "forty",
        "fifty",
        "sixty",
        "seventy",
        "eighty",
        "ninety",
    ]
    thousands = ["", "thousand", "million", "billion", "trillion"]

    def convert_hundreds(num):
        if num == 0:
            return ""
        elif num < 10:
            return ones[num]
        elif num < 20:
            return teens[num - 10]
        elif num < 100:
            return tens[num // 10] + (" " + ones[num % 10] if num % 10 else "")
        else:
            return (
                ones[num // 100]
                + " hundred"
                + (" " + convert_hundreds(num % 100) if num % 100 else "")
            )

    def convert_number(num: int):
        if num == 0:
            return "zero"

        groups = []
        group_idx = 0

        while num > 0:
            group = num % 1000
            if group != 0:
                group_words = convert_hundreds(group)
                if thousands[group_idx]:
                    group_words += " " + thousands[group_idx]
                groups.append(group_words)
            num //= 1000
            group_idx += 1

        return " ".join(reversed(groups))

    return convert_number(n)


# @lru_cache(maxsize=1)
# def get_normalizers():
#     """Lazy load normalizers"""
#     from wetext import Normalizer  # type: ignore

#     return (
#         Normalizer(remove_erhua=False, lang="zh", operator="tn"),
#         Normalizer(lang="en", operator="tn"),
#     )


def normalize_chinese(text: str) -> str:
    # zh_normalizer, _ = get_normalizers()

    text = expand_contractions(text.rstrip())
    text, pinyin_map = save_and_replace(text, PINYIN_PATTERN, "pinyin")
    text, name_map = save_and_replace(text, NAME_PATTERN, "n")

    try:
        result = text  # TODO: improve Chinese normalizers
        # result = zh_normalizer.normalize(text)
    except Exception:
        return ""

    result = restore_placeholders(result, name_map)
    result = restore_placeholders(result, pinyin_map, correct_pinyin)
    result = replace_chars(result, ZH_CHAR_MAP)

    return result


def normalize_english(text: str) -> str:
    # _, en_normalizer = get_normalizers()

    text = expand_contractions(text)

    try:
        # currently dollar only
        def process_currency(match):
            digits = extract_all_digits(match.group(0))
            if not digits:
                return match.group(0)

            num = int(digits)
            word_form = number_to_words(num)

            return f"{word_form} dollar{'s' if num != 1 else ''} "

        text = re.sub(r"\$\s*[0-9,.\s]+", process_currency, text).rstrip()

        def process_digits(match):
            parts = match.group(0).split()
            if all(len(part) == 1 and part.isdigit() for part in parts):
                return " ".join(number_to_words(int(digit)) for digit in parts)
            return number_to_words(int(extract_all_digits(match.group(0))))

        text = re.sub(r"\b\d(\s+\d)+\b", process_digits, text)

        def process_number(match):
            digits = extract_all_digits(match.group(0))
            if digits:
                return number_to_words(int(digits))
            return match.group(0)

        text = re.sub(r"\b\d+(?:,\d+)*\b", process_number, text)

        result = re.sub(r"\s+", " ", text).strip()
    except Exception:
        result = text

    return replace_chars(result, CHAR_MAP)


def normalize(text: str) -> str:
    normalize_fn = normalize_chinese if use_chinese(text) else normalize_english
    return normalize_fn(text)


def tokenize_by_CJK_char(line: str, do_upper_case=True) -> str:
    """
    Tokenize a line of text with CJK char.

    Note: All return charaters will be upper case.

    Example:
      input = "ä½ å¥½ä¸–ç•Œæ˜¯ hello world çš„ä¸­æ–‡"
      output = "ä½  å¥½ ä¸– ç•Œ æ˜¯ HELLO WORLD çš„ ä¸­ æ–‡"

    Args:
      line:
        The input text.

    Return:
      A new string tokenize by CJK char.
    """
    # The CJK ranges is from https://github.com/alvations/nltk/blob/79eed6ddea0d0a2c212c1060b477fc268fec4d4b/nltk/tokenize/util.py
    CJK_RANGE_PATTERN = r"([\u1100-\u11ff\u2e80-\ua4cf\ua840-\uD7AF\uF900-\uFAFF\uFE30-\uFE4F\uFF65-\uFFDC\U00020000-\U0002FFFF])"
    chars = re.split(CJK_RANGE_PATTERN, line.strip())
    return " ".join(
        [w.strip().upper() if do_upper_case else w.strip() for w in chars if w.strip()]
    )



================================================
FILE: mlx_audio/tts/models/indextts/perceiver.py
================================================
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.indextts.attention import MultiHeadAttention


# gated gelu feedforward
class FeedForward(nn.Module):
    def __init__(self, dim: int, d_ff: int, use_bias: bool = True):
        super().__init__()
        self.w_1 = nn.Linear(dim, d_ff * 2, bias=use_bias)
        self.activation = nn.GELU()
        self.w_2 = nn.Linear(d_ff, dim, bias=use_bias)

    def __call__(self, x: mx.array) -> mx.array:
        x, gate = mx.split(self.w_1(x), 2, axis=-1)
        return self.w_2(self.activation(gate) * x)


class PerceiverResampler(nn.Module):
    def __init__(
        self,
        n_dim: int,
        n_depth=2,
        n_dim_context: Optional[int] = None,
        n_latents=32,
        n_dim_head=64,
        n_heads=8,
        n_ff_mult=4,
    ):
        super().__init__()

        n_dim_context = n_dim if n_dim_context is None else n_dim_context

        self.proj_context = (
            nn.Linear(n_dim_context, n_dim) if n_dim_context != n_dim else nn.Identity()
        )
        self.latents = mx.zeros((n_latents, n_dim))
        self.layers = [
            [
                MultiHeadAttention(n_heads, n_dim, False, n_dim_head),
                FeedForward(n_dim, (n_dim * n_ff_mult * 2) // 3),
            ]
            for _ in range(n_depth)
        ]
        self.norm = nn.RMSNorm(n_dim)

    def __call__(self, x, mask=None):
        B = x.shape[0]

        latents = mx.broadcast_to(self.latents, (B, *self.latents.shape))

        x = self.proj_context(x)

        for attn, ff in self.layers:
            kv = mx.concat([x, latents], axis=-2)
            latents += attn(latents, kv, kv, mask=mask)
            latents += ff(latents)

        return self.norm(latents)



================================================
FILE: mlx_audio/tts/models/indextts/ecapa_tdnn/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/models/indextts/ecapa_tdnn/asp.py
================================================
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.indextts.ecapa_tdnn.tdnn import TDNN


class AttentiveStatisticsPooling(nn.Module):
    def __init__(
        self, channels: int, attention_channels: int, global_context: bool = True
    ):
        super().__init__()

        self.eps = 1e-12
        self.global_context = global_context

        self.tdnn = TDNN(
            channels * 3 if global_context else channels, attention_channels, 1
        )
        self.tanh = nn.Tanh()
        self.conv = nn.Conv1d(attention_channels, channels, 1)

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None):  # NLC
        N, L, C = x.shape

        if mask is not None:
            mask = mask[:, :, None]
        else:
            mask = mx.ones((N, L, 1))

        if self.global_context:
            global_mean = (x * mask).sum(1, keepdims=True) / (
                mask.sum(1, keepdims=True) + self.eps
            )
            global_std = mx.sqrt(
                ((x - global_mean) ** 2 * mask).sum(1, keepdims=True)
                / (mask.sum(1, keepdims=True) + self.eps)
                + self.eps
            )
            attn = mx.concat(
                [
                    x,
                    mx.repeat(global_mean, L, axis=1),
                    mx.repeat(global_std, L, axis=1),
                ],
                axis=2,
            )
        else:
            attn = x

        attn = self.conv(self.tanh(self.tdnn(attn)))

        attn = mx.softmax(mx.where(mask == 0, -mx.inf, attn), axis=1)

        mean = (x * attn).sum(1, keepdims=True)
        std = mx.sqrt(((x - mean) ** 2 * attn).sum(1, keepdims=True) + self.eps)

        return mx.concat([mean, std], axis=2)



================================================
FILE: mlx_audio/tts/models/indextts/ecapa_tdnn/ecapa_tdnn.py
================================================
from dataclasses import dataclass, field
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.indextts.ecapa_tdnn.asp import AttentiveStatisticsPooling
from mlx_audio.tts.models.indextts.ecapa_tdnn.se_res2net import SeRes2Net
from mlx_audio.tts.models.indextts.ecapa_tdnn.tdnn import TDNN


@dataclass
class ECPATDNNArgs:
    input_size: int
    lin_neurons: int = 192
    channels: list[int] = field(default_factory=lambda: [512, 512, 512, 512, 1536])
    kernel_sizes: list[int] = field(default_factory=lambda: [5, 3, 3, 3, 1])
    dilations: list[int] = field(default_factory=lambda: [1, 2, 3, 4, 1])
    attention_channels: int = 128
    res2net_scale: int = 8
    se_channels: int = 128
    global_context: bool = True
    groups: list[int] = field(default_factory=lambda: [1, 1, 1, 1, 1])


class ECPATDNN(nn.Module):
    def __init__(self, args: ECPATDNNArgs):
        super().__init__()
        assert len(args.channels) == len(args.kernel_sizes) and len(
            args.channels
        ) == len(args.dilations)

        self.args = args

        self.blocks = [
            TDNN(
                args.input_size,
                args.channels[0],
                args.kernel_sizes[0],
                dilation=args.dilations[0],
                groups=args.groups[0],
            )
        ] + [
            SeRes2Net(
                args.channels[i - 1],
                args.channels[i],
                scale=args.res2net_scale,
                attention_channels=args.se_channels,
                kernel_size=args.kernel_sizes[i],
                dilation=args.dilations[i],
                groups=args.groups[i],
            )
            for i in range(1, len(args.channels) - 1)
        ]
        self.mfa = TDNN(
            args.channels[-2] * (len(args.channels) - 2),
            args.channels[-1],
            args.kernel_sizes[-1],
            dilation=args.dilations[-1],
            groups=args.groups[-1],
        )
        self.asp = AttentiveStatisticsPooling(
            args.channels[-1],
            attention_channels=args.attention_channels,
            global_context=args.global_context,
        )
        self.asp_bn = nn.BatchNorm(args.channels[-1] * 2)
        self.fc = nn.Conv1d(
            in_channels=args.channels[-1] * 2,
            out_channels=args.lin_neurons,
            kernel_size=1,
        )

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None):  #
        xl = []
        for layer in self.blocks:
            if isinstance(layer, SeRes2Net):
                x = layer(x, mask=mask)
                xl.append(mx.array(x))
            else:
                x = layer(x)

        x = mx.concat(xl, axis=2)
        x = self.mfa(x)

        x = self.asp(x, mask=mask)
        x = self.asp_bn(x)

        x = self.fc(x)

        return x



================================================
FILE: mlx_audio/tts/models/indextts/ecapa_tdnn/se_res2net.py
================================================
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.indextts.ecapa_tdnn.tdnn import TDNN


class Res2Net(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        scale: int,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        # just make sure it's dividable
        assert in_channels % scale == out_channels % scale == 0

        self.scale = scale

        self.blocks = [
            TDNN(
                in_channels // scale,
                out_channels // scale,
                kernel_size,
                dilation,
                groups,
                bias,
            )
            for _ in range(scale - 1)
        ]

    def __call__(self, x: mx.array) -> mx.array:  # NLC
        segments = mx.split(x, self.scale, axis=-1)

        y = [segments[0]]

        for i in range(1, len(segments)):
            prev = y[-1] if i > 1 else 0
            y.append(self.blocks[i - 1](segments[i] + prev))

        return mx.concat(y, axis=-1)


class SE(nn.Module):
    def __init__(
        self,
        in_channels: int,
        se_channels: int,
        out_channels: int,
    ):
        super().__init__()

        self.conv1 = nn.Conv1d(in_channels, se_channels, 1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv1d(se_channels, out_channels, 1)
        self.sigmoid = nn.Sigmoid()

    def __call__(
        self, x: mx.array, mask: Optional[mx.array] = None
    ) -> mx.array:  # NLC, NL
        if mask is not None:
            mask = mask[:, :, None]  # NL1
            masked_x = x * mask
            s = masked_x.sum(1, keepdims=True) / mask.sum(1, keepdims=True)
        else:
            s = x.mean(1, keepdims=True)

        s = self.sigmoid(self.conv2(self.relu(self.conv1(s))))

        return s * x


class SeRes2Net(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        scale: int,
        attention_channels: int,
        kernel_size: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        self.out_channels = out_channels

        self.tdnn1 = TDNN(
            in_channels,
            out_channels,
            kernel_size=1,
            dilation=1,
            groups=groups,
        )
        self.res2net_block = Res2Net(
            out_channels,
            out_channels,
            kernel_size,
            scale,
            dilation=dilation,
        )
        self.tdnn2 = TDNN(
            out_channels,
            out_channels,
            kernel_size=1,
            dilation=1,
            groups=groups,
        )
        self.se_block = SE(out_channels, attention_channels, out_channels)
        self.shortcut = (
            nn.Conv1d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=1,
            )
            if in_channels != out_channels
            else nn.Identity()
        )

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None) -> mx.array:
        x = self.shortcut(x)

        x += self.se_block(self.tdnn2(self.res2net_block(self.tdnn1(x))), mask)

        return x



================================================
FILE: mlx_audio/tts/models/indextts/ecapa_tdnn/tdnn.py
================================================
import mlx.core as mx
import mlx.nn as nn


# essentially just conv with relu & norm
class TDNN(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()

        self.kernel_size = kernel_size
        self.padding = ((kernel_size - 1) * dilation) // 2

        self.conv = nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            1,
            0,
            dilation,
            groups,
            bias,
        )
        self.activation = nn.ReLU()
        self.norm = nn.BatchNorm(out_channels)

    def __call__(self, x: mx.array) -> mx.array:  # NLC
        # reflect padding
        top_pad = x[:, 1 : self.padding + 1, :][:, ::-1, :]
        bottom_pad = x[:, -(self.padding + 1) : -1, :][:, ::-1, :]
        x = mx.concat([top_pad, x, bottom_pad], axis=1)

        res = self.norm(self.activation(self.conv(x)))

        return res



================================================
FILE: mlx_audio/tts/models/kokoro/__init__.py
================================================
from .kokoro import Model, ModelConfig
from .pipeline import KokoroPipeline

__all__ = ["KokoroPipeline", "Model", "ModelConfig"]



================================================
FILE: mlx_audio/tts/models/kokoro/istftnet.py
================================================
import math
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from mlx_audio.utils import istft, stft

from ..base import check_array_shape
from ..interpolate import interpolate


def get_padding(kernel_size: int, dilation: int = 1) -> int:
    return int((kernel_size * dilation - dilation) / 2)


def compute_norm(
    x: mx.array,
    p: int,
    dim: Optional[Union[int, List[int]]] = None,
    keepdim: bool = False,
) -> mx.array:
    """
    Compute the p-norm of a tensor along specified dimensions.

    Args:
        x: Input array
        p: Order of the norm (1 or 2)
        dim: Dimension(s) along which to compute the norm
        keepdim: Whether to keep the reduced dimensions

    Returns:
        MLX array containing the computed norm
    """
    if p not in [1, 2]:
        raise ValueError("Only p-norms with p of 1 or 2 are supported")

    # Handle dimension input
    if dim is None:
        dim = tuple(range(x.ndim))
    elif isinstance(dim, int):
        dim = (dim,)

    if p == 1:
        # L1 norm
        return mx.sum(mx.abs(x), axis=dim, keepdims=keepdim)
    else:
        # L2 norm
        return mx.sqrt(mx.sum(x * x, axis=dim, keepdims=keepdim))


def weight_norm(
    weight_v: mx.array, weight_g: mx.array, dim: Optional[int] = None
) -> mx.array:
    """
    Applies weight normalization to the input tensor.

    Weight normalization reparameterizes weight vectors in a neural network
    as a magnitude scalar times a direction vector: w = g * v/||v||

    Args:
        weight_v: Weight direction tensor (v)
        weight_g: Weight magnitude tensor (g)
        dim: Dimension along which to normalize. If None, normalize over all dims
            except dim=-1

    Returns:
        Normalized weight tensor
    """
    rank = len(weight_v.shape)

    if dim is not None:
        # Adjust negative dim
        if dim < -1:
            dim += rank

        # Create list of axes to normalize over
        axes = list(range(rank))
        if dim != -1:
            axes.remove(dim)
    else:
        # Default behavior: normalize over all dimensions
        axes = list(range(rank))

    # Compute L2 norm of v along specified axes
    norm_v = compute_norm(weight_v, p=2, dim=axes, keepdim=True)

    # Normalize and scale by g: w = g * (v / ||v||)
    normalized_weight = weight_v / (
        norm_v + 1e-7
    )  # Add epsilon for numerical stability
    return normalized_weight * weight_g


class ConvWeighted(nn.Module):
    """Conv1d with weight normalization"""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        encode: bool = False,
    ):
        super().__init__()

        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        # Initialize weight magnitude (g) and direction (v) vectors
        self.weight_g = mx.ones(
            (out_channels, 1, 1)
        )  # Scalar magnitude per output channel
        self.weight_v = mx.ones(
            (out_channels, kernel_size, in_channels)
        )  # Direction vectors

        self.bias = mx.zeros(in_channels if encode else out_channels) if bias else None

    def __call__(self, x, conv):

        weight = weight_norm(self.weight_v, self.weight_g, dim=0)

        if self.bias is not None:
            bias = self.bias.reshape(1, 1, -1)
        else:
            bias = None

        def apply_conv(x, weight_to_use):
            if self.bias is not None:
                return (
                    conv(
                        x,
                        weight_to_use,
                        stride=self.stride,
                        padding=self.padding,
                        dilation=self.dilation,
                        groups=self.groups,
                    )
                    + bias
                )
            return conv(
                x,
                weight_to_use,
                stride=self.stride,
                padding=self.padding,
                dilation=self.dilation,
                groups=self.groups,
            )

        try:
            # Check if channels last match or if groups > 1 for ConvTransposed1d
            if x.shape[-1] == weight.shape[-1] or self.groups > 1:
                # Input is channels first, use weight as-is
                return apply_conv(x, weight)
            else:
                # Input is channels last, need to transpose weight
                return apply_conv(x, weight.T)
        except Exception as e:
            print(f"Error: {e}")
            print(f"x.shape: {x.shape}, weight.shape: {weight.shape}")
            raise e


class _InstanceNorm(nn.Module):
    def __init__(
        self,
        num_features: int,
        eps: float = 1e-5,
        momentum: float = 0.1,
        affine: bool = False,
        track_running_stats: bool = False,
    ) -> None:
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # Initialize parameters
        if self.affine:
            self.weight = mx.ones((num_features,))
            self.bias = mx.zeros((num_features,))
        else:
            self.weight = None
            self.bias = None

        if self.track_running_stats:
            self.running_mean = mx.zeros((num_features,))
            self.running_var = mx.ones((num_features,))
        else:
            self.running_mean = None
            self.running_var = None

    def _check_input_dim(self, input):
        raise NotImplementedError

    def _get_no_batch_dim(self):
        raise NotImplementedError

    def _handle_no_batch_input(self, input):
        # Add batch dimension, apply norm, then remove batch dimension
        expanded = mx.expand_dims(input, axis=0)
        result = self._apply_instance_norm(expanded)
        return mx.squeeze(result, axis=0)

    def _apply_instance_norm(self, input):
        # MLX doesn't have a direct instance_norm function like PyTorch
        # So we need to implement it manually

        # Get dimensions
        dims = list(range(input.ndim))
        feature_dim = dims[-self._get_no_batch_dim()]

        # Compute statistics along all dims except batch and feature dims
        reduce_dims = [d for d in dims if d != 0 and d != feature_dim]

        if self.training or not self.track_running_stats:
            # Compute mean and variance for normalization
            mean = mx.mean(input, axis=reduce_dims, keepdims=True)
            var = mx.var(input, axis=reduce_dims, keepdims=True)

            # Update running stats if tracking
            if self.track_running_stats and self.training:
                # Compute overall mean and variance (across batch too)
                overall_mean = mx.mean(mean, axis=0)
                overall_var = mx.mean(var, axis=0)

                # Update running statistics
                self.running_mean = (
                    1 - self.momentum
                ) * self.running_mean + self.momentum * overall_mean
                self.running_var = (
                    1 - self.momentum
                ) * self.running_var + self.momentum * overall_var
        else:
            # Use running statistics
            mean_shape = [1] * input.ndim
            mean_shape[feature_dim] = self.num_features
            var_shape = mean_shape.copy()

            mean = mx.reshape(self.running_mean, mean_shape)
            var = mx.reshape(self.running_var, var_shape)

        # Normalize
        x_norm = (input - mean) / mx.sqrt(var + self.eps)

        # Apply affine transform if needed
        if self.affine:
            weight_shape = [1] * input.ndim
            weight_shape[feature_dim] = self.num_features
            bias_shape = weight_shape.copy()

            weight = mx.reshape(self.weight, weight_shape)
            bias = mx.reshape(self.bias, bias_shape)

            return x_norm * weight + bias
        else:
            return x_norm

    def __call__(self, input):
        self._check_input_dim(input)

        feature_dim = input.ndim - self._get_no_batch_dim()
        if input.shape[feature_dim] != self.num_features:
            if self.affine:
                raise ValueError(
                    f"expected input's size at dim={feature_dim} to match num_features"
                    f" ({self.num_features}), but got: {input.shape[feature_dim]}."
                )
            else:
                print(
                    f"input's size at dim={feature_dim} does not match num_features. "
                    "You can silence this warning by not passing in num_features, "
                    "which is not used because affine=False"
                )

        if input.ndim == self._get_no_batch_dim():
            return self._handle_no_batch_input(input)

        return self._apply_instance_norm(input)


class InstanceNorm1d(_InstanceNorm):
    """Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input.

    This implementation follows the algorithm described in the paper
    "Instance Normalization: The Missing Ingredient for Fast Stylization".

    Args:
        num_features: Number of features or channels (C) of the input
        eps: A value added to the denominator for numerical stability. Default: 1e-5
        momentum: The value used for the running_mean and running_var computation. Default: 0.1
        affine: When True, this module has learnable affine parameters. Default: False
        track_running_stats: When True, this module tracks running statistics. Default: False

    Shape:
        - Input: (N, C, L) or (C, L)
        - Output: Same shape as input

    Examples:
        >>> # Without Learnable Parameters
        >>> m = nn.InstanceNorm1d(100)
        >>> # With Learnable Parameters
        >>> m = nn.InstanceNorm1d(100, affine=True)
        >>> input = mx.random.normal((20, 100, 40))
        >>> output = m(input)
    """

    def _get_no_batch_dim(self):
        return 2

    def _check_input_dim(self, input):
        if input.ndim not in (2, 3):
            raise ValueError(f"expected 2D or 3D input (got {input.ndim}D input)")


class AdaIN1d(nn.Module):
    def __init__(self, style_dim: int, num_features: int):
        super().__init__()
        self.norm = InstanceNorm1d(num_features, affine=False)
        self.fc = nn.Linear(style_dim, num_features * 2)

    def __call__(self, x: mx.array, s: mx.array) -> mx.array:
        h = self.fc(s)
        h = mx.expand_dims(h, axis=2)  # Equivalent to view(..., 1)
        gamma, beta = mx.split(h, 2, axis=1)
        x = (1 + gamma) * self.norm(x) + beta
        return x


class AdaINResBlock1(nn.Module):
    def __init__(
        self,
        channels: int,
        kernel_size: int = 3,
        dilation: Tuple[int, int, int] = (1, 3, 5),
        style_dim: int = 64,
    ):
        super(AdaINResBlock1, self).__init__()
        self.convs1 = [
            ConvWeighted(
                channels,
                channels,
                kernel_size,
                stride=1,
                padding=get_padding(kernel_size, dilation[i]),
                dilation=dilation[i],
            )
            for i in range(3)
        ]
        self.convs2 = [
            ConvWeighted(
                channels,
                channels,
                kernel_size,
                stride=1,
                padding=get_padding(kernel_size, 1),
                dilation=1,
            )
            for _ in range(3)
        ]
        self.adain1 = [AdaIN1d(style_dim, channels) for _ in range(3)]
        self.adain2 = [AdaIN1d(style_dim, channels) for _ in range(3)]
        self.alpha1 = [mx.ones((1, channels, 1)) for _ in range(len(self.convs1))]
        self.alpha2 = [mx.ones((1, channels, 1)) for _ in range(len(self.convs2))]

    def __call__(self, x: mx.array, s: mx.array) -> mx.array:
        for c1, c2, n1, n2, a1, a2 in zip(
            self.convs1, self.convs2, self.adain1, self.adain2, self.alpha1, self.alpha2
        ):
            xt = n1(x, s)
            xt = xt + (1 / a1) * (mx.sin(a1 * xt) ** 2)  # Snake1D

            xt = xt.swapaxes(2, 1)
            xt = c1(xt, mx.conv1d)
            xt = xt.swapaxes(2, 1)

            xt = n2(xt, s)
            xt = xt + (1 / a2) * (mx.sin(a2 * xt) ** 2)  # Snake1D

            xt = xt.swapaxes(2, 1)
            xt = c2(xt, mx.conv1d)
            xt = xt.swapaxes(2, 1)

            x = xt + x
        return x


def mlx_angle(z, deg=False):
    z = mx.array(z)

    if z.dtype == mx.complex64:
        zimag = mx.imag(z)
        zreal = mx.real(z)
    else:
        zimag = mx.zeros_like(z)
        zreal = z

    a = mx.arctan2(zimag, zreal)

    if deg:
        a = a * (180.0 / math.pi)

    return a


def mlx_unwrap(p, discont=None, axis=-1, period=2 * math.pi):
    if discont is None:
        discont = period / 2

    discont = max(discont, period / 2)

    slice_indices = [slice(None)] * p.ndim

    slice_indices[axis] = slice(1, None)
    after_slice = tuple(slice_indices)

    slice_indices[axis] = slice(None, -1)
    before_slice = tuple(slice_indices)

    dd = p[after_slice] - p[before_slice]

    interval_high = period / 2
    interval_low = -interval_high

    ddmod = dd - period * mx.floor((dd - interval_low) / period)
    ddmod = mx.where(
        (mx.abs(dd - interval_high) < 1e-10) & (dd > 0), interval_high, ddmod
    )

    ph_correct = ddmod - dd
    ph_correct = mx.where(mx.abs(dd) < discont, 0, ph_correct)

    padding_shape = list(ph_correct.shape)
    padding_shape[axis] = 1
    zero_padding = mx.zeros(padding_shape)
    padded_corrections = mx.concatenate([zero_padding, ph_correct], axis=axis)
    cumulative_corrections = mx.cumsum(padded_corrections, axis=axis)

    return p + cumulative_corrections


class MLXSTFT:
    def __init__(
        self, filter_length=800, hop_length=200, win_length=800, window="hann"
    ):
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length

        self.window = window

    def transform(self, input_data):
        # Ensure 2D
        if input_data.ndim == 1:
            input_data = input_data[None, :]

        magnitudes = []
        phases = []

        for batch_idx in range(input_data.shape[0]):
            # Compute STFT
            x_stft = stft(
                input_data[batch_idx],
                n_fft=self.filter_length,
                hop_length=self.hop_length,
                win_length=self.win_length,
                window=self.window,
                center=True,
                pad_mode="reflect",
            ).transpose(1, 0)

            # Get magnitude
            magnitude = mx.abs(x_stft)

            # Get phase
            phase = mlx_angle(x_stft)

            magnitudes.append(magnitude)
            phases.append(phase)

        magnitudes = mx.stack(magnitudes, axis=0)
        phases = mx.stack(phases, axis=0)

        return magnitudes, phases

    def inverse(self, magnitude, phase):
        reconstructed = []

        for batch_idx in range(magnitude.shape[0]):
            # Unwrap phases for reconstruction
            phase_cont = mlx_unwrap(phase[batch_idx], axis=1)

            # Combine magnitude and phase
            real_part = magnitude[batch_idx] * mx.cos(phase_cont)
            imag_part = magnitude[batch_idx] * mx.sin(phase_cont)
            x_stft = real_part + 1j * imag_part

            # Inverse STFT
            audio = istft(
                x_stft,
                hop_length=self.hop_length,
                win_length=self.win_length,
                window=self.window,
                center=True,
                length=None,
            )

            reconstructed.append(audio)

        reconstructed = mx.stack(reconstructed, axis=0)[:, None, :]

        return reconstructed

    def __call__(self, input_data: mx.array) -> mx.array:
        self.magnitude, self.phase = self.transform(input_data)
        reconstruction = self.inverse(self.magnitude, self.phase)
        return mx.expand_dims(reconstruction, axis=-2)


class SineGen:
    def __init__(
        self,
        samp_rate: int,
        upsample_scale: int,
        harmonic_num: int = 0,
        sine_amp: float = 0.1,
        noise_std: float = 0.003,
        voiced_threshold: float = 0,
        flag_for_pulse: bool = False,
    ):
        super().__init__()
        self.sine_amp = sine_amp
        self.noise_std = noise_std
        self.harmonic_num = harmonic_num
        self.dim = self.harmonic_num + 1
        self.sampling_rate = samp_rate
        self.voiced_threshold = voiced_threshold
        self.flag_for_pulse = flag_for_pulse
        self.upsample_scale = upsample_scale

    def _f02uv(self, f0: mx.array) -> mx.array:
        return mx.array(f0 > self.voiced_threshold, dtype=mx.float32)

    def _f02sine(self, f0_values: mx.array) -> mx.array:
        """f0_values: (batchsize, length, dim)
        where dim indicates fundamental tone and overtones
        """
        # convert to F0 in rad. The interger part n can be ignored
        # because 2 * np.pi * n doesn't affect phase
        rad_values = (f0_values / self.sampling_rate) % 1
        # initial phase noise (no noise for fundamental component)
        rand_ini = mx.random.normal((f0_values.shape[0], f0_values.shape[2]))
        rand_ini[:, 0] = 0
        rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini
        # instantanouse phase sine[t] = sin(2*pi \sum_i=1 ^{t} rad)
        if not self.flag_for_pulse:
            rad_values = interpolate(
                rad_values.transpose(0, 2, 1),
                scale_factor=1 / self.upsample_scale,
                mode="linear",
            ).transpose(0, 2, 1)
            phase = mx.cumsum(rad_values, axis=1) * 2 * mx.pi
            phase = interpolate(
                phase.transpose(0, 2, 1) * self.upsample_scale,
                scale_factor=self.upsample_scale,
                mode="linear",
            ).transpose(0, 2, 1)
            sines = mx.sin(phase)
        else:
            # If necessary, make sure that the first time step of every
            # voiced segments is sin(pi) or cos(0)
            # This is used for pulse-train generation
            # identify the last time step in unvoiced segments
            uv = self._f02uv(f0_values)
            uv_1 = mx.roll(uv, shifts=-1, axis=1)
            uv_1[:, -1, :] = 1
            u_loc = (uv < 1) * (uv_1 > 0)
            # get the instantanouse phase
            tmp_cumsum = mx.cumsum(rad_values, axis=1)
            # different batch needs to be processed differently
            for idx in range(f0_values.shape[0]):
                temp_sum = tmp_cumsum[idx, u_loc[idx, :, 0], :]
                temp_sum[1:, :] = temp_sum[1:, :] - temp_sum[0:-1, :]
                # stores the accumulation of i.phase within
                # each voiced segments
                tmp_cumsum[idx, :, :] = 0
                tmp_cumsum[idx, u_loc[idx, :, 0], :] = temp_sum
            # rad_values - tmp_cumsum: remove the accumulation of i.phase
            # within the previous voiced segment.
            i_phase = mx.cumsum(rad_values - tmp_cumsum, axis=1)
            # get the sines
            sines = mx.cos(i_phase * 2 * mx.pi)
        return sines

    def __call__(self, f0: mx.array) -> Tuple[mx.array, mx.array, mx.array]:
        f0_buf = mx.zeros((f0.shape[0], f0.shape[1], self.dim))

        # Fundamental component
        fn = f0 * mx.arange(1, self.harmonic_num + 2)[None, None, :]

        # Generate sine waveforms
        sine_waves = self._f02sine(fn) * self.sine_amp

        # Generate UV signal
        uv = self._f02uv(f0)

        # Generate noise
        noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3
        noise = noise_amp * mx.random.normal(sine_waves.shape)

        sine_waves = sine_waves * uv + noise
        return sine_waves, uv, noise


class SourceModuleHnNSF(nn.Module):
    """SourceModule for hn-nsf
    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0)
    sampling_rate: sampling_rate in Hz
    harmonic_num: number of harmonic above F0 (default: 0)
    sine_amp: amplitude of sine source signal (default: 0.1)
    add_noise_std: std of additive Gaussian noise (default: 0.003)
        note that amplitude of noise in unvoiced is decided
        by sine_amp
    voiced_threshold: threhold to set U/V given F0 (default: 0)
    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
    F0_sampled (batchsize, length, 1)
    Sine_source (batchsize, length, 1)
    noise_source (batchsize, length 1)
    uv (batchsize, length, 1)
    """

    def __init__(
        self,
        sampling_rate,
        upsample_scale,
        harmonic_num=0,
        sine_amp=0.1,
        add_noise_std=0.003,
        voiced_threshod=0,
    ):
        super(SourceModuleHnNSF, self).__init__()
        self.sine_amp = sine_amp
        self.noise_std = add_noise_std
        # to produce sine waveforms
        self.l_sin_gen = SineGen(
            sampling_rate,
            upsample_scale,
            harmonic_num,
            sine_amp,
            add_noise_std,
            voiced_threshod,
        )
        # to merge source harmonics into a single excitation
        self.l_linear = nn.Linear(harmonic_num + 1, 1)

    def __call__(self, x):
        """
        Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
        F0_sampled (batchsize, length, 1)
        Sine_source (batchsize, length, 1)
        noise_source (batchsize, length 1)
        """
        # source for harmonic branch
        sine_wavs, uv, _ = self.l_sin_gen(x)
        sine_merge = mx.tanh(self.l_linear(sine_wavs))
        # source for noise branch, in the same shape as uv
        noise = mx.random.normal(uv.shape) * self.sine_amp / 3
        return sine_merge, noise, uv


class ReflectionPad1d(nn.Module):
    def __init__(self, padding):
        super().__init__()
        self.padding = padding

    def __call__(self, x):
        return mx.pad(x, ((0, 0), (0, 0), (self.padding[0], self.padding[1])))


def leaky_relu(x, negative_slope=0.01):
    return mx.where(x > 0, x, x * negative_slope)


class Generator(nn.Module):
    def __init__(
        self,
        style_dim,
        resblock_kernel_sizes,
        upsample_rates,
        upsample_initial_channel,
        resblock_dilation_sizes,
        upsample_kernel_sizes,
        gen_istft_n_fft,
        gen_istft_hop_size,
    ):
        super(Generator, self).__init__()
        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_rates)
        upsample_rates = mx.array(upsample_rates)
        self.m_source = SourceModuleHnNSF(
            sampling_rate=24000,
            upsample_scale=mx.prod(upsample_rates) * gen_istft_hop_size,
            harmonic_num=8,
            voiced_threshod=10,
        )
        self.f0_upsamp = nn.Upsample(
            scale_factor=mx.prod(upsample_rates) * gen_istft_hop_size
        )
        self.noise_convs = []
        self.noise_res = []
        self.ups = []
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(
                ConvWeighted(
                    upsample_initial_channel // (2 ** (i + 1)),
                    upsample_initial_channel // (2**i),
                    int(k),
                    int(u),
                    padding=int((k - u) // 2),
                    encode=True,
                )
            )
        self.resblocks = []
        for i in range(len(self.ups)):
            ch = upsample_initial_channel // (2 ** (i + 1))
            for j, (k, d) in enumerate(
                zip(resblock_kernel_sizes, resblock_dilation_sizes)
            ):
                self.resblocks.append(AdaINResBlock1(ch, k, d, style_dim))
            c_cur = upsample_initial_channel // (2 ** (i + 1))
            if i + 1 < len(upsample_rates):
                stride_f0 = int(mx.prod(upsample_rates[i + 1 :]))
                self.noise_convs.append(
                    nn.Conv1d(
                        gen_istft_n_fft + 2,
                        c_cur,
                        kernel_size=stride_f0 * 2,
                        stride=stride_f0,
                        padding=(stride_f0 + 1) // 2,
                    )
                )
                self.noise_res.append(AdaINResBlock1(c_cur, 7, [1, 3, 5], style_dim))
            else:
                self.noise_convs.append(
                    nn.Conv1d(gen_istft_n_fft + 2, c_cur, kernel_size=1)
                )
                self.noise_res.append(AdaINResBlock1(c_cur, 11, [1, 3, 5], style_dim))
        self.post_n_fft = gen_istft_n_fft
        self.conv_post = ConvWeighted(ch, self.post_n_fft + 2, 7, 1, padding=3)
        self.reflection_pad = ReflectionPad1d((1, 0))
        self.stft = MLXSTFT(
            filter_length=gen_istft_n_fft,
            hop_length=gen_istft_hop_size,
            win_length=gen_istft_n_fft,
        )

    def __call__(self, x, s, f0):
        f0 = self.f0_upsamp(f0[:, None].transpose(0, 2, 1))  # bs,n,t
        har_source, noi_source, uv = self.m_source(f0)
        har_source = mx.squeeze(har_source.transpose(0, 2, 1), axis=1)
        har_spec, har_phase = self.stft.transform(har_source)
        har = mx.concatenate([har_spec, har_phase], axis=1)
        har = har.swapaxes(2, 1)
        for i in range(self.num_upsamples):
            x = leaky_relu(x, negative_slope=0.1)
            x_source = self.noise_convs[i](har)
            x_source = x_source.swapaxes(2, 1)
            x_source = self.noise_res[i](x_source, s)

            x = x.swapaxes(2, 1)
            x = self.ups[i](x, mx.conv_transpose1d)
            x = x.swapaxes(2, 1)

            if i == self.num_upsamples - 1:
                x = self.reflection_pad(x)
            x = x + x_source

            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i * self.num_kernels + j](x, s)
                else:
                    xs += self.resblocks[i * self.num_kernels + j](x, s)
            x = xs / self.num_kernels

        x = leaky_relu(x, negative_slope=0.01)

        x = x.swapaxes(2, 1)
        x = self.conv_post(x, mx.conv1d)
        x = x.swapaxes(2, 1)

        spec = mx.exp(x[:, : self.post_n_fft // 2 + 1, :])
        phase = mx.sin(x[:, self.post_n_fft // 2 + 1 :, :])
        result = self.stft.inverse(spec, phase)
        return result


class UpSample1d(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type
        self.interpolate = nn.Upsample(
            scale_factor=2, mode="nearest", align_corners=True
        )

    def __call__(self, x):
        if self.layer_type == "none":
            return x
        else:
            return self.interpolate(x)


class AdainResBlk1d(nn.Module):
    def __init__(
        self,
        dim_in,
        dim_out,
        style_dim=64,
        actv=nn.LeakyReLU(0.2),
        upsample="none",
        dropout_p=0.0,
        bias=False,
        conv_type=None,
    ):
        super().__init__()
        self.actv = actv
        self.dim_in = dim_in
        self.conv_type = conv_type
        self.upsample_type = upsample
        self.upsample = UpSample1d(upsample)
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out, style_dim)
        self.dropout = nn.Dropout(dropout_p)
        if upsample == "none":
            self.pool = nn.Identity()
        else:
            self.pool = ConvWeighted(
                1, dim_in, kernel_size=3, stride=2, padding=1, groups=dim_in
            )

    def _build_weights(self, dim_in, dim_out, style_dim):
        self.conv1 = ConvWeighted(dim_in, dim_out, kernel_size=3, stride=1, padding=1)
        self.conv2 = ConvWeighted(dim_out, dim_out, kernel_size=3, stride=1, padding=1)
        self.norm1 = AdaIN1d(style_dim, dim_in)
        self.norm2 = AdaIN1d(style_dim, dim_out)
        if self.learned_sc:
            self.conv1x1 = ConvWeighted(
                dim_in, dim_out, kernel_size=1, stride=1, padding=0, bias=False
            )

    def _shortcut(self, x):
        x = x.swapaxes(2, 1)
        x = self.upsample(x)
        x = x.swapaxes(2, 1)

        if self.learned_sc:
            x = x.swapaxes(2, 1)
            x = self.conv1x1(x, mx.conv1d)
            x = x.swapaxes(2, 1)
        return x

    def _residual(self, x, s):
        x = self.norm1(x, s)
        x = self.actv(x)

        # Manually implement grouped ConvTranspose1d since MLX doesn't support groups
        x = x.swapaxes(2, 1)
        x = self.pool(x, mx.conv_transpose1d) if self.upsample_type != "none" else x
        x = mx.pad(x, ((0, 0), (1, 0), (0, 0))) if self.upsample_type != "none" else x
        x = x.swapaxes(2, 1)

        x = x.swapaxes(2, 1)
        x = self.conv1(self.dropout(x), mx.conv1d)
        x = x.swapaxes(2, 1)

        x = self.norm2(x, s)
        x = self.actv(x)

        x = x.swapaxes(2, 1)
        x = self.conv2(x, mx.conv1d)
        x = x.swapaxes(2, 1)
        return x

    def __call__(self, x, s):
        out = self._residual(x, s)
        out = (out + self._shortcut(x)) / mx.sqrt(2)
        return out


class Decoder(nn.Module):
    def __init__(
        self,
        dim_in,
        style_dim,
        dim_out,
        resblock_kernel_sizes,
        upsample_rates,
        upsample_initial_channel,
        resblock_dilation_sizes,
        upsample_kernel_sizes,
        gen_istft_n_fft,
        gen_istft_hop_size,
    ):
        super().__init__()
        self.encode = AdainResBlk1d(dim_in + 2, 1024, style_dim, conv_type=mx.conv1d)
        self.decode = []
        self.decode.append(
            AdainResBlk1d(1024 + 2 + 64, 1024, style_dim, conv_type=mx.conv1d)
        )
        self.decode.append(
            AdainResBlk1d(1024 + 2 + 64, 1024, style_dim, conv_type=mx.conv1d)
        )
        self.decode.append(
            AdainResBlk1d(1024 + 2 + 64, 1024, style_dim, conv_type=mx.conv1d)
        )
        self.decode.append(
            AdainResBlk1d(
                1024 + 2 + 64, 512, style_dim, upsample=True, conv_type=mx.conv1d
            )
        )
        self.F0_conv = ConvWeighted(1, 1, kernel_size=3, stride=2, padding=1, groups=1)
        self.N_conv = ConvWeighted(1, 1, kernel_size=3, stride=2, padding=1, groups=1)
        self.asr_res = [ConvWeighted(512, 64, kernel_size=1, padding=0)]
        self.generator = Generator(
            style_dim,
            resblock_kernel_sizes,
            upsample_rates,
            upsample_initial_channel,
            resblock_dilation_sizes,
            upsample_kernel_sizes,
            gen_istft_n_fft,
            gen_istft_hop_size,
        )

    def __call__(self, asr, F0_curve, N, s):
        s = mx.array(s)
        F0 = self.F0_conv(F0_curve[:, None, :].swapaxes(2, 1), mx.conv1d).swapaxes(2, 1)
        N = self.N_conv(N[:, None, :].swapaxes(2, 1), mx.conv1d).swapaxes(2, 1)
        x = mx.concatenate([asr, F0, N], axis=1)
        x = self.encode(x, s)
        asr_res = self.asr_res[0](asr.swapaxes(2, 1), mx.conv1d).swapaxes(2, 1)
        res = True
        for block in self.decode:  # Working in MLX
            if res:
                x = mx.concatenate([x, asr_res, F0, N], axis=1)
            x = block(x, s)
            # Check if this block has upsampling
            if hasattr(block, "upsample_type") and block.upsample_type != "none":
                res = False
        x = self.generator(x, s, F0_curve)  # Working in MLX
        return x

    def sanitize(self, key, weights):
        sanitized_weights = None
        if "noise_convs" in key and key.endswith(".weight"):
            sanitized_weights = weights.transpose(0, 2, 1)

        elif "weight_v" in key:
            if check_array_shape(weights):
                sanitized_weights = weights
            else:
                sanitized_weights = weights.transpose(0, 2, 1)

        else:
            sanitized_weights = weights

        return sanitized_weights



================================================
FILE: mlx_audio/tts/models/kokoro/kokoro.py
================================================
import json
import sys
import time
from dataclasses import dataclass
from numbers import Number
from typing import Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from loguru import logger

from ..base import BaseModelArgs, GenerationResult, check_array_shape
from .istftnet import Decoder
from .modules import AlbertModelArgs, CustomAlbert, ProsodyPredictor, TextEncoder
from .pipeline import KokoroPipeline

# Force reset logger configuration at the top of your file
logger.remove()  # Remove all handlers
logger.configure(
    handlers=[{"sink": sys.stderr, "level": "DEBUG"}]
)  # Add back with explicit level


def sanitize_lstm_weights(key: str, state_dict: mx.array) -> dict:
    """Convert PyTorch LSTM weight keys to MLX LSTM weight keys."""
    base_key = key.rsplit(".", 1)[0]

    # Mapping of PyTorch LSTM weight suffixes to MLX weight names
    weight_map = {
        "weight_ih_l0_reverse": "Wx_backward",
        "weight_hh_l0_reverse": "Wh_backward",
        "bias_ih_l0_reverse": "bias_ih_backward",
        "bias_hh_l0_reverse": "bias_hh_backward",
        "weight_ih_l0": "Wx_forward",
        "weight_hh_l0": "Wh_forward",
        "bias_ih_l0": "bias_ih_forward",
        "bias_hh_l0": "bias_hh_forward",
    }

    for suffix, new_suffix in weight_map.items():
        if key.endswith(suffix):
            return {f"{base_key}.{new_suffix}": state_dict}

    return {key: state_dict}


@dataclass
class ModelConfig(BaseModelArgs):
    istftnet: dict
    dim_in: int
    dropout: float
    hidden_dim: int
    max_conv_dim: int
    max_dur: int
    multispeaker: bool
    n_layer: int
    n_mels: int
    n_token: int
    style_dim: int
    text_encoder_kernel_size: int
    plbert: dict
    vocab: Dict[str, int]
    sample_rate: int = 24000


class Model(nn.Module):
    """
    KokoroModel is a mlx.nn.Module with 2 main responsibilities:
    1. Init weights, downloading config.json + model.pth from HF if needed
    2. forward(phonemes: str, ref_s: FloatTensor) -> (audio: FloatTensor)

    You likely only need one KokoroModel instance, and it can be reused across
    multiple KokoroPipelines to avoid redundant memory allocation.

    Unlike KokoroPipeline, KokoroModel is language-blind.

    KokoroModel stores self.vocab and thus knows how to map phonemes -> input_ids,
    so there is no need to repeatedly download config.json outside of KokoroModel.
    """

    REPO_ID = "prince-canuma/Kokoro-82M"

    def __init__(self, config: ModelConfig, repo_id: str = None):
        super().__init__()
        self.repo_id = repo_id
        self.config = config
        self.vocab = config.vocab
        self.bert = CustomAlbert(
            AlbertModelArgs(vocab_size=config.n_token, **config.plbert)
        )

        self.bert_encoder = nn.Linear(self.bert.config.hidden_size, config.hidden_dim)
        self.context_length = self.bert.config.max_position_embeddings
        self.predictor = ProsodyPredictor(
            style_dim=config.style_dim,
            d_hid=config.hidden_dim,
            nlayers=config.n_layer,
            max_dur=config.max_dur,
            dropout=config.dropout,
        )
        self.text_encoder = TextEncoder(
            channels=config.hidden_dim,
            kernel_size=config.text_encoder_kernel_size,
            depth=config.n_layer,
            n_symbols=config.n_token,
        )
        self.decoder = Decoder(
            dim_in=config.hidden_dim,
            style_dim=config.style_dim,
            dim_out=config.n_mels,
            **config.istftnet,
        )
        self._pipelines: Dict[str, KokoroPipeline] = {}  # Cache for pipelines

    @dataclass
    class Output:
        audio: mx.array
        pred_dur: Optional[mx.array] = None

    def __call__(
        self,
        phonemes: str,
        ref_s: mx.array,
        speed: Number = 1,
        return_output: bool = False,  # MARK: BACKWARD COMPAT
        decoder=None,
    ) -> Union["KokoroModel.Output", mx.array]:
        input_ids = list(
            filter(lambda i: i is not None, map(lambda p: self.vocab.get(p), phonemes))
        )
        assert len(input_ids) + 2 <= self.context_length, (
            len(input_ids) + 2,
            self.context_length,
        )
        input_ids = mx.array([[0, *input_ids, 0]])
        input_lengths = mx.array([input_ids.shape[-1]])
        text_mask = mx.arange(int(input_lengths.max()))[None, ...]
        text_mask = mx.repeat(text_mask, input_lengths.shape[0], axis=0).astype(
            input_lengths.dtype
        )
        text_mask = text_mask + 1 > input_lengths[:, None]
        bert_dur, _ = self.bert(input_ids, attention_mask=(~text_mask).astype(mx.int32))
        d_en = self.bert_encoder(bert_dur).transpose(0, 2, 1)
        ref_s = ref_s
        s = ref_s[:, 128:]
        d = self.predictor.text_encoder(d_en, s, input_lengths, text_mask)
        x, _ = self.predictor.lstm(d)
        duration = self.predictor.duration_proj(x)
        duration = mx.sigmoid(duration).sum(axis=-1) / speed
        pred_dur = mx.clip(mx.round(duration), a_min=1, a_max=None).astype(mx.int32)[0]
        indices = mx.concatenate(
            [mx.repeat(mx.array(i), int(n)) for i, n in enumerate(pred_dur)]
        )
        pred_aln_trg = mx.zeros((input_ids.shape[1], indices.shape[0]))
        pred_aln_trg[indices, mx.arange(indices.shape[0])] = 1
        pred_aln_trg = pred_aln_trg[None, :]
        en = d.transpose(0, 2, 1) @ pred_aln_trg
        F0_pred, N_pred = self.predictor.F0Ntrain(en, s)
        t_en = self.text_encoder(
            input_ids, input_lengths, text_mask
        )  # Working fine in MLX
        asr = t_en @ pred_aln_trg

        decoder = mx.compile(decoder) if decoder is not None else self.decoder
        audio = decoder(asr, F0_pred, N_pred, ref_s[:, :128])[0]  # Working fine in MLX

        # Evaluate the computation graph for audio and pred_dur before returning
        mx.eval(audio, pred_dur)

        return self.Output(audio=audio, pred_dur=pred_dur) if return_output else audio

    def sanitize(self, weights):
        sanitized_weights = {}
        for key, state_dict in weights.items():

            if key.startswith("bert"):
                if "position_ids" in key:
                    # Remove unused position_ids
                    continue
                else:
                    # print(k, v.shape)
                    sanitized_weights[key] = state_dict

            if key.startswith("bert_encoder"):
                sanitized_weights[key] = state_dict

            if key.startswith("text_encoder"):

                if key.endswith((".gamma", ".beta")):
                    base_key = key.rsplit(".", 1)[0]
                    if key.endswith(".gamma"):
                        new_key = f"{base_key}.weight"
                    else:
                        new_key = f"{base_key}.bias"

                    sanitized_weights[new_key] = state_dict
                elif "weight_v" in key:
                    if check_array_shape(state_dict):
                        sanitized_weights[key] = state_dict
                    else:
                        sanitized_weights[key] = state_dict.transpose(0, 2, 1)

                # Replace weight_ih_l0_reverse and weight_hh_l0_reverse with Wx and Wh
                elif key.endswith(
                    (
                        ".weight_ih_l0_reverse",
                        ".weight_hh_l0_reverse",
                        ".bias_ih_l0_reverse",
                        ".bias_hh_l0_reverse",
                        ".weight_ih_l0",
                        ".weight_hh_l0",
                        ".bias_ih_l0",
                        ".bias_hh_l0",
                    )
                ):
                    sanitized_weights.update(sanitize_lstm_weights(key, state_dict))
                else:
                    sanitized_weights[key] = state_dict

            if key.startswith("predictor"):
                if "F0_proj.weight" in key:
                    sanitized_weights[key] = state_dict.transpose(0, 2, 1)

                elif "N_proj.weight" in key:
                    sanitized_weights[key] = state_dict.transpose(0, 2, 1)

                elif "weight_v" in key:
                    if check_array_shape(state_dict):
                        sanitized_weights[key] = state_dict
                    else:
                        sanitized_weights[key] = state_dict.transpose(0, 2, 1)

                    # Replace weight_ih_l0_reverse and weight_hh_l0_reverse with Wx and Wh
                elif key.endswith(
                    (
                        ".weight_ih_l0_reverse",
                        ".weight_hh_l0_reverse",
                        ".bias_ih_l0_reverse",
                        ".bias_hh_l0_reverse",
                        ".weight_ih_l0",
                        ".weight_hh_l0",
                        ".bias_ih_l0",
                        ".bias_hh_l0",
                    )
                ):
                    sanitized_weights.update(sanitize_lstm_weights(key, state_dict))
                else:
                    sanitized_weights[key] = state_dict

            if key.startswith("decoder"):
                sanitized_weights[key] = self.decoder.sanitize(key, state_dict)
        return sanitized_weights

    @property
    def sample_rate(self):
        return self.config.sample_rate

    def _get_pipeline(self, lang_code: str) -> KokoroPipeline:
        """Retrieves or creates a cached KokoroPipeline for the given language code."""
        if lang_code not in self._pipelines:
            logger.info(f"Creating new KokoroPipeline for language: {lang_code}")
            self._pipelines[lang_code] = KokoroPipeline(
                model=self,
                repo_id=self.REPO_ID if self.repo_id is None else self.repo_id,
                lang_code=lang_code,
            )
        return self._pipelines[lang_code]

    def generate(
        self,
        text: str,
        voice: str = None,
        speed: float = 1.0,
        lang_code: str = "a",
        split_pattern: str = r"\n+",
        **kwargs,
    ):
        pipeline = self._get_pipeline(lang_code)

        pipeline.voices = {}  # Reset voices

        if voice is None:
            voice = "af_heart"

        # Track overall generation time
        start_time = time.time()

        for segment_idx, (graphenes, phonemes, audio) in enumerate(
            pipeline(text, voice=voice, speed=speed, split_pattern=split_pattern)
        ):
            # Track per-segment generation time
            segment_time = time.time() - start_time

            samples = audio.shape[0] if audio is not None else 0
            assert samples > 0, "No audio generated"

            # Calculate token count
            token_count = len(phonemes) if phonemes is not None else 0

            # Calculate audio duration in seconds
            sample_rate = self.config.sample_rate

            audio_duration_seconds = samples / sample_rate * audio.shape[1]

            # Calculate real-time factor (RTF)
            rtf = (
                segment_time / audio_duration_seconds
                if audio_duration_seconds > 0
                else 0
            )

            # Format duration as HH:MM:SS.mmm
            duration_mins = int(audio_duration_seconds // 60)
            duration_secs = int(audio_duration_seconds % 60)
            duration_ms = int((audio_duration_seconds % 1) * 1000)
            duration_hours = int(audio_duration_seconds // 3600)
            duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

            yield GenerationResult(
                audio=audio[0],
                samples=samples,
                sample_rate=sample_rate,
                segment_idx=segment_idx,
                token_count=token_count,
                audio_duration=duration_str,
                real_time_factor=round(rtf, 2),
                prompt={
                    "tokens": token_count,
                    "tokens-per-sec": (
                        round(token_count / segment_time, 2) if segment_time > 0 else 0
                    ),
                },
                audio_samples={
                    "samples": samples,
                    "samples-per-sec": (
                        round(samples / segment_time, 2) if segment_time > 0 else 0
                    ),
                },
                processing_time_seconds=segment_time,
                peak_memory_usage=mx.get_peak_memory() / 1e9,
            )

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/kokoro/modules.py
================================================
import math
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from ..base import BaseModelArgs
from .istftnet import AdainResBlk1d, ConvWeighted


class LinearNorm(nn.Module):
    def __init__(self, in_dim, out_dim, bias=True, w_init_gain="linear"):
        super().__init__()
        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)

    def __call__(self, x):
        return self.linear_layer(x)


class TextEncoder(nn.Module):
    def __init__(self, channels, kernel_size, depth, n_symbols, actv=nn.LeakyReLU(0.2)):
        super().__init__()
        self.embedding = nn.Embedding(n_symbols, channels)
        padding = (kernel_size - 1) // 2
        self.cnn = []
        for _ in range(depth):
            self.cnn.append(
                [
                    ConvWeighted(
                        channels, channels, kernel_size=kernel_size, padding=padding
                    ),
                    nn.LayerNorm(channels),
                    actv,
                    nn.Dropout(0.2),
                ]
            )
        # MLX doesn't have built-in LSTM, so we'll implement a simplified version
        self.lstm = LSTM(channels, channels // 2)

    def __call__(self, x, input_lengths, m):
        x = self.embedding(x)
        x = mx.transpose(x, (0, 2, 1))
        m = mx.expand_dims(m, axis=1)
        x = mx.where(m, 0.0, x)

        for conv in self.cnn:
            for layer in conv:
                if isinstance(layer, (ConvWeighted, nn.LayerNorm)):
                    x = x.swapaxes(2, 1)
                    x = (
                        layer(x, mx.conv1d)
                        if isinstance(layer, ConvWeighted)
                        else layer(x)
                    )
                    x = x.swapaxes(2, 1)
                else:
                    x = layer(x)

                x = mx.where(m, 0.0, x)

        x = x.swapaxes(2, 1)
        x, _ = self.lstm(x)
        x = x.swapaxes(2, 1)
        x_pad = mx.zeros([x.shape[0], x.shape[1], m.shape[-1]])
        x_pad[:, :, : x.shape[-1]] = x
        x = mx.where(m, 0.0, x_pad)
        return x


class AdaLayerNorm(nn.Module):
    # Works fine in MLX
    def __init__(self, style_dim, channels, eps=1e-5):
        super().__init__()
        self.channels = channels
        self.eps = eps
        self.fc = nn.Linear(style_dim, channels * 2)

    def __call__(self, x, s):
        h = self.fc(s)
        h = mx.reshape(h, (h.shape[0], h.shape[1], 1))
        gamma, beta = mx.split(h, 2, axis=1)
        gamma = gamma.transpose(2, 0, 1)
        beta = beta.transpose(2, 0, 1)

        mean = mx.mean(x, axis=-1, keepdims=True)
        var = mx.var(x, axis=-1, keepdims=True)
        x = (x - mean) / mx.sqrt(var + self.eps)

        return (1 + gamma) * x + beta


class LSTM(nn.Module):
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        bias: bool = True,
        batch_first: bool = True,
    ):
        super().__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias
        self.batch_first = batch_first
        # Initialize scale for weight initialization
        scale = 1.0 / math.sqrt(hidden_size)

        # Forward direction weights and biases
        self.Wx_forward = mx.random.uniform(
            low=-scale, high=scale, shape=(4 * hidden_size, input_size)
        )
        self.Wh_forward = mx.random.uniform(
            low=-scale, high=scale, shape=(4 * hidden_size, hidden_size)
        )
        self.bias_ih_forward = (
            mx.random.uniform(low=-scale, high=scale, shape=(4 * hidden_size,))
            if bias
            else None
        )
        self.bias_hh_forward = (
            mx.random.uniform(low=-scale, high=scale, shape=(4 * hidden_size,))
            if bias
            else None
        )

        # Backward direction weights and biases
        self.Wx_backward = mx.random.uniform(
            low=-scale, high=scale, shape=(4 * hidden_size, input_size)
        )
        self.Wh_backward = mx.random.uniform(
            low=-scale, high=scale, shape=(4 * hidden_size, hidden_size)
        )
        self.bias_ih_backward = (
            mx.random.uniform(low=-scale, high=scale, shape=(4 * hidden_size,))
            if bias
            else None
        )
        self.bias_hh_backward = (
            mx.random.uniform(low=-scale, high=scale, shape=(4 * hidden_size,))
            if bias
            else None
        )

    def _extra_repr(self):
        return (
            f"input_size={self.input_size}, "
            f"hidden_size={self.hidden_size}, bias={self.bias}"
        )

    def _forward_direction(self, x, hidden=None, cell=None):
        """Process sequence in forward direction"""
        # Pre-compute input projections
        if self.bias_ih_forward is not None and self.bias_hh_forward is not None:
            x_proj = mx.addmm(
                self.bias_ih_forward + self.bias_hh_forward, x, self.Wx_forward.T
            )
        else:
            x_proj = x @ self.Wx_forward.T

        all_hidden = []
        all_cell = []

        seq_len = x.shape[-2]

        # Initialize hidden and cell states if not provided
        if hidden is None:
            hidden = mx.zeros((x.shape[0], self.hidden_size))
        if cell is None:
            cell = mx.zeros((x.shape[0], self.hidden_size))

        # Process sequence in forward direction (0 to seq_len-1)
        for idx in range(seq_len):
            ifgo = x_proj[..., idx, :]
            ifgo = ifgo + hidden @ self.Wh_forward.T

            # Split gates
            i, f, g, o = mx.split(ifgo, 4, axis=-1)

            # Apply activations
            i = mx.sigmoid(i)
            f = mx.sigmoid(f)
            g = mx.tanh(g)
            o = mx.sigmoid(o)

            # Update cell and hidden states
            cell = f * cell + i * g
            hidden = o * mx.tanh(cell)

            all_cell.append(cell)
            all_hidden.append(hidden)

        return mx.stack(all_hidden, axis=-2), mx.stack(all_cell, axis=-2)

    def _backward_direction(self, x, hidden=None, cell=None):
        """Process sequence in backward direction"""
        # Pre-compute input projections
        if self.bias_ih_backward is not None and self.bias_hh_backward is not None:
            x_proj = mx.addmm(
                self.bias_ih_backward + self.bias_hh_backward, x, self.Wx_backward.T
            )
        else:
            x_proj = x @ self.Wx_backward.T

        all_hidden = []
        all_cell = []

        seq_len = x.shape[-2]

        # Initialize hidden and cell states if not provided
        if hidden is None:
            hidden = mx.zeros((x.shape[0], self.hidden_size))
        if cell is None:
            cell = mx.zeros((x.shape[0], self.hidden_size))

        # Process sequence in backward direction (seq_len-1 to 0)
        for idx in range(seq_len - 1, -1, -1):
            ifgo = x_proj[..., idx, :]
            ifgo = ifgo + hidden @ self.Wh_backward.T

            # Split gates
            i, f, g, o = mx.split(ifgo, 4, axis=-1)

            # Apply activations
            i = mx.sigmoid(i)
            f = mx.sigmoid(f)
            g = mx.tanh(g)
            o = mx.sigmoid(o)

            # Update cell and hidden states
            cell = f * cell + i * g
            hidden = o * mx.tanh(cell)

            # Insert at beginning to maintain original sequence order
            all_cell.insert(0, cell)
            all_hidden.insert(0, hidden)

        return mx.stack(all_hidden, axis=-2), mx.stack(all_cell, axis=-2)

    def __call__(
        self,
        x,
        hidden_forward=None,
        cell_forward=None,
        hidden_backward=None,
        cell_backward=None,
    ):
        """
        Process input sequence in both directions and concatenate the results.

        Args:
            x: Input tensor of shape (batch_size, seq_len, input_size)
            hidden_forward: Initial hidden state for forward direction
            cell_forward: Initial cell state for forward direction
            hidden_backward: Initial hidden state for backward direction
            cell_backward: Initial cell state for backward direction

        Returns:
            Tuple of:
                - Combined output hidden states (batch_size, seq_len, 2*hidden_size)
                - Tuple of final states ((forward_hidden, forward_cell), (backward_hidden, backward_cell))
        """

        if x.ndim == 2:
            x = mx.expand_dims(x, axis=0)  # (1, seq_len, input_size)

        # Forward direction
        forward_hidden, forward_cell = self._forward_direction(
            x, hidden_forward, cell_forward
        )

        # Backward direction
        backward_hidden, backward_cell = self._backward_direction(
            x, hidden_backward, cell_backward
        )

        # Concatenate outputs along the feature dimension
        output = mx.concatenate([forward_hidden, backward_hidden], axis=-1)

        # Return combined output and final states for both directions
        return output, (
            (forward_hidden[..., -1, :], forward_cell[..., -1, :]),
            (backward_hidden[..., 0, :], backward_cell[..., 0, :]),
        )


class ProsodyPredictor(nn.Module):
    def __init__(self, style_dim, d_hid, nlayers, max_dur=50, dropout=0.1):
        super().__init__()
        self.text_encoder = DurationEncoder(
            sty_dim=style_dim, d_model=d_hid, nlayers=nlayers, dropout=dropout
        )
        self.lstm = LSTM(d_hid + style_dim, d_hid // 2)
        self.duration_proj = LinearNorm(d_hid, max_dur)
        self.shared = LSTM(d_hid + style_dim, d_hid // 2)

        # F0 and N blocks
        self.F0 = [
            AdainResBlk1d(
                d_hid, d_hid, style_dim, dropout_p=dropout, conv_type=mx.conv1d
            ),
            AdainResBlk1d(
                d_hid,
                d_hid // 2,
                style_dim,
                upsample=True,
                dropout_p=dropout,
                conv_type=mx.conv1d,
            ),
            AdainResBlk1d(
                d_hid // 2,
                d_hid // 2,
                style_dim,
                dropout_p=dropout,
                conv_type=mx.conv1d,
            ),
        ]

        self.N = [
            AdainResBlk1d(
                d_hid, d_hid, style_dim, dropout_p=dropout, conv_type=mx.conv1d
            ),
            AdainResBlk1d(
                d_hid,
                d_hid // 2,
                style_dim,
                upsample=True,
                dropout_p=dropout,
                conv_type=mx.conv1d,
            ),
            AdainResBlk1d(
                d_hid // 2,
                d_hid // 2,
                style_dim,
                dropout_p=dropout,
                conv_type=mx.conv1d,
            ),
        ]

        self.F0_proj = nn.Conv1d(d_hid // 2, 1, 1, padding=0)
        self.N_proj = nn.Conv1d(d_hid // 2, 1, 1, padding=0)

    def __call__(self, texts, style, text_lengths, alignment, m):
        d = self.text_encoder(texts, style, text_lengths, m)
        x, _ = self.lstm(d, text_lengths)

        # Apply dropout during inference
        x = mx.dropout(x, p=0.5)

        duration = self.duration_proj(x)
        en = mx.matmul(mx.transpose(d), alignment)
        return mx.squeeze(duration, axis=-1), en

    def F0Ntrain(self, x, s):
        x = mx.array(x)
        s = mx.array(s)
        x, _ = self.shared(mx.transpose(x, (0, 2, 1)))

        # F0 prediction
        F0 = mx.transpose(x, (0, 2, 1))
        for block in self.F0:
            F0 = block(F0, s)

        F0 = F0.swapaxes(2, 1)
        F0 = self.F0_proj(F0)
        F0 = F0.swapaxes(2, 1)

        # N prediction
        N = mx.transpose(x, (0, 2, 1))
        for block in self.N:
            N = block(N, s)
        N = N.swapaxes(2, 1)
        N = self.N_proj(N)
        N = N.swapaxes(2, 1)

        return mx.squeeze(F0, axis=1), mx.squeeze(N, axis=1)


class DurationEncoder(nn.Module):
    def __init__(self, sty_dim, d_model, nlayers, dropout=0.1):
        super().__init__()
        self.lstms = []
        for _ in range(nlayers):
            self.lstms.extend(
                [LSTM(d_model + sty_dim, d_model // 2), AdaLayerNorm(sty_dim, d_model)]
            )
        self.dropout = dropout
        self.d_model = d_model
        self.sty_dim = sty_dim

    def __call__(self, x, style, text_lengths, m):
        x = x.transpose(2, 0, 1)
        s = mx.broadcast_to(style, (x.shape[0], x.shape[1], style.shape[-1]))
        x = mx.concatenate([x, s], axis=-1)
        x = mx.where(m[..., None].transpose(1, 0, 2), 0.0, x)
        x = x.transpose(1, 2, 0)

        for block in self.lstms:
            if isinstance(block, AdaLayerNorm):
                x = block(x.transpose(0, 2, 1), style).transpose(0, 2, 1)
                x = mx.concatenate([x, s.transpose(1, 2, 0)], axis=1)
                x = mx.where(m[..., None].transpose(0, 2, 1), 0.0, x)
            else:
                x = x.transpose(0, 2, 1)[0]
                x, _ = block(x)
                x = x.transpose(0, 2, 1)
                x_pad = mx.zeros([x.shape[0], x.shape[1], m.shape[-1]])
                x_pad[:, :, : x.shape[-1]] = x
                x = x_pad
        return x.transpose(0, 2, 1)


# https://github.com/yl4579/StyleTTS2/blob/main/Utils/PLBERT/util.py
# TODO: Implement this in MLX


@dataclass
class AlbertModelArgs(BaseModelArgs):
    num_hidden_layers: int
    num_attention_heads: int
    hidden_size: int
    intermediate_size: int
    max_position_embeddings: int
    model_type: str = "albert"
    embedding_size: int = 128
    inner_group_num: int = 1
    num_hidden_groups: int = 1
    hidden_dropout_prob: float = 0.1
    attention_probs_dropout_prob: float = 0.1
    type_vocab_size: int = 2
    initializer_range: float = 0.02
    layer_norm_eps: float = 1e-12
    vocab_size: int = 30522
    dropout: float = 0.0


class AlbertEmbeddings(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size)
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.embedding_size
        )
        self.token_type_embeddings = nn.Embedding(
            config.type_vocab_size, config.embedding_size
        )
        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def __call__(self, input_ids, token_type_ids=None, position_ids=None):
        seq_length = input_ids.shape[1]
        if position_ids is None:
            position_ids = mx.arange(seq_length, dtype=mx.int32)[None, :]
        if token_type_ids is None:
            token_type_ids = mx.zeros_like(input_ids)

        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class AlbertSelfAttention(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)

        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.shape[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        x = x.reshape(new_x_shape)
        return x.transpose(0, 2, 1, 3)

    def __call__(self, hidden_states, attention_mask=None):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        attention_scores = mx.matmul(query_layer, key_layer.transpose(0, 1, 3, 2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = mx.softmax(attention_scores, axis=-1)
        attention_probs = self.dropout(attention_probs)

        context_layer = mx.matmul(attention_probs, value_layer)
        context_layer = context_layer.transpose(0, 2, 1, 3)
        new_context_layer_shape = context_layer.shape[:-2] + (self.all_head_size,)
        context_layer = context_layer.reshape(new_context_layer_shape)

        context_layer = self.dense(context_layer)
        context_layer = self.LayerNorm(context_layer + hidden_states)

        return context_layer


class AlbertSelfOutput(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def __call__(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class AlbertIntermediate(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.intermediate_act_fn = nn.GELU()

    def __call__(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class AlbertOutput(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def __call__(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class AlbertLayer(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.attention = AlbertSelfAttention(config)

        self.seq_len_dim = 1
        self.full_layer_layer_norm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )
        self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)
        self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)
        self.activation = nn.GELU()

    def __call__(self, hidden_states, attention_mask=None):
        attention_output = self.attention(hidden_states, attention_mask)
        ffn_output = self.ff_chunk(attention_output)
        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output)
        return hidden_states

    def ff_chunk(self, attention_output: mx.array) -> mx.array:
        ffn_output = self.ffn(attention_output)
        ffn_output = self.activation(ffn_output)
        ffn_output = self.ffn_output(ffn_output)
        return ffn_output


class AlbertLayerGroup(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.albert_layers = [
            AlbertLayer(config) for _ in range(config.inner_group_num)
        ]

    def __call__(self, hidden_states, attention_mask=None):
        for layer_module in self.albert_layers:
            hidden_states = layer_module(hidden_states, attention_mask)
        return hidden_states


class AlbertEncoder(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.config = config
        self.embedding_hidden_mapping_in = nn.Linear(
            config.embedding_size, config.hidden_size
        )
        self.albert_layer_groups = [
            AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)
        ]

    def __call__(self, hidden_states, attention_mask=None):
        hidden_states = self.embedding_hidden_mapping_in(hidden_states)
        for i in range(self.config.num_hidden_layers):
            # Number of layers in a hidden group
            layers_per_group = int(
                self.config.num_hidden_layers / self.config.num_hidden_groups
            )

            # Index of the hidden group
            group_idx = int(
                i / (self.config.num_hidden_layers / self.config.num_hidden_groups)
            )

            layer_group_output = self.albert_layer_groups[group_idx](
                hidden_states, attention_mask
            )
            hidden_states = layer_group_output
        return hidden_states


class CustomAlbert(nn.Module):
    def __init__(self, config: AlbertModelArgs):
        super().__init__()
        self.config = config
        self.embeddings = AlbertEmbeddings(config)
        self.encoder = AlbertEncoder(config)
        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)

    def __call__(self, input_ids, token_type_ids=None, attention_mask=None):
        embedding_output = self.embeddings(input_ids, token_type_ids)

        if attention_mask is not None:
            attention_mask = attention_mask[:, None, None, :]
            attention_mask = (1.0 - attention_mask) * -10000.0

        encoder_outputs = self.encoder(embedding_output, attention_mask)
        sequence_output = encoder_outputs
        pooled_output = nn.tanh(self.pooler(sequence_output[:, 0]))

        return sequence_output, pooled_output

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if "position_ids" in k:
                # Remove unused position_ids
                continue
            else:
                sanitized_weights[k] = v
        return sanitized_weights



================================================
FILE: mlx_audio/tts/models/kokoro/pipeline.py
================================================
import logging
import re
from dataclasses import dataclass
from numbers import Number
from typing import Any, Generator, List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import hf_hub_download
from misaki import en, espeak

from .voice import load_voice_tensor

ALIASES = {
    "en-us": "a",
    "en-gb": "b",
    "es": "e",
    "fr-fr": "f",
    "hi": "h",
    "it": "i",
    "pt-br": "p",
    "ja": "j",
    "zh": "z",
}

LANG_CODES = dict(
    # pip install misaki[en]
    a="American English",
    b="British English",
    # espeak-ng
    e="es",
    f="fr-fr",
    h="hi",
    i="it",
    p="pt-br",
    # pip install misaki[ja]
    j="Japanese",
    # pip install misaki[zh]
    z="Mandarin Chinese",
)


class KokoroPipeline:
    """
    KokoroPipeline is a language-aware support class with 2 main responsibilities:
    1. Perform language-specific G2P, mapping (and chunking) text -> phonemes
    2. Manage and store voices, lazily downloaded from HF if needed

    You are expected to have one KokoroPipeline per language. If you have multiple
    KokoroPipeline instances, you should reuse one KokoroModel instance across all of them.

    KokoroPipeline is designed to work with a KokoroModel, but this is not required.
    There are 2 ways to pass an existing model into a pipeline:
    1. On init: us_pipeline = KokoroPipeline(lang_code='a', model=model)
    2. On call: us_pipeline(text, voice, model=model)

    By default, KokoroPipeline will automatically initialize its own KokoroModel. To
    suppress this, construct a "quiet" KokoroPipeline with model=False.

    A "quiet" KokoroPipeline yields (graphemes, phonemes, None) without generating
    any audio. You can use this to phonemize and chunk your text in advance.

    A "loud" KokoroPipeline _with_ a model yields (graphemes, phonemes, audio).
    """

    def __init__(
        self,
        lang_code: str,
        model: nn.Module,
        repo_id: str,
        trf: bool = False,
    ):
        """Initialize a KokoroPipeline.

        Args:
            lang_code: Language code for G2P processing
            model: KokoroModel instance, True to create new model, False for no model
            trf: Whether to use transformer-based G2P
            device: Override default device selection ('cuda' or 'cpu', or None for auto)
                   If None, will auto-select cuda if available
                   If 'cuda' and not available, will explicitly raise an error
        """
        lang_code = lang_code.lower()
        lang_code = ALIASES.get(lang_code, lang_code)
        assert lang_code in LANG_CODES, (lang_code, LANG_CODES)
        self.lang_code = lang_code
        self.repo_id = repo_id
        if repo_id is None:
            raise ValueError("repo_id is required to load voices")
        self.model = model
        self.voices = {}
        if lang_code in "ab":
            try:
                fallback = espeak.EspeakFallback(british=lang_code == "b")
            except Exception as e:
                logging.warning("EspeakFallback not Enabled: OOD words will be skipped")
                logging.warning({str(e)})
                fallback = None
            self.g2p = en.G2P(
                trf=trf, british=lang_code == "b", fallback=fallback, unk=""
            )
        elif lang_code == "j":
            try:
                from misaki import ja

                self.g2p = ja.JAG2P()
            except ImportError:
                logging.error(
                    "You need to `pip install misaki[ja]` to use lang_code='j'"
                )
                raise
        elif lang_code == "z":
            try:
                from misaki import zh

                self.g2p = zh.ZHG2P()
            except ImportError:
                logging.error(
                    "You need to `pip install misaki[zh]` to use lang_code='z'"
                )
                raise
        else:
            language = LANG_CODES[lang_code]
            logging.warning(
                f"Using EspeakG2P(language='{language}'). Chunking logic not yet implemented, so long texts may be truncated unless you split them with '\\n'."
            )
            self.g2p = espeak.EspeakG2P(language=language)

    def load_single_voice(self, voice: str) -> mx.array:
        if voice in self.voices:
            return self.voices[voice]
        if voice.endswith(".pt"):
            f = voice
        else:
            f = hf_hub_download(repo_id=self.repo_id, filename=f"voices/{voice}.pt")
            if not voice.startswith(self.lang_code):
                v = LANG_CODES.get(voice, voice)
                p = LANG_CODES.get(self.lang_code, self.lang_code)
                logging.warning(
                    f"Language mismatch, loading {v} voice into {p} pipeline."
                )
        pack = mx.array(load_voice_tensor(f))
        self.voices[voice] = pack
        return pack

    """
    load_voice is a helper function that lazily downloads and loads a voice:
    Single voice can be requested (e.g. 'af_bella') or multiple voices (e.g. 'af_bella,af_jessica').
    If multiple voices are requested, they are averaged.
    Delimiter is optional and defaults to ','.
    """

    def load_voice(self, voice: str, delimiter: str = ",") -> mx.array:
        if voice in self.voices:
            return self.voices[voice]
        logging.debug(f"Loading voice: {voice}")
        packs = [self.load_single_voice(v) for v in voice.split(delimiter)]
        if len(packs) == 1:
            return packs[0]
        self.voices[voice] = mx.mean(mx.stack(packs), axis=0)
        return self.voices[voice]

    @classmethod
    def tokens_to_ps(cls, tokens: List[en.MToken]) -> str:
        return "".join(
            t.phonemes + (" " if t.whitespace else "") for t in tokens
        ).strip()

    @classmethod
    def waterfall_last(
        cls,
        tokens: List[en.MToken],
        next_count: int,
        waterfall: List[str] = ["!.?â€¦", ":;", ",â€”"],
        bumps: List[str] = [")", "â€"],
    ) -> int:
        for w in waterfall:
            z = next(
                (
                    i
                    for i, t in reversed(list(enumerate(tokens)))
                    if t.phonemes in set(w)
                ),
                None,
            )
            if z is None:
                continue
            z += 1
            if z < len(tokens) and tokens[z].phonemes in bumps:
                z += 1
            if next_count - len(cls.tokens_to_ps(tokens[:z])) <= 510:
                return z
        return len(tokens)

    @classmethod
    def tokens_to_text(cls, tokens: List[en.MToken]) -> str:
        return "".join(t.text + t.whitespace for t in tokens).strip()

    def en_tokenize(
        self, tokens: List[en.MToken]
    ) -> Generator[Tuple[str, str, List[en.MToken]], None, None]:
        tks = []
        pcount = 0
        for t in tokens:
            # American English: É¾ => T
            t.phonemes = "" if t.phonemes is None else t.phonemes.replace("É¾", "T")
            next_ps = t.phonemes + (" " if t.whitespace else "")
            next_pcount = pcount + len(next_ps.rstrip())
            if next_pcount > 510:
                z = KokoroPipeline.waterfall_last(tks, next_pcount)
                text = KokoroPipeline.tokens_to_text(tks[:z])
                logging.debug(
                    f"Chunking text at {z}: '{text[:30]}{'...' if len(text) > 30 else ''}'"
                )
                ps = KokoroPipeline.tokens_to_ps(tks[:z])
                yield text, ps, tks[:z]
                tks = tks[z:]
                pcount = len(KokoroPipeline.tokens_to_ps(tks))
                if not tks:
                    next_ps = next_ps.lstrip()
            tks.append(t)
            pcount += len(next_ps)
        if tks:
            text = KokoroPipeline.tokens_to_text(tks)
            ps = KokoroPipeline.tokens_to_ps(tks)
            yield "".join(text).strip(), "".join(ps).strip(), tks

    @classmethod
    def infer(
        cls,
        model: nn.Module,
        ps: str,
        pack: mx.array,
        speed: Number = 1,
    ):
        return model(ps, pack[len(ps) - 1], speed, return_output=True)

    def generate_from_tokens(
        self,
        tokens: Union[str, List[en.MToken]],
        voice: str,
        speed: Number = 1,
        model: Optional[nn.Module] = None,
    ) -> Generator["KokoroPipeline.Result", None, None]:
        """Generate audio from either raw phonemes or pre-processed tokens.

        Args:
            tokens: Either a phoneme string or list of pre-processed MTokens
            voice: The voice to use for synthesis
            speed: Speech speed modifier (default: 1)
            model: Optional Model instance (uses pipeline's model if not provided)

        Yields:
            KokoroPipeline.Result containing the input tokens and generated audio

        Raises:
            ValueError: If no voice is provided or token sequence exceeds model limits
        """
        model = model or self.model
        if model and voice is None:
            raise ValueError(
                'Specify a voice: pipeline.generate_from_tokens(..., voice="af_heart")'
            )

        pack = self.load_voice(voice) if model else None

        # Handle raw phoneme string
        if isinstance(tokens, str):
            logging.debug("Processing phonemes from raw string")
            if len(tokens) > 510:
                raise ValueError(f"Phoneme string too long: {len(tokens)} > 510")
            output = KokoroPipeline.infer(model, tokens, pack, speed) if model else None
            yield self.Result(graphemes="", phonemes=tokens, output=output)
            return

        logging.debug("Processing MTokens")
        # Handle pre-processed tokens
        for gs, ps, tks in self.en_tokenize(tokens):
            if not ps:
                continue
            elif len(ps) > 510:
                logging.warning(
                    f"Unexpected len(ps) == {len(ps)} > 510 and ps == '{ps}'"
                )
                logging.warning("Truncating to 510 characters")
                ps = ps[:510]
            output = KokoroPipeline.infer(model, ps, pack, speed) if model else None
            if output is not None and output.pred_dur is not None:
                KokoroPipeline.join_timestamps(tks, output.pred_dur)
            yield self.Result(graphemes=gs, phonemes=ps, tokens=tks, output=output)

    @classmethod
    def join_timestamps(cls, tokens: List[en.MToken], pred_dur: mx.array):
        # Multiply by 600 to go from pred_dur frames to sample_rate 24000
        # Equivalent to dividing pred_dur frames by 40 to get timestamp in seconds
        # We will count nice round half-frames, so the divisor is 80
        MAGIC_DIVISOR = 80
        if not tokens or len(pred_dur) < 3:
            # We expect at least 3: <bos>, token, <eos>
            return
        # We track 2 counts, measured in half-frames: (left, right)
        # This way we can cut space characters in half
        # TODO: Is -3 an appropriate offset?
        left = right = 2 * max(0, pred_dur[0].item() - 3)
        # Updates:
        # left = right + (2 * token_dur) + space_dur
        # right = left + space_dur
        i = 1
        for t in tokens:
            if i >= len(pred_dur) - 1:
                break
            if not t.phonemes:
                if t.whitespace:
                    i += 1
                    left = right + pred_dur[i].item()
                    right = left + pred_dur[i].item()
                    i += 1
                continue
            j = i + len(t.phonemes)
            if j >= len(pred_dur):
                break
            t.start_ts = left / MAGIC_DIVISOR
            token_dur = pred_dur[i:j].sum().item()
            space_dur = pred_dur[j].item() if t.whitespace else 0
            left = right + (2 * token_dur) + space_dur
            t.end_ts = left / MAGIC_DIVISOR
            right = left + space_dur
            i = j + (1 if t.whitespace else 0)

    @dataclass
    class Result:
        graphemes: str
        phonemes: str
        tokens: Optional[List[en.MToken]] = None
        output: Optional[Any] = None
        text_index: Optional[int] = None

        @property
        def audio(self) -> Optional[mx.array]:
            return None if self.output is None else self.output.audio

        @property
        def pred_dur(self) -> Optional[mx.array]:
            return None if self.output is None else self.output.pred_dur

        ### MARK: BEGIN BACKWARD COMPAT ###
        def __iter__(self):
            yield self.graphemes
            yield self.phonemes
            yield self.audio

        def __getitem__(self, index):
            return [self.graphemes, self.phonemes, self.audio][index]

        def __len__(self):
            return 3

    def __call__(
        self,
        text: Union[str, List[str]],
        voice: Optional[str] = None,
        speed: Number = 1,
        split_pattern: Optional[str] = r"\n+",
    ) -> Generator["KokoroPipeline.Result", None, None]:
        if voice is None:
            raise ValueError(
                'Specify a voice: en_us_pipeline(text="Hello world!", voice="af_heart")'
            )
        pack = self.load_voice(voice) if self.model else None
        if isinstance(text, str):
            text = re.split(split_pattern, text.strip()) if split_pattern else [text]
        # Process each segment
        for graphemes_index, graphemes in enumerate(text):
            if not graphemes.strip():  # Skip empty segments
                continue

            # English processing (unchanged)
            if self.lang_code in "ab":
                # print(f"Processing English text: {graphemes[:50]}{'...' if len(graphemes) > 50 else ''}")
                _, tokens = self.g2p(graphemes)
                for gs, ps, tks in self.en_tokenize(tokens):
                    if not ps:
                        continue
                    elif len(ps) > 510:
                        logging.warning(
                            f"Unexpected len(ps) == {len(ps)} > 510 and ps == '{ps}'"
                        )
                        ps = ps[:510]
                    output = (
                        KokoroPipeline.infer(self.model, ps, pack, speed)
                        if self.model
                        else None
                    )
                    if output is not None and output.pred_dur is not None:
                        KokoroPipeline.join_timestamps(tks, output.pred_dur)
                    yield self.Result(
                        graphemes=gs,
                        phonemes=ps,
                        tokens=tks,
                        output=output,
                        text_index=graphemes_index,
                    )

            # Non-English processing with chunking
            else:
                # Split long text into smaller chunks (roughly 400 characters each)
                # Using sentence boundaries when possible
                chunk_size = 400
                chunks = []

                # Try to split on sentence boundaries first
                sentences = re.split(r"([.!?]+)", graphemes)
                current_chunk = ""

                for i in range(0, len(sentences), 2):
                    sentence = sentences[i]
                    # Add the punctuation back if it exists
                    if i + 1 < len(sentences):
                        sentence += sentences[i + 1]

                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += sentence
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence

                if current_chunk:
                    chunks.append(current_chunk.strip())

                # If no chunks were created (no sentence boundaries), fall back to character-based chunking
                if not chunks:
                    chunks = [
                        graphemes[i : i + chunk_size]
                        for i in range(0, len(graphemes), chunk_size)
                    ]

                # Process each chunk
                for chunk in chunks:
                    if not chunk.strip():
                        continue

                    ps, _ = self.g2p(chunk)
                    if not ps:
                        continue
                    elif len(ps) > 510:
                        logging.warning(f"Truncating len(ps) == {len(ps)} > 510")
                        ps = ps[:510]

                    output = (
                        KokoroPipeline.infer(self.model, ps, pack, speed)
                        if self.model
                        else None
                    )
                    yield self.Result(
                        graphemes=chunk,
                        phonemes=ps,
                        output=output,
                        text_index=graphemes_index,
                    )



================================================
FILE: mlx_audio/tts/models/kokoro/voice.py
================================================
import io
import pickle
import sys
import zipfile

import numpy as np


def load_voice_tensor(path: str) -> np.ndarray:
    """
    Load a voice pack .pt file into a NumPy array.
    Handles either flat layout or one extra top-level folder.
    """
    # map PyTorch storage names to NumPy dtypes
    _STORAGE_TO_DTYPE = {
        "FloatStorage": np.float32,
        "DoubleStorage": np.float64,
        "HalfStorage": np.float16,
        "IntStorage": np.int32,
        "LongStorage": np.int64,
        "ByteStorage": np.uint8,
        "CharStorage": np.int8,
        "ShortStorage": np.int16,
        "BoolStorage": np.bool_,
    }
    storages: dict[str, np.ndarray] = {}

    with zipfile.ZipFile(path, "r") as zf:
        names = zf.namelist()

        # detect optional top-level prefix
        if "byteorder" in names:
            prefix = ""
        else:
            tops = {n.split("/", 1)[0] for n in names if "/" in n}
            prefix = (tops.pop() + "/") if len(tops) == 1 else ""

        try:
            byteorder = zf.read(f"{prefix}byteorder").decode("ascii").strip()
        except KeyError:
            byteorder = sys.byteorder

        # build a helper for retrieving raw storage blobs
        def _persistent_load(pid):
            typename, storage_type, root_key, _, numel = pid
            if typename != "storage":
                raise RuntimeError(f"Unknown persistent id: {typename}")
            if root_key not in storages:
                raw = zf.read(f"{prefix}data/{root_key}")
                name = storage_type.__name__
                try:
                    dtype = _STORAGE_TO_DTYPE[name]
                except KeyError:
                    raise RuntimeError(f"Unsupported storage type: {name}")
                if byteorder != sys.byteorder:
                    dtype = dtype.newbyteorder()
                storages[root_key] = np.frombuffer(raw, dtype=dtype, count=numel)
            return storages[root_key]

        # mimic torch._utils._rebuild_tensor_v2
        def _rebuild_tensor_v2(
            storage, storage_offset, size, stride, requires_grad, backward_hooks
        ):
            count = 1
            for d in size:
                count *= d
            segment = storage[storage_offset : storage_offset + count]
            return segment.reshape(size)

        class _NoTorchUnpickler(pickle.Unpickler):
            def persistent_load(self, pid):
                return _persistent_load(pid)

            def find_class(self, module, name):
                if module == "torch._utils" and name == "_rebuild_tensor_v2":
                    return _rebuild_tensor_v2
                return super().find_class(module, name)

        data_pkl = zf.read(f"{prefix}data.pkl")
        unpickler = _NoTorchUnpickler(io.BytesIO(data_pkl))
        return unpickler.load()



================================================
FILE: mlx_audio/tts/models/llama/__init__.py
================================================
from .llama import Model, ModelConfig

__all__ = ["Model", "ModelConfig"]



================================================
FILE: mlx_audio/tts/models/llama/llama.py
================================================
import time
from dataclasses import dataclass
from typing import List, Optional

import mlx.core as mx
from mlx_lm.generate import stream_generate
from mlx_lm.models.llama import Model as LlamaModel
from mlx_lm.models.llama import ModelArgs as LlamaModelConfig
from mlx_lm.sample_utils import make_logits_processors, make_sampler
from tqdm import tqdm
from transformers import AutoTokenizer

from mlx_audio.codec.models.snac import SNAC

from ..base import GenerationResult


@dataclass
class ModelConfig(LlamaModelConfig):
    tokenizer_name: str = "mlx-community/orpheus-3b-0.1-ft-bf16"
    sample_rate: int = 24000

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


snac_model = SNAC.from_pretrained("mlx-community/snac_24khz").eval()


def decode_audio_from_codes(code_list):
    layer_1 = []
    layer_2 = []
    layer_3 = []
    for i in range((len(code_list) + 1) // 7):
        layer_1.append(code_list[7 * i])
        layer_2.append(code_list[7 * i + 1] - 4096)
        layer_3.append(code_list[7 * i + 2] - (2 * 4096))
        layer_3.append(code_list[7 * i + 3] - (3 * 4096))
        layer_2.append(code_list[7 * i + 4] - (4 * 4096))
        layer_3.append(code_list[7 * i + 5] - (5 * 4096))
        layer_3.append(code_list[7 * i + 6] - (6 * 4096))
    codes = [
        mx.expand_dims(mx.array(layer_1), 0),
        mx.expand_dims(mx.array(layer_2), 0),
        mx.expand_dims(mx.array(layer_3), 0),
    ]
    audio_hat = snac_model.decode(codes).squeeze(-1)
    return audio_hat


def encode_audio_to_codes(audio):
    audio = audio[None, None, :]

    codes = snac_model.encode(audio)

    layer_1 = codes[0].squeeze(0).tolist()
    layer_2 = codes[1].squeeze(0).tolist()
    layer_3 = codes[2].squeeze(0).tolist()

    code_list = []
    num_groups = len(layer_1)
    for i in range(num_groups):
        code_list.append(layer_1[i])
        code_list.append(layer_2[2 * i] + 4096)
        code_list.append(layer_3[4 * i] + 2 * 4096)
        code_list.append(layer_3[4 * i + 1] + 3 * 4096)
        code_list.append(layer_2[2 * i + 1] + 4 * 4096)
        code_list.append(layer_3[4 * i + 2] + 5 * 4096)
        code_list.append(layer_3[4 * i + 3] + 6 * 4096)

    return mx.array(code_list)[None, :]


class Model(LlamaModel):
    def __init__(self, config: ModelConfig, **kwargs):
        super().__init__(config)
        self.config = config
        self.model_type = config.model_type
        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)

    @property
    def layers(self):
        return self.model.layers

    @property
    def sample_rate(self):
        return self.config.sample_rate

    def parse_output(self, input_ids):
        token_to_find = 128257
        token_to_remove = 128258

        # MLX doesn't have nonzero, so we need to create indices manually
        mask = input_ids == token_to_find
        indices = []
        for i in range(mask.shape[0]):
            for j in range(mask.shape[1]):
                if mask[i, j]:
                    indices.append((i, j))
        token_indices = [[], []]
        for i, j in indices:
            token_indices[0].append(i)
            token_indices[1].append(j)

        token_indices = mx.array(token_indices)

        if len(token_indices[1]) > 0:
            last_occurrence_idx = int(token_indices[1][-1])
            cropped_tensor = input_ids[:, last_occurrence_idx + 1 :]
        else:
            cropped_tensor = input_ids

        mask = cropped_tensor != token_to_remove

        processed_rows = []

        for row in cropped_tensor:
            # Create a mask and filter manually since boolean indexing isn't supported
            row_list = row.tolist()
            masked_row = mx.array([val for val in row_list if val != token_to_remove])
            processed_rows.append(masked_row)

        code_lists = []

        for row in processed_rows:
            row_length = row.shape[0]
            new_length = (row_length // 7) * 7
            trimmed_row = row[:new_length]
            trimmed_row = [t - 128266 for t in trimmed_row]
            code_lists.append(trimmed_row)

        return code_lists

    def prepare_input_ids(
        self,
        prompts: List[str],
        voice: Optional[str] = None,
        ref_audio: Optional[mx.array] = None,
        ref_text: Optional[str] = None,
    ):
        audio_input_ids = None
        if ref_audio is not None and ref_text is not None:
            print(
                "\033[93mWARNING: Audio cloning doesn't work reliably on Orpheus.\033[0m \nA known issue affecting Torch and MLX versions. \nWill be fixed once the Canopy labs repo update their code or the model."
            )
            audio_input_ids = encode_audio_to_codes(ref_audio) + 128266
            audio_transcript_ids = self.tokenizer(
                ref_text, return_tensors="mlx"
            ).input_ids
        elif voice is not None:
            prompts = [f"{voice}: " + p for p in prompts]

        start_token = mx.array([[128259]], dtype=mx.int64)  # Start of human
        end_tokens = mx.array(
            [[128009, 128260]], dtype=mx.int64
        )  # End of text, End of human

        prompt_input_ids = []
        for prompt in prompts:
            prompt_input_ids.append(
                self.tokenizer(prompt, return_tensors="mlx").input_ids
            )

        batch_input_ids = []
        pad_token = mx.array([128263], dtype=mx.int64)
        max_len = max([p.shape[1] for p in prompt_input_ids])

        for input_ids in prompt_input_ids:
            modified_input_ids = []

            padding_len = max_len - input_ids.shape[1]
            if padding_len > 0:
                modified_input_ids.append(mx.repeat(pad_token, padding_len)[None, :])

            # reference audio and transcript
            if audio_input_ids is not None:
                audio_start_tokens = mx.array([[128261, 128257]], dtype=mx.int64)
                audio_end_tokens = mx.array([[128258, 128262]], dtype=mx.int64)
                ref_input_ids = mx.concatenate(
                    [
                        start_token,
                        audio_transcript_ids,
                        end_tokens,
                        audio_start_tokens,
                        audio_input_ids,
                        audio_end_tokens,
                    ],
                    axis=1,
                )
                modified_input_ids.append(ref_input_ids)

            # prompt
            one_prompt_input_ids = mx.concatenate(
                [start_token, input_ids, end_tokens], axis=1
            )  # SOH SOT Text EOT EOH
            modified_input_ids.append(one_prompt_input_ids)

            batch_input_ids.append(mx.concatenate(modified_input_ids, axis=1))

        batch_input_ids = mx.concatenate(batch_input_ids, axis=0)
        batch_mask = mx.where(batch_input_ids == pad_token, False, True)

        return batch_input_ids, batch_mask

    def generate(
        self,
        text,
        voice: str,
        temperature: float = 0.6,
        top_p: float = 0.8,
        split_pattern: str = "\n",
        max_tokens: int = 1200,
        verbose: bool = False,
        ref_audio: mx.array = None,
        ref_text: Optional[str] = None,
        **kwargs,
    ):
        prompt = text.replace("\\n", "\n").replace("\\t", "\t")
        prompts = prompt.split(split_pattern)

        input_ids, _ = self.prepare_input_ids(
            prompts,
            voice,
            ref_audio,
            ref_text,
        )

        sampler = make_sampler(temperature, top_p, top_k=kwargs.get("top_k", -1))
        logits_processors = make_logits_processors(
            kwargs.get("logit_bias", None),
            kwargs.get("repetition_penalty", 1.3),
            kwargs.get("repetition_context_size", 20),
        )

        time_start = time.time()
        # TODO: Support batch processing as in the Colab: https://github.com/canopyai/Orpheus-TTS
        for i, response in enumerate(
            tqdm(
                stream_generate(
                    self,
                    tokenizer=self.tokenizer,
                    prompt=input_ids.squeeze(0),
                    max_tokens=max_tokens,
                    sampler=sampler,
                    logits_processors=logits_processors,
                ),
                total=max_tokens,
                disable=not verbose,
            )
        ):
            next_token = mx.array([response.token])
            input_ids = mx.concatenate([input_ids, next_token[None, :]], axis=1)
            if i % 50 == 0:
                mx.clear_cache()

            if next_token == 128258:
                break

        code_lists = self.parse_output(input_ids)

        my_samples = []
        for code_list in code_lists:
            samples = decode_audio_from_codes(code_list)
            my_samples.append(samples)

        time_end = time.time()

        if len(prompts) != len(my_samples):
            raise Exception("Number of prompts and samples do not match")
        else:
            for i in range(len(my_samples)):
                audio = my_samples[i][0]

                samples = audio.shape[0] if audio is not None else 0
                assert samples > 0, "No audio generated"

                # Calculate token count
                token_count = input_ids.shape[1] if input_ids is not None else 0

                # Calculate audio duration in seconds
                sample_rate = self.config.sample_rate
                audio_duration_seconds = samples / sample_rate

                # Calculate real-time factor (RTF)
                rtf = audio_duration_seconds / (time_end - time_start)

                # Format duration as HH:MM:SS.mmm
                duration_mins = int(audio_duration_seconds // 60)
                duration_secs = int(audio_duration_seconds % 60)
                duration_ms = int((audio_duration_seconds % 1) * 1000)
                duration_hours = int(audio_duration_seconds // 3600)
                duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

                yield GenerationResult(
                    audio=audio,
                    samples=samples,
                    sample_rate=sample_rate,
                    segment_idx=i,
                    token_count=token_count,
                    audio_duration=duration_str,
                    real_time_factor=rtf,
                    prompt={
                        "tokens": token_count,
                        "tokens-per-sec": (
                            round(token_count / audio_duration_seconds, 2)
                            if audio_duration_seconds > 0
                            else 0
                        ),
                    },
                    audio_samples={
                        "samples": samples,
                        "samples-per-sec": (
                            round(samples / audio_duration_seconds, 2)
                            if audio_duration_seconds > 0
                            else 0
                        ),
                    },
                    processing_time_seconds=time_end - time_start,
                    peak_memory_usage=mx.get_peak_memory() / 1e9,
                )

                # Clear cache after each segment to avoid memory leaks
                mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/outetts/__init__.py
================================================
from .outetts import Model, ModelConfig



================================================
FILE: mlx_audio/tts/models/outetts/audio_processor.py
================================================
import io
import json
import os
from dataclasses import asdict
from typing import Union

import mlx.core as mx
import numpy as np

from mlx_audio.stt.utils import SAMPLE_RATE as WHISPER_SAMPLE_RATE
from mlx_audio.stt.utils import load_model, resample_audio

from .dac_interface import DacInterface
from .prompt_processor import PromptProcessor


def calculate_pitch(
    audio_array: mx.array,
    sr: int,
    min_freq: float = 75.0,
    max_freq: float = 600.0,
    frame_length: int = 400,
    hop_length: int = 160,
    threshold: float = 0.3,
) -> mx.array:
    """
    Calculate pitch frequencies for short audio clips using autocorrelation.

    Args:
        audio_array: Input audio array (1D or 2D [channels, samples])
        sr: Sampling rate
        min_freq: Minimum detectable frequency (Hz)
        max_freq: Maximum detectable frequency (Hz)
        frame_length: Analysis frame length in samples
        hop_length: Hop size in samples
        threshold: Voicing threshold (0.0-1.0)

    Returns:
        Array of pitch values (Hz) per frame
    """
    audio_np = np.array(audio_array)

    # convert to mono and ensure 1D
    if len(audio_np.shape) > 1:
        audio_np = np.mean(audio_np, axis=0)
    audio_np = np.squeeze(audio_np)

    num_samples = audio_np.shape[-1]
    pad_len = (frame_length - (num_samples % hop_length)) % hop_length
    audio_np = np.pad(audio_np, (0, pad_len))

    num_frames = (len(audio_np) - frame_length) // hop_length + 1
    frames = np.zeros((num_frames, frame_length))
    for i in range(num_frames):
        frames[i] = audio_np[i * hop_length : i * hop_length + frame_length]

    window = np.hanning(frame_length)
    frames_windowed = frames * window

    # compute autocorrelation using FFT
    fft_frames = np.fft.rfft(frames_windowed, n=2 * frame_length, axis=1)
    power_spectrum = fft_frames.real**2 + fft_frames.imag**2
    autocorr = np.fft.irfft(power_spectrum, axis=1)[:, :frame_length]

    # find valid frequency range indices
    min_idx = max(1, int(sr / max_freq))
    max_idx = min(frame_length, int(sr / min_freq))

    # find peak indices in valid range
    relevant_autocorr = autocorr[:, min_idx:max_idx]
    peak_indices = np.argmax(relevant_autocorr, axis=1) + min_idx
    peak_values = np.array([autocorr[i, peak_indices[i]] for i in range(num_frames)])

    # parabolic interpolation for sub-sample accuracy
    indices = np.clip(peak_indices, 1, frame_length - 2)
    alpha = np.array([autocorr[i, indices[i] - 1] for i in range(num_frames)])
    beta = np.array([autocorr[i, indices[i]] for i in range(num_frames)])
    gamma = np.array([autocorr[i, indices[i] + 1] for i in range(num_frames)])

    delta = 0.5 * (alpha - gamma) / (alpha - 2 * beta + gamma + 1e-8)
    valid_mask = (peak_indices > 0) & (peak_indices < frame_length - 1)
    delta = np.where(valid_mask, delta, 0.0)

    # calculate final periods and pitches
    best_period = (peak_indices + delta) / sr
    pitch = np.where(best_period > 0, 1.0 / best_period, 0.0)

    # apply voicing threshold
    autocorr_0 = autocorr[:, 0]
    voiced = (peak_values / (autocorr_0 + 1e-8)) > threshold
    pitch = np.where(voiced, pitch, 0.0)

    # clamp valid frequencies
    pitch = np.clip(pitch, min_freq, max_freq)

    return mx.array(pitch)


def extract_single_pitch_value(
    audio_array: mx.array,
    sr: int,
    min_freq: float = 75.0,
    max_freq: float = 600.0,
    frame_length: int = 400,
    hop_length: int = 160,
    threshold: float = 0.3,
) -> float:
    """
    Calculates the average pitch of an audio array and normalizes it to 0-1 range.

    Args:
        audio_array: Input audio array (1D or 2D [channels, samples])
        sr: Sampling rate
        min_freq: Minimum detectable frequency (Hz)
        max_freq: Maximum detectable frequency (Hz)
        frame_length: Analysis frame length in samples
        hop_length: Hop size in samples
        threshold: Voicing threshold (0.0-1.0)

    Returns:
        A single float value representing the normalized average pitch (0.0-1.0).
    """
    pitch_array = calculate_pitch(
        audio_array, sr, min_freq, max_freq, frame_length, hop_length, threshold
    )

    # calculate the average pitch across frames
    average_pitch = float(mx.mean(pitch_array))

    # normalize to 0-1 range
    normalized_pitch = (average_pitch - min_freq) / (max_freq - min_freq)

    # clamp to ensure it's strictly within 0-1
    normalized_pitch = min(max(normalized_pitch, 0.0), 1.0)

    return normalized_pitch


class Features:
    def __init__(self):
        self.eps = 1e-10

    def scale_values(self, value: float) -> int:
        """
        Scale a value from [0,1] to [0,100] and round to nearest integer
        """
        return round(value * 100)

    def features_to_tokens(self, features: dict) -> list:
        """
        Convert features to token strings in format <|feature_value|>
        """
        return [f"<|{name}_{value}|>" for name, value in features.items()]

    def validate_audio(self, audio: mx.array) -> bool:
        if audio is None or not isinstance(audio, mx.array):
            return False
        if audio.size == 0:  # Check if array is empty
            return False
        audio_np = np.array(audio)
        if np.isnan(audio_np).any() or np.isinf(audio_np).any():
            return False
        return True

    def get_default_features(self) -> dict:
        """
        Return default feature values when audio is invalid
        """
        return {"energy": 0, "spectral_centroid": 0, "pitch": 0}

    def extract_audio_features(self, audio: mx.array, sr: int) -> dict:
        """
        Extract fast-to-compute features from audio segments.
        Each feature is normalized to [0, 1] range.

        Args:
            audio: Audio array of shape [channels, samples]
            sr: Sample rate

        Returns:
            Dictionary of features, each as a single float value
        """
        if not self.validate_audio(audio):
            return self.get_default_features()

        audio_np = np.array(audio)

        # convert to mono if stereo
        if len(audio_np.shape) == 2 and audio_np.shape[0] > 1:
            audio_np = np.mean(audio_np, axis=0, keepdims=True)

        audio = mx.array(audio_np)

        features = {}

        # rms energy (loudness) - normalized to [0, 1]
        features["energy"] = float(mx.sqrt(mx.mean(audio**2)))

        # spectral centroid - normalized to [0, 1]
        spec_np = np.abs(np.fft.rfft(audio_np))
        freqs_np = np.linspace(0, sr / 2, spec_np.shape[-1])
        spec_sum = np.sum(spec_np) + self.eps
        centroid = np.sum(freqs_np * spec_np.squeeze()) / spec_sum
        features["spectral_centroid"] = float(centroid / (sr / 2))

        # pitch - normalized to [0, 1]
        features["pitch"] = extract_single_pitch_value(audio, sr)

        # scale values to 0-100 range
        for name, value in features.items():
            features[name] = self.scale_values(value)

        return features


class AudioProcessor:
    def __init__(
        self, audio_codec_path: str = "mlx-community/dac-speech-24khz-1.5kbps"
    ):
        self.features = Features()
        self.audio_codec = DacInterface(audio_codec_path)

    def create_speaker_from_whisper(
        self,
        audio: str,
        whisper_model: str = "mlx-community/whisper-large-v3-turbo",
    ):
        if isinstance(audio, str):
            audio = self.audio_codec.load_audio(audio)
        else:
            # resample audio to 16000 for whisper
            resampled_audio = resample_audio(
                audio[..., None], self.audio_codec.sr, WHISPER_SAMPLE_RATE
            )
            resampled_audio = mx.array(resampled_audio, dtype=mx.float32).mean(axis=1)

            # convert to 2d array
            audio = audio[None, None, ...]

        seconds = audio.flatten().shape[0] / self.audio_codec.sr
        if seconds > 20:
            print(
                "Speaker audio is longer than 20 seconds. Use a shorter clip for best results."
            )
        if seconds > 15:
            print(
                "Speaker audio is longer than 15 seconds. For best results, consider using an audio clip up to 15 seconds."
            )

        # load whisper model
        whisper_model = load_model(whisper_model)

        # transcribe audio
        data = whisper_model.generate(resampled_audio.flatten(), word_timestamps=True)
        data = asdict(data)

        # clear memory
        del whisper_model
        mx.clear_cache()

        text = PromptProcessor.text_normalizations(data["text"])
        words = []
        for s in data["segments"]:
            words.extend(
                [
                    {
                        "word": i["word"].strip(),
                        "start": float(i["start"]),
                        "end": float(i["end"]),
                    }
                    for i in s["words"]
                ]
            )

        return self.create_speaker_from_dict(
            {"audio": {"bytes": audio}, "text": text, "words": words}
        )

    def create_speaker_from_dict(self, data: dict):
        audio = data["audio"]["bytes"]
        if isinstance(audio, str):
            audio = io.BytesIO(audio)
            audio = self.audio_codec.load_audio(audio)

        full_codes = self.audio_codec.encode(audio, verbose=True).tolist()[0]

        c1 = full_codes[0]
        c2 = full_codes[1]

        sr = self.audio_codec.sr
        text = data["text"]
        words = data["words"]

        tps = 75

        audio = audio.squeeze(0)
        global_features = self.features.extract_audio_features(audio, sr)

        start = None
        word_codes = []
        max_extension = 20

        for idx, i in enumerate(words):
            if start is None:
                start = max(0, int(i["start"] * tps) - max_extension)
            word = i["word"].strip()
            if idx == len(words) - 1:
                end = min(len(c1), int(i["end"] * tps) + max_extension)
            else:
                end = int(i["end"] * tps)

            word_c1 = c1[start:end]
            word_c2 = c2[start:end]

            word_audio = audio[:, int(i["start"] * sr) : int(i["end"] * sr)]
            features = self.features.extract_audio_features(word_audio, sr)

            start = end

            word_codes.append(
                {
                    "word": word,
                    "duration": round(len(word_c1) / tps, 2),
                    "c1": word_c1,
                    "c2": word_c2,
                    "features": features,
                }
            )

        return {"text": text, "words": word_codes, "global_features": global_features}

    def save_speaker(self, speaker: dict, path: str):
        # Expand ~ to home directory to save in ~/.cache/mlx_audio/voices
        path = os.path.expanduser(path)
        os.makedirs(os.path.dirname(path), exist_ok=True)

        with open(path, "w") as f:
            json.dump(speaker, f)

        print(f"Speaker saved to: {path}")

    def load_speaker(self, path: str):
        # Expand ~ to home directory to load from ~/.cache/mlx_audio/voices
        path = os.path.expanduser(path)
        if not os.path.exists(path):
            raise FileNotFoundError(f"Speaker file not found: {path}")

        with open(path, "r") as f:
            return json.load(f)

        print(f"Speaker loaded from: {path}")



================================================
FILE: mlx_audio/tts/models/outetts/dac_interface.py
================================================
import math

import mlx.core as mx
import numpy as np
import pyloudnorm as pyln
import scipy.signal
import soundfile as sf

from mlx_audio.codec import DAC


def process_audio_array(
    audio: mx.array,
    sample_rate: int = 24000,
    target_loudness: float = -18.0,
    peak_limit: float = -1,
    block_size: float = 0.400,
) -> mx.array:
    audio_np = np.array(audio)

    # handle multi-channel audio
    if len(audio_np.shape) > 1:
        if audio_np.shape[1] > 1:
            audio_np = np.mean(audio_np, axis=1)
        else:
            audio_np = np.squeeze(audio_np)

    original_length = len(audio_np)
    min_samples = int(block_size * sample_rate)

    if original_length < min_samples:
        pad_length = min_samples - original_length
        audio_padded = np.pad(audio_np, (0, pad_length), mode="constant")
    else:
        audio_padded = audio_np

    # measure and normalize loudness
    meter = pyln.Meter(sample_rate, block_size=block_size)
    measured_loudness = meter.integrated_loudness(audio_padded)
    normalized = pyln.normalize.loudness(
        audio_padded, measured_loudness, target_loudness
    )

    # apply peak limiting if necessary
    peak_value = np.max(np.abs(normalized))
    threshold_value = 10 ** (peak_limit / 20)
    if peak_value > threshold_value:
        normalized = pyln.normalize.peak(normalized, peak_limit)

    if original_length < min_samples:
        normalized = normalized[:original_length]

    normalized_array = mx.array(normalized).reshape(1, 1, -1)
    return normalized_array


class DacInterface:
    def __init__(self, repo_id: str = "mlx-community/dac-speech-24khz-1.5kbps"):
        self.model = DAC.from_pretrained(repo_id)
        self.sr = 24000

    def convert_audio(
        self, audio: mx.array, sr: int, target_sr: int, target_channels: int
    ):
        audio_np = np.array(audio)

        if len(audio_np.shape) < 2:
            audio_np = audio_np.reshape(1, -1)

        channels, length = audio_np.shape[-2:]

        if target_channels == 1:
            if channels > 1:
                audio_np = np.mean(audio_np, axis=-2, keepdims=True)
        elif target_channels == 2:
            if channels == 1:
                audio_np = np.repeat(audio_np, 2, axis=-2)
            elif channels > 2:
                audio_np = audio_np[..., :2, :]

        if sr != target_sr:
            new_length = int(length * target_sr / sr)
            resampled = np.zeros((target_channels, new_length))

            for ch in range(target_channels):
                resampled[ch] = scipy.signal.resample(audio_np[ch], new_length)

            audio_np = resampled

        return mx.array(audio_np)

    def convert_audio_array(self, audio: mx.array, sr):
        return self.convert_audio(audio, sr, self.sr, 1)

    def load_audio(self, path):
        audio_np, sr = sf.read(path)
        audio = mx.array(audio_np)
        if len(audio.shape) == 1:
            audio = audio.reshape(1, -1)
        # if stereo, reshape to channels-first format
        elif len(audio.shape) > 1 and audio.shape[0] > audio.shape[1]:
            audio = audio.T
        return self.convert_audio_array(audio, sr).reshape(1, 1, -1)

    def preprocess(self, audio_data):
        length = audio_data.shape[-1]
        hop_length = self.model.hop_length
        right_pad = math.ceil(length / hop_length) * hop_length - length
        audio_data = mx.pad(audio_data, [(0, 0), (0, 0), (0, right_pad)])
        return audio_data

    def encode(self, x: mx.array, win_duration: int = 5.0, verbose: bool = False):
        x = process_audio_array(x)
        nb, nac, nt = x.shape
        x = x.reshape(nb * nac, 1, nt)
        n_samples = int(win_duration * self.sr)
        n_samples = int(
            math.ceil(n_samples / self.model.hop_length) * self.model.hop_length
        )
        hop = n_samples
        codes_list = []

        if verbose:
            from tqdm import trange

            range_fn = trange
        else:
            range_fn = range

        for i in range_fn(0, nt, hop):
            chunk = x[..., i : i + n_samples]
            audio_data = self.preprocess(chunk)
            _, c, _, _, _ = self.model.encode(audio_data, None)
            codes_list.append(c)

        codes = mx.concatenate(codes_list, axis=-1)
        return codes

    def decode(self, codes: mx.array, verbose: bool = False) -> mx.array:
        model = self.model
        chunk_length = 4096
        recons = []

        if verbose:
            from tqdm import trange

            range_fn = trange
        else:
            range_fn = range

        @mx.compile
        def decode_chunk(codes):
            z = model.quantizer.from_codes(codes)[0]
            r = model.decode(z)
            return r

        for i in range_fn(0, codes.shape[-1], chunk_length):
            c = codes[..., i : i + chunk_length]
            recons.append(decode_chunk(c))

        recons = mx.concatenate(recons, axis=-1)
        return process_audio_array(recons.swapaxes(1, 2))



================================================
FILE: mlx_audio/tts/models/outetts/default_speaker.json
================================================
{
  "text": "The cat watched from the windowsill, tail flicking with quiet curiosity as the first snowflakes of winter began to fall, dusting the world in fragile white.",
  "words": [
    {
      "word": "The",
      "duration": 0.2,
      "c1": [
        720, 720, 474, 691, 607, 126, 597, 607, 897, 288, 362, 903, 333, 1009,
        79
      ],
      "c2": [
        658, 663, 237, 915, 74, 74, 966, 721, 893, 722, 630, 516, 861, 385, 149
      ],
      "features": {
        "energy": 10,
        "spectral_centroid": 15,
        "pitch": 45
      }
    },
    {
      "word": "cat",
      "duration": 0.33,
      "c1": [
        700, 597, 639, 838, 622, 336, 975, 326, 67, 375, 853, 761, 35, 363, 31,
        1000, 982, 192, 647, 564, 329, 1002, 275, 480, 551
      ],
      "c2": [
        34, 810, 457, 546, 42, 631, 339, 867, 115, 1011, 509, 369, 473, 85, 190,
        715, 391, 518, 562, 986, 749, 193, 530, 327, 820
      ],
      "features": {
        "energy": 14,
        "spectral_centroid": 21,
        "pitch": 35
      }
    },
    {
      "word": "watched",
      "duration": 0.44,
      "c1": [
        625, 668, 168, 524, 462, 151, 549, 951, 597, 820, 489, 329, 377, 144,
        112, 16, 481, 133, 195, 744, 144, 750, 288, 500, 1000, 58, 916, 597, 72,
        336, 224, 476, 581
      ],
      "c2": [
        204, 421, 318, 677, 74, 953, 903, 413, 809, 37, 634, 824, 933, 200, 14,
        1007, 111, 17, 435, 718, 559, 783, 415, 821, 958, 247, 14, 721, 158,
        235, 276, 875, 683
      ],
      "features": {
        "energy": 19,
        "spectral_centroid": 21,
        "pitch": 26
      }
    },
    {
      "word": "from",
      "duration": 0.2,
      "c1": [
        528, 668, 738, 985, 126, 924, 1003, 325, 393, 86, 114, 392, 638, 915,
        549
      ],
      "c2": [
        929, 872, 332, 296, 983, 406, 867, 568, 374, 328, 419, 348, 177, 379,
        181
      ],
      "features": {
        "energy": 10,
        "spectral_centroid": 29,
        "pitch": 14
      }
    },
    {
      "word": "the",
      "duration": 0.12,
      "c1": [470, 985, 152, 474, 967, 558, 460, 728, 470],
      "c2": [596, 246, 314, 246, 756, 238, 606, 262, 499],
      "features": {
        "energy": 23,
        "spectral_centroid": 10,
        "pitch": 23
      }
    },
    {
      "word": "windowsill,",
      "duration": 0.75,
      "c1": [
        217, 126, 549, 700, 198, 891, 95, 683, 158, 680, 16, 769, 402, 776, 295,
        258, 68, 213, 669, 865, 719, 29, 949, 329, 216, 481, 284, 224, 221, 359,
        328, 311, 415, 443, 410, 359, 600, 590, 932, 611, 905, 304, 292, 72,
        388, 333, 66, 943, 489, 648, 630, 648, 402, 972, 392, 558
      ],
      "c2": [
        911, 19, 1007, 169, 185, 182, 399, 849, 656, 963, 265, 80, 453, 768,
        919, 1010, 501, 794, 141, 123, 93, 694, 499, 174, 768, 689, 598, 686,
        10, 381, 282, 556, 126, 672, 872, 650, 990, 556, 913, 635, 174, 819,
        999, 423, 64, 272, 112, 600, 453, 678, 791, 301, 206, 187, 819, 948
      ],
      "features": {
        "energy": 17,
        "spectral_centroid": 25,
        "pitch": 24
      }
    },
    {
      "word": "tail",
      "duration": 0.6,
      "c1": [
        669, 94, 917, 202, 607, 720, 625, 597, 126, 607, 885, 700, 474, 480,
        126, 126, 551, 720, 126, 551, 720, 607, 572, 234, 114, 963, 963, 975,
        587, 119, 378, 696, 730, 375, 46, 827, 515, 447, 979, 138, 22, 267, 43,
        495, 16
      ],
      "c2": [
        1011, 336, 157, 39, 1000, 721, 862, 413, 557, 569, 74, 569, 141, 493,
        124, 775, 204, 588, 74, 588, 810, 124, 102, 1021, 83, 848, 297, 339,
        335, 684, 400, 905, 909, 710, 460, 115, 81, 628, 224, 663, 892, 247,
        392, 234, 132
      ],
      "features": {
        "energy": 15,
        "spectral_centroid": 23,
        "pitch": 34
      }
    },
    {
      "word": "flicking",
      "duration": 0.45,
      "c1": [
        978, 489, 630, 588, 436, 798, 4, 975, 245, 325, 415, 4, 393, 4, 4, 997,
        982, 437, 444, 180, 861, 868, 225, 440, 780, 597, 720, 639, 168, 426,
        114, 621, 854, 869
      ],
      "c2": [
        571, 321, 376, 232, 301, 678, 904, 630, 990, 772, 690, 870, 719, 694,
        332, 558, 301, 194, 279, 443, 852, 64, 709, 401, 401, 14, 74, 873, 134,
        754, 1002, 595, 540, 525
      ],
      "features": {
        "energy": 9,
        "spectral_centroid": 22,
        "pitch": 23
      }
    },
    {
      "word": "with",
      "duration": 0.23,
      "c1": [
        621, 392, 756, 459, 433, 881, 786, 198, 702, 847, 490, 27, 680, 146, 58,
        808, 997
      ],
      "c2": [
        460, 840, 840, 303, 847, 534, 801, 99, 662, 666, 510, 132, 376, 96, 639,
        240, 668
      ],
      "features": {
        "energy": 11,
        "spectral_centroid": 15,
        "pitch": 20
      }
    },
    {
      "word": "quiet",
      "duration": 0.37,
      "c1": [
        969, 291, 572, 720, 625, 85, 698, 478, 811, 956, 232, 85, 962, 817, 986,
        483, 835, 526, 77, 187, 178, 50, 440, 16, 198, 237, 418, 862
      ],
      "c2": [
        498, 606, 24, 629, 662, 181, 119, 678, 340, 736, 217, 204, 935, 796,
        118, 478, 818, 791, 329, 209, 5, 234, 337, 647, 110, 922, 933, 1011
      ],
      "features": {
        "energy": 12,
        "spectral_centroid": 12,
        "pitch": 43
      }
    },
    {
      "word": "curiosity",
      "duration": 0.71,
      "c1": [
        321, 402, 215, 607, 720, 224, 731, 621, 491, 720, 551, 456, 336, 688,
        476, 953, 718, 806, 410, 786, 976, 664, 855, 433, 756, 396, 699, 776,
        443, 739, 932, 22, 305, 353, 503, 564, 978, 407, 395, 798, 324, 168,
        909, 328, 328, 443, 738, 114, 962, 681, 535, 701, 382
      ],
      "c2": [
        777, 665, 629, 327, 831, 764, 162, 725, 810, 170, 629, 774, 108, 948,
        972, 449, 600, 905, 81, 765, 601, 422, 820, 746, 450, 346, 733, 77, 733,
        81, 722, 576, 286, 271, 714, 95, 346, 133, 514, 799, 122, 900, 568, 666,
        209, 668, 558, 630, 165, 587, 423, 904, 629
      ],
      "features": {
        "energy": 10,
        "spectral_centroid": 29,
        "pitch": 22
      }
    },
    {
      "word": "as",
      "duration": 0.48,
      "c1": [
        474, 936, 336, 589, 254, 854, 79, 140, 863, 854, 701, 260, 929, 140,
        669, 808, 411, 232, 434, 542, 597, 126, 551, 126, 607, 1011, 774, 681,
        94, 25, 971, 288, 305, 347, 355, 415
      ],
      "c2": [
        267, 813, 232, 361, 77, 607, 252, 933, 508, 658, 846, 849, 873, 496,
        832, 167, 440, 124, 557, 124, 736, 588, 569, 983, 497, 360, 810, 274,
        588, 365, 517, 934, 957, 839, 646, 720
      ],
      "features": {
        "energy": 7,
        "spectral_centroid": 31,
        "pitch": 23
      }
    },
    {
      "word": "the",
      "duration": 0.13,
      "c1": [359, 568, 700, 985, 80, 580, 274, 129, 600, 794],
      "c2": [423, 833, 245, 690, 209, 688, 765, 453, 677, 615],
      "features": {
        "energy": 9,
        "spectral_centroid": 26,
        "pitch": 20
      }
    },
    {
      "word": "first",
      "duration": 0.36,
      "c1": [
        997, 325, 147, 4, 780, 669, 621, 896, 30, 686, 526, 399, 210, 783, 216,
        144, 329, 448, 481, 288, 132, 600, 168, 221, 415, 415, 528
      ],
      "c2": [
        325, 666, 627, 629, 240, 665, 650, 481, 962, 328, 128, 358, 166, 264,
        555, 30, 815, 10, 669, 525, 450, 746, 919, 621, 647, 16, 601
      ],
      "features": {
        "energy": 13,
        "spectral_centroid": 28,
        "pitch": 22
      }
    },
    {
      "word": "snowflakes",
      "duration": 0.76,
      "c1": [
        1003, 680, 607, 720, 126, 668, 336, 224, 114, 997, 426, 997, 147, 221,
        359, 328, 1003, 738, 974, 151, 782, 179, 190, 553, 453, 761, 778, 23,
        128, 643, 125, 7, 345, 223, 275, 524, 325, 764, 114, 953, 70, 75, 449,
        513, 783, 830, 825, 365, 819, 920, 669, 700, 700, 720, 220, 209, 221
      ],
      "c2": [
        276, 489, 810, 975, 775, 913, 1022, 818, 340, 481, 690, 366, 924, 782,
        366, 481, 400, 998, 872, 556, 688, 719, 78, 952, 119, 412, 286, 847, 60,
        381, 86, 694, 779, 55, 246, 374, 143, 91, 209, 640, 313, 873, 295, 355,
        333, 705, 468, 1008, 317, 87, 105, 511, 260, 650, 574, 88, 690
      ],
      "features": {
        "energy": 12,
        "spectral_centroid": 29,
        "pitch": 22
      }
    },
    {
      "word": "of",
      "duration": 0.15,
      "c1": [443, 328, 528, 85, 313, 145, 588, 140, 114, 325, 325],
      "c2": [924, 835, 400, 832, 397, 1011, 695, 716, 366, 489, 487],
      "features": {
        "energy": 7,
        "spectral_centroid": 34,
        "pitch": 13
      }
    },
    {
      "word": "winter",
      "duration": 0.29,
      "c1": [
        559, 71, 549, 64, 902, 609, 206, 386, 428, 529, 92, 1020, 148, 456, 605,
        673, 958, 897, 250, 716, 236, 232
      ],
      "c2": [
        891, 358, 1016, 185, 558, 392, 63, 45, 238, 404, 603, 520, 657, 628,
        748, 649, 629, 298, 772, 483, 1008, 401
      ],
      "features": {
        "energy": 18,
        "spectral_centroid": 16,
        "pitch": 31
      }
    },
    {
      "word": "began",
      "duration": 0.24,
      "c1": [
        490, 6, 596, 669, 1011, 700, 583, 349, 666, 783, 215, 126, 61, 22, 945,
        773, 920, 975
      ],
      "c2": [
        194, 225, 140, 243, 14, 650, 929, 671, 323, 365, 556, 298, 707, 483,
        550, 57, 127, 886
      ],
      "features": {
        "energy": 11,
        "spectral_centroid": 12,
        "pitch": 18
      }
    },
    {
      "word": "to",
      "duration": 0.2,
      "c1": [
        265, 1021, 113, 178, 698, 561, 97, 402, 25, 916, 766, 660, 159, 945, 967
      ],
      "c2": [
        141, 976, 455, 403, 760, 738, 519, 123, 327, 721, 690, 904, 689, 140,
        615
      ],
      "features": {
        "energy": 13,
        "spectral_centroid": 19,
        "pitch": 20
      }
    },
    {
      "word": "fall,",
      "duration": 0.39,
      "c1": [
        781, 325, 4, 114, 997, 415, 4, 443, 953, 781, 399, 993, 489, 383, 920,
        383, 272, 755, 843, 450, 763, 392, 411, 682, 895, 443, 490, 863, 79
      ],
      "c2": [
        143, 990, 209, 990, 990, 556, 462, 952, 914, 702, 301, 833, 779, 982,
        26, 458, 519, 9, 264, 74, 304, 110, 646, 905, 185, 959, 53, 543, 909
      ],
      "features": {
        "energy": 13,
        "spectral_centroid": 14,
        "pitch": 18
      }
    },
    {
      "word": "dusting",
      "duration": 0.89,
      "c1": [
        27, 669, 490, 691, 691, 625, 625, 572, 474, 885, 215, 215, 215, 215,
        215, 215, 75, 718, 94, 924, 232, 818, 14, 232, 985, 547, 955, 4, 627,
        524, 524, 579, 462, 104, 597, 720, 720, 491, 597, 571, 802, 864, 315,
        515, 832, 219, 133, 923, 773, 245, 415, 328, 590, 80, 528, 322, 808,
        551, 625, 716, 158, 562, 712, 477, 905, 920, 424
      ],
      "c2": [
        206, 521, 77, 447, 260, 810, 74, 301, 243, 775, 243, 775, 880, 862,
        1017, 806, 806, 631, 873, 806, 806, 722, 14, 531, 630, 500, 990, 240,
        690, 431, 240, 815, 449, 273, 903, 569, 325, 629, 872, 239, 686, 189,
        774, 264, 314, 628, 107, 120, 560, 929, 1008, 610, 24, 929, 400, 949,
        431, 721, 447, 443, 774, 392, 923, 855, 747, 144, 460
      ],
      "features": {
        "energy": 14,
        "spectral_centroid": 28,
        "pitch": 30
      }
    },
    {
      "word": "the",
      "duration": 0.12,
      "c1": [396, 433, 276, 530, 316, 117, 112, 7, 531],
      "c2": [332, 479, 262, 239, 123, 239, 453, 499, 545],
      "features": {
        "energy": 23,
        "spectral_centroid": 11,
        "pitch": 30
      }
    },
    {
      "word": "world",
      "duration": 0.32,
      "c1": [
        217, 489, 897, 607, 402, 383, 496, 937, 247, 206, 790, 32, 406, 856,
        715, 458, 278, 481, 503, 399, 871, 453, 858, 392
      ],
      "c2": [
        593, 959, 461, 546, 242, 438, 81, 99, 939, 361, 269, 571, 525, 542, 246,
        10, 613, 228, 913, 252, 132, 132, 287, 559
      ],
      "features": {
        "energy": 22,
        "spectral_centroid": 11,
        "pitch": 31
      }
    },
    {
      "word": "in",
      "duration": 0.23,
      "c1": [
        558, 497, 436, 598, 607, 416, 311, 906, 955, 905, 448, 54, 92, 487, 770,
        298, 490
      ],
      "c2": [
        838, 399, 420, 819, 325, 929, 124, 214, 1021, 728, 975, 688, 132, 718,
        724, 911, 536
      ],
      "features": {
        "energy": 14,
        "spectral_centroid": 16,
        "pitch": 22
      }
    },
    {
      "word": "fragile",
      "duration": 0.41,
      "c1": [
        415, 325, 953, 359, 325, 838, 359, 764, 842, 341, 706, 674, 971, 592,
        507, 16, 628, 481, 626, 691, 1011, 610, 336, 476, 528, 637, 472, 251,
        945, 811, 406
      ],
      "c2": [
        126, 990, 374, 143, 629, 868, 338, 91, 346, 393, 407, 987, 987, 1009,
        617, 854, 824, 439, 789, 311, 810, 497, 664, 549, 135, 908, 702, 639,
        320, 698, 414
      ],
      "features": {
        "energy": 13,
        "spectral_centroid": 20,
        "pitch": 18
      }
    },
    {
      "word": "white.",
      "duration": 0.75,
      "c1": [
        26, 432, 1, 651, 998, 716, 998, 727, 978, 311, 85, 895, 279, 392, 669,
        916, 549, 1011, 97, 597, 296, 392, 526, 998, 835, 468, 871, 405, 26,
        759, 524, 107, 77, 22, 260, 682, 621, 79, 682, 411, 701, 972, 691, 720,
        551, 597, 660, 224, 236, 70, 652, 215, 126, 474, 597, 625
      ],
      "c2": [
        475, 778, 695, 612, 913, 315, 536, 593, 55, 371, 19, 560, 821, 646, 151,
        801, 821, 413, 14, 922, 629, 380, 417, 679, 487, 562, 821, 706, 324,
        896, 169, 594, 810, 864, 810, 588, 862, 969, 14, 105, 528, 165, 420,
        170, 821, 423, 977, 904, 690, 235, 702, 14, 124, 350, 74, 413
      ],
      "features": {
        "energy": 13,
        "spectral_centroid": 11,
        "pitch": 23
      }
    }
  ],
  "global_features": {
    "energy": 13,
    "spectral_centroid": 20,
    "pitch": 28
  },
  "interface_version": 3
}



================================================
FILE: mlx_audio/tts/models/outetts/outetts.py
================================================
import json
import re
import time
import uuid
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.generate import stream_generate
from mlx_lm.models.llama import Model as LlamaModel
from mlx_lm.models.llama import ModelArgs as LlamaModelConfig
from mlx_lm.models.qwen2 import Model as Qwen2Model
from mlx_lm.models.qwen2 import ModelArgs as Qwen2ModelConfig
from mlx_lm.models.qwen3 import Model as Qwen3Model
from mlx_lm.models.qwen3 import ModelArgs as Qwen3ModelConfig
from mlx_lm.sample_utils import make_logits_processors, make_sampler
from tqdm import tqdm
from transformers import AutoTokenizer

from ..base import GenerationResult
from .audio_processor import AudioProcessor
from .dac_interface import DacInterface
from .prompt_processor import PromptProcessor


@dataclass
class ModelConfig(LlamaModelConfig, Qwen2ModelConfig, Qwen3ModelConfig):
    tokenizer_name: str = "OuteAI/Llama-OuteTTS-1.0-1B"
    sample_rate: int = 24000


class Model(nn.Module):
    def __init__(self, config: ModelConfig, **kwargs):
        super().__init__()
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)
        self.model = self._initialize_model(config, **kwargs)

    def _initialize_model(self, config: ModelConfig, **kwargs) -> nn.Module:

        model_map = {"llama": LlamaModel, "qwen2": Qwen2Model, "qwen3": Qwen3Model}

        if config.model_type not in model_map:
            raise ValueError(f"Unsupported model type: {config.model_type}")

        return model_map[config.model_type](config, **kwargs)

    def sanitize(self, weights):
        weights = self.model.sanitize(weights)
        return {
            (
                f"model.{k}"
                if not k.startswith("model.model.")
                and not k.startswith("model.lm_head")
                else k
            ): v
            for k, v in weights.items()
        }

    @property
    def layers(self):
        return self.model.layers

    @property
    def sample_rate(self):
        return self.config.sample_rate

    def __call__(self, *args, **kwargs):
        return self.model(*args, **kwargs)

    def get_speaker(self, voice: Optional[str], ref_audio: Optional[str]) -> dict:
        if voice is None and ref_audio is None:
            voice = f"{Path(__file__).parent}/default_speaker.json"
            return self.audio_processor.load_speaker(voice)

        if voice is not None:
            return self.audio_processor.load_speaker(voice)

        speaker = self.audio_processor.create_speaker_from_whisper(ref_audio)
        file_id = str(uuid.uuid4())
        save_path = f"~/.cache/mlx_audio/voices/outetts_{file_id}.json"
        self.audio_processor.save_speaker(speaker, save_path)
        return speaker

    def chunk_text(self, text: str, max_words: int = 30) -> List[str]:
        sentences = re.split(r"[.!?ã€‚ï¼ï¼Ÿï¸•ï¸–]+", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            words = sentence.split()
            if current_length + len(words) > max_words:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.extend(words)
            current_length += len(words)
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        return chunks

    def generate_result(
        self, audio, start_time: float, token_count: int, segment_idx: int, **kwargs
    ) -> GenerationResult:
        samples = audio.shape[0] if audio is not None else 0
        assert samples > 0, "No audio generated"

        sample_rate = (
            self.config.sample_rate
            if kwargs.get("sample_rate") is None
            else kwargs.get("sample_rate")
        )
        audio_duration_seconds = samples / sample_rate

        elapsed_time = time.perf_counter() - start_time
        rtf = audio_duration_seconds / elapsed_time

        duration_mins = int(audio_duration_seconds // 60)
        duration_secs = int(audio_duration_seconds % 60)
        duration_ms = int((audio_duration_seconds % 1) * 1000)
        duration_hours = int(audio_duration_seconds // 3600)
        duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

        return GenerationResult(
            audio=audio,
            samples=samples,
            sample_rate=sample_rate,
            segment_idx=segment_idx,
            token_count=token_count,
            audio_duration=duration_str,
            real_time_factor=rtf,
            prompt={
                "tokens": token_count,
                "tokens-per-sec": (
                    round(token_count / elapsed_time, 2) if elapsed_time > 0 else 0
                ),
            },
            audio_samples={
                "samples": samples,
                "samples-per-sec": (
                    round(samples / elapsed_time, 2) if elapsed_time > 0 else 0
                ),
            },
            processing_time_seconds=elapsed_time,
            peak_memory_usage=mx.get_peak_memory() / 1e9,
        )

    def generate(
        self,
        text,
        voice: Optional[str] = None,
        temperature: float = 0.4,
        top_p: float = 0.9,
        split_pattern: str = "\n",
        max_tokens: int = 1200,
        verbose: bool = False,
        ref_audio: Optional[str] = None,
        stream: bool = False,
        streaming_interval: float = 2.0,
        **kwargs,
    ):

        prompts = self.chunk_text(text)

        self.prompt_processor = PromptProcessor(self.tokenizer)
        self.audio_processor = AudioProcessor()

        speaker = self.get_speaker(voice, ref_audio)

        sampler = make_sampler(
            temperature,
            top_p,
            min_p=kwargs.get("min_p", 0.05),
            top_k=kwargs.get("top_k", 40),
        )
        logits_processors = make_logits_processors(
            kwargs.get("logit_bias", None),
            kwargs.get("repetition_penalty", 1.1),
            kwargs.get("repetition_context_size", 64),
        )

        for prompt in prompts:
            completion_prompt = self.prompt_processor.get_completion_prompt(
                prompt, speaker
            )
            input_ids = self.tokenizer.encode(
                completion_prompt, add_special_tokens=False, return_tensors="mlx"
            )
            input_length = input_ids.shape[1]

            generated_token_count = 0
            yielded_token_count = 0
            streaming_token_interval = int(streaming_interval * 137.5)
            yielded_frame_count = 0

            time_start = time.perf_counter()

            for i, response in enumerate(
                tqdm(
                    stream_generate(
                        self.model,
                        tokenizer=self.tokenizer,
                        prompt=input_ids.squeeze(0),
                        max_tokens=max_tokens,
                        sampler=sampler,
                        logits_processors=logits_processors,
                    ),
                    total=max_tokens,
                    disable=not verbose,
                )
            ):
                next_token = mx.array([response.token])
                input_ids = mx.concatenate([input_ids, next_token[None, :]], axis=1)
                generated_token_count += 1

                # send a partial result in streaming mode
                if stream and generated_token_count % streaming_token_interval == 0:
                    output_ids = input_ids[:, input_length:].tolist()[0]
                    output = self.prompt_processor.extract_audio_from_tokens(output_ids)
                    audio = self.audio_processor.audio_codec.decode(mx.array([output]))[
                        -1, -1, :
                    ]

                    yield self.generate_result(
                        audio=audio[yielded_frame_count:],
                        start_time=time_start,
                        token_count=len(output_ids) - yielded_token_count,
                        segment_idx=i,
                        **kwargs,
                    )
                    yielded_token_count = len(output_ids)
                    yielded_frame_count = audio.shape[0]
                    time_start = time.perf_counter()

            output_ids = input_ids[:, input_length:].tolist()[0]
            output = self.prompt_processor.extract_audio_from_tokens(output_ids)

            audio = self.audio_processor.audio_codec.decode(mx.array([output]))[
                -1, -1, :
            ]
            if audio.shape[0] > yielded_frame_count:
                yield self.generate_result(
                    audio=audio[yielded_frame_count:],
                    start_time=time_start,
                    token_count=len(output_ids) - yielded_token_count,
                    segment_idx=i,
                    **kwargs,
                )

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/outetts/prompt_processor.py
================================================
import re
from typing import Union

from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast

from .tokens import SpecialTokens


class PromptProcessor:
    def __init__(
        self, tokenizer: Union[str, PreTrainedTokenizer, PreTrainedTokenizerFast]
    ):
        self.special_tokens = SpecialTokens()

        if tokenizer:
            if isinstance(tokenizer, (PreTrainedTokenizer, PreTrainedTokenizerFast)):
                self.tokenizer = tokenizer
            elif isinstance(tokenizer, str):
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)
            else:
                raise ValueError(f"Invalid tokenizer: {type(tokenizer)}")

            self.c1 = {}
            self.c2 = {}
            self.get_audio_token_map()

        self.input_prompt = "{bos}\n{text_start}{text}{text_end}\n{audio_start}\n"
        self.global_features = "{fs}{codes}{fe}\n"

    def get_audio_token_map(self):
        self.c1 = {
            self.tokenizer.encode(
                self.special_tokens.c1.format(i), add_special_tokens=False
            )[0]: i
            for i in range(1025)
        }
        self.c2 = {
            self.tokenizer.encode(
                self.special_tokens.c2.format(i), add_special_tokens=False
            )[0]: i
            for i in range(1025)
        }

    def get_features(self, f: dict):
        features = {
            "energy": f.get("energy", 0),
            "spectral_centroid": f.get("spectral_centroid", 0),
            "pitch": f.get("pitch", 0),
        }
        return [f"<|{k}_{v}|>" for k, v in features.items()]

    def get_global_features(self, f: dict):
        return self.global_features.format(
            fs=self.special_tokens.global_features_start,
            codes="".join(self.get_features(f)),
            fe=self.special_tokens.global_features_end,
        )

    def create_codes(self, words: dict):
        codes = []
        for i in words:
            word = (
                i["word"]
                + self.special_tokens.features
                + self.special_tokens.time.format(i["duration"])
            )
            word += "".join(self.get_features(i["features"]))
            pairs = []

            for idx in range(len(i["c1"])):
                c1 = self.special_tokens.c1.format(i["c1"][idx])
                c2 = self.special_tokens.c2.format(i["c2"][idx])
                pairs.append(f"{c1}{c2}")

            word += self.special_tokens.code + "".join(pairs)
            codes.append(
                self.special_tokens.word_start + word + self.special_tokens.word_end
            )

        return "\n".join(codes)

    def _init_prompt(self, text):
        return self.input_prompt.format(
            bos=self.special_tokens.bos,
            text_start=self.special_tokens.text_start,
            text=text,
            text_end=self.special_tokens.text_end,
            audio_start=self.special_tokens.audio_start,
        )

    def _get_separator(self, text: str) -> str:
        has_hiragana = any("\u3040" <= c <= "\u309f" for c in text)
        has_katakana = any("\u30a0" <= c <= "\u30ff" for c in text)
        has_han = any("\u4e00" <= c <= "\u9fff" for c in text)
        has_hangul = any("\uac00" <= c <= "\ud7af" for c in text)

        if has_hiragana or has_katakana or has_han:
            return "ã€‚"
        elif has_hangul:
            return ". "
        else:
            return ". "

    def merge_speaker_text(self, input_text: str, speaker_text: str) -> str:
        speaker_text = speaker_text.strip()
        separator = self._get_separator(speaker_text)

        # Determine allowed endings based on the separator
        if separator == "ã€‚":
            allowed_ends = ["ã€‚", "ï¼Ÿ", "ï¼", "?", "!"]
        else:
            allowed_ends = [".", "?", "!"]

        rs = ""
        if speaker_text:
            last_char = speaker_text[-1]
            if last_char not in allowed_ends:
                rs = separator
            else:
                if separator != "ã€‚":
                    rs = " "

        output = speaker_text.strip() + rs + input_text.strip()

        return output, rs.strip()

    @staticmethod
    def text_normalizations(text: str) -> str:
        # Normalize whitespace characters (newlines, tabs, etc.) to single spaces
        text = re.sub(r"\s+", " ", text)
        text = text.replace("â€¦", "...")  # Replace ellipsis character with three dots

        # Strip leading/trailing whitespace
        text = text.strip()

        # Normalize common Unicode characters to ASCII equivalents
        text = re.sub(r"[â€œâ€]", '"', text)  # Curly quotes to straight quotes
        text = re.sub(r"[â€˜â€™]", "'", text)  # Curly single quotes
        text = re.sub(r"[â€“â€”]", "-", text)  # Various dashes to hyphen

        # Remove control characters
        text = re.sub(r"[\x00-\x1F\x7F-\x9F]", "", text)

        return text

    def get_completion_prompt(self, text: str, speaker: dict = None):
        text = self.text_normalizations(text)

        if speaker is not None:
            text, separator = self.merge_speaker_text(text, speaker["text"])
            speaker["words"][-1]["word"] += separator
            codes = self.create_codes(speaker["words"])

        prompt = self._init_prompt(text)

        if speaker is not None:
            prompt += codes + "\n" + self.special_tokens.word_start

        return prompt

    def get_training_prompt(self, speaker: dict) -> str:
        text = self.text_normalizations(speaker["text"])
        words = speaker["words"]
        global_features = speaker["global_features"]

        prompt = self._init_prompt(text)
        prompt += self.get_global_features(global_features)
        prompt += self.create_codes(words)
        prompt += (
            "\n" + self.special_tokens.audio_end + "\n" + self.special_tokens.eos + "\n"
        )

        return prompt

    def extract_audio_from_tokens(self, tokens: list[int]):
        codebook1 = [self.c1[i] for i in tokens if i in self.c1]
        codebook2 = [self.c2[i] for i in tokens if i in self.c2]
        t = min(len(codebook1), len(codebook2))
        codebook1 = codebook1[:t]
        codebook2 = codebook2[:t]
        return [codebook1, codebook2]



================================================
FILE: mlx_audio/tts/models/outetts/tokens.py
================================================
from dataclasses import asdict, dataclass
from typing import Dict


@dataclass
class SpecialTokens:
    """
    Dataclass containing special tokens used for text and audio processing.
    """

    bos: str = "<|im_start|>"
    eos: str = "<|im_end|>"
    c1: str = "<|c1_{}|>"
    c2: str = "<|c2_{}|>"
    text_start: str = "<|text_start|>"
    text_end: str = "<|text_end|>"
    voice_characteristic_start: str = "<|voice_characteristic_start|>"
    voice_characteristic_end: str = "<|voice_characteristic_end|>"
    emotion_start: str = "<|emotion_start|>"
    emotion_end: str = "<|emotion_end|>"
    audio_start: str = "<|audio_start|>"
    audio_end: str = "<|audio_end|>"
    time: str = "<|t_{:.2f}|>"
    code: str = "<|code|>"
    energy: str = "<|energy_{}|>"
    spectral_centroid: str = "<|spectral_centroid_{}|>"
    pitch: str = "<|pitch_{}|>"
    word_start: str = "<|word_start|>"
    word_end: str = "<|word_end|>"
    features: str = "<|features|>"
    global_features_start: str = "<|global_features_start|>"
    global_features_end: str = "<|global_features_end|>"

    def to_dict(self) -> Dict[str, str]:
        """Convert the dataclass instance to a dictionary using asdict."""
        return asdict(self)



================================================
FILE: mlx_audio/tts/models/sesame/__init__.py
================================================
from .sesame import Model

__all__ = ["Model"]



================================================
FILE: mlx_audio/tts/models/sesame/attention.py
================================================
import math
from typing import Any, Optional

import mlx.core as mx
from mlx import nn
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.llama import ModelArgs


class Llama3ScaledRoPE(nn.Module):
    def __init__(
        self,
        dim: int,
        max_seq_len: int = 2048,
        base: float = 500_000.0,
        scale_factor: float = 32.0,
        low_freq_factor: int = 1,
        high_freq_factor: int = 4,
        old_context_len: int = 8192,
    ) -> None:
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_seq_len = max_seq_len

        self.scale_factor = scale_factor
        self.low_freq_factor = low_freq_factor
        self.high_freq_factor = high_freq_factor
        self.old_context_len = old_context_len
        self.is_cache_built = False
        self.rope_init()

    def rope_init(self):
        freqs = 1.0 / (
            self.base
            ** (
                mx.arange(0, self.dim, 2)[: (self.dim // 2)].astype(mx.float32)
                / self.dim
            )
        )

        theta = self.apply_scaling(
            freqs,
            self.scale_factor,
            self.low_freq_factor,
            self.high_freq_factor,
            self.old_context_len,
        )
        self._theta = theta
        self.build_rope_cache(self.max_seq_len)
        self.is_cache_built = True

    def build_rope_cache(self, max_seq_len: int = 4096) -> None:
        seq_idx = mx.arange(max_seq_len, dtype=self._theta.dtype)
        idx_theta = mx.einsum("i, j -> ij", seq_idx, self._theta).astype(mx.float32)
        cache = mx.stack([mx.cos(idx_theta), mx.sin(idx_theta)], axis=-1)
        self._cache = cache

    def apply_scaling(
        self,
        freqs: mx.array,
        scale_factor: float,
        low_freq_factor: int,
        high_freq_factor: int,
        old_context_len: int,
    ):
        low_freq_wavelen = old_context_len / low_freq_factor
        high_freq_wavelen = old_context_len / high_freq_factor
        new_freqs = []
        for freq in freqs:
            wavelen = 2 * math.pi / freq
            if wavelen < high_freq_wavelen:
                new_freqs.append(freq)
            elif wavelen > low_freq_wavelen:
                new_freqs.append(freq / scale_factor)
            else:
                assert low_freq_wavelen != high_freq_wavelen
                smooth = (old_context_len / wavelen - low_freq_factor) / (
                    high_freq_factor - low_freq_factor
                )
                new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)
        return mx.array(new_freqs, dtype=freqs.dtype)

    def __call__(self, x: mx.array, *, offset: int) -> mx.array:
        if not self.is_cache_built:
            raise RuntimeError(
                "RoPE cache is not built. Please call rope_init() first."
            )

        seq_len = x.shape[1]
        rope_cache = (
            self._cache[:seq_len]
            if offset is None
            else self._cache[None, offset : offset + seq_len]
        )
        xshaped = x.astype(mx.float32).reshape(*x.shape[:-1], -1, 2)
        rope_cache = rope_cache.reshape(-1, xshaped.shape[1], 1, xshaped.shape[3], 2)

        x_out = mx.stack(
            [
                xshaped[..., 0] * rope_cache[..., 0]
                - xshaped[..., 1] * rope_cache[..., 1],
                xshaped[..., 1] * rope_cache[..., 0]
                + xshaped[..., 0] * rope_cache[..., 1],
            ],
            -1,
        )

        x_out = x_out.flatten(3)
        return x_out.astype(x.dtype)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads or n_heads

        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads

        self.scale = head_dim**-0.5
        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)

        self.rope = Llama3ScaledRoPE(
            self.head_dim,
            base=args.rope_theta,
            scale_factor=args.rope_scaling.get("factor", 1.0),
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        b, s_x, _ = x.shape
        y = x

        s_y = y.shape[1] if y is not None else 0

        q = self.q_proj(x)

        q_per_kv = self.n_heads // self.n_kv_heads
        q = q.reshape(b, s_x, self.n_kv_heads * q_per_kv, self.head_dim)

        if self.rope is not None:
            q = self.rope(q, offset=cache.offset if cache else 0)

        q = q.swapaxes(1, 2)

        k = self.k_proj(y)
        v = self.v_proj(y)

        k = k.reshape(b, s_y, -1, self.head_dim)
        v = v.reshape(b, s_y, -1, self.head_dim)
        if self.rope is not None:
            k = self.rope(k, offset=cache.offset if cache else 0)

        k = k.swapaxes(1, 2)
        v = v.swapaxes(1, 2)

        if cache:
            k, v = cache.update_and_fetch(k, v)

        if self.n_heads != self.n_kv_heads:
            q_per_kv = self.n_heads // self.n_kv_heads

            k = mx.expand_dims(k, axis=2)
            v = mx.expand_dims(v, axis=2)

            k_expand_shape = (b, self.n_kv_heads, q_per_kv) + k.shape[3:]
            v_expand_shape = (b, self.n_kv_heads, q_per_kv) + v.shape[3:]

            k = mx.broadcast_to(k, k_expand_shape)
            v = mx.broadcast_to(v, v_expand_shape)

            k = k.reshape(b, self.n_kv_heads * q_per_kv, *k.shape[3:])
            v = v.reshape(b, self.n_kv_heads * q_per_kv, *v.shape[3:])

        output = scaled_dot_product_attention(
            q, k, v, cache=cache, scale=self.scale, mask=mask
        )

        output = output.swapaxes(1, 2).reshape(b, s_x, -1)
        return self.o_proj(output)



================================================
FILE: mlx_audio/tts/models/sesame/sesame.py
================================================
from __future__ import annotations

import re
import time
from dataclasses import dataclass
from typing import Callable, Dict, List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import soundfile as sf
from huggingface_hub import hf_hub_download
from mlx_lm.models.cache import make_prompt_cache
from mlx_lm.models.llama import LlamaModel
from mlx_lm.models.llama import ModelArgs as LlamaModelArgs
from mlx_lm.sample_utils import make_sampler
from scipy import signal
from tokenizers.processors import TemplateProcessing
from tqdm import tqdm
from transformers import AutoTokenizer

from mlx_audio.codec.models.mimi import Mimi, MimiStreamingDecoder

from ..base import GenerationResult
from .attention import Attention

try:
    from .watermarking import CSM_1B_GH_WATERMARK, load_watermarker, watermark
except ImportError:
    print(
        "Watermarking module not found. Please install silentcipher to use watermarking."
    )

MIMI_REPO = "kyutai/moshiko-pytorch-bf16"
TOKENIZER_REPO = "unsloth/Llama-3.2-1B"


def create_causal_mask(seq_len: int) -> mx.array:
    return mx.tril(mx.ones((seq_len, seq_len), dtype=mx.bool_))


def index_causal_mask(mask: mx.array, input_pos: mx.array) -> mx.array:
    mask_indexed = mx.take(mask, input_pos, axis=0)

    seq_len = input_pos.shape[1]
    mask_indexed = mask_indexed[:, :, :seq_len]

    # reshape to (batch_size, 1, seq_len, seq_len) for broadcasting across heads
    return mx.expand_dims(mask_indexed, axis=1)


def resample_audio(audio: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:
    gcd = np.gcd(orig_sr, target_sr)
    up = target_sr // gcd
    down = orig_sr // gcd
    resampled = signal.resample_poly(audio, up, down, padtype="edge")
    return resampled


@dataclass
class SesameModelArgs:
    model_type: str
    backbone_flavor: str
    decoder_flavor: str
    text_vocab_size: int
    audio_vocab_size: int
    audio_num_codebooks: int

    def __init__(
        self,
        model_type,
        backbone_flavor,
        decoder_flavor,
        text_vocab_size,
        audio_vocab_size,
        audio_num_codebooks,
        **kwargs,
    ):
        self.model_type = model_type
        self.backbone_flavor = backbone_flavor
        self.decoder_flavor = decoder_flavor
        self.text_vocab_size = text_vocab_size
        self.audio_vocab_size = audio_vocab_size
        self.audio_num_codebooks = audio_num_codebooks


def create_llama_model_args(flavor: str) -> LlamaModelArgs:
    if flavor == "llama-1B":
        return LlamaModelArgs(
            model_type="llama",
            num_hidden_layers=16,
            num_attention_heads=32,
            num_key_value_heads=8,
            head_dim=64,
            hidden_size=2048,
            intermediate_size=8192,
            rms_norm_eps=1e-5,
            vocab_size=128_256,
            max_position_embeddings=2048,
            attention_bias=False,
            mlp_bias=False,
            rope_theta=500_000,
            rope_scaling={
                "factor": 32.0,
                "low_freq_factor": 1.0,
                "high_freq_factor": 4.0,
                "original_max_position_embeddings": 8192,
                "rope_type": "llama3",
            },
        )
    elif flavor == "llama-100M":
        return LlamaModelArgs(
            model_type="llama",
            num_hidden_layers=4,
            num_attention_heads=8,
            num_key_value_heads=2,
            head_dim=128,
            hidden_size=1024,
            intermediate_size=8192,
            rms_norm_eps=1e-5,
            vocab_size=128_256,
            max_position_embeddings=2048,
            attention_bias=False,
            mlp_bias=False,
            rope_theta=500_000,
            rope_scaling={
                "factor": 32.0,
                "low_freq_factor": 1.0,
                "high_freq_factor": 4.0,
                "original_max_position_embeddings": 8192,
                "rope_type": "llama3",
            },
        )
    else:
        raise ValueError(f"Unknown flavor: {flavor}")


class SesameModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        args = SesameModelArgs(**config)
        self.args = args

        backbone_args = create_llama_model_args(args.backbone_flavor)
        decoder_args = create_llama_model_args(args.decoder_flavor)

        self.backbone = LlamaModel(backbone_args)
        self.decoder = LlamaModel(decoder_args)

        backbone_dim = backbone_args.hidden_size
        decoder_dim = decoder_args.hidden_size

        self.backbone.embed_tokens = nn.Identity()
        self.decoder.embed_tokens = nn.Identity()

        for layer in self.backbone.layers:
            layer.self_attn = Attention(backbone_args)
        for layer in self.decoder.layers:
            layer.self_attn = Attention(decoder_args)

        self.text_embeddings = nn.Embedding(args.text_vocab_size, backbone_dim)
        self.audio_embeddings = nn.Embedding(
            args.audio_vocab_size * args.audio_num_codebooks, backbone_dim
        )

        self.projection = nn.Linear(backbone_dim, decoder_dim, bias=False)
        self.codebook0_head = nn.Linear(backbone_dim, args.audio_vocab_size, bias=False)
        self.audio_head = mx.zeros(
            (args.audio_num_codebooks - 1, decoder_dim, args.audio_vocab_size)
        )

        self._backbone_causal_mask = None
        self._decoder_causal_mask = None

        self.backbone_cache = None
        self.decoder_cache = None
        self.caches_enabled = False

    def setup_caches(self, max_batch_size: int):
        backbone_args = create_llama_model_args(self.args.backbone_flavor)

        self._backbone_causal_mask = create_causal_mask(
            backbone_args.max_position_embeddings
        )
        self._decoder_causal_mask = create_causal_mask(self.args.audio_num_codebooks)

        self.backbone_cache = make_prompt_cache(self.backbone)
        self.decoder_cache = make_prompt_cache(self.decoder)
        self.caches_enabled = True

    def caches_are_enabled(self):
        return self.caches_enabled

    def reset_caches(self):
        if self.backbone_cache is not None:
            self.backbone_cache = make_prompt_cache(self.backbone)

        if self.decoder_cache is not None:
            self.decoder_cache = make_prompt_cache(self.decoder)

    def generate_frame(
        self,
        tokens: mx.array,
        tokens_mask: mx.array,
        input_pos: mx.array,
        sampler: Callable[..., mx.array],
    ) -> mx.array:
        assert self.caches_are_enabled(), "backbone caches are not enabled"

        curr_backbone_mask = index_causal_mask(self._backbone_causal_mask, input_pos)
        embeds = self._embed_tokens(tokens)
        masked_embeds = embeds * mx.expand_dims(tokens_mask, -1)
        h = mx.sum(masked_embeds, axis=2)
        h = self.backbone(h, mask=curr_backbone_mask, cache=self.backbone_cache)

        last_h = h[:, -1, :]
        c0_logits = self.codebook0_head(last_h)
        c0_sample = mx.expand_dims(sampler(c0_logits), axis=-1)
        c0_embed = self._embed_audio(0, c0_sample)

        curr_h = mx.concat([mx.expand_dims(last_h, 1), c0_embed], axis=1)
        curr_sample = c0_sample
        curr_pos = mx.arange(curr_h.shape[1], dtype=mx.int32)
        curr_pos = mx.expand_dims(curr_pos, 0)
        curr_pos = mx.broadcast_to(curr_pos, (curr_h.shape[0], curr_h.shape[1]))

        # reset decoder cache for new frame

        self.decoder_cache = make_prompt_cache(self.decoder)

        for i in range(1, self.args.audio_num_codebooks):
            curr_decoder_mask = index_causal_mask(self._decoder_causal_mask, curr_pos)
            decoder_h = self.decoder(
                self.projection(curr_h),
                mask=curr_decoder_mask,
                cache=self.decoder_cache,
            )

            ci_logits = mx.matmul(decoder_h[:, -1, :], self.audio_head[i - 1])
            ci_sample = mx.expand_dims(sampler(ci_logits), axis=-1)
            ci_embed = self._embed_audio(i, ci_sample)

            curr_h = ci_embed
            curr_sample = mx.concat([curr_sample, ci_sample], axis=1)
            curr_pos = curr_pos[:, -1:] + 1

        return curr_sample

    def _embed_audio(self, codebook: int, tokens: mx.array) -> mx.array:
        return self.audio_embeddings(tokens + codebook * self.args.audio_vocab_size)

    def _embed_tokens(self, tokens: mx.array) -> mx.array:
        text_embeds = self.text_embeddings(tokens[:, :, -1])
        text_embeds = mx.expand_dims(text_embeds, axis=-2)

        codebook_indices = mx.arange(self.args.audio_num_codebooks, dtype=mx.int32)
        codebook_offsets = codebook_indices * self.args.audio_vocab_size

        audio_tokens = tokens[:, :, :-1] + mx.reshape(codebook_offsets, (1, 1, -1))
        audio_embeds_flat = self.audio_embeddings(audio_tokens.flatten())

        audio_embeds = mx.reshape(
            audio_embeds_flat,
            (tokens.shape[0], tokens.shape[1], self.args.audio_num_codebooks, -1),
        )

        return mx.concat([audio_embeds, text_embeds], axis=-2)


@dataclass
class Segment:
    speaker: int
    text: str
    # (num_samples,), sample_rate = 24_000
    audio: mx.array


def load_llama3_tokenizer(path_or_hf_repo: str):
    tokenizer = AutoTokenizer.from_pretrained(path_or_hf_repo)
    bos = tokenizer.bos_token
    eos = tokenizer.eos_token
    tokenizer._tokenizer.post_processor = TemplateProcessing(
        single=f"{bos}:0 $A:0 {eos}:0",
        pair=f"{bos}:0 $A:0 {eos}:0 {bos}:1 $B:1 {eos}:1",
        special_tokens=[
            (f"{bos}", tokenizer.bos_token_id),
            (f"{eos}", tokenizer.eos_token_id),
        ],
    )
    return tokenizer


class Model(nn.Module):
    def __init__(
        self,
        config: Dict,
    ):
        super().__init__()
        self.model = SesameModel(config)
        self.model.setup_caches(1)

        self._text_tokenizer = load_llama3_tokenizer(TOKENIZER_REPO)
        mimi = Mimi.from_pretrained(MIMI_REPO)
        self._audio_tokenizer = mimi
        self._streaming_decoder = MimiStreamingDecoder(mimi)

        try:
            self._watermarker = load_watermarker()
        except Exception:
            self._watermarker = None

        self._sample_rate = mimi.cfg.sample_rate

    def model_quant_predicate(self, p, m, config):
        """
        Model modules to skip during quantization
        """
        return not p.startswith("_audio_tokenizer")

    @property
    def layers(self):
        """Return the backbone layers of the model."""
        return self.model.backbone.layers

    @property
    def sample_rate(self):
        return self._sample_rate

    def _tokenize_text_segment(
        self, text: str, speaker: int
    ) -> Tuple[mx.array, mx.array]:
        frame_tokens = []
        frame_masks = []

        text_tokens = self._text_tokenizer.encode(
            f"[{speaker}]{text}", return_tensors="mlx"
        ).squeeze(0)
        text_frame = mx.zeros((len(text_tokens), 33)).astype(mx.int32)
        text_frame_mask = mx.zeros((len(text_tokens), 33)).astype(mx.bool_)
        text_frame[:, -1] = text_tokens
        text_frame_mask[:, -1] = True

        frame_tokens.append(text_frame)
        frame_masks.append(text_frame_mask)

        return mx.concat(frame_tokens, axis=0), mx.concat(frame_masks, axis=0)

    def _tokenize_audio(self, audio: mx.array) -> Tuple[mx.array, mx.array]:
        frame_tokens = []
        frame_masks = []

        # (K, T)
        audio_tokens = self._audio_tokenizer.encode(
            mx.expand_dims(mx.expand_dims(audio, 0), 0)
        )[0]

        # add EOS frame
        eos_frame = mx.zeros((audio_tokens.shape[0], 1))
        audio_tokens = mx.concat([audio_tokens, eos_frame], axis=1)

        audio_frame = mx.zeros((audio_tokens.shape[1], 33)).astype(mx.int32)
        audio_frame_mask = mx.zeros((audio_tokens.shape[1], 33)).astype(mx.bool_)
        audio_frame[:, :-1] = audio_tokens.swapaxes(0, 1)
        audio_frame_mask[:, :-1] = True

        frame_tokens.append(audio_frame)
        frame_masks.append(audio_frame_mask)

        return mx.concat(frame_tokens, axis=0), mx.concat(frame_masks, axis=0)

    def _tokenize_segment(self, segment: Segment) -> Tuple[mx.array, mx.array]:
        """
        Returns:
            (seq_len, 33), (seq_len, 33)
        """
        text_tokens, text_masks = self._tokenize_text_segment(
            segment.text, segment.speaker
        )
        audio_tokens, audio_masks = self._tokenize_audio(segment.audio)

        return mx.concat([text_tokens, audio_tokens], axis=0), mx.concat(
            [text_masks, audio_masks], axis=0
        )

    def sanitize(self, weights):
        sanitized_weights = {}

        for k, v in weights.items():
            if not k.startswith("model."):
                k = "model." + k

            if "attn" in k and not "self_attn" in k:
                k = k.replace("attn", "self_attn")
                k = k.replace("output_proj", "o_proj")

            if "mlp" in k:
                k = k.replace("w1", "gate_proj")
                k = k.replace("w2", "down_proj")
                k = k.replace("w3", "up_proj")

            if "sa_norm" in k or "mlp_norm" in k:
                k = k.replace("sa_norm", "input_layernorm").replace("scale", "weight")
                k = k.replace("mlp_norm", "post_attention_layernorm").replace(
                    "scale", "weight"
                )

            if "decoder.norm" in k or "backbone.norm" in k:
                k = k.replace("scale", "weight")

            sanitized_weights[k] = v

        return sanitized_weights

    def prepare_prompt(
        self, text: str, speaker: int, audio_path: str, sample_rate: int
    ) -> Segment:
        audio, sr = sf.read(audio_path)
        if sr != sample_rate:
            audio = resample_audio(audio, sr, sample_rate)
        return Segment(text=text, speaker=speaker, audio=mx.array(audio))

    def default_speaker_prompt(self, voice: str) -> List[Segment]:
        SPEAKER_PROMPTS = {
            "conversational_a": {
                "text": (
                    "like revising for an exam I'd have to try and like keep up the momentum because I'd "
                    "start really early I'd be like okay I'm gonna start revising now and then like "
                    "you're revising for ages and then I just like start losing steam I didn't do that "
                    "for the exam we had recently to be fair that was a more of a last minute scenario "
                    "but like yeah I'm trying to like yeah I noticed this yesterday that like Mondays I "
                    "sort of start the day with this not like a panic but like a"
                ),
            },
            "conversational_b": {
                "text": (
                    "like a super Mario level. Like it's very like high detail. And like, once you get "
                    "into the park, it just like, everything looks like a computer game and they have all "
                    "these, like, you know, if, if there's like a, you know, like in a Mario game, they "
                    "will have like a question block. And if you like, you know, punch it, a coin will "
                    "come out. So like everyone, when they come into the park, they get like this little "
                    "bracelet and then you can go punching question blocks around."
                ),
            },
        }

        prompt_path = hf_hub_download(
            repo_id="sesame/csm-1b", filename=f"prompts/{voice}.wav"
        )
        prompt = self.prepare_prompt(
            SPEAKER_PROMPTS[voice]["text"], 0, prompt_path, 24_000
        )
        return [prompt]

    def generate_result(
        self, samples, start_time: float, stream: bool = False
    ) -> GenerationResult:
        token_count = len(samples)
        transposed = mx.transpose(mx.stack(samples), axes=[1, 2, 0])
        if stream:
            audio = (
                self._streaming_decoder.decode_frames(transposed).squeeze(0).squeeze(0)
            )
        else:
            audio = self._audio_tokenizer.decode(transposed).squeeze(0).squeeze(0)

        # This applies an imperceptible watermark to identify audio as AI-generated.
        # Watermarking ensures transparency, dissuades misuse, and enables traceability.
        # Please be a responsible AI citizen and keep the watermarking in place.
        # If using CSM 1B in another application, use your own private key and keep it secret.
        if self._watermarker is not None:
            audio = watermark(
                self._watermarker,
                audio,
                self._sample_rate,
                CSM_1B_GH_WATERMARK,
            )
            audio = mx.array(audio, dtype=mx.float32)

        mx.eval(audio)

        segment_time = time.perf_counter() - start_time

        samples = audio.shape[0] if audio is not None else 0
        assert samples > 0, "No audio generated"

        # Calculate audio duration in seconds
        sample_rate = 24000
        audio_duration_seconds = samples / sample_rate

        # Calculate real-time factor (RTF)
        rtf = segment_time / audio_duration_seconds if audio_duration_seconds > 0 else 0

        # Format duration as HH:MM:SS.mmm
        duration_mins = int(audio_duration_seconds // 60)
        duration_secs = int(audio_duration_seconds % 60)
        duration_ms = int((audio_duration_seconds % 1) * 1000)
        duration_hours = int(audio_duration_seconds // 3600)
        duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

        return GenerationResult(
            audio=audio,
            samples=samples,
            sample_rate=sample_rate,
            segment_idx=0,
            token_count=token_count,
            audio_duration=duration_str,
            real_time_factor=round(rtf, 2),
            prompt={
                "tokens": token_count,
                "tokens-per-sec": (
                    round(token_count / segment_time, 2) if segment_time > 0 else 0
                ),
            },
            audio_samples={
                "samples": samples,
                "samples-per-sec": (
                    round(samples / segment_time, 2) if segment_time > 0 else 0
                ),
            },
            processing_time_seconds=segment_time,
            peak_memory_usage=mx.get_peak_memory() / 1e9,
        )

    def generate(
        self,
        text: List[str] | str,
        voice: Optional[str] = None,
        speaker: int = 0,
        context: List[Segment] = [],
        split_pattern: Optional[str] = r"\n+",
        sampler: Callable[..., mx.array] = None,
        max_audio_length_ms: float = 90_000,
        ref_audio: mx.array = None,
        ref_text: str = None,
        stream: bool = False,
        streaming_interval: float = 2.0,
        **kwargs,
    ):
        # if reference audio is provided, use it as the first segment
        if len(context) == 0 and ref_audio is not None and ref_text is not None:
            context = [Segment(speaker=speaker, text=ref_text, audio=ref_audio)]
        elif ref_audio is None:
            # otherwise, use the provided or default voice
            if voice is None:
                voice = "conversational_a"
            context = self.default_speaker_prompt(voice)

        sampler = sampler or make_sampler(temp=0.9, top_k=50)
        max_audio_frames = int(max_audio_length_ms / 80)
        streaming_interval_tokens = int(streaming_interval * 12.5)

        if isinstance(text, str):
            text = re.split(split_pattern, text.strip()) if split_pattern else [text]

        for prompt in text:
            start_time = time.perf_counter()

            self.model.reset_caches()
            if stream:
                self._streaming_decoder.reset()

            tokens, tokens_mask = [], []
            for segment in context:
                segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)
                tokens.append(segment_tokens)
                tokens_mask.append(segment_tokens_mask)

            gen_segment_tokens, gen_segment_tokens_mask = self._tokenize_text_segment(
                prompt, speaker
            )
            tokens.append(gen_segment_tokens)
            tokens_mask.append(gen_segment_tokens_mask)

            prompt_tokens = mx.concat(tokens, axis=0).astype(mx.int32)
            prompt_tokens_mask = mx.concat(tokens_mask, axis=0).astype(mx.bool_)

            samples = []
            curr_tokens = mx.expand_dims(prompt_tokens, axis=0)
            curr_tokens_mask = mx.expand_dims(prompt_tokens_mask, axis=0)
            curr_pos = mx.expand_dims(
                mx.arange(0, prompt_tokens.shape[0]), axis=0
            ).astype(mx.int32)
            generated_frame_count = 0
            yielded_frame_count = 0

            max_seq_len = 2048 - max_audio_frames
            if curr_tokens.shape[1] >= max_seq_len:
                raise ValueError(
                    f"Inputs too long, must be below max_seq_len - max_audio_frames: {max_seq_len}"
                )

            for _ in tqdm(range(max_audio_frames)):
                sample = self.model.generate_frame(
                    curr_tokens, curr_tokens_mask, curr_pos, sampler
                )
                if mx.all(sample == 0):
                    break  # eos

                samples.append(sample)

                curr_tokens = mx.expand_dims(
                    mx.concat([sample, mx.zeros((1, 1)).astype(mx.int32)], axis=1),
                    axis=1,
                )
                curr_tokens_mask = mx.expand_dims(
                    mx.concat(
                        [
                            mx.ones_like(sample).astype(mx.bool_),
                            mx.zeros((1, 1)).astype(mx.bool_),
                        ],
                        axis=1,
                    ),
                    axis=1,
                )
                curr_pos = curr_pos[:, -1:] + 1
                generated_frame_count += 1

                # send a partial result in streaming mode
                if (
                    stream
                    and (generated_frame_count - yielded_frame_count)
                    >= streaming_interval_tokens
                ):
                    yielded_frame_count = generated_frame_count
                    yield self.generate_result(samples, start_time, stream=True)
                    samples = []
                    start_time = time.perf_counter()

            if len(samples) > 0:
                yield self.generate_result(samples, start_time, stream=stream)

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/sesame/watermarking.py
================================================
import argparse

import mlx.core as mx
import numpy as np
import silentcipher
import soundfile as sf
from scipy import signal

# This watermark key is public, it is not secure.
# If using CSM 1B in another application, use a new private key and keep it secret.
CSM_1B_GH_WATERMARK = [212, 211, 146, 56, 201]


def cli_check_audio() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--audio_path", type=str, required=True)
    args = parser.parse_args()
    check_audio_from_file(args.audio_path)


def load_watermarker() -> silentcipher.server.Model:
    model = silentcipher.get_model(
        model_type="44.1k",
    )
    return model


def resample_audio(audio: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:
    gcd = np.gcd(orig_sr, target_sr)
    up = target_sr // gcd
    down = orig_sr // gcd
    resampled = signal.resample_poly(audio, up, down, padtype="edge")
    return resampled


def watermark(
    watermarker: silentcipher.server.Model,
    audio_array: mx.array,
    sample_rate: int,
    watermark_key: list[int],
) -> tuple[mx.array, int]:
    audio_array = np.array(audio_array, dtype=np.float32)

    if sample_rate != 44100:
        audio_array_44khz = resample_audio(audio_array, sample_rate, 44100)
    else:
        audio_array_44khz = audio_array

    encoded, *_ = watermarker.encode_wav(
        audio_array_44khz, 44100, watermark_key, calc_sdr=False, message_sdr=36
    )

    if sample_rate != 44100:
        encoded = resample_audio(encoded, 44100, sample_rate)

    return encoded


def verify(
    watermarker: silentcipher.server.Model,
    watermarked_audio: mx.array,
    sample_rate: int,
    watermark_key: list[int],
) -> bool:
    if sample_rate != 44100:
        watermarked_audio_44khz = resample_audio(watermarked_audio, sample_rate, 44100)
    else:
        watermarked_audio_44khz = watermarked_audio

    result = watermarker.decode_wav(
        watermarked_audio_44khz, 44100, phase_shift_decoding=True
    )

    is_watermarked = result["status"]
    if is_watermarked:
        is_csm_watermarked = result["messages"][0] == watermark_key
    else:
        is_csm_watermarked = False

    return is_watermarked and is_csm_watermarked


def check_audio_from_file(audio_path: str) -> None:
    watermarker = load_watermarker()
    audio_array, sample_rate = load_audio(audio_path)
    is_watermarked = verify(watermarker, audio_array, sample_rate, CSM_1B_GH_WATERMARK)
    outcome = "Watermarked" if is_watermarked else "Not watermarked"
    print(f"{outcome}: {audio_path}")


def load_audio(audio_path: str) -> tuple[mx.array, int]:
    audio_array_np, sample_rate = sf.read(audio_path, always_2d=True)

    if audio_array_np.shape[1] > 1:
        audio_array_np = audio_array_np.mean(axis=1)
    else:
        audio_array_np = audio_array_np.squeeze()

    audio_array = mx.array(audio_array_np)

    return audio_array, int(sample_rate)


if __name__ == "__main__":
    cli_check_audio()



================================================
FILE: mlx_audio/tts/models/spark/__init__.py
================================================
from .spark import Model, ModelConfig



================================================
FILE: mlx_audio/tts/models/spark/audio_tokenizer.py
================================================
from pathlib import Path
from typing import Any, Dict, Tuple, Union

import mlx.core as mx
import numpy as np

from mlx_audio.stt.models.wav2vec.feature_extractor import Wav2Vec2FeatureExtractor
from mlx_audio.stt.models.wav2vec.wav2vec import Wav2Vec2Model

from .bicodec import BiCodec
from .utils.audio import load_audio
from .utils.file import load_config


class BiCodecTokenizer:
    """BiCodec tokenizer for handling audio input and tokenization."""

    def __init__(self, model_dir: Path, **kwargs):
        super().__init__()
        """
        Args:
            model_dir: Path to the model directory.
            device: Device to run the model on (default is GPU if available).
        """
        self.model_dir = model_dir
        self.config = load_config(f"{model_dir}/audio_tokenizer_config.yaml")
        self._initialize_model()

    def _initialize_model(self):
        """Load and initialize the BiCodec model and Wav2Vec2 feature extractor."""
        self.model = BiCodec.load_from_checkpoint(f"{self.model_dir}/BiCodec")
        self.processor = Wav2Vec2FeatureExtractor.from_pretrained(
            f"{self.model_dir}/wav2vec2-large-xlsr-53"
        )
        self.feature_extractor = Wav2Vec2Model.from_pretrained(
            f"{self.model_dir}/wav2vec2-large-xlsr-53"
        )
        self.feature_extractor.config.output_hidden_states = True

    def get_ref_clip(self, wav: np.ndarray) -> np.ndarray:
        """Get reference audio clip for speaker embedding."""
        ref_segment_length = (
            int(self.config["sample_rate"] * self.config["ref_segment_duration"])
            // self.config["latent_hop_length"]
            * self.config["latent_hop_length"]
        )
        wav_length = len(wav)

        if ref_segment_length > wav_length:
            # Repeat and truncate to handle insufficient length
            wav = np.tile(wav, ref_segment_length // wav_length + 1)

        return wav[:ref_segment_length]

    def process_audio(
        self, wav_path: Union[Path, mx.array]
    ) -> Tuple[np.ndarray, mx.array]:
        """load auido and get reference audio from wav path"""
        if isinstance(wav_path, Path) or isinstance(wav_path, str):
            wav = load_audio(
                wav_path,
                sampling_rate=self.config["sample_rate"],
                volume_normalize=self.config["volume_normalize"],
            )
        elif isinstance(wav_path, mx.array):
            wav = wav_path
        else:
            raise ValueError(f"Invalid input type: {type(wav_path)}")

        wav_ref = self.get_ref_clip(wav)

        return wav, wav_ref[None, ...]

    def extract_wav2vec2_features(self, wavs: mx.array) -> mx.array:
        """extract wav2vec2 features"""
        inputs = self.processor(
            wavs,
            sampling_rate=16000,
            return_tensors="mx",
            padding=True,
            output_hidden_states=True,
        )["input_values"]
        feat = self.feature_extractor(inputs)
        feats_mix = (
            feat.hidden_states[11] + feat.hidden_states[14] + feat.hidden_states[16]
        ) / 3

        return feats_mix

    def tokenize_batch(self, batch: Dict[str, Any]) -> Tuple[mx.array, mx.array]:
        """tokenize the batch of audio

        Args:
            batch:
                wavs (List[np.ndarray]): batch of audio
                ref_wavs (mx.array): reference audio. shape: (batch_size, seq_len)

        Returns:
            semantic_tokens: semantic tokens. shape: (batch_size, seq_len, latent_dim)
            global_tokens: global tokens. shape: (batch_size, seq_len, global_dim)
        """
        feats = self.extract_wav2vec2_features(batch["wav"])
        batch["feat"] = feats

        semantic_tokens, global_tokens = self.model.tokenize(batch)

        return global_tokens, semantic_tokens

    def tokenize(self, audio_path: str) -> Tuple[mx.array, mx.array]:
        """tokenize the audio"""
        wav, ref_wav = self.process_audio(audio_path)
        feat = self.extract_wav2vec2_features(wav)
        batch = {
            "wav": wav[None, ...],
            "ref_wav": ref_wav,
            "feat": feat,
        }
        semantic_tokens, global_tokens = self.model.tokenize(batch)

        return global_tokens, semantic_tokens

    def detokenize(
        self, global_tokens: mx.array, semantic_tokens: mx.array
    ) -> np.array:
        """detokenize the tokens to waveform

        Args:
            global_tokens: global tokens. shape: (batch_size, global_dim)
            semantic_tokens: semantic tokens. shape: (batch_size, latent_dim)

        Returns:
            wav_rec: waveform. shape: (batch_size, seq_len) for batch or (seq_len,) for single
        """
        global_tokens = mx.expand_dims(global_tokens, 1)

        # convert to mlx array
        wav_rec = self.model.detokenize(semantic_tokens, global_tokens)
        return wav_rec.squeeze()



================================================
FILE: mlx_audio/tts/models/spark/bicodec.py
================================================
from pathlib import Path
from typing import Any, Dict, Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from omegaconf import DictConfig
from safetensors.torch import load_file

from mlx_audio.tts.models.spark.modules.encoder_decoder.feat_decoder import Decoder
from mlx_audio.tts.models.spark.modules.encoder_decoder.feat_encoder import Encoder
from mlx_audio.tts.models.spark.modules.encoder_decoder.wave_generator import (
    WaveGenerator,
)
from mlx_audio.tts.models.spark.modules.residual import FactorizedVectorQuantize
from mlx_audio.tts.models.spark.modules.speaker.speaker_encoder import SpeakerEncoder
from mlx_audio.tts.models.spark.utils.file import load_config
from mlx_audio.tts.utils import get_model_path
from mlx_audio.utils import hanning, mel_filters, stft


def mel_spectrogram(
    audio: mx.array,
    sample_rate: int = 16_000,
    n_mels: int = 128,
    n_fft: int = 1024,
    f_min: int = 10,
    f_max: Optional[int] = None,
    hop_length: int = 320,
    win_length: int = 640,
    padding: int = 0,
):
    if not isinstance(audio, mx.array):
        audio = mx.array(audio)
    if padding > 0:
        audio = mx.pad(audio, (0, padding))
    window = hanning(win_length + 1)[:-1]
    freqs = stft(
        audio, window=window, win_length=win_length, hop_length=hop_length, n_fft=n_fft
    )
    magnitudes = freqs.abs()
    filters = mel_filters(
        sample_rate=sample_rate,
        n_fft=n_fft,
        n_mels=n_mels,
        f_min=f_min,
        f_max=f_max,
        norm="slaney",
        mel_scale="slaney",
    )
    mel_spec = magnitudes @ filters.T
    return mx.expand_dims(mel_spec, axis=0)


class BiCodec(nn.Module):
    """
    BiCodec model for speech synthesis, incorporating a speaker encoder, feature encoder/decoder,
    quantizer, and wave generator.
    """

    def __init__(
        self,
        mel_params: Dict[str, Any],
        encoder: nn.Module,
        decoder: nn.Module,
        quantizer: nn.Module,
        speaker_encoder: nn.Module,
        prenet: nn.Module,
        postnet: nn.Module,
        **kwargs,
    ) -> None:
        """
        Initializes the BiCodec model with the required components.

        Args:
            mel_params (dict): Parameters for the mel-spectrogram transformer.
            encoder (nn.Module): Encoder module.
            decoder (nn.Module): Decoder module.
            quantizer (nn.Module): Quantizer module.
            speaker_encoder (nn.Module): Speaker encoder module.
            prenet (nn.Module): Prenet network.
            postnet (nn.Module): Postnet network.
        """
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.quantizer = quantizer
        self.speaker_encoder = speaker_encoder
        self.prenet = prenet
        self.postnet = postnet
        self.mel_params = mel_params

    @classmethod
    def load_from_checkpoint(cls, model_dir: Path, **kwargs) -> "BiCodec":
        """
        Loads the model from a checkpoint.

        Args:
            model_dir (Path): Path to the model directory containing checkpoint and config.

        Returns:
            BiCodec: The initialized BiCodec model.
        """
        ckpt_path = f"{model_dir}/model.safetensors"
        config = load_config(f"{model_dir}/config.yaml")["audio_tokenizer"]
        mel_params = config["mel_params"]

        encoder = Encoder(**config["encoder"])
        quantizer = FactorizedVectorQuantize(**config["quantizer"])
        prenet = Decoder(**config["prenet"])
        postnet = Decoder(**config["postnet"])
        decoder = WaveGenerator(**config["decoder"])
        speaker_encoder = SpeakerEncoder(**config["speaker_encoder"])

        model = cls(
            mel_params=mel_params,
            encoder=encoder,
            decoder=decoder,
            quantizer=quantizer,
            speaker_encoder=speaker_encoder,
            prenet=prenet,
            postnet=postnet,
        )

        weights = load_file(ckpt_path)

        # Convert PyTorch weights to MLX arrays and sanitize
        weights = {
            k: mx.array(v) for k, v in weights.items() if "num_batches_tracked" not in k
        }

        for module in [encoder, decoder, quantizer, speaker_encoder]:
            if hasattr(module, "sanitize"):
                weights = module.sanitize(weights)

        model.load_weights(list(weights.items()), strict=True)

        return model

    def __call__(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """
        Performs a forward pass through the model.

        Args:
            batch (dict): A dictionary containing features, reference waveform, and target waveform.

        Returns:
            dict: A dictionary containing the reconstruction, features, and other metrics.
        """
        feat = batch["feat"]
        ref_wav = batch["ref_wav"]
        mel = self.get_mel_spectrogram(ref_wav)
        z = self.encoder(feat.transpose(0, 2, 1))
        vq_outputs = self.quantizer(z)

        x_vector, d_vector = self.speaker_encoder(mel)

        conditions = d_vector
        with_speaker_loss = False

        # Ensure conditions is an integer type for embedding lookup
        # The error shows that the embedding layer expects integral indices
        if isinstance(conditions, mx.array) and conditions.dtype == mx.float32:
            # Convert to integer type if needed for the embedding layer
            # or ensure it's properly formatted for the prenet
            conditions = conditions.astype(mx.int32)

        x = self.prenet(vq_outputs["z_q"], conditions)
        pred_feat = self.postnet(x)
        x = x + conditions[..., None]
        wav_recon = self.decoder(x)

        return {
            "vq_loss": vq_outputs["vq_loss"],
            "perplexity": vq_outputs["perplexity"],
            "cluster_size": vq_outputs["active_num"],
            "recons": wav_recon,
            "pred_feat": pred_feat,
            "x_vector": x_vector,
            "d_vector": d_vector,
            "audios": batch["wav"][:, None],
            "with_speaker_loss": with_speaker_loss,
        }

    def tokenize(self, batch: Dict[str, Any]):
        """
        Tokenizes the input audio into semantic and global tokens.

        Args:
            batch (dict): The input audio features and reference waveform.

        Returns:
            tuple: Semantic tokens and global tokens.
        """
        feat = batch["feat"]
        ref_wav = mx.array(batch["ref_wav"])
        mel = self.get_mel_spectrogram(ref_wav)
        z = self.encoder(feat.transpose(0, 2, 1))
        semantic_tokens = self.quantizer.tokenize(z)
        global_tokens = self.speaker_encoder.tokenize(mel)

        return semantic_tokens, global_tokens

    def detokenize(self, semantic_tokens, global_tokens):
        """
        Detokenizes the semantic and global tokens into a waveform.

        Args:
            semantic_tokens (tensor): Semantic tokens.
            global_tokens (tensor): Global tokens.

        Returns:
            tensor: Reconstructed waveform.
        """

        z_q = self.quantizer.detokenize(semantic_tokens.transpose(0, 1)).transpose(
            0, 2, 1
        )
        d_vector = self.speaker_encoder.detokenize(global_tokens)
        x = self.prenet(z_q, d_vector)
        x = x + d_vector[..., None]
        wav_recon = self.decoder(x)

        return wav_recon  # Return MLX array directly

    def get_mel_spectrogram(self, wav):
        mels = []
        for i in range(wav.shape[0]):
            audio_sample = mx.squeeze(wav[i])
            mel = mel_spectrogram(
                audio=audio_sample,
                sample_rate=self.mel_params["sample_rate"],
                n_mels=self.mel_params["num_mels"],
                n_fft=self.mel_params["n_fft"],
                hop_length=self.mel_params["hop_length"],
                win_length=self.mel_params["win_length"],
                f_min=self.mel_params["mel_fmin"],
                f_max=self.mel_params["mel_fmax"],
            )
            mels.append(mel)
        return mx.concatenate(mels, axis=0)


if __name__ == "__main__":

    model_path = get_model_path("SparkAudio/Spark-TTS-0.5B")

    model = BiCodec.load_from_checkpoint(model_path / "BiCodec")
    model.eval()

    # Generate random inputs for testing
    duration = 0.96
    x = mx.random.normal((20, 1, int(duration * 16000)), dtype=mx.float32)
    feat = mx.random.normal((20, int(duration * 50), 1024), dtype=mx.float32)
    inputs = {"feat": feat, "wav": x, "ref_wav": x}

    # Forward pass
    outputs = model(inputs)
    semantic_tokens, global_tokens = model.tokenize(inputs)

    wav_recon = model.detokenize(semantic_tokens, global_tokens)

    print(outputs["recons"].shape)
    print(wav_recon.shape)

    if np.allclose(outputs["recons"], wav_recon):
        print("Test successful")
    else:
        print("Test failed")



================================================
FILE: mlx_audio/tts/models/spark/spark.py
================================================
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.generate import stream_generate
from mlx_lm.models.qwen2 import Model as Qwen2Model
from mlx_lm.sample_utils import make_logits_processors, make_sampler
from mlx_lm.tokenizer_utils import load_tokenizer
from tqdm import tqdm

from mlx_audio.tts.models.base import BaseModelArgs, GenerationResult
from mlx_audio.tts.utils import get_model_path

from .audio_tokenizer import BiCodecTokenizer
from .utils.token_parser import GENDER_MAP, LEVELS_MAP, TASK_TOKEN_MAP

PITCH_MAP = SPEED_MAP = {
    0.0: "very_low",
    0.5: "low",
    1.0: "moderate",
    1.5: "high",
    2.0: "very_high",
}


@dataclass
class ModelConfig(BaseModelArgs):
    model_path: Path = None
    sample_rate: int = 16000
    bos_token_id: int = 151643
    eos_token_id: int = 151645
    hidden_act: str = "silu"
    hidden_size: int = 896
    initializer_range: float = 0.02
    intermediate_size: int = 4864
    max_position_embeddings: int = 32768
    max_window_layers: int = 21
    model_type: str = "qwen2"
    num_attention_heads: int = 14
    num_hidden_layers: int = 24
    num_key_value_heads: int = 2
    rms_norm_eps: float = 1e-06
    rope_theta: float = 1000000.0
    sliding_window: int = 32768
    tie_word_embeddings: bool = True
    torch_dtype: str = "bfloat16"
    transformers_version: str = "4.43.1"
    use_sliding_window: bool = False
    vocab_size: int = 166000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None


class Model(nn.Module):
    """
    Spark-TTS for text-to-speech generation.
    """

    def __init__(self, config: ModelConfig):
        """
        Initializes the SparkTTS model with the provided configurations and device.

        Args:
            config (ModelConfig): The configuration for the model.
        """
        self.config = config

        model_dir = config.model_path

        self.model = Qwen2Model(config)
        self.tokenizer = load_tokenizer(model_dir, eos_token_ids=config.eos_token_id)

        self._audio_tokenizer = BiCodecTokenizer(model_dir)

    def load_weights(self, weights, strict=True):
        self.model.load_weights(weights, strict=strict)

    def parameters(self):
        return self.model.parameters()

    def model_type(self):
        return "spark"

    def sanitize(self, weights):
        return self.model.sanitize(weights)

    @property
    def sample_rate(self):
        return self.config.sample_rate

    @property
    def layers(self):
        return self.model.layers

    def model_quant_predicate(self, p, m, config):
        """
        Model modules to skip during quantization
        """
        return not p.startswith("_audio_tokenizer")

    def process_prompt(
        self,
        text: str,
        ref_audio: Path,
        ref_text: str,
    ) -> Tuple[str, mx.array]:
        """
        Process input for voice cloning.

        Args:
            text (str): The text input to be converted to speech.
            ref_audio (Path): Path to the audio file used as a reference.
            ref_text (str, optional): Transcript of the reference audio.

        Return:
            Tuple[str, mx.array]: Input prompt; global tokens
        """

        global_token_ids, semantic_token_ids = self._audio_tokenizer.tokenize(ref_audio)
        global_tokens = "".join(
            [f"<|bicodec_global_{i}|>" for i in global_token_ids.squeeze()]
        )

        # Prepare the input tokens for the model
        if ref_text is not None:
            semantic_tokens = "".join(
                [f"<|bicodec_semantic_{i}|>" for i in semantic_token_ids.squeeze()]
            )
            inputs = [
                TASK_TOKEN_MAP["tts"],
                "<|start_content|>",
                ref_text,
                text,
                "<|end_content|>",
                "<|start_global_token|>",
                global_tokens,
                "<|end_global_token|>",
                "<|start_semantic_token|>",
                semantic_tokens,
            ]
        else:
            inputs = [
                TASK_TOKEN_MAP["tts"],
                "<|start_content|>",
                text,
                "<|end_content|>",
                "<|start_global_token|>",
                global_tokens,
                "<|end_global_token|>",
            ]

        inputs = "".join(inputs)

        return inputs, global_token_ids

    def process_prompt_control(
        self,
        gender: str,
        pitch: str,
        speed: str,
        text: str,
    ):
        """
        Process input for voice creation.

        Args:
            gender (str): female | male.
            pitch (str): very_low | low | moderate | high | very_high
            speed (str): very_low | low | moderate | high | very_high
            text (str): The text input to be converted to speech.

        Return:
            str: Input prompt
        """
        assert gender in GENDER_MAP.keys()
        assert pitch in LEVELS_MAP.keys()
        assert speed in LEVELS_MAP.keys()

        gender_id = GENDER_MAP[gender]
        pitch_level_id = LEVELS_MAP[pitch]
        speed_level_id = LEVELS_MAP[speed]

        pitch_label_tokens = f"<|pitch_label_{pitch_level_id}|>"
        speed_label_tokens = f"<|speed_label_{speed_level_id}|>"
        gender_tokens = f"<|gender_{gender_id}|>"

        attribte_tokens = "".join(
            [gender_tokens, pitch_label_tokens, speed_label_tokens]
        )

        control_tts_inputs = [
            TASK_TOKEN_MAP["controllable_tts"],
            "<|start_content|>",
            text,
            "<|end_content|>",
            "<|start_style_label|>",
            attribte_tokens,
            "<|end_style_label|>",
        ]

        return "".join(control_tts_inputs)

    def generate(
        self,
        text: str,
        ref_audio: Path = None,
        ref_text: str = None,
        gender: str = "male",
        pitch: float = 1.0,
        speed: float = 1.0,
        temperature: float = 0.8,
        top_k: float = 50,
        top_p: float = 0.95,
        max_tokens: int = 3000,
        verbose: bool = False,
        split_pattern: str = "\n",
        **kwargs,
    ) -> GenerationResult:
        """
        Performs inference to generate speech from text, incorporating prompt audio and/or text.

        Args:
            text (str): The text input to be converted to speech.
            ref_audio (Path): Path to the audio file used as a reference.
            ref_text (str, optional): Transcript of the reference audio.
            gender (str): female | male.
            pitch (str): very_low | low | moderate | high | very_high
            speed (str): very_low | low | moderate | high | very_high
            temperature (float, optional): Sampling temperature for controlling randomness. Default is 0.8.
            top_k (float, optional): Top-k sampling parameter. Default is 50.
            top_p (float, optional): Top-p (nucleus) sampling parameter. Default is 0.95.

        Returns:
            GenerationResult: Generated waveform as a tensor.
        """

        speed_factor = SPEED_MAP[speed]
        pitch_factor = PITCH_MAP[pitch]

        if ref_audio is not None:  # voice cloning
            gender = None

        text_splits = text.split(split_pattern)

        for text_split in text_splits:
            if gender is not None:
                prompt = self.process_prompt_control(
                    gender, pitch_factor, speed_factor, text_split
                )

            else:
                prompt, global_token_ids = self.process_prompt(
                    text_split, ref_audio, ref_text
                )

            inputs = self.tokenizer._tokenizer([prompt], return_tensors="pt")

            input_ids = mx.array(inputs.input_ids)

            sampler = make_sampler(temperature, top_p=top_p, top_k=top_k)
            logits_processors = make_logits_processors(
                kwargs.get("logit_bias", None),
                kwargs.get("repetition_penalty", 1.3),
                kwargs.get("repetition_context_size", 20),
            )

            time_start = time.time()

            generated_ids = []

            # Generate speech using the model
            for i, response in enumerate(
                tqdm(
                    stream_generate(
                        self.model,
                        tokenizer=self.tokenizer,
                        prompt=input_ids.squeeze(0),
                        max_tokens=max_tokens,
                        sampler=sampler,
                        logits_processors=logits_processors,
                    ),
                    total=max_tokens,
                    disable=not verbose,
                )
            ):
                next_token = mx.array([response.token])
                input_ids = mx.concatenate([input_ids, next_token[None, :]], axis=1)
                if i % 50 == 0:
                    mx.clear_cache()

                if next_token == 128258:
                    break

            time_end = time.time()
            # Trim the output tokens to remove the input tokens
            generated_ids = mx.array(
                [
                    output[len(input) :]
                    for input, output in zip(inputs.input_ids, input_ids)
                ]
            ).tolist()

            # Decode the generated tokens into text
            predicts = self.tokenizer._tokenizer.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]

            # Extract semantic token IDs from the generated text
            pred_semantic_ids = mx.array(
                [
                    int(token)
                    for token in re.findall(r"bicodec_semantic_(\d+)", predicts)
                ]
            )[None, ...]

            if gender is not None:
                global_token_ids = mx.array(
                    [
                        int(token)
                        for token in re.findall(r"bicodec_global_(\d+)", predicts)
                    ]
                )[None, ...]

            # Convert semantic tokens back to waveform
            audio = self._audio_tokenizer.detokenize(
                global_token_ids.astype(mx.int32),
                pred_semantic_ids.astype(mx.int32),
            )

            # Clear cache
            mx.clear_cache()

            audio_samples = len(audio)
            audio_duration_seconds = audio_samples / self.config.sample_rate

            # Format duration as HH:MM:SS.mmm
            duration_mins = int(audio_duration_seconds // 60)
            duration_secs = int(audio_duration_seconds % 60)
            duration_ms = int((audio_duration_seconds % 1) * 1000)
            duration_hours = int(audio_duration_seconds // 3600)
            duration_str = f"{duration_hours:02d}:{duration_mins:02d}:{duration_secs:02d}.{duration_ms:03d}"

            yield GenerationResult(
                audio=audio,
                sample_rate=self.config.sample_rate,
                samples=audio_samples,
                segment_idx=0,  # Default segment index
                token_count=len(pred_semantic_ids.squeeze()),
                audio_samples={
                    "samples": audio_samples,
                    "samples-per-sec": (
                        round(audio_samples / audio_duration_seconds, 2)
                        if audio_duration_seconds > 0
                        else 0
                    ),
                },
                audio_duration=duration_str,
                real_time_factor=(
                    audio_duration_seconds / (time_end - time_start)
                    if (time_end - time_start) > 0
                    else 0
                ),
                prompt={
                    "tokens": len(pred_semantic_ids.squeeze()),
                    "tokens-per-sec": (
                        round(
                            len(pred_semantic_ids.squeeze()) / audio_duration_seconds, 2
                        )
                        if audio_duration_seconds > 0
                        else 0
                    ),
                },
                processing_time_seconds=time_end - time_start,
                peak_memory_usage=mx.get_peak_memory() / 1e9,
            )

            # Clear cache after each segment to avoid memory leaks
            mx.clear_cache()



================================================
FILE: mlx_audio/tts/models/spark/modules/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/models/spark/modules/finite_scalar_quantization.py
================================================
"""
Finite Scalar Quantization: VQ-VAE Made Simple - https://arxiv.org/abs/2309.15505
Code adapted from Jax version in Appendix A.1
"""

from __future__ import annotations

from contextlib import nullcontext
from functools import partial, wraps
from typing import List, Tuple

import mlx.core as mx
import mlx.nn as nn

# helper functions


def exists(v):
    return v is not None


def default(*args):
    for arg in args:
        if exists(arg):
            return arg
    return None


def maybe(fn):
    @wraps(fn)
    def inner(x, *args, **kwargs):
        if not exists(x):
            return x
        return fn(x, *args, **kwargs)

    return inner


# tensor helpers


def round_ste(z: mx.array) -> mx.array:
    """Round with straight through gradients."""
    zhat = z.round()
    return z + (zhat - z)


# main class


class FSQ(nn.Module):
    def __init__(
        self,
        levels: List[int],
        dim: int | None = None,
        num_codebooks=1,
        keep_num_codebooks_dim: bool | None = None,
        scale: float | None = None,
        allowed_dtypes: Tuple[mx.dtype, ...] = (mx.float32, mx.float64),
        channel_first: bool = False,
        projection_has_bias: bool = True,
        return_indices=True,
        force_quantization_f32=True,
    ):
        super().__init__()
        _levels = mx.array(list(levels), dtype=mx.int32)
        self._levels = _levels

        _basis = mx.cumprod(mx.array([1] + list(levels[:-1])), axis=0)
        self._basis = _basis

        self.scale = scale

        codebook_dim = len(levels)
        self.codebook_dim = codebook_dim

        effective_codebook_dim = codebook_dim * num_codebooks
        self.num_codebooks = num_codebooks
        self.effective_codebook_dim = effective_codebook_dim

        keep_num_codebooks_dim = default(keep_num_codebooks_dim, num_codebooks > 1)
        assert not (num_codebooks > 1 and not keep_num_codebooks_dim)
        self.keep_num_codebooks_dim = keep_num_codebooks_dim

        self.dim = default(dim, len(_levels) * num_codebooks)

        self.channel_first = channel_first

        has_projections = self.dim != effective_codebook_dim
        self.project_in = (
            nn.Linear(self.dim, effective_codebook_dim, bias=projection_has_bias)
            if has_projections
            else nn.Identity()
        )
        self.project_out = (
            nn.Linear(effective_codebook_dim, self.dim, bias=projection_has_bias)
            if has_projections
            else nn.Identity()
        )

        self.has_projections = has_projections

        self.return_indices = return_indices
        if return_indices:
            self.codebook_size = self._levels.prod().item()
            implicit_codebook = self._indices_to_codes(mx.arange(self.codebook_size))
            self._implicit_codebook = implicit_codebook

        self.allowed_dtypes = allowed_dtypes
        self.force_quantization_f32 = force_quantization_f32

    def atanh(self, x):
        return mx.log((1 + x) / (1 - x)) / 2

    def bound(self, z, eps: float = 1e-3):
        """Bound `z`, an array of shape (..., d)."""
        half_l = (self._levels - 1) * (1 + eps) / 2
        offset = mx.where(self._levels % 2 == 0, 0.5, 0.0)
        shift = self.atanh(offset / half_l)  # original atanh
        return mx.tanh(z + shift) * half_l - offset

    def quantize(self, z):
        """Quantizes z, returns quantized zhat, same shape as z."""
        quantized = round_ste(self.bound(z))
        half_width = self._levels // 2  # Renormalize to [-1, 1].
        return quantized / half_width

    def _scale_and_shift(self, zhat_normalized):
        half_width = self._levels // 2
        return (zhat_normalized * half_width) + half_width

    def _scale_and_shift_inverse(self, zhat):
        half_width = self._levels // 2
        return (zhat - half_width) / half_width

    def _indices_to_codes(self, indices):
        level_indices = self.indices_to_level_indices(indices)
        codes = self._scale_and_shift_inverse(level_indices)
        return codes

    def codes_to_indices(self, zhat):
        """Converts a `code` to an index in the codebook."""
        assert zhat.shape[-1] == self.codebook_dim
        zhat = self._scale_and_shift(zhat)
        return (zhat * self._basis).sum(axis=-1).astype(mx.int32)

    def indices_to_level_indices(self, indices):
        """Converts indices to indices at each level, perhaps needed for a transformer with factorized embeddings"""
        indices = mx.reshape(indices, (indices.shape[0], -1, 1))
        codes_non_centered = (indices // self._basis) % self._levels
        return codes_non_centered

    def indices_to_codes(self, indices):
        """Inverse of `codes_to_indices`."""
        assert exists(indices)

        is_img_or_video = indices.ndim >= (3 + int(self.keep_num_codebooks_dim))

        codes = self._indices_to_codes(indices)

        if self.keep_num_codebooks_dim:
            codes = mx.reshape(codes, (codes.shape[0], -1))

        codes = self.project_out(codes)

        if is_img_or_video or self.channel_first:
            codes = mx.reshape(codes, (codes.shape[0], -1, codes.shape[-1]))

        return codes

    def __call__(self, z):
        """
        einstein notation
        b - batch
        n - sequence (or flattened spatial dimensions)
        d - feature dimension
        c - number of codebook dim
        """

        is_img_or_video = z.ndim >= 4
        need_move_channel_last = is_img_or_video or self.channel_first

        # standardize image or video into (batch, seq, dimension)

        if need_move_channel_last:
            z = mx.reshape(z, (z.shape[0], -1, z.shape[-1]))
            # z = mx.reshape(z, (z.shape[0], -1))

        assert (
            z.shape[-1] == self.dim
        ), f"expected dimension of {self.dim} but found dimension of {z.shape[-1]}"

        z = self.project_in(z)

        z = mx.reshape(z, (z.shape[0], z.shape[1], self.num_codebooks, z.shape[-1]))

        # whether to force quantization step to be full precision or not

        force_f32 = self.force_quantization_f32

        orig_dtype = z.dtype

        if force_f32 and orig_dtype not in self.allowed_dtypes:
            z = z.float()

        codes = self.quantize(z)

        # returning indices could be optional

        indices = None

        if self.return_indices:
            indices = self.codes_to_indices(codes)

        codes = mx.reshape(codes, (codes.shape[0], codes.shape[1], -1))

        codes = codes.astype(orig_dtype)

        # project out

        out = self.project_out(codes)

        # reconstitute image or video dimensions

        if need_move_channel_last:
            out = mx.reshape(out, (out.shape[0], -1, out.shape[-1]))
            out = mx.reshape(
                out, (out.shape[0], out.shape[1], out.shape[2], out.shape[-1])
            )

            indices = mx.reshape(indices, (indices.shape[0], -1, indices.shape[-1]))

        if not self.keep_num_codebooks_dim and self.return_indices:
            indices = mx.reshape(indices, (indices.shape[0], -1, indices.shape[-1]))

        # return quantized output and indices

        return out, indices



================================================
FILE: mlx_audio/tts/models/spark/modules/residual.py
================================================
from typing import Any, Dict, List

import mlx.core as mx
import mlx.nn as nn
from einops.array_api import rearrange

from mlx_audio.codec.models.descript.nn.layers import WNConv1d


def exists(val):
    return val is not None


def default(val, d):
    return val if exists(val) else d


class FactorizedVectorQuantize(nn.Module):
    def __init__(
        self,
        input_dim: int,
        codebook_size: int,
        codebook_dim: int,
        commitment: float,
        codebook_loss_weight: float = 1.0,
        decay: float = 0.99,
        threshold_ema_dead_code: float = 2,
        momentum: float = 0.99,
        **kwargs,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.codebook_size = codebook_size
        self.commitment = commitment
        self.codebook_dim = codebook_dim
        self.codebook_loss_weight = codebook_loss_weight
        self.decay = decay
        self.threshold_ema_dead_code = threshold_ema_dead_code
        self.momentum = momentum

        requires_projection = input_dim != codebook_dim

        self.in_project = (
            WNConv1d(in_channels=input_dim, out_channels=codebook_dim, kernel_size=1)
            if requires_projection
            else nn.Identity()
        )
        self.out_project = (
            WNConv1d(in_channels=codebook_dim, out_channels=input_dim, kernel_size=1)
            if requires_projection
            else nn.Identity()
        )

        self.codebook = nn.Embedding(self.codebook_size, codebook_dim)
        self.cluster_size = mx.zeros((self.codebook_size,))

    def __call__(self, z: mx.array) -> Dict[str, Any]:
        """Quantized the input tensor using a fixed codebook and returns
        the corresponding codebook vectors

        Parameters
        ----------
        z : Tensor[B x D x T]

        Returns
        -------
        Tensor[B x D x T]
            Quantized continuous representation of input
        Tensor[1]
            Commitment loss to train encoder to predict vectors closer to codebook
            entries
        Tensor[1]
            Codebook loss to update the codebook
        Tensor[B x T]
            Codebook indices (quantized discrete representation of input)
        Tensor[B x D x T]
            Projected latents (continuous representation of input before quantization)
        """
        # transpose since we use linear

        # Factorized codes project input into low-dimensional space if self.input_dim != self.codebook_dim
        z_e = self.in_project(z.transpose(0, 2, 1)).transpose(0, 2, 1)
        z_q, indices, dists = self.decode_latents(z_e)

        # statistic the usage of codes
        embed_onehot = mx.zeros(
            (indices.shape[0], indices.shape[1], self.codebook_size), dtype=z_e.dtype
        )
        for i in range(indices.shape[0]):
            for j in range(indices.shape[1]):
                embed_onehot[i, j, indices[i, j]] = 1.0
        avg_probs = mx.mean(embed_onehot.reshape(-1, self.codebook_size), axis=0)
        perplexity = mx.exp(-mx.sum(avg_probs * mx.log(avg_probs + 1e-10)))

        active_num = (embed_onehot.sum(0).sum(0) > 0).sum()

        commit_loss = mx.zeros(0)
        codebook_loss = mx.zeros(0)

        z_q = z_e + (
            z_q - z_e
        )  # noop in forward pass, straight-through gradient estimator in backward pass

        z_q = self.out_project(z_q.transpose(0, 2, 1)).transpose(0, 2, 1)

        vq_loss = (commit_loss + codebook_loss).mean()

        return {
            "z_q": z_q,
            "indices": indices,
            "dists": dists,
            "vq_loss": vq_loss,
            "perplexity": perplexity,
            "active_num": active_num.astype(mx.float32),
        }

    def vq2emb(self, vq, out_proj=True):
        emb = self.embed_code(vq)
        if out_proj:
            emb = self.out_project(emb)
        return emb

    def tokenize(self, z: mx.array) -> mx.array:
        """tokenize the input tensor"""
        z_e = self.in_project(z.transpose(0, 2, 1)).transpose(0, 2, 1)
        _, indices, _ = self.decode_latents(z_e)
        return indices

    def detokenize(self, indices):
        """detokenize the input indices"""
        # Check if indices are empty
        if indices.shape[0] == 0 or indices.shape[1] == 0:
            # Return an appropriate empty or placeholder tensor
            return mx.zeros((1, self.input_dim, 1))

        z_q = self.decode_code(indices).transpose(0, 2, 1)
        z_q = self.out_project(z_q)
        return z_q

    def get_emb(self):
        return self.codebook.weight

    def embed_code(self, embed_id):
        return mx.take(self.codebook.weight, embed_id, axis=0)

    def decode_code(self, embed_id):

        return self.embed_code(embed_id).transpose(0, 2, 1)

    def normalize(self, x):
        """Normalize input tensor along dimension 1."""
        norm = mx.sqrt(mx.sum(mx.power(x, 2), axis=1, keepdims=True))
        return x / mx.maximum(norm, 1e-12)

    def decode_latents(self, latents):
        encodings = rearrange(latents, "b d t -> (b t) d")
        codebook = self.codebook.weight

        # L2 normalize encodings and codebook
        encodings = self.normalize(encodings)
        codebook = self.normalize(codebook)

        # Compute euclidean distance between encodings and codebook,
        # with L2 normalization, the distance is equal to cosine distance
        dist = (
            mx.sum(mx.power(encodings, 2), axis=1, keepdims=True)
            - 2 * encodings @ codebook.T
            + mx.sum(mx.power(codebook, 2), axis=1, keepdims=True).T
        )
        min_encoding_indices = mx.argmax(-dist, axis=1)
        indices = mx.reshape(min_encoding_indices, (latents.shape[0], latents.shape[2]))
        z_q = self.decode_code(indices)

        return z_q, indices, dist

    def get_codes_from_indices(self, indices):
        """Get codebook vectors from indices.

        Args:
            indices: Tensor of shape [B, T]

        Returns:
            Tensor of shape [B, D, T]
        """
        return self.decode_code(indices)

    def get_output_from_indices(self, indices):
        """Get output from indices.

        Args:
            indices: Tensor of shape [B, T]

        Returns:
            Tensor of shape [B, D, T]
        """
        z_q = self.get_codes_from_indices(indices)
        return self.out_project(z_q.transpose(0, 2, 1)).transpose(0, 2, 1)

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if "weight_v" in k:
                if v.shape[1] > v.shape[-1]:
                    sanitized_weights[k] = v.transpose(0, 2, 1)
                else:
                    sanitized_weights[k] = v
            else:
                sanitized_weights[k] = v
        return sanitized_weights



================================================
FILE: mlx_audio/tts/models/spark/modules/residual_fsq.py
================================================
import random
from typing import List

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.spark.modules.finite_scalar_quantization import FSQ


def exists(val):
    return val is not None


def first(l):
    return l[0]


def default(val, d):
    return val if exists(val) else d


def round_up_multiple(num, mult):
    return ceil(num / mult) * mult


class ResidualFSQ(nn.Module):
    """Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf"""

    def __init__(
        self,
        *,
        levels: List[int],
        num_quantizers,
        dim=None,
        is_channel_first=False,
        quantize_dropout=False,
        quantize_dropout_cutoff_index=0,
        quantize_dropout_multiple_of=1,
        **kwargs,
    ):
        super().__init__()
        codebook_dim = len(levels)
        dim = default(dim, codebook_dim)

        requires_projection = codebook_dim != dim
        self.project_in = (
            nn.Linear(dim, codebook_dim) if requires_projection else nn.Identity()
        )
        self.project_out = (
            nn.Linear(codebook_dim, dim) if requires_projection else nn.Identity()
        )
        self.has_projections = requires_projection

        self.is_channel_first = is_channel_first
        self.num_quantizers = num_quantizers

        self.levels = levels
        self.layers = []

        # Convert ListConfig to a regular list before passing to mx.array
        levels_tensor = mx.array(list(levels))

        scales = []

        for ind in range(num_quantizers):
            scales.append((levels_tensor - 1) ** -ind)

            fsq = FSQ(levels=levels, dim=codebook_dim, **kwargs)

            self.layers.append(fsq)

        assert all([not fsq.has_projections for fsq in self.layers])

        self.codebook_size = self.layers[0].codebook_size

        self._scales = mx.array(scales)

        self.quantize_dropout = quantize_dropout and num_quantizers > 1

        assert quantize_dropout_cutoff_index >= 0

        self.quantize_dropout_cutoff_index = quantize_dropout_cutoff_index
        self.quantize_dropout_multiple_of = quantize_dropout_multiple_of  # encodec paper proposes structured dropout, believe this was set to 4

    @property
    def codebooks(self):
        codebooks = [layer._implicit_codebook for layer in self.layers]
        codebooks = mx.stack(codebooks, axis=0)
        return codebooks

    def get_codes_from_indices(self, indices):
        batch, quantize_dim = indices.shape[0], indices.shape[-1]

        # may also receive indices in the shape of 'b h w q' (accept_image_fmap)

        # MLX doesn't have pack function, so we need to reshape manually
        original_shape = indices.shape
        indices = mx.reshape(indices, (indices.shape[0], -1, indices.shape[-1]))

        # because of quantize dropout, one can pass in indices that are coarse
        # and the network should be able to reconstruct

        if quantize_dim < self.num_quantizers:
            assert (
                self.quantize_dropout > 0.0
            ), "quantize dropout must be greater than 0 if you wish to reconstruct from a signal with less fine quantizations"
            indices = mx.pad(
                indices,
                ((0, 0), (0, 0), (0, self.num_quantizers - quantize_dim)),
                constant_value=-1,
            )

        # take care of quantizer dropout

        mask = indices == -1
        # MLX doesn't have masked_fill, so we use where
        indices = mx.where(
            mask, mx.zeros_like(indices), indices
        )  # have it fetch a dummy code to be masked out later

        # MLX doesn't have get_at function, so we need to manually gather codes
        all_codes = []
        for q in range(self.codebooks.shape[0]):
            q_codes = []
            for b in range(indices.shape[0]):
                n_codes = []
                for n in range(indices.shape[1]):
                    idx = indices[b, n, q]
                    n_codes.append(self.codebooks[q, idx])
                q_codes.append(mx.stack(n_codes))
            all_codes.append(mx.stack(q_codes))
        all_codes = mx.stack(all_codes)[:, :, :, 0, :]  # Shape: (q, b, n, d)

        # mask out any codes that were dropout-ed
        # Reshape mask for broadcasting: q b n 1

        mask_reshaped = mx.reshape(
            mask, (mask.shape[2], mask.shape[0], mask.shape[1], 1)
        )

        all_codes = mx.where(mask_reshaped, mx.zeros_like(all_codes), all_codes)

        # scale the codes
        # Reshape scales for broadcasting: q 1 1 d
        scales = mx.reshape(
            self._scales, (self._scales.shape[0], 1, 1, self._scales.shape[1])
        )
        all_codes = all_codes * scales

        # if (accept_image_fmap = True) then return shape (quantize, batch, height, width, dimension)
        # Reshape all_codes back to original dimensions
        if len(original_shape) > 3:  # If we had height, width dimensions
            all_codes = mx.reshape(
                all_codes,
                (
                    all_codes.shape[0],
                    original_shape[0],
                    *original_shape[1:-1],
                    all_codes.shape[-1],
                ),
            )

        return all_codes

    def get_output_from_indices(self, indices):
        codes = self.get_codes_from_indices(indices)
        codes_summed = mx.sum(codes, axis=0)
        return self.project_out(codes_summed)

    def __call__(
        self, x, return_all_codes=False, rand_quantize_dropout_fixed_seed=None
    ):
        num_quant, quant_dropout_multiple_of = (
            self.num_quantizers,
            self.quantize_dropout_multiple_of,
        )

        # handle channel first

        if self.is_channel_first:
            # Manually implement rearrange and pack functionality
            # First, move dimension d from position 1 to the end
            shape = x.shape
            # Assuming shape is (b, d, ...)
            new_shape = (shape[0],) + shape[2:] + (shape[1],)
            x = mx.transpose(x, (0,) + tuple(range(2, len(shape))) + (1,))

            # Pack operation: flatten all dimensions between b and d
            # This is equivalent to pack([x], "b * d")
            ps = x.shape
            middle_dims = x.shape[1:-1]
            flattened_dim = 1
            for dim in middle_dims:
                flattened_dim *= dim
            x = mx.reshape(x, (x.shape[0], flattened_dim, x.shape[-1]))

        # maybe project in

        x = self.project_in(x)

        quantized_out = 0.0
        residual = x

        all_indices = []

        should_quantize_dropout = self.training and self.quantize_dropout

        # sample a layer index at which to dropout further residual quantization
        # also prepare null indices

        if should_quantize_dropout:

            # check if seed is manually passed in

            rand = random.Random(rand_quantize_dropout_fixed_seed)

            rand_quantize_dropout_index = rand.randrange(
                self.quantize_dropout_cutoff_index, num_quant
            )

            if quant_dropout_multiple_of != 1:
                rand_quantize_dropout_index = (
                    round_up_multiple(
                        rand_quantize_dropout_index + 1, quant_dropout_multiple_of
                    )
                    - 1
                )

            null_indices = mx.full(x.shape[:2], -1, dtype=mx.int32)

        # go through the layers
        for quantizer_index, (layer, scale) in enumerate(
            zip(self.layers, self._scales)
        ):

            if (
                should_quantize_dropout
                and quantizer_index > rand_quantize_dropout_index
            ):
                all_indices.append(null_indices)
                continue

            quantized, indices = layer(residual / scale)

            quantized = quantized * scale

            residual = residual - quantized
            quantized_out = quantized_out + quantized

            all_indices.append(indices)

        # project out, if needed

        quantized_out = self.project_out(quantized_out)

        # stack all indices

        all_indices = mx.stack(all_indices, axis=-1)

        # channel first out

        if self.is_channel_first:
            # MLX doesn't have unpack, so we need to reshape manually
            # Assuming ps contains the original batch dimensions
            # Reshape to combine all dimensions between batch and the last dimension
            batch_size = ps[0] if isinstance(ps, tuple) else ps
            quantized_out = mx.reshape(
                quantized_out, (batch_size, -1, quantized_out.shape[-1])
            ).swapaxes(
                2, 1
            )  # swap to match torch output
            all_indices = mx.reshape(
                all_indices, (batch_size, -1, all_indices.shape[-1])
            ).swapaxes(
                2, 1
            )  # swap to match torch output

        # return
        ret = (quantized_out, all_indices)

        if not return_all_codes:
            return ret

        # whether to return all codes from all codebooks across layers

        all_codes = self.get_codes_from_indices(all_indices)

        # will return all codes in shape (quantizer, batch, sequence length, codebook dimension)

        return (*ret, all_codes)


if __name__ == "__main__":
    model = ResidualFSQ(
        levels=[4, 4, 4, 4, 4, 4],
        num_quantizers=1,
        dim=30,
        is_channel_first=True,
        quantize_dropout=False,
    )
    x = mx.random.normal((2, 30, 10))
    quantize, embed_ind = model(x)

    emb_from_ind = model.get_output_from_indices(embed_ind.transpose(0, 2, 1))

    print(quantize == emb_from_ind.transpose(0, 2, 1))

    print("quantize shape", quantize.shape)
    print("embed_ind", embed_ind)



================================================
FILE: mlx_audio/tts/models/spark/modules/blocks/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/models/spark/modules/blocks/sampler.py
================================================
import math

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.codec.models.descript.nn.layers import WNConvTranspose1d


class SamplingBlock(nn.Module):
    """Sampling block for upsampling or downsampling"""

    def __init__(
        self,
        dim: int,
        groups: int = 1,
        upsample_scale: int = 1,
        downsample_scale: int = 1,
    ) -> None:
        """
        Args:
            dim: input dimension
            groups: number of groups
            upsample_scale: upsampling scale
            downsample_scale: downsampling scale
        """
        super(SamplingBlock, self).__init__()

        self.upsample_scale = upsample_scale
        self.downsample_scale = downsample_scale

        if self.upsample_scale > 1:
            self.de_conv_upsampler = nn.Sequential(
                nn.LeakyReLU(0.2),
                WNConvTranspose1d(
                    dim,
                    dim,
                    kernel_size=upsample_scale * 2,
                    stride=upsample_scale,
                    padding=upsample_scale // 2 + upsample_scale % 2,
                    groups=groups,
                ),
            )

        if self.downsample_scale > 1:
            self.conv_downsampler = nn.Sequential(
                nn.LeakyReLU(0.2),
                nn.Conv1d(
                    dim,
                    dim,
                    kernel_size=2 * downsample_scale,
                    stride=downsample_scale,
                    padding=downsample_scale // 2 + downsample_scale % 2,
                    groups=groups,
                ),
            )

    @staticmethod
    def repeat_upsampler(x, upsample_scale):
        # MLX doesn't have repeat_interleave, so we need to implement it manually
        batch_size, seq_len, channels = x.shape
        # Create a new tensor with the expanded shape
        output = mx.zeros((batch_size, seq_len * upsample_scale, channels))
        # Fill the output tensor by repeating each element
        for i in range(seq_len):
            for j in range(upsample_scale):
                output[:, i * upsample_scale + j, :] = x[:, i, :]
        return output

    @staticmethod
    def skip_downsampler(x, downsample_scale):
        return nn.AvgPool1d(kernel_size=downsample_scale, stride=downsample_scale)(x)

    def __call__(self, x):
        x = x.transpose(0, 2, 1)
        if self.upsample_scale > 1:
            repeat_res = self.repeat_upsampler(x, self.upsample_scale)
            deconv_res = self.de_conv_upsampler(x)
            upmerge_res = repeat_res + deconv_res
        else:
            upmerge_res = x
            repeat_res = x

        if self.downsample_scale > 1:
            conv_res = self.conv_downsampler(upmerge_res)
            skip2_res = self.skip_downsampler(upmerge_res, self.downsample_scale)
            skip1_res = self.skip_downsampler(repeat_res, self.downsample_scale)
        else:
            conv_res = upmerge_res
            skip2_res = upmerge_res
            skip1_res = repeat_res

        final_res = conv_res + skip1_res + skip2_res

        return final_res.transpose(0, 2, 1)


# test
if __name__ == "__main__":
    test_input = mx.random.randint(
        0, 100, (8, 1024, 50)
    )  # Batch size = 8, 1024 channels, length = 50
    model = SamplingBlock(1024, 1024, upsample_scale=2)
    model_down = SamplingBlock(1024, 1024, downsample_scale=2)
    output = model(test_input)
    output_down = model_down(test_input)
    print("shape after upsample * 2", output.shape)  # torch.Size([8, 1024, 100])
    print("shape after downsample * 2", output_down.shape)  # torch.Size([8, 1024, 25])
    if output.shape == (8, 1024, 100) and output_down.shape == (8, 1024, 25):
        print("test successful")
    else:
        print("test failed")



================================================
FILE: mlx_audio/tts/models/spark/modules/encoder_decoder/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/models/spark/modules/encoder_decoder/feat_decoder.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from collections import OrderedDict
from typing import List

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.codec.models.vocos.vocos import VocosBackbone
from mlx_audio.tts.models.spark.modules.blocks.sampler import SamplingBlock


class Decoder(nn.Module):
    """Decoder module with convnext and upsampling blocks

    Args:
        sample_ratios (List[int]): sample ratios
            example: [2, 2] means downsample by 2x and then upsample by 2x
    """

    def __init__(
        self,
        input_channels: int,
        vocos_dim: int,
        vocos_intermediate_dim: int,
        vocos_num_layers: int,
        out_channels: int,
        condition_dim: int = None,
        sample_ratios: List[int] = [1, 1],
        use_tanh_at_final: bool = False,
    ):
        super().__init__()

        self.linear_pre = nn.Linear(input_channels, vocos_dim)
        modules = []
        for ratio in sample_ratios:
            module_list = [
                SamplingBlock(
                    dim=vocos_dim,
                    groups=vocos_dim,
                    upsample_scale=ratio,
                ),
                VocosBackbone(
                    input_channels=vocos_dim,
                    dim=vocos_dim,
                    intermediate_dim=vocos_intermediate_dim,
                    num_layers=2,
                ),
            ]
            modules.append(module_list)

        self.downsample = modules

        self.vocos_backbone = VocosBackbone(
            input_channels=vocos_dim,
            dim=vocos_dim,
            intermediate_dim=vocos_intermediate_dim,
            num_layers=vocos_num_layers,
            adanorm_num_embeddings=condition_dim,
        )
        self.linear = nn.Linear(vocos_dim, out_channels)
        self.use_tanh_at_final = use_tanh_at_final

    def __call__(self, x: mx.array, c: mx.array = None):
        """encoder forward.

        Args:
            x (mx.array): (batch_size, input_channels, length)

        Returns:
            x (mx.array): (batch_size, encode_channels, length)
        """
        x = self.linear_pre(x.transpose(0, 2, 1))
        for modules in self.downsample:
            for module in modules:
                x = module(x)

        x = self.vocos_backbone(x.transpose(0, 2, 1), bandwidth_id=c)
        x = self.linear(x).transpose(0, 2, 1)
        if self.use_tanh_at_final:
            x = mx.tanh(x)

        return x


# test
if __name__ == "__main__":
    test_input = mx.random.normal(
        (8, 1024, 50), dtype=mx.float32
    )  # Batch size = 8, 1024 channels, length = 50
    condition = mx.random.randint(0, 100, (256, 8))  # 8, 256
    decoder = Decoder(
        input_channels=1024,
        vocos_dim=384,
        vocos_intermediate_dim=2048,
        vocos_num_layers=12,
        out_channels=256,
        condition_dim=256,
        sample_ratios=[2, 2],
    )
    output = decoder(test_input, condition)
    print(output.shape)  # torch.Size([8, 256, 200])
    if output.shape == (8, 256, 200):
        print("Decoder test passed")
    else:
        print("Decoder test failed")



================================================
FILE: mlx_audio/tts/models/spark/modules/encoder_decoder/feat_encoder.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from typing import List

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.codec.models.vocos.vocos import VocosBackbone
from mlx_audio.tts.models.spark.modules.blocks.sampler import SamplingBlock


class Encoder(nn.Module):
    """Encoder module with convnext and downsampling blocks"""

    def __init__(
        self,
        input_channels: int,
        vocos_dim: int,
        vocos_intermediate_dim: int,
        vocos_num_layers: int,
        out_channels: int,
        sample_ratios: List[int] = [1, 1],
    ):
        super().__init__()
        """
        Encoder module with VocosBackbone and sampling blocks.

        Args:
            sample_ratios (List[int]): sample ratios
                example: [2, 2] means downsample by 2x and then upsample by 2x
        """

        self.encoder = VocosBackbone(
            input_channels=input_channels,
            dim=vocos_dim,
            intermediate_dim=vocos_intermediate_dim,
            num_layers=vocos_num_layers,
        )

        modules = []

        for ratio in sample_ratios:
            modules.append(
                [
                    SamplingBlock(
                        dim=vocos_dim,
                        groups=vocos_dim,
                        downsample_scale=ratio,
                    ),
                    VocosBackbone(
                        input_channels=vocos_dim,
                        dim=vocos_dim,
                        intermediate_dim=vocos_intermediate_dim,
                        num_layers=2,
                        bias=True,
                    ),
                ]
            )

        self.downsample = modules

        self.project = nn.Linear(vocos_dim, out_channels)

    def __call__(self, x: mx.array, *args):
        """
        Args:
            x (mx.array): (batch_size, input_channels, length)

        Returns:
            x (mx.array): (batch_size, encode_channels, length)
        """

        x = self.encoder(x)

        for modules in self.downsample:
            for module in modules:
                x = x.transpose(0, 2, 1)
                x = module(x)

        x = self.project(x)
        return x.transpose(0, 2, 1)

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if "dwconv.weight" in k:
                if v.shape[1] < v.shape[-1]:
                    sanitized_weights[k] = v.transpose(0, 2, 1)
                else:
                    sanitized_weights[k] = v
            elif "embed.weight" in k:
                if v.shape[1] > v.shape[-1]:
                    sanitized_weights[k] = v.transpose(0, 2, 1)
                else:
                    sanitized_weights[k] = v

            else:
                sanitized_weights[k] = v

        return sanitized_weights


# test
if __name__ == "__main__":
    test_input = mx.random.normal(
        (8, 1024, 50), dtype=mx.float32
    )  # Batch size = 8, 1024 channels, length = 50
    encoder = Encoder(
        input_channels=1024,
        vocos_dim=384,
        vocos_intermediate_dim=2048,
        vocos_num_layers=12,
        out_channels=256,
        sample_ratios=[2, 2],
    )

    output = encoder(test_input)
    print(output.shape)  # torch.Size([8, 256, 12])
    if output.shape == (8, 256, 12):
        print("test successful")
    else:
        print("test failed")



================================================
FILE: mlx_audio/tts/models/spark/modules/encoder_decoder/wave_generator.py
================================================
import mlx.core as mx
import mlx.nn as nn

from mlx_audio.codec.models.descript.dac import (
    ResidualUnit,
    Snake1d,
    WNConv1d,
    WNConvTranspose1d,
)


class DecoderBlock(nn.Module):
    def __init__(
        self,
        input_dim: int = 16,
        output_dim: int = 8,
        kernel_size: int = 2,
        stride: int = 1,
    ):
        super().__init__()
        self.block = nn.Sequential(
            Snake1d(input_dim),
            WNConvTranspose1d(
                input_dim,
                output_dim,
                kernel_size=kernel_size,
                stride=stride,
                padding=(kernel_size - stride) // 2,
            ),
            ResidualUnit(output_dim, dilation=1),
            ResidualUnit(output_dim, dilation=3),
            ResidualUnit(output_dim, dilation=9),
        )

    def __call__(self, x):
        return self.block(x)


class WaveGenerator(nn.Module):
    def __init__(
        self,
        input_channel,
        channels,
        rates,
        kernel_sizes,
        d_out: int = 1,
    ):
        super().__init__()

        # Add first conv layer
        layers = [WNConv1d(input_channel, channels, kernel_size=7, padding=3)]

        # Add upsampling + MRF blocks
        for i, (kernel_size, stride) in enumerate(zip(kernel_sizes, rates)):
            input_dim = channels // 2**i
            output_dim = channels // 2 ** (i + 1)
            layers += [DecoderBlock(input_dim, output_dim, kernel_size, stride)]

        # Add final conv layer
        layers += [
            Snake1d(output_dim),
            WNConv1d(output_dim, d_out, kernel_size=7, padding=3),
            nn.Tanh(),
        ]

        self.model = layers

    def __call__(self, x):
        x = x.transpose(0, 2, 1)
        for module in self.model:
            x = module(x)
        return x.transpose(0, 2, 1)

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():

            if "decoder.model" in k:
                if "block.layers" not in k:
                    k = k.replace("block", "block.layers")
                    sanitized_weights[k] = v

            if ".alpha" in k:
                if v.shape[1] > v.shape[-1]:
                    sanitized_weights[k] = v.transpose(0, 2, 1)
                else:
                    sanitized_weights[k] = v

            elif (
                "decoder.model" in k
                and "block.layers.1" in k
                and ("weight_v" in k or "weight_g" in k)
                and k.count("block") == 1
            ):

                if v.shape[0] > v.shape[-1]:
                    sanitized_weights[k] = v.transpose(1, 2, 0)
                else:
                    sanitized_weights[k] = v
            else:
                sanitized_weights[k] = v
        return sanitized_weights


if __name__ == "__main__":
    test_input = mx.random.normal((8, 1024, 50), dtype=mx.float32)
    wave_generator = WaveGenerator(1024, 16, [2, 2], [7, 7])
    output = wave_generator(test_input)
    print(output.shape)
    if output.shape == (8, 1, 203):
        print("WaveGenerator test passed")
    else:
        print("WaveGenerator test failed")



================================================
FILE: mlx_audio/tts/models/spark/modules/speaker/__init__.py
================================================




================================================
FILE: mlx_audio/tts/models/spark/modules/speaker/ecapa_tdnn.py
================================================
# Copyright (c) 2021 Zhengyang Chen (chenzhengyang117@gmail.com)
#               2022 Hongji Wang (jijijiang77@gmail.com)
#               2023 Bing Han (hanbing97@sjtu.edu.cn)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" This implementation is adapted from github repo:
    https://github.com/lawlict/ECAPA-TDNN.
"""

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.spark.modules.speaker import pooling_layers as pooling_layers


class Res2Conv1dReluBn(nn.Module):
    """
    in_channels == out_channels == channels
    """

    def __init__(
        self,
        channels,
        kernel_size=1,
        stride=1,
        padding=0,
        dilation=1,
        bias=True,
        scale=4,
    ):
        super().__init__()
        assert channels % scale == 0, "{} % {} != 0".format(channels, scale)
        self.scale = scale
        self.width = channels // scale
        self.channels = channels
        self.nums = scale if scale == 1 else scale - 1

        self.convs = []
        self.bns = []
        for i in range(self.nums):
            self.convs.append(
                nn.Conv1d(
                    self.width,
                    self.width,
                    kernel_size,
                    stride,
                    padding,
                    dilation,
                    bias=bias,
                )
            )
            self.bns.append(nn.BatchNorm(self.width))
        # self.convs = [*self.convs]  # nn.ModuleList(self.convs)
        # self.bns = [*self.bns]  # nn.ModuleList(self.bns)

    def __call__(self, x):
        out = []

        spx = mx.split(x, self.scale, axis=1)
        sp = spx[0]
        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
            # Order: conv -> relu -> bn
            if i >= 1:
                sp = sp + spx[i]

            sp = conv(sp.transpose(0, 2, 1))
            sp = bn(nn.relu(sp)).transpose(0, 2, 1)
            out.append(sp)
        if self.scale != 1:
            out.append(spx[self.nums])
        out = mx.concatenate(out, axis=1)
        return out


""" Conv1d + BatchNorm1d + ReLU
"""


class Conv1dReluBn(nn.Module):

    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size=1,
        stride=1,
        padding=0,
        dilation=1,
        bias=True,
    ):
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias
        )
        self.bn = nn.BatchNorm(out_channels)

    def __call__(self, x):
        x = self.conv(x.swapaxes(1, 2)).swapaxes(1, 2)
        x = nn.relu(x)
        x = self.bn(x.swapaxes(1, 2)).swapaxes(1, 2)
        return x


""" The SE connection of 1D case.
"""


class SE_Connect(nn.Module):

    def __init__(self, channels, se_bottleneck_dim=128):
        super().__init__()
        self.linear1 = nn.Linear(channels, se_bottleneck_dim)
        self.linear2 = nn.Linear(se_bottleneck_dim, channels)

    def __call__(self, x):
        out = mx.mean(x, axis=2)
        out = nn.relu(self.linear1(out))
        out = mx.sigmoid(self.linear2(out))
        out = x * out[:, :, None]
        return out


""" SE-Res2Block of the ECAPA-TDNN architecture.
"""


class SE_Res2Block(nn.Module):

    def __init__(self, channels, kernel_size, stride, padding, dilation, scale):
        super().__init__()
        self.se_res2block = [
            Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),
            Res2Conv1dReluBn(
                channels, kernel_size, stride, padding, dilation, scale=scale
            ),
            Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),
            SE_Connect(channels),
        ]

    def __call__(self, x):
        res = x
        for module in self.se_res2block:
            x = module(x)
        return x + res


class ECAPA_TDNN(nn.Module):

    def __init__(
        self,
        channels=512,
        feat_dim=80,
        embed_dim=192,
        pooling_func="ASTP",
        global_context_att=False,
        emb_bn=False,
    ):
        super().__init__()

        self.layer1 = Conv1dReluBn(feat_dim, channels, kernel_size=5, padding=2)
        self.layer2 = SE_Res2Block(
            channels, kernel_size=3, stride=1, padding=2, dilation=2, scale=8
        )
        self.layer3 = SE_Res2Block(
            channels, kernel_size=3, stride=1, padding=3, dilation=3, scale=8
        )
        self.layer4 = SE_Res2Block(
            channels, kernel_size=3, stride=1, padding=4, dilation=4, scale=8
        )

        cat_channels = channels * 3
        out_channels = 512 * 3
        self.conv = nn.Conv1d(cat_channels, out_channels, kernel_size=1)
        self.pool = getattr(pooling_layers, pooling_func)(
            in_dim=out_channels, global_context_att=global_context_att
        )
        self.pool_out_dim = self.pool.get_out_dim()
        self.bn = nn.BatchNorm(self.pool_out_dim)
        self.linear = nn.Linear(self.pool_out_dim, embed_dim)
        self.emb_bn = emb_bn
        if emb_bn:  # better in SSL for SV
            self.bn2 = nn.BatchNorm(embed_dim)
        else:
            self.bn2 = nn.Identity()

    def __call__(self, x, return_latent=False):
        x = x.transpose(0, 2, 1)  # (B,T,F) -> (B,F,T)

        out1 = self.layer1(x)
        out2 = self.layer2(out1)
        out3 = self.layer3(out2)
        out4 = self.layer4(out3)

        out = mx.concatenate([out2, out3, out4], axis=1)

        out = self.conv(out.transpose(0, 2, 1)).transpose(0, 2, 1)
        latent = nn.relu(out)
        out = self.pool(latent)
        out = self.bn(out)
        out = self.linear(out)
        if self.emb_bn:
            out = self.bn2(out)

        if return_latent:
            return out, latent
        return out


def ECAPA_TDNN_c1024(feat_dim, embed_dim, pooling_func="ASTP", emb_bn=False):
    return ECAPA_TDNN(
        channels=1024,
        feat_dim=feat_dim,
        embed_dim=embed_dim,
        pooling_func=pooling_func,
        emb_bn=emb_bn,
    )


def ECAPA_TDNN_GLOB_c1024(feat_dim, embed_dim, pooling_func="ASTP", emb_bn=False):
    return ECAPA_TDNN(
        channels=1024,
        feat_dim=feat_dim,
        embed_dim=embed_dim,
        pooling_func=pooling_func,
        global_context_att=True,
        emb_bn=emb_bn,
    )


def ECAPA_TDNN_c512(feat_dim, embed_dim, pooling_func="ASTP", emb_bn=False):
    return ECAPA_TDNN(
        channels=512,
        feat_dim=feat_dim,
        embed_dim=embed_dim,
        pooling_func=pooling_func,
        emb_bn=emb_bn,
    )


def ECAPA_TDNN_GLOB_c512(feat_dim, embed_dim, pooling_func="ASTP", emb_bn=False):
    return ECAPA_TDNN(
        channels=512,
        feat_dim=feat_dim,
        embed_dim=embed_dim,
        pooling_func=pooling_func,
        global_context_att=True,
        emb_bn=emb_bn,
    )


if __name__ == "__main__":
    from mlx.utils import tree_flatten

    x = mx.zeros(shape=(1, 200, 100))
    model = ECAPA_TDNN_GLOB_c512(feat_dim=100, embed_dim=256, pooling_func="ASTP")
    model.eval()
    out, latent = model(x, True)
    print(out.shape)
    print(latent.shape)
    # Count parameters for MLX model
    num_params = 0

    weights = dict(tree_flatten(model.parameters()))

    for k, v in weights.items():
        num_params += v.size
    print("{} M".format(num_params / 1e6))

    # from thop import profile
    # x_np = torch.randn(1, 200, 80)
    # flops, params = profile(model, inputs=(x_np, ))
    # print("FLOPs: {} G, Params: {} M".format(flops / 1e9, params / 1e6))



================================================
FILE: mlx_audio/tts/models/spark/modules/speaker/perceiver_encoder.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Adapted from https://github.com/lucidrains/naturalspeech2-pytorch/blob/659bec7f7543e7747e809e950cc2f84242fbeec7/naturalspeech2_pytorch/naturalspeech2_pytorch.py#L532

from collections import namedtuple
from functools import wraps

import mlx.core as mx
import mlx.nn as nn
from einops import rearrange, repeat


def exists(val):
    return val is not None


def once(fn):
    called = False

    @wraps(fn)
    def inner(x):
        nonlocal called
        if called:
            return
        called = True
        return fn(x)

    return inner


print_once = once(print)

# main class


class Attend(nn.Module):
    def __init__(self, dropout=0.0, causal=False):
        super().__init__()
        self.dropout = dropout
        self.attn_dropout = nn.Dropout(dropout)
        self.causal = causal
        self.mask = None

    def get_mask(self, n, device=None):
        if exists(self.mask) and self.mask.shape[-1] >= n:
            return self.mask[:n, :n]

        mask = mx.triu(mx.ones((n, n), dtype=mx.bool_), 1)
        self.mask = mask
        return mask

    def __call__(self, q, k, v, mask=None):
        """
        einstein notation
        b - batch
        h - heads
        n, i, j - sequence length (base sequence length, source, target)
        d - feature dimension
        """
        n = q.shape[-2]

        scale = q.shape[-1] ** -0.5

        # Handle different dimensions for k and v
        kv_einsum_eq = "b j d" if k.ndim == 3 else "b h j d"

        # similarity
        if k.ndim == 3:
            k = mx.expand_dims(k, axis=1)
            k = mx.broadcast_to(k, q.shape)

        if v.ndim == 3:
            v = mx.expand_dims(v, axis=1)
            v = mx.broadcast_to(v, q.shape[:-1] + (v.shape[-1],))

        # q: [b h i d], k: [b h j d]
        sim = mx.matmul(q, mx.transpose(k, (0, 1, 3, 2))) * scale

        # key padding mask
        if exists(mask):
            mask = mx.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))
            sim = mx.where(mask, sim, -1e9)

        # causal mask
        if self.causal:
            causal_mask = self.get_mask(n)
            sim = mx.where(causal_mask, -1e9, sim)

        # attention
        attn = mx.softmax(sim, axis=-1)

        if self.dropout > 0 and self.training:
            attn = self.attn_dropout(attn)

        # aggregate values
        out = mx.matmul(attn, v)

        return out


def Sequential(*mods):
    return nn.Sequential(*[mod for mod in mods if exists(mod)])


def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d


class RMSNorm(nn.Module):
    def __init__(self, dim, scale=True, dim_cond=None):
        super().__init__()
        self.cond = exists(dim_cond)
        self.to_gamma_beta = nn.Linear(dim_cond, dim * 2) if self.cond else None

        self.scale = dim**0.5
        self.gamma = mx.ones((dim,)) if scale else None

    def __call__(self, x, cond=None):
        def normalize(input, p=2.0, dim=1, eps=1e-12):
            norm = mx.power(
                mx.sum(mx.power(mx.abs(input), p), axis=dim, keepdims=True), 1 / p
            )
            return input / mx.maximum(norm, eps)

        gamma = default(self.gamma, 1)
        out = normalize(x, dim=-1) * self.scale * gamma

        if not self.cond:
            return out

        assert exists(cond)
        gamma, beta = mx.split(self.to_gamma_beta(cond), 2, axis=-1)
        gamma = mx.expand_dims(gamma, axis=1)
        beta = mx.expand_dims(beta, axis=1)
        return out * gamma + beta


class CausalConv1d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, stride=1):
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation
        )
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.stride = stride
        assert stride == 1
        self.causal_padding = dilation * (kernel_size - 1)

    def __call__(self, x):
        causal_padded_x = mx.pad(x, [(0, 0), (0, 0), (self.causal_padding, 0)])
        return self.conv(causal_padded_x)


class GEGLU(nn.Module):
    def __call__(self, x):
        x, gate = mx.split(x, 2, axis=-1)
        return nn.gelu(gate) * x


def FeedForward(dim, mult=4, causal_conv=False):
    dim_inner = int(dim * mult * 2 / 3)

    conv = None
    if causal_conv:
        conv = [
            lambda x: mx.transpose(x, (0, 2, 1)),  # b n d -> b d n
            CausalConv1d(dim_inner, dim_inner, 3),
            lambda x: mx.transpose(x, (0, 2, 1)),  # b d n -> b n d
        ]

        return [
            nn.Linear(dim, dim_inner * 2),
            GEGLU(),
            conv,
            nn.Linear(dim_inner, dim),
        ]
    else:
        return [
            nn.Linear(dim, dim_inner * 2),
            GEGLU(),
            nn.Linear(dim_inner, dim),
        ]


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        *,
        dim_context=None,
        causal=False,
        dim_head=64,
        heads=8,
        dropout=0.0,
        cross_attn_include_queries=False,
    ):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        self.cross_attn_include_queries = cross_attn_include_queries

        dim_inner = dim_head * heads
        dim_context = default(dim_context, dim)

        self.attend = Attend(causal=causal, dropout=dropout)
        self.to_q = nn.Linear(dim, dim_inner, bias=False)
        self.to_kv = nn.Linear(dim_context, dim_inner * 2, bias=False)
        self.to_out = nn.Linear(dim_inner, dim, bias=False)

    def __call__(self, x, context=None, mask=None):
        h, has_context = self.heads, exists(context)

        context = default(context, x)

        if has_context and self.cross_attn_include_queries:
            context = mx.concatenate([x, context], axis=-2)

        q = self.to_q(x)
        kv = self.to_kv(context)
        k, v = mx.split(kv, 2, axis=-1)

        # Reshape for multi-head attention
        q = mx.reshape(q, (q.shape[0], q.shape[1], h, -1))
        q = mx.transpose(q, (0, 2, 1, 3))  # b n (h d) -> b h n d

        k = mx.reshape(k, (k.shape[0], k.shape[1], h, -1))
        k = mx.transpose(k, (0, 2, 1, 3))  # b n (h d) -> b h n d

        v = mx.reshape(v, (v.shape[0], v.shape[1], h, -1))
        v = mx.transpose(v, (0, 2, 1, 3))  # b n (h d) -> b h n d

        out = self.attend(q, k, v, mask=mask)

        out = mx.transpose(out, (0, 2, 1, 3))  # b h n d -> b n h d
        out = mx.reshape(out, (out.shape[0], out.shape[1], -1))  # b n h d -> b n (h d)

        return self.to_out(out)


class PerceiverResampler(nn.Module):
    def __init__(
        self,
        *,
        dim,
        depth=2,
        dim_context=None,
        num_latents=32,
        dim_head=64,
        heads=8,
        ff_mult=4,
    ):
        super().__init__()
        dim_context = default(dim_context, dim)

        self.proj_context = (
            nn.Linear(dim_context, dim) if dim_context != dim else nn.Identity()
        )

        self.latents = mx.random.normal(shape=(num_latents, dim), scale=0.02)

        self.layers = []
        for _ in range(depth):
            self.layers.append(
                [
                    Attention(
                        dim=dim,
                        dim_head=dim_head,
                        heads=heads,
                        cross_attn_include_queries=True,
                    ),
                    FeedForward(dim=dim, mult=ff_mult),
                ]
            )

        self.norm = RMSNorm(dim)

    def __call__(self, x, mask=None):
        batch = x.shape[0]

        x = self.proj_context(x)

        latents = mx.broadcast_to(self.latents, (batch,) + self.latents.shape)

        for attn, ff in self.layers:
            latents = attn(latents, x, mask=mask) + latents
            skip_connect = latents
            for module in ff:
                latents = module(latents)

            latents = skip_connect + latents

        return self.norm(latents)


if __name__ == "__main__":
    from mlx.utils import tree_flatten

    model = PerceiverResampler(dim=256, dim_context=80)
    x = mx.random.normal(shape=(8, 200, 80))
    out = model(x)
    print("Output shape:", out.shape)  # [8, 32, 80]

    # Count parameters for MLX model
    num_params = 0

    weights = dict(tree_flatten(model.parameters()))

    for k, v in weights.items():
        num_params += v.size
    print("{} M".format(num_params / 1e6))



================================================
FILE: mlx_audio/tts/models/spark/modules/speaker/pooling_layers.py
================================================
# Copyright (c) 2021 Shuai Wang (wsstriving@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Pooling functions to aggregate frame-level deep features
into segment-level speaker embeddings

High-order statistics are surprisingly effective, TSDP acts similarly as TSTP,
even though we remove the mean statistic, on Voxceleb.
"""
import mlx.core as mx
import mlx.nn as nn


class TAP(nn.Module):
    """
    Temporal average pooling, only first-order mean is considered
    """

    def __init__(self, in_dim=0, **kwargs):
        super(TAP, self).__init__()
        self.in_dim = in_dim

    def __call__(self, x):
        pooling_mean = mx.mean(x, axis=-1)
        # To be compatable with 2D input
        pooling_mean = pooling_mean.flatten(start_axis=1)
        return pooling_mean

    def get_out_dim(self):
        self.out_dim = self.in_dim
        return self.out_dim


class TSDP(nn.Module):
    """
    Temporal standard deviation pooling, only second-order std is considered
    """

    def __init__(self, in_dim=0, **kwargs):
        super(TSDP, self).__init__()
        self.in_dim = in_dim

    def __call__(self, x):
        # The last dimension is the temporal axis
        pooling_std = mx.sqrt(mx.var(x, axis=-1) + 1e-7)
        pooling_std = pooling_std.flatten(start_axis=1)
        return pooling_std

    def get_out_dim(self):
        self.out_dim = self.in_dim
        return self.out_dim


class TSTP(nn.Module):
    """
    Temporal statistics pooling, concatenate mean and std, which is used in
    x-vector
    Comment: simple concatenation can not make full use of both statistics
    """

    def __init__(self, in_dim=0, **kwargs):
        super(TSTP, self).__init__()
        self.in_dim = in_dim

    def __call__(self, x):
        # The last dimension is the temporal axis
        pooling_mean = mx.mean(x, axis=-1)
        pooling_std = mx.sqrt(mx.var(x, axis=-1) + 1e-7)
        pooling_mean = pooling_mean.flatten(start_axis=1)
        pooling_std = pooling_std.flatten(start_axis=1)
        stats = mx.concatenate((pooling_mean, pooling_std), axis=1)
        return stats

    def get_out_dim(self):
        self.out_dim = self.in_dim * 2
        return self.out_dim


class ASTP(nn.Module):
    """Attentive statistics pooling: Channel- and context-dependent
    statistics pooling, first used in ECAPA_TDNN.
    """

    def __init__(self, in_dim, bottleneck_dim=128, global_context_att=False, **kwargs):
        super(ASTP, self).__init__()
        self.in_dim = in_dim
        self.global_context_att = global_context_att

        # Use Conv1d with stride == 1 rather than Linear, then we don't
        # need to transpose inputs.
        if global_context_att:
            self.linear1 = nn.Conv1d(
                in_dim * 3, bottleneck_dim, kernel_size=1
            )  # equals W and b in the paper
        else:
            self.linear1 = nn.Conv1d(
                in_dim, bottleneck_dim, kernel_size=1
            )  # equals W and b in the paper
        self.linear2 = nn.Conv1d(
            bottleneck_dim, in_dim, kernel_size=1
        )  # equals V and k in the paper

    def __call__(self, x):
        """
        x: a 3-dimensional tensor in tdnn-based architecture (B,F,T)
            or a 4-dimensional tensor in resnet architecture (B,C,F,T)
            0-dim: batch-dimension, last-dim: time-dimension (frame-dimension)
        """
        if len(x.shape) == 4:
            x = x.reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])
        assert len(x.shape) == 3

        if self.global_context_att:
            context_mean = mx.mean(x, axis=-1)[:, :, None]
            context_mean = mx.broadcast_to(context_mean, x.shape)
            context_std = mx.sqrt(mx.var(x, axis=-1) + 1e-7)[:, :, None]
            context_std = mx.broadcast_to(context_std, x.shape)
            x_in = mx.concatenate((x, context_mean, context_std), axis=1)
        else:
            x_in = x

        # DON'T use ReLU here! ReLU may be hard to converge.
        alpha = mx.tanh(
            self.linear1(x_in.transpose(0, 2, 1)).transpose(0, 2, 1)
        )  # alpha = F.relu(self.linear1(x_in))
        alpha = mx.softmax(
            self.linear2(alpha.transpose(0, 2, 1)).transpose(0, 2, 1), axis=2
        )
        mean = mx.sum(alpha * x, axis=2)
        var = mx.sum(alpha * (x**2), axis=2) - mean**2
        std = mx.sqrt(mx.clip(var, 1e-7, None))
        return mx.concatenate([mean, std], axis=1)

    def get_out_dim(self):
        self.out_dim = 2 * self.in_dim
        return self.out_dim


class MHASTP(nn.Module):
    """Multi head attentive statistics pooling
    Reference:
        Self Multi-Head Attention for Speaker Recognition
        https://arxiv.org/pdf/1906.09890.pdf
    """

    def __init__(
        self, in_dim, layer_num=2, head_num=2, d_s=1, bottleneck_dim=64, **kwargs
    ):
        super(MHASTP, self).__init__()
        assert (
            in_dim % head_num
        ) == 0  # make sure that head num can be divided by input_dim
        self.in_dim = in_dim
        self.head_num = head_num
        d_model = int(in_dim / head_num)
        channel_dims = [bottleneck_dim for i in range(layer_num + 1)]
        if d_s > 1:
            d_s = d_model
        else:
            d_s = 1
        self.d_s = d_s
        channel_dims[0], channel_dims[-1] = d_model, d_s
        self.heads_att_trans = []
        for i in range(self.head_num):
            layers = []
            for j in range(layer_num - 1):
                layers.extend(
                    [
                        nn.Conv1d(channel_dims[j], channel_dims[j + 1], 1, 1),
                        nn.Tanh(),
                    ]
                )
            layers.append(
                nn.Conv1d(channel_dims[layer_num - 1], channel_dims[layer_num], 1, 1)
            )
            self.heads_att_trans.append(nn.Sequential(*layers))

    def __call__(self, input):
        """
        input: a 3-dimensional tensor in xvector architecture
            or a 4-dimensional tensor in resnet architecture
            0-dim: batch-dimension, last-dim: time-dimension (frame-dimension)
        """
        if len(input.shape) == 4:  # B x F x T
            input = input.reshape(
                input.shape[0], input.shape[1] * input.shape[2], input.shape[3]
            )
        assert len(input.shape) == 3
        bs, f_dim, t_dim = input.shape
        chunks = mx.split(input, self.head_num, axis=1)
        # split
        chunks_out = []
        for i, layer in enumerate(self.heads_att_trans):
            att_score = layer(chunks[i].transpose(0, 2, 1)).transpose(0, 2, 1)
            alpha = mx.softmax(att_score, axis=-1)
            mean = mx.sum(alpha * chunks[i], axis=2)
            var = mx.sum(alpha * chunks[i] ** 2, axis=2) - mean**2
            std = mx.sqrt(mx.clip(var, 1e-7, None))
            chunks_out.append(mx.concatenate((mean, std), axis=1))
        out = mx.concatenate(chunks_out, axis=1)
        return out

    def get_out_dim(self):
        self.out_dim = 2 * self.in_dim
        return self.out_dim


class MQMHASTP(nn.Module):
    """An attentive pooling
    Reference:
        multi query multi head attentive statistics pooling
        https://arxiv.org/pdf/2110.05042.pdf
    Args:
        in_dim: the feature dimension of input
        layer_num: the number of layer in the pooling layer
        query_num: the number of querys
        head_num: the number of heads
        bottleneck_dim: the bottleneck dimension

    SA (H = 1, Q = 1, n = 2, d_s = 1) ref:
        https://www.danielpovey.com/files/2018_interspeech_xvector_attention.pdf
    MHA (H > 1, Q = 1, n = 1, d_s = 1) ref:
        https://arxiv.org/pdf/1906.09890.pdf
    AS (H = 1, Q > 1, n = 2, d_s = 1) ref:
        https://arxiv.org/pdf/1803.10963.pdf
    VSA (H = 1, Q > 1, n = 2, d_s = d_h) ref:
        http://www.interspeech2020.org/uploadfile/pdf/Mon-2-10-5.pdf
    """

    def __init__(
        self,
        in_dim,
        layer_num=2,
        query_num=2,
        head_num=8,
        d_s=2,
        bottleneck_dim=64,
        **kwargs
    ):
        super(MQMHASTP, self).__init__()
        self.n_query = [
            MHASTP(
                in_dim,
                layer_num=layer_num,
                head_num=head_num,
                d_s=d_s,
                bottleneck_dim=bottleneck_dim,
            )
            for i in range(query_num)
        ]
        self.query_num = query_num
        self.in_dim = in_dim

    def __call__(self, input):
        """
        input: a 3-dimensional tensor in xvector architecture
            or a 4-dimensional tensor in resnet architecture
            0-dim: batch-dimension, last-dim: time-dimension (frame-dimension)
        """
        if len(input.shape) == 4:  # B x F x T
            input = input.reshape(
                input.shape[0], input.shape[1] * input.shape[2], input.shape[3]
            )
        assert len(input.shape) == 3
        res = []
        for i, layer in enumerate(self.n_query):
            res.append(layer(input))
        out = mx.concatenate(res, axis=-1)
        return out

    def get_out_dim(self):
        self.out_dim = self.in_dim * 2 * self.query_num
        return self.out_dim


if __name__ == "__main__":
    data = mx.random.normal(shape=(16, 512, 10, 35))
    # model = StatisticsPooling()
    model = MQMHASTP(512 * 10)
    model = MHASTP(512 * 10)
    model = MQMHASTP(512 * 10, context=False)
    print(model)

    out = model(data)
    print(out.shape)
    print(model.get_out_dim())



================================================
FILE: mlx_audio/tts/models/spark/modules/speaker/speaker_encoder.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import List, Tuple

import mlx.core as mx
import mlx.nn as nn

from mlx_audio.tts.models.spark.modules.residual_fsq import ResidualFSQ
from mlx_audio.tts.models.spark.modules.speaker.ecapa_tdnn import ECAPA_TDNN_GLOB_c512
from mlx_audio.tts.models.spark.modules.speaker.perceiver_encoder import (
    PerceiverResampler,
)

# from mlx_audio.codec.models.descript.nn.quantize import ResidualVectorQuantize


"""
x-vector + d-vector
"""


class SpeakerEncoder(nn.Module):

    def __init__(
        self,
        input_dim: int = 100,
        out_dim: int = 512,
        latent_dim: int = 128,
        token_num: int = 32,
        fsq_levels: List[int] = [4, 4, 4, 4, 4, 4],
        fsq_num_quantizers: int = 1,
    ):
        super(SpeakerEncoder, self).__init__()

        self.speaker_encoder = ECAPA_TDNN_GLOB_c512(
            feat_dim=input_dim, embed_dim=out_dim
        )
        self.perceiver_sampler = PerceiverResampler(
            dim=latent_dim, dim_context=512 * 3, num_latents=token_num
        )
        self.quantizer = ResidualFSQ(
            dim=latent_dim,
            num_quantizers=fsq_num_quantizers,
            levels=fsq_levels,
            is_channel_first=True,
            quantize_dropout=False,
        )

        self.project = nn.Linear(latent_dim * token_num, out_dim)

    def get_codes_from_indices(self, indices: mx.array) -> mx.array:
        zq = self.quantizer.get_codes_from_indices(indices.transpose(1, 2))
        return zq.transpose(0, 2, 1)

    def get_indices(self, mels: mx.array) -> mx.array:
        mels = mels.transpose(0, 2, 1)
        x = self.perceiver_sampler(mels).transpose(0, 2, 1)
        zq, indices = self.quantizer(x)
        return indices

    def __call__(self, mels: mx.array) -> Tuple[mx.array, mx.array]:
        """
        Args:
            mels: (B, D_mel, T1)

        Return:
            x_vector: (B, out_dim)
            d_vector: (B, out_dim)
        """
        # mels = mels.transpose(1,2)

        x_vector, features = self.speaker_encoder(mels, True)
        x = self.perceiver_sampler(features.transpose(0, 2, 1)).transpose(0, 2, 1)
        z_q, indices = self.quantizer(x)  # zq: (B, latent_dim, T2, latent_dim)
        x = z_q.reshape(z_q.shape[0], -1)
        d_vector = self.project(x)

        return x_vector, d_vector

    def tokenize(self, mels: mx.array) -> mx.array:
        """tokenize the input mel spectrogram"""
        _, features = self.speaker_encoder(mels, True)
        x = self.perceiver_sampler(features.transpose(0, 2, 1)).transpose(0, 2, 1)
        z_q, indices = self.quantizer(x)
        return indices

    def detokenize(self, indices: mx.array) -> mx.array:
        zq = self.quantizer.get_output_from_indices(indices.swapaxes(-1, -2)).swapaxes(
            -1, -2
        )
        x = zq.reshape(zq.shape[0], -1)
        d_vector = self.project(x)
        return d_vector

    def sanitize(self, weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if (
                ".conv.weight" in k
                or ("convs." in k and "weight" in k)
                or ("speaker_encoder.pool.linear" in k and "weight" in k)
            ):
                if v.shape[1] > v.shape[-1]:
                    sanitized_weights[k] = v.transpose(0, 2, 1)
                else:
                    sanitized_weights[k] = v
            else:
                sanitized_weights[k] = v
        return sanitized_weights


if __name__ == "__main__":
    from mlx.utils import tree_flatten

    model = SpeakerEncoder(
        input_dim=100,
        latent_dim=128,
        token_num=32,
        fsq_levels=[4, 4, 4, 4, 4, 4],
        fsq_num_quantizers=1,
    )
    mel = mx.random.normal(shape=(8, 200, 100), scale=1.0)
    x_vector, d_vector = model(mel)
    print("x-vector shape", x_vector.shape)
    print("d-vector shape", d_vector.shape)

    indices = model.tokenize(mel)
    print("indices shape", indices.shape)
    d_vector_post = model.detokenize(indices)
    print("d-vector shape", d_vector_post.shape)
    if d_vector_post.all() == d_vector.all():
        print("d-vector post and d-vector are the same")
    else:
        print("d-vector post and d-vector are different")

    num_params = 0

    weights = dict(tree_flatten(model.parameters()))

    for k, v in weights.items():
        num_params += v.size
    print("{} M".format(num_params / 1e6))



================================================
FILE: mlx_audio/tts/models/spark/utils/audio.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Description:
    This script contains a collection of functions designed to handle various
    audio processing.
"""

import random
from pathlib import Path
from typing import Tuple

import mlx.core as mx
import numpy as np
import soundfile
import soxr
from numpy.lib.stride_tricks import sliding_window_view


def audio_volume_normalize(audio: np.ndarray, coeff: float = 0.2) -> np.ndarray:
    """
    Normalize the volume of an audio signal.

    Parameters:
        audio (numpy array): Input audio signal array.
        coeff (float): Target coefficient for normalization, default is 0.2.

    Returns:
        numpy array: The volume-normalized audio signal.
    """
    # Sort the absolute values of the audio signal
    temp = np.sort(np.abs(audio))

    # If the maximum value is less than 0.1, scale the array to have a maximum of 0.1
    if temp[-1] < 0.1:
        scaling_factor = max(
            temp[-1], 1e-3
        )  # Prevent division by zero with a small constant
        audio = audio / scaling_factor * 0.1

    # Filter out values less than 0.01 from temp
    temp = temp[temp > 0.01]
    L = temp.shape[0]  # Length of the filtered array

    # If there are fewer than or equal to 10 significant values, return the audio without further processing
    if L <= 10:
        return audio

    # Compute the average of the top 10% to 1% of values in temp
    volume = np.mean(temp[int(0.9 * L) : int(0.99 * L)])

    # Normalize the audio to the target coefficient level, clamping the scale factor between 0.1 and 10
    audio = audio * np.clip(coeff / volume, a_min=0.1, a_max=10)

    # Ensure the maximum absolute value in the audio does not exceed 1
    max_value = np.max(np.abs(audio))
    if max_value > 1:
        audio = audio / max_value

    return audio


def load_audio(
    adfile: Path,
    sampling_rate: int = None,
    length: int = None,
    volume_normalize: bool = False,
    segment_duration: int = None,
) -> np.ndarray:
    r"""Load audio file with target sampling rate and lsength

    Args:
        adfile (Path): path to audio file.
        sampling_rate (int, optional): target sampling rate. Defaults to None.
        length (int, optional): target audio length. Defaults to None.
        volume_normalize (bool, optional): whether perform volume normalization. Defaults to False.
        segment_duration (int): random select a segment with duration of {segment_duration}s.
                                Defualt to None which means the whole audio will be used.

    Returns:
        audio (np.ndarray): audio
    """

    audio, sr = soundfile.read(adfile)
    if len(audio.shape) > 1:
        audio = audio[:, 0]

    if sampling_rate is not None and sr != sampling_rate:
        audio = soxr.resample(audio, sr, sampling_rate, quality="VHQ")
        sr = sampling_rate

    if segment_duration is not None:
        seg_length = int(sr * segment_duration)
        audio = random_select_audio_segment(audio, seg_length)

    # Audio volume normalize
    if volume_normalize:
        audio = audio_volume_normalize(audio)
    # check the audio length
    if length is not None:
        assert abs(audio.shape[0] - length) < 1000
        if audio.shape[0] > length:
            audio = audio[:length]
        else:
            audio = np.pad(audio, (0, int(length - audio.shape[0])))
    return audio


def random_select_audio_segment(audio: np.ndarray, length: int) -> np.ndarray:
    """get an audio segment given the length

    Args:
        audio (np.ndarray):
        length (int): audio length = sampling_rate * duration
    """
    if audio.shape[0] < length:
        audio = np.pad(audio, (0, int(length - audio.shape[0])))
    start_index = random.randint(0, audio.shape[0] - length)
    end_index = int(start_index + length)

    return audio[start_index:end_index]


def detect_speech_boundaries(
    wav: np.ndarray,
    sample_rate: int,
    window_duration: float = 0.1,
    energy_threshold: float = 0.01,
    margin_factor: int = 2,
) -> Tuple[int, int]:
    """Detect the start and end points of speech in an audio signal using RMS energy.

    Args:
        wav: Input audio signal array with values in [-1, 1]
        sample_rate: Audio sample rate in Hz
        window_duration: Duration of detection window in seconds
        energy_threshold: RMS energy threshold for speech detection
        margin_factor: Factor to determine extra margin around detected boundaries

    Returns:
        tuple: (start_index, end_index) of speech segment

    Raises:
        ValueError: If the audio contains only silence
    """
    window_size = int(window_duration * sample_rate)
    margin = margin_factor * window_size
    step_size = window_size // 10

    # Create sliding windows using stride tricks to avoid loops
    windows = sliding_window_view(wav, window_size)[::step_size]

    # Calculate RMS energy for each window
    energy = np.sqrt(np.mean(windows**2, axis=1))
    speech_mask = energy >= energy_threshold

    if not np.any(speech_mask):
        raise ValueError("No speech detected in audio (only silence)")

    start = max(0, np.argmax(speech_mask) * step_size - margin)
    end = min(
        len(wav),
        (len(speech_mask) - 1 - np.argmax(speech_mask[::-1])) * step_size + margin,
    )

    return start, end


def remove_silence_on_both_ends(
    wav: np.ndarray,
    sample_rate: int,
    window_duration: float = 0.1,
    volume_threshold: float = 0.01,
) -> np.ndarray:
    """Remove silence from both ends of an audio signal.

    Args:
        wav: Input audio signal array
        sample_rate: Audio sample rate in Hz
        window_duration: Duration of detection window in seconds
        volume_threshold: Amplitude threshold for silence detection

    Returns:
        np.ndarray: Audio signal with silence removed from both ends

    Raises:
        ValueError: If the audio contains only silence
    """
    start, end = detect_speech_boundaries(
        wav, sample_rate, window_duration, volume_threshold
    )
    return wav[start:end]


def hertz_to_mel(pitch: float) -> float:
    """
    Converts a frequency from the Hertz scale to the Mel scale.

    Parameters:
    - pitch: float or ndarray
        Frequency in Hertz.

    Returns:
    - mel: float or ndarray
        Frequency in Mel scale.
    """
    mel = 2595 * np.log10(1 + pitch / 700)
    return mel



================================================
FILE: mlx_audio/tts/models/spark/utils/file.py
================================================
# Copyright (c) 2025 SparkAudio
#               2025 Xinsheng Wang (w.xinshawn@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Description:
    This script contains a collection of functions designed to handle various
    file reading and writing operations. It provides utilities to read from files,
    write data to files, and perform file manipulation tasks.
"""


import csv
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Set, Union

from omegaconf import DictConfig, OmegaConf
from tqdm import tqdm


def resolve_symbolic_link(symbolic_link_path: Path) -> Path:
    """
    Resolves the absolute path of a symbolic link.

    Args:
        symbolic_link_path (Path): The path to the symbolic link.

    Returns:
        Path: The absolute path that the symbolic link points to.
    """

    link_directory = os.path.dirname(symbolic_link_path)
    target_path_relative = os.readlink(symbolic_link_path)
    return os.path.join(link_directory, target_path_relative)


def write_jsonl(metadata: List[dict], file_path: Path) -> None:
    """Writes a list of dictionaries to a JSONL file.

    Args:
    metadata : List[dict]
        A list of dictionaries, each representing a piece of meta.
    file_path : Path
        The file path to save the JSONL file

    This function writes each dictionary in the list to a new line in the specified file.
    """
    with open(file_path, "w", encoding="utf-8") as f:
        for meta in tqdm(metadata, desc="writing jsonl"):
            # Convert dictionary to JSON string and write it to the file with a newline
            json_str = json.dumps(meta, ensure_ascii=False) + "\n"
            f.write(json_str)
    print(f"jsonl saved to {file_path}")


def read_jsonl(file_path: Path) -> List[dict]:
    """
    Reads a JSONL file and returns a list of dictionaries.

    Args:
    file_path : Path
        The path to the JSONL file to be read.

    Returns:
    List[dict]
        A list of dictionaries parsed from each line of the JSONL file.
    """
    metadata = []
    # Open the file for reading
    with open(file_path, "r", encoding="utf-8") as f:
        # Split the file into lines
        lines = f.read().splitlines()
    # Process each line
    for line in lines:
        # Convert JSON string back to dictionary and append to list
        meta = json.loads(line)
        metadata.append(meta)
    # Return the list of metadata
    return metadata


def read_json_as_jsonl(file_path: Path) -> List[dict]:
    metadata = []
    with open(file_path, "r", encoding="utf-8") as infile:
        data = json.load(infile)
    for k in sorted(data.keys()):
        meta = {"index": k}
        meta.update(data[k])
        metadata.append(meta)
    return metadata


def decode_unicode_strings(meta: Dict[str, Any]) -> Dict[str, Any]:
    processed_meta = {}
    for k, v in meta.items():
        if isinstance(v, str):
            processed_meta[k] = v.encode("utf-8").decode("unicode_escape")
        else:
            processed_meta[k] = v
    return processed_meta


def load_config(config_path: Path) -> DictConfig:
    """Loads a configuration file and optionally merges it with a base configuration.

    Args:
    config_path (Path): Path to the configuration file.
    """
    # Load the initial configuration from the given path
    config = OmegaConf.load(config_path)

    # Check if there is a base configuration specified and merge if necessary
    if config.get("base_config", None) is not None:
        base_config = OmegaConf.load(config["base_config"])
        config = OmegaConf.merge(base_config, config)

    return config


def jsonl_to_csv(jsonl_file_path: str, csv_file_path: str) -> None:
    """
    Converts a JSONL file to a CSV file.

    This function reads a JSONL file, determines all unique keys present in the file,
    and writes the data to a CSV file with columns for all these keys.
    """

    all_keys = set()
    data_rows = []

    # Read the JSONL file once to extract keys and collect data
    with open(jsonl_file_path, "r") as file:
        for line in file:
            data = json.loads(line.strip())
            data_rows.append(data)
            all_keys.update(data.keys())

    # Convert the set of keys to a sorted list for consistent column order
    sorted_keys = sorted(all_keys)

    # Write the data to a CSV file
    with open(csv_file_path, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=sorted_keys)

        # Write the header row
        writer.writeheader()

        # Write each row of data
        for data in data_rows:
            writer.writerow(data)

    print(f"CSV file has been created at {csv_file_path}")


def save_metadata(data, filename, headers=None):
    """
    Save metadata to a file.

    Args:
        data (list of dict): Metadata to be saved.
        filename (str): Name of the file to save the metadata.
        headers (list of str): The order of column names to be saved; defaults to the keys from the first dictionary in data if not provided.
    """
    # Set headers to keys from the first dictionary in data if not explicitly provided
    if headers is None:
        headers = list(data[0].keys())

    with open(filename, "w", encoding="utf-8") as file:
        # Write the headers to the file
        file.write("|".join(headers) + "\n")
        for entry in data:
            # Retrieve values in the order of headers, replacing any '|' characters with a space to prevent formatting errors
            formatted_values = [
                str(entry.get(key, "")).replace("|", " ") for key in headers
            ]
            # Write the formatted values to the file
            file.write("|".join(formatted_values) + "\n")


def read_metadata(filename, headers=None):
    """
    Read metadata from a file.

    Args:
        filename (str): The file from which to read the metadata.

    Returns:
        list of dict: The metadata read from the file.
        list of str: The headers used in the file.
    """
    with open(filename, "r", encoding="utf-8") as file:
        lines = file.readlines()

    data = []
    # Set headers from the first line of the file if not provided
    if headers is None:
        headers = lines[0].strip().split("|")
        lines = lines[1:]

    for line in lines:
        line = line.strip()
        # Skip empty lines
        if not line:
            continue
        # Split the line by '|' and pair with headers to form a dictionary
        entry_data = dict(zip(headers, line.split("|")))
        data.append(entry_data)

    return data, headers



================================================
FILE: mlx_audio/tts/models/spark/utils/token_parser.py
================================================
TASK_TOKEN_MAP = {
    "vc": "<|task_vc|>",
    "tts": "<|task_tts|>",
    "asr": "<|task_asr|>",
    "s2s": "<|task_s2s|>",
    "t2s": "<|task_t2s|>",
    "understand": "<|task_understand|>",
    "caption": "<|task_cap|>",
    "controllable_tts": "<|task_controllable_tts|>",
    "prompt_tts": "<|task_prompt_tts|>",
    "speech_edit": "<|task_edit|>",
}

LEVELS_MAP = {
    "very_low": 0,
    "low": 1,
    "moderate": 2,
    "high": 3,
    "very_high": 4,
}

LEVELS_MAP_UI = {1: "very_low", 2: "low", 3: "moderate", 4: "high", 5: "very_high"}

GENDER_MAP = {
    "female": 0,
    "male": 1,
}

AGE_MAP = {"Child": 0, "Teenager": 1, "Youth-Adult": 2, "Middle-aged": 3, "Elderly": 4}

EMO_MAP = {
    "UNKNOWN": 0,
    "NEUTRAL": 1,
    "ANGRY": 2,
    "HAPPY": 3,
    "SAD": 4,
    "FEARFUL": 5,
    "DISGUSTED": 6,
    "SURPRISED": 7,
    "SARCASTIC": 8,
    "EXCITED": 9,
    "SLEEPY": 10,
    "CONFUSED": 11,
    "EMPHASIS": 12,
    "LAUGHING": 13,
    "SINGING": 14,
    "WORRIED": 15,
    "WHISPER": 16,
    "ANXIOUS": 17,
    "NO-AGREEMENT": 18,
    "APOLOGETIC": 19,
    "CONCERNED": 20,
    "ENUNCIATED": 21,
    "ASSERTIVE": 22,
    "ENCOURAGING": 23,
    "CONTEMPT": 24,
}


class TokenParser:
    """Turn label to special token"""

    def __init__(self):
        pass

    """Parse the attributes of a person."""

    def __init__(self):
        pass

    @staticmethod
    def age(age: str) -> str:
        """Turn age token."""
        age_id = AGE_MAP[age]
        return f"<|age_{age_id}|>"

    @staticmethod
    def gender(gender: str) -> str:
        """Turn gender token."""
        gender_id = GENDER_MAP[gender]
        return f"<|gender_{gender_id}|>"

    @staticmethod
    def mel_value(mel: int):
        """Turn special token of mel scale pitch."""
        mel = max(0, int(mel))
        mel = min(1000, int(mel))
        return f"<|pitch_value_{mel}|>"

    @staticmethod
    def mel_level(level: str):
        """Turn special token of mel level."""
        level_tag = LEVELS_MAP[level]
        return f"<|pitch_label_{level_tag}|>"

    @staticmethod
    def pitch_var_value(pitch_std: int):
        """Turn special token of pitch_std value."""
        assert isinstance(pitch_std, int)
        pitch_std = max(0, int(pitch_std))
        pitch_std = min(10, int(pitch_std))
        return f"<|pitch_var_value_{pitch_std}|>"

    @staticmethod
    def pitch_var_level(level: str):
        """Turn special token of pitch std level."""
        level_tag = LEVELS_MAP[level]
        return f"<|pitch_var_label_{level_tag}|>"

    @staticmethod
    def loudness_value(loudness: int):
        """Turn special toak of loudness value [0, 30]"""
        assert loudness >= 0
        loudness = max(0, int(loudness))
        loudness = min(30, int(loudness))
        return f"<|loudness_value_{loudness}|>"

    @staticmethod
    def loudness_level(level: str):
        """Turn special token of loudness level."""
        level_tag = LEVELS_MAP[level]
        return f"<|loudness_label_{level_tag}|>"

    @staticmethod
    def speed_value(speed: int):
        """Turn special token of speed value."""
        speed = max(0, int(speed))
        speed = min(10, int(speed))
        return f"<|speed_value_{speed}|>"

    @staticmethod
    def speed_level(level: str):
        """Turn special token of speed level."""
        level_tag = LEVELS_MAP[level]
        return f"<|speed_label_{level_tag}|>"

    @staticmethod
    def task(task: str) -> str:
        """Turn special token of task."""
        assert task in TASK_TOKEN_MAP.keys()

        return TASK_TOKEN_MAP[task]

    @staticmethod
    def emotion(emotion: str):
        emo_id = EMO_MAP[emotion]

        return f"<|emotion_{emo_id}|>"


# test
if __name__ == "__main__":
    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(
        "/aifs4su/xinshengwang/code/StyleCraft/tokenizer/stylecraft-bicodec-pitch-loudness-speed-emotion-tokenizer"
    )

    tasks = ["tts", "tts", "understand", "controllable_tts", "prompt_tts"]
    ages = ["Child", "Teenager", "Youth-Adult", "Middle-aged", "Elderly"]
    genders = ["female", "female", "female", "male", "male"]
    mels = [100, 200, 300, 400, 500]
    mel_levels = ["very_low", "low", "moderate", "high", "very_high"]
    loudnesses = [1, 10, 23, 19, 30]
    loudness_levels = ["very_low", "low", "moderate", "high", "very_high"]
    emotions = ["UNKNOWN", "NEUTRAL", "ANGRY", "HAPPY", "SAD"]

    for i in range(5):
        task = TokenParser.task(tasks[i])
        age = TokenParser.age(ages[i])
        gender = TokenParser.gender(genders[i])
        mel = TokenParser.mel_value(mels[i])
        mel_level = TokenParser.mel_level(mel_levels[i])
        loudness = TokenParser.loudness_value(loudnesses[i])
        loudness_level = TokenParser.loudness_level(loudness_levels[i])
        emotion = TokenParser.emotion(emotions[i])
        inputs = [task, age, gender, mel, mel_level, loudness, loudness_level, emotion]
        inputs = "".join(inputs)
        ids = tokenizer.encode(inputs, add_special_tokens=False)
        print(ids)
        print("decode", tokenizer.decode(ids))



================================================
FILE: mlx_audio/tts/tests/__init__.py
================================================
[Empty file]


================================================
FILE: mlx_audio/tts/tests/test_base.py
================================================
import unittest

import mlx.core as mx
import numpy as np

from mlx_audio.tts.models.base import BaseModelArgs, check_array_shape


class TestBaseModel(unittest.TestCase):
    def test_base_model_args_from_dict(self):
        """Test BaseModelArgs.from_dict method."""

        # Define a test subclass
        class TestArgs(BaseModelArgs):
            def __init__(self, param1, param2, param3=None):
                self.param1 = param1
                self.param2 = param2
                self.param3 = param3

        # Test with all parameters
        params = {"param1": 1, "param2": "test", "param3": True}
        args = TestArgs.from_dict(params)
        self.assertEqual(args.param1, 1)
        self.assertEqual(args.param2, "test")
        self.assertEqual(args.param3, True)

        # Test with extra parameters (should be ignored)
        params = {"param1": 1, "param2": "test", "param3": True, "extra": "ignored"}
        args = TestArgs.from_dict(params)
        self.assertEqual(args.param1, 1)
        self.assertEqual(args.param2, "test")
        self.assertEqual(args.param3, True)
        self.assertFalse(hasattr(args, "extra"))

        # Test with missing optional parameter
        params = {"param1": 1, "param2": "test"}
        args = TestArgs.from_dict(params)
        self.assertEqual(args.param1, 1)
        self.assertEqual(args.param2, "test")
        self.assertIsNone(args.param3)

    def test_check_array_shape(self):
        """Test check_array_shape function."""
        # Valid shape: out_channels >= kH == kW
        valid_array = mx.array(np.zeros((64, 3, 3)))
        self.assertTrue(check_array_shape(valid_array))

        # Invalid shape: kH != kW
        invalid_array1 = mx.array(np.zeros((64, 3, 4)))
        self.assertFalse(check_array_shape(invalid_array1))

        # Invalid shape: out_channels < kH
        invalid_array2 = mx.array(np.zeros((2, 3, 3)))
        self.assertFalse(check_array_shape(invalid_array2))

        # Invalid shape: wrong number of dimensions
        invalid_array3 = mx.array(np.zeros((64, 3)))
        self.assertFalse(check_array_shape(invalid_array3))

        # Invalid shape: wrong number of dimensions
        invalid_array4 = mx.array(np.zeros((64, 3, 3, 3)))
        self.assertFalse(check_array_shape(invalid_array4))


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/tts/tests/test_convert.py
================================================
import sys  # Import sys to patch argv
import unittest
from unittest.mock import MagicMock, patch

from mlx_audio.tts.convert import configure_parser, main


class TestConvert(unittest.TestCase):
    def setUp(self):
        self.parser = configure_parser()

        # Mock the actual convert function
        self.convert_mock = MagicMock()
        self.patcher = patch("mlx_audio.tts.convert.convert", new=self.convert_mock)
        self.patcher.start()

    def tearDown(self):
        self.patcher.stop()

    def test_basic_conversion(self):
        test_args = [
            "--hf-path",
            "dummy_hf",
            "--mlx-path",
            "dummy_mlx",
            "--dtype",
            "float16",
        ]
        # Patch sys.argv for this test run
        with patch.object(sys, "argv", ["convert.py"] + test_args):
            main()

        self.convert_mock.assert_called_once_with(
            hf_path="dummy_hf",
            mlx_path="dummy_mlx",
            quantize=False,
            q_group_size=64,
            q_bits=4,
            quant_predicate=None,
            dtype="float16",
            upload_repo=None,
            dequantize=False,
        )

    def test_quantized_conversion(self):
        test_args = [
            "--hf-path",
            "dummy_hf",
            "--quantize",
            "--q-group-size",
            "128",
            "--q-bits",
            "8",
        ]
        # Patch sys.argv for this test run
        with patch.object(sys, "argv", ["convert.py"] + test_args):
            main()

        self.convert_mock.assert_called_once_with(
            hf_path="dummy_hf",
            mlx_path="mlx_model",  # Default mlx_path
            quantize=True,
            q_group_size=128,
            q_bits=8,
            quant_predicate=None,
            dtype="float16",  # Should be ignored when quantize=True
            upload_repo=None,
            dequantize=False,
        )

    def test_quantized_conversion_invalid_group_size_raises_error(self):
        """Tests if main raises ValueError for invalid group size."""
        test_args = [
            "--hf-path",
            "dummy_hf",
            "--quantize",
            "--q-group-size",
            "100",  # Invalid: not 64 or 128
            "--q-bits",
            "4",
        ]

        # Configure the mock to raise ValueError when called with q_group_size=100
        def side_effect(*args, **kwargs):
            if kwargs.get("q_group_size") == 100:
                raise ValueError(
                    "[quantize] The requested group size 100 is not supported."
                )
            return MagicMock()  # Default return for other calls if needed

        self.convert_mock.side_effect = side_effect

        # Patch sys.argv and assert ValueError is raised
        with patch.object(sys, "argv", ["convert.py"] + test_args):
            with self.assertRaisesRegex(
                ValueError, "requested group size 100 is not supported"
            ):
                main()

        # Verify the mock was called (even though it raised an error)
        self.convert_mock.assert_called_once_with(
            hf_path="dummy_hf",
            mlx_path="mlx_model",
            quantize=True,
            q_group_size=100,
            q_bits=4,
            quant_predicate=None,
            dtype="float16",
            upload_repo=None,
            dequantize=False,
        )

    def test_quantization_recipes(self):
        for recipe in ["mixed_2_6", "mixed_3_6", "mixed_4_6"]:
            with self.subTest(recipe=recipe):
                self.convert_mock.reset_mock()  # Reset mock for each subtest
                test_args = ["--hf-path", "dummy_hf", "--quant-predicate", recipe]
                # Patch sys.argv for this test run
                with patch.object(sys, "argv", ["convert.py"] + test_args):
                    main()

                self.convert_mock.assert_called_once_with(  # Changed to assert_called_once_with
                    hf_path="dummy_hf",
                    mlx_path="mlx_model",  # Default mlx_path
                    quantize=False,  # Default quantize
                    q_group_size=64,  # Default q_group_size
                    q_bits=4,  # Default q_bits
                    quant_predicate=recipe,
                    dtype="float16",  # Default dtype
                    upload_repo=None,  # Default upload_repo
                    dequantize=False,  # Default dequantize
                )
                # No need to reset mock here, it's handled at the start of the loop

    def test_dequantize_flag(self):
        test_args = ["--hf-path", "dummy_hf", "--dequantize"]
        # Patch sys.argv for this test run
        with patch.object(sys, "argv", ["convert.py"] + test_args):
            main()

        self.convert_mock.assert_called_once_with(
            hf_path="dummy_hf",
            mlx_path="mlx_model",  # Default mlx_path
            quantize=False,
            q_group_size=64,
            q_bits=4,
            quant_predicate=None,
            dtype="float16",
            upload_repo=None,
            dequantize=True,
        )

    def test_upload_repo_argument(self):
        test_args = ["--hf-path", "dummy_hf", "--upload-repo", "my/repo"]
        # Patch sys.argv for this test run
        with patch.object(sys, "argv", ["convert.py"] + test_args):
            main()

        self.convert_mock.assert_called_once_with(
            hf_path="dummy_hf",
            mlx_path="mlx_model",  # Default mlx_path
            quantize=False,
            q_group_size=64,
            q_bits=4,
            quant_predicate=None,
            dtype="float16",
            upload_repo="my/repo",
            dequantize=False,
        )


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/tts/tests/test_interpolate.py
================================================
import unittest

import mlx.core as mx
import numpy as np

from mlx_audio.tts.models.interpolate import interpolate, interpolate1d


class TestInterpolate(unittest.TestCase):
    def test_interpolate_input_validation(self):
        """Test input validation in interpolate function."""
        # Test input with less than 3 dimensions
        with self.assertRaises(ValueError):
            interpolate(mx.array(np.zeros((2, 3))), size=4)

        # Test with both size and scale_factor defined
        with self.assertRaises(ValueError):
            interpolate(mx.array(np.zeros((2, 3, 4))), size=8, scale_factor=2)

        # Test with neither size nor scale_factor defined
        with self.assertRaises(ValueError):
            interpolate(mx.array(np.zeros((2, 3, 4))))

        # Test with unsupported dimensions
        with self.assertRaises(ValueError):
            interpolate(mx.array(np.zeros((2, 3, 4, 5))), size=8)

    def test_interpolate_size_handling(self):
        """Test size handling in interpolate function."""
        # Test with single size value
        x = mx.array(np.zeros((2, 3, 4)))
        result = interpolate(x, size=8)
        self.assertEqual(result.shape, (2, 3, 8))

        # Test with scale_factor
        x = mx.array(np.zeros((2, 3, 4)))
        result = interpolate(x, scale_factor=2)
        self.assertEqual(result.shape, (2, 3, 8))

    def test_interpolate1d_nearest(self):
        """Test 1D nearest neighbor interpolation."""
        # Create a simple test array
        x = mx.array(np.array([[[1.0, 2.0, 3.0, 4.0]]]))  # Shape: (1, 1, 4)

        # Test upsampling
        result = interpolate1d(x, size=8, mode="nearest")
        self.assertEqual(result.shape, (1, 1, 8))

        # Expected values for nearest neighbor interpolation
        expected = mx.array(np.array([[[1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]]]))
        np.testing.assert_allclose(result.tolist(), expected.tolist(), rtol=1e-5)

        # Test downsampling
        result = interpolate1d(x, size=2, mode="nearest")
        self.assertEqual(result.shape, (1, 1, 2))

        # Expected values for nearest neighbor downsampling
        expected = mx.array(np.array([[[1.0, 3.0]]]))
        np.testing.assert_allclose(result.tolist(), expected.tolist(), rtol=1e-5)

    def test_interpolate1d_linear(self):
        """Test 1D linear interpolation."""
        # Create a simple test array
        x = mx.array(np.array([[[1.0, 3.0, 5.0, 7.0]]]))  # Shape: (1, 1, 4)

        # Test upsampling with align_corners=True
        result = interpolate1d(x, size=7, mode="linear", align_corners=True)
        self.assertEqual(result.shape, (1, 1, 7))

        # Expected values for linear interpolation with align_corners=True
        expected = mx.array(np.array([[[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]]]))
        np.testing.assert_allclose(result.tolist(), expected.tolist(), rtol=1e-5)

        # Test upsampling with align_corners=False
        result = interpolate1d(x, size=7, mode="linear", align_corners=False)
        # Shape should be correct
        self.assertEqual(result.shape, (1, 1, 7))

        # Test edge case: input width = 1
        x_single = mx.array(np.array([[[5.0]]]))  # Shape: (1, 1, 1)
        result = interpolate1d(x_single, size=4, mode="linear")
        self.assertEqual(result.shape, (1, 1, 4))
        expected = mx.array(np.array([[[5.0, 5.0, 5.0, 5.0]]]))
        np.testing.assert_allclose(result.tolist(), expected.tolist(), rtol=1e-5)


if __name__ == "__main__":
    unittest.main()



================================================
FILE: mlx_audio/tts/tests/test_models.py
================================================
import importlib.resources
import unittest
from unittest.mock import MagicMock, patch

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from misaki import en


# Create a patch for the deprecated open_text function
def patched_open_text(package, resource):
    """Replacement for deprecated open_text using files() API"""
    return importlib.resources.files(package).joinpath(resource).open("r")


# Apply the patch at the module level
@patch("importlib.resources.open_text", patched_open_text)
class TestSanitizeLSTMWeights(unittest.TestCase):
    def test_sanitize_lstm_weights(self):
        """Test sanitize_lstm_weights function."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.kokoro import sanitize_lstm_weights

        # Test weight_ih_l0_reverse
        key = "lstm.weight_ih_l0_reverse"
        weights = mx.array(np.zeros((10, 10)))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.Wx_backward")

        # Test weight_hh_l0_reverse
        key = "lstm.weight_hh_l0_reverse"
        weights = mx.array(np.zeros((10, 10)))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.Wh_backward")

        # Test bias_ih_l0_reverse
        key = "lstm.bias_ih_l0_reverse"
        weights = mx.array(np.zeros(10))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.bias_ih_backward")

        # Test bias_hh_l0_reverse
        key = "lstm.bias_hh_l0_reverse"
        weights = mx.array(np.zeros(10))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.bias_hh_backward")

        # Test weight_ih_l0
        key = "lstm.weight_ih_l0"
        weights = mx.array(np.zeros((10, 10)))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.Wx_forward")

        # Test weight_hh_l0
        key = "lstm.weight_hh_l0"
        weights = mx.array(np.zeros((10, 10)))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.Wh_forward")

        # Test bias_ih_l0
        key = "lstm.bias_ih_l0"
        weights = mx.array(np.zeros(10))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.bias_ih_forward")

        # Test bias_hh_l0
        key = "lstm.bias_hh_l0"
        weights = mx.array(np.zeros(10))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "lstm.bias_hh_forward")

        # Test unknown key
        key = "unknown.key"
        weights = mx.array(np.zeros(10))
        result = sanitize_lstm_weights(key, weights)
        self.assertEqual(list(result.keys())[0], "unknown.key")


@patch("importlib.resources.open_text", patched_open_text)
class TestKokoroModel(unittest.TestCase):
    @patch("mlx_audio.tts.models.kokoro.kokoro.json.load")
    @patch("builtins.open", new_callable=MagicMock)
    @patch("mlx_audio.tts.models.kokoro.kokoro.mx.load")
    @patch.object(nn.Module, "load_weights")
    def test_init(self, mock_load_weights, mock_mx_load, mock_open, mock_json_load):
        """Test KokoroModel initialization."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.kokoro import Model, ModelConfig

        # Mock the config loading
        config = {
            "istftnet": {
                "upsample_kernel_sizes": [20, 12],
                "upsample_rates": [10, 6],
                "gen_istft_hop_size": 5,
                "gen_istft_n_fft": 20,
                "resblock_dilation_sizes": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
                "resblock_kernel_sizes": [3, 7, 11],
                "upsample_initial_channel": 512,
            },
            "dim_in": 64,
            "dropout": 0.2,
            "hidden_dim": 512,
            "max_conv_dim": 512,
            "max_dur": 50,
            "multispeaker": True,
            "n_layer": 3,
            "n_mels": 80,
            "n_token": 178,
            "style_dim": 128,
            "text_encoder_kernel_size": 5,
            "plbert": {
                "hidden_size": 768,
                "num_attention_heads": 12,
                "intermediate_size": 2048,
                "max_position_embeddings": 512,
                "num_hidden_layers": 12,
                "dropout": 0.1,
            },
            "vocab": {"a": 1, "b": 2},
        }
        mock_json_load.return_value = config

        # Mock the weights loading
        mock_mx_load.return_value = {"key": mx.array(np.zeros(10))}

        # Make load_weights return the module
        mock_load_weights.return_value = None

        # Initialize the model with the config parameter
        model = Model(ModelConfig.from_dict(config))

        # Check that the model was initialized correctly
        self.assertIsInstance(model, nn.Module)
        self.assertEqual(model.vocab, {"a": 1, "b": 2})

    def test_output_dataclass(self):
        """Test KokoroModel.Output dataclass."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.kokoro import Model

        # Create a mock output
        audio = mx.array(np.zeros((1, 1000)))
        pred_dur = mx.array(np.zeros((1, 100)))

        # Mock __init__ to return None
        with patch.object(Model, "__init__", return_value=None):
            output = Model.Output(audio=audio, pred_dur=pred_dur)

        # Check that the output was created correctly
        self.assertIs(output.audio, audio)
        self.assertIs(output.pred_dur, pred_dur)


@patch("importlib.resources.open_text", patched_open_text)
class TestKokoroPipeline(unittest.TestCase):
    def test_aliases_and_lang_codes(self):
        """Test ALIASES and LANG_CODES constants."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.pipeline import ALIASES, LANG_CODES

        # Check that all aliases map to valid language codes
        for alias_key, alias_value in ALIASES.items():
            self.assertIn(alias_value, LANG_CODES)

        # Check specific mappings
        self.assertEqual(ALIASES["en-us"], "a")
        self.assertEqual(ALIASES["ja"], "j")
        self.assertEqual(LANG_CODES["a"], "American English")
        self.assertEqual(LANG_CODES["j"], "Japanese")

    def test_init(self):
        """Test KokoroPipeline initialization."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.pipeline import LANG_CODES, KokoroPipeline

        # Mock the KokoroModel - fix the import path
        with patch("mlx_audio.tts.models.kokoro.kokoro.Model") as mock_kokoro_model:
            with patch(
                "mlx_audio.tts.models.kokoro.pipeline.isinstance"
            ) as mock_isinstance:
                mock_model = MagicMock()
                mock_kokoro_model.return_value = mock_model

                # Simply make isinstance always return True when checking for KokoroModel
                mock_isinstance.return_value = True

                # Initialize with default model
                pipeline = KokoroPipeline(
                    lang_code="a", model=mock_model, repo_id="mock"
                )
                self.assertEqual(pipeline.lang_code, "a")
                self.assertEqual(LANG_CODES[pipeline.lang_code], "American English")

                # Initialize with provided model
                model = mock_model
                pipeline = KokoroPipeline(lang_code="a", model=model, repo_id="mock")
                self.assertEqual(pipeline.model, model)

                # Initialize with no model
                pipeline = KokoroPipeline(lang_code="a", model=False, repo_id="mock")
                self.assertIs(pipeline.model, False)

    def test_load_voice(self):
        """Test load_voice method."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.pipeline import KokoroPipeline

        # Setup the pipeline
        with patch.object(KokoroPipeline, "__init__", return_value=None):
            with patch(
                "mlx_audio.tts.models.kokoro.pipeline.load_voice_tensor"
            ) as load_voice_tensor:
                with patch(
                    "mlx_audio.tts.models.kokoro.pipeline.hf_hub_download"
                ) as mock_hf_hub_download:
                    pipeline = KokoroPipeline.__new__(KokoroPipeline)
                    pipeline.lang_code = "a"
                    pipeline.voices = {}
                    # Add the missing repo_id attribute
                    pipeline.repo_id = "mlx-community/kokoro-tts"

                    # Mock the load voice return value
                    load_voice_tensor.return_value = mx.zeros((512, 1, 256))

                    # Test loading a single voice
                    pipeline.load_single_voice("voice1")
                    mock_hf_hub_download.assert_called_once()
                    self.assertIn("voice1", pipeline.voices)

                    # Test loading multiple voices
                    mock_hf_hub_download.reset_mock()
                    pipeline.voices = {}  # Reset voices
                    result = pipeline.load_voice("voice1,voice2")
                    self.assertEqual(mock_hf_hub_download.call_count, 2)
                    self.assertIn("voice1", pipeline.voices)
                    self.assertIn("voice2", pipeline.voices)

    def test_tokens_to_ps(self):
        """Test tokens_to_ps method."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.pipeline import KokoroPipeline

        # Create mock tokens with whitespace attribute
        token1 = MagicMock(spec=en.MToken)
        token1.ps = "p1"
        token1.whitespace = " "
        token1.phonemes = "p1"

        token2 = MagicMock(spec=en.MToken)
        token2.ps = "p2"
        token2.whitespace = ""
        token2.phonemes = "p2"

        tokens = [token1, token2]

        # Test the method
        with patch.object(KokoroPipeline, "__init__", return_value=None):
            with patch.object(KokoroPipeline, "tokens_to_ps", return_value="p1 p2"):
                result = KokoroPipeline.tokens_to_ps(tokens)
                self.assertEqual(result, "p1 p2")

    def test_tokens_to_text(self):
        """Test tokens_to_text method."""
        # Import inside the test method
        from mlx_audio.tts.models.kokoro.pipeline import KokoroPipeline

        # Create mock tokens with whitespace attribute
        token1 = MagicMock(spec=en.MToken)
        token1.text = "Hello"
        token1.whitespace = " "

        token2 = MagicMock(spec=en.MToken)
        token2.text = "world"
        token2.whitespace = ""

        tokens = [token1, token2]

        # Test the method
        with patch.object(KokoroPipeline, "__init__", return_value=None):
            with patch.object(
                KokoroPipeline, "tokens_to_text", return_value="Hello world"
            ):
                result = KokoroPipeline.tokens_to_text(tokens)
                self.assertEqual(result, "Hello world")

    def test_result_dataclass(self):
        """Test KokoroPipeline.Result dataclass."""
        # Import inside the test methods
        from mlx_audio.tts.models.kokoro.kokoro import Model
        from mlx_audio.tts.models.kokoro.pipeline import KokoroPipeline

        # Create a mock output
        audio = mx.array(np.zeros((1, 1000)))
        pred_dur = mx.array(np.zeros((1, 100)))
        model_output = Model.Output(audio=audio, pred_dur=pred_dur)

        # Create a Result instance
        result = KokoroPipeline.Result(
            graphemes="Hello",
            phonemes="HH EH L OW",
            tokens=[MagicMock()],
            output=model_output,
            text_index=0,
        )

        # Check properties
        self.assertEqual(result.graphemes, "Hello")
        self.assertEqual(result.phonemes, "HH EH L OW")
        self.assertIs(result.audio, audio)
        self.assertIs(result.pred_dur, pred_dur)

        # Test backward compatibility
        self.assertEqual(len(result), 3)
        self.assertEqual(result[0], "Hello")
        self.assertEqual(result[1], "HH EH L OW")
        self.assertIs(result[2], audio)

        # Test iteration
        items = list(result)
        self.assertEqual(items[0], "Hello")
        self.assertEqual(items[1], "HH EH L OW")
        self.assertIs(items[2], audio)


@patch("importlib.resources.open_text", patched_open_text)
class TestBarkModel(unittest.TestCase):
    @patch("mlx_audio.tts.models.bark.bark.BertTokenizer")
    def test_init(self, mock_tokenizer):
        """Test BarkModel initialization."""
        from mlx_audio.tts.models.bark.bark import (
            CoarseAcousticsConfig,
            CodecConfig,
            FineAcousticsConfig,
            Model,
            ModelConfig,
            SemanticConfig,
        )

        # Create mock configs
        semantic_config = SemanticConfig()
        coarse_config = CoarseAcousticsConfig()
        fine_config = FineAcousticsConfig()
        codec_config = CodecConfig()

        config = ModelConfig(
            semantic_config=semantic_config,
            coarse_acoustics_config=coarse_config,
            fine_acoustics_config=fine_config,
            codec_config=codec_config,
        )

        # Initialize model
        model = Model(config)

        # Check that components were initialized correctly
        self.assertIsNotNone(model.semantic)
        self.assertIsNotNone(model.coarse_acoustics)
        self.assertIsNotNone(model.fine_acoustics)
        self.assertIsNotNone(model.tokenizer)

    def test_sanitize_weights(self):
        """Test weight sanitization."""
        from mlx_audio.tts.models.bark.bark import Model, ModelConfig

        # Create a minimal config
        config = ModelConfig(
            semantic_config={},
            coarse_acoustics_config={},
            fine_acoustics_config={},
            codec_config={},
        )

        model = Model(config)

        # Test with transformer weights
        weights = {
            "_orig_mod.transformer.h.0.mlp.weight": mx.zeros((10, 10)),
            "_orig_mod.transformer.h.1.mlp.weight": mx.zeros((10, 10)),
            "lm_head.weight": mx.zeros((10, 10)),
        }

        sanitized = model.sanitize(weights)

        # Check that weights were properly renamed
        self.assertIn("layers.0.mlp.weight", sanitized)
        self.assertIn("layers.1.mlp.weight", sanitized)
        self.assertIn("lm_head.weight", sanitized)


@patch("importlib.resources.open_text", patched_open_text)
class TestBarkPipeline(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures."""
        from mlx_audio.tts.models.bark.bark import (
            CoarseAcousticsConfig,
            CodecConfig,
            FineAcousticsConfig,
            Model,
            ModelConfig,
            SemanticConfig,
        )
        from mlx_audio.tts.models.bark.pipeline import Pipeline

        # Create mock model with required attributes
        self.mock_model = MagicMock(spec=Model)

        # Add the required mock attributes/methods
        self.mock_model.semantic = MagicMock()
        self.mock_model.coarse_acoustics = MagicMock()
        self.mock_model.fine_acoustics = MagicMock()
        self.mock_model.codec_model = MagicMock()

        self.mock_tokenizer = MagicMock()

        # Initialize pipeline
        self.pipeline = Pipeline(
            model=self.mock_model,
            tokenizer=self.mock_tokenizer,
            config=ModelConfig(
                semantic_config=SemanticConfig(),
                coarse_acoustics_config=CoarseAcousticsConfig(),
                fine_acoustics_config=FineAcousticsConfig(),
                codec_config=CodecConfig(),
            ),
        )

    def test_generate_text_semantic(self):
        """Test semantic token generation."""
        # Mock tokenizer output
        self.mock_tokenizer.encode.return_value = [1, 2, 3]

        # Create logits with proper shape including SEMANTIC_PAD_TOKEN
        logits = mx.zeros((1, 1, 129596))  # Large enough to include SEMANTIC_PAD_TOKEN
        # Mock model output
        self.mock_model.semantic.return_value = (
            logits,  # logits with correct shape
            None,  # kv_cache
        )

        # Test generation
        semantic_tokens, text_tokens = self.pipeline.generate_text_semantic(
            "test text",
            temperature=0.7,
            use_kv_caching=True,
            voice=None,
        )

        # Verify tokenizer was called
        self.mock_tokenizer.encode.assert_called_once_with(
            "test text", add_special_tokens=False
        )

        # Verify model was called
        self.mock_model.semantic.assert_called()

        # Check output types
        self.assertIsInstance(semantic_tokens, mx.array)
        self.assertIsInstance(text_tokens, mx.array)

    @patch("mlx.core.random.categorical")  # Add this patch since we use mx alias
    def test_generate_coarse(self, mock_mlx_categorical):
        """Test coarse token generation."""
        # Create mock semantic tokens
        semantic_tokens = mx.array([1, 2, 3])

        # Create logits with proper shape
        logits = mx.zeros((1, 1, 12096))

        # Mock both categorical functions to return predictable values
        mock_mlx_categorical.return_value = mx.array([10000])  # Return token index

        # Set up the mock to return proper values for each call
        self.mock_model.coarse_acoustics.return_value = (logits, None)

        # Test generation with minimal parameters to reduce test time
        coarse_tokens = self.pipeline.generate_coarse(
            semantic_tokens,
            temperature=0.7,
            use_kv_caching=True,
            voice=None,
            max_coarse_history=60,
            sliding_window_len=2,  # Reduce this to minimum
        )

        # Verify model was called at least once
        self.mock_model.coarse_acoustics.assert_called()

        # Check output type and shape
        self.assertIsInstance(coarse_tokens, mx.array)
        self.assertEqual(coarse_tokens.shape[0], 2)  # N_COARSE_CODEBOOKS

    def test_generate_fine(self):
        """Test fine token generation."""
        # Create mock coarse tokens
        coarse_tokens = mx.zeros((2, 100))  # N_COARSE_CODEBOOKS x sequence_length

        # Mock model output with proper shape
        self.mock_model.fine_acoustics.return_value = mx.zeros((1, 1024, 1024))

        # Test generation
        fine_tokens = self.pipeline.generate_fine(coarse_tokens, temperature=0.7)

        # Verify model was called
        self.mock_model.fine_acoustics.assert_called()

        # Check output type and shape
        self.assertIsInstance(fine_tokens, mx.array)
        self.assertEqual(
            fine_tokens.shape[0], 8
        )  # N_FINE_CODEBOOKS (corrected from 10 to 8)
        self.assertEqual(fine_tokens.shape[1], 100)  # sequence_length


class TestLlamaModel(unittest.TestCase):
    @property
    def _default_config(self):
        return {
            "attention_bias": False,
            "head_dim": 128,
            "hidden_size": 3072,
            "intermediate_size": 8192,
            "max_position_embeddings": 131072,
            "mlp_bias": False,
            "model_type": "llama",
            "num_attention_heads": 24,
            "num_hidden_layers": 28,
            "num_key_value_heads": 8,
            "rms_norm_eps": 1e-05,
            "rope_scaling": {
                "factor": 32.0,
                "high_freq_factor": 4.0,
                "low_freq_factor": 1.0,
                "original_max_position_embeddings": 8192,
                "rope_type": "llama3",
            },
            "rope_theta": 500000.0,
            "tie_word_embeddings": True,
            "vocab_size": 156940,
        }

    @patch("transformers.LlamaTokenizer")
    def test_init(self, mock_tokenizer):
        """Test LlamaModel initialization."""
        from mlx_audio.tts.models.llama.llama import Model, ModelConfig

        # Mock the tokenizer instance
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.return_value = mock_tokenizer_instance

        # Create a minimal config
        config = ModelConfig(**self._default_config)

        # Initialize model
        model = Model(config)

        # Check that model was created
        self.assertIsInstance(model, Model)

    @patch("transformers.LlamaTokenizer")
    def test_generate(self, mock_tokenizer):
        """Test generate method."""
        from mlx_audio.tts.models.llama.llama import Model, ModelConfig

        # Mock tokenizer instance
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.return_value = mock_tokenizer_instance

        config = ModelConfig(**self._default_config)
        model = Model(config)

        # Verify batched input creation with a voice
        input_ids, input_mask = model.prepare_input_ids(["Foo", "Bar Baz"], voice="zoe")
        self.assertEqual(input_ids.shape[0], 2)
        self.assertEqual(input_mask.shape[0], 2)

        logits = model(input_ids)
        self.assertEqual(logits.shape, (2, 9, config.vocab_size))

        # Verify batched input creation with reference audio
        input_ids, input_mask = model.prepare_input_ids(
            ["Foo", "Bar Baz"], ref_audio=mx.zeros((100,)), ref_text="Caption"
        )
        self.assertEqual(input_ids.shape[0], 2)
        self.assertEqual(input_mask.shape[0], 2)

        logits = model(input_ids)
        self.assertEqual(logits.shape, (2, 22, config.vocab_size))

    @patch("transformers.LlamaTokenizer")
    def test_sanitize(self, mock_tokenizer):
        """Test sanitize method."""
        from mlx_audio.tts.models.llama.llama import Model, ModelConfig

        # Mock tokenizer instance
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.return_value = mock_tokenizer_instance

        # Create a config with tie_word_embeddings=True
        config = ModelConfig(
            model_type="llama",
            hidden_size=4096,
            num_hidden_layers=32,
            intermediate_size=16384,
            num_attention_heads=32,
            rms_norm_eps=1e-5,
            vocab_size=32000,
            head_dim=128,
            max_position_embeddings=1024,
            num_key_value_heads=32,
            attention_bias=True,
            mlp_bias=True,
            rope_theta=500000.0,
            rope_traditional=False,
            rope_scaling=None,
            tie_word_embeddings=True,
        )

        # Initialize the model with a patched __init__
        with patch.object(Model, "__init__", return_value=None):
            model = Model.__new__(Model)
            model.config = config

            # Add the sanitize method from actual implementation
            def mock_sanitize(weights):
                result = {}
                for k, v in weights.items():
                    if "rotary_emb" in k:
                        continue
                    if "lm_head.weight" in k and config.tie_word_embeddings:
                        continue
                    result[k] = v
                return result

            model.sanitize = mock_sanitize

            # Create test weights with rotary embeddings and lm_head
            weights = {
                "self_attn.rotary_emb.inv_freq": mx.zeros(10),
                "lm_head.weight": mx.zeros((32000, 4096)),
                "model.layers.0.input_layernorm.weight": mx.zeros(4096),
            }

            # Test sanitize method
            sanitized = model.sanitize(weights)

            # Assert rotary embeddings are removed
            self.assertNotIn("self_attn.rotary_emb.inv_freq", sanitized)

            # Assert lm_head weights are removed with tie_word_embeddings=True
            self.assertNotIn("lm_head.weight", sanitized)

            # Assert other weights remain
            self.assertIn("model.layers.0.input_layernorm.weight", sanitized)

            # Now test with tie_word_embeddings=False
            config.tie_word_embeddings = False

            # Test sanitize again
            sanitized2 = model.sanitize(weights)

            # lm_head should be kept with tie_word_embeddings=False
            self.assertIn("lm_head.weight", sanitized2)


class TestOuteTTSModel(unittest.TestCase):
    @property
    def _default_config(self):
        return {
            "attention_bias": False,
            "head_dim": 64,
            "hidden_size": 2048,
            "intermediate_size": 8192,
            "max_position_embeddings": 131072,
            "mlp_bias": False,
            "model_type": "llama",
            "num_attention_heads": 32,
            "num_hidden_layers": 16,
            "num_key_value_heads": 8,
            "rms_norm_eps": 1e-05,
            "rope_scaling": {
                "factor": 32.0,
                "high_freq_factor": 4.0,
                "low_freq_factor": 1.0,
                "original_max_position_embeddings": 8192,
                "rope_type": "llama3",
            },
            "rope_theta": 500000.0,
            "tie_word_embeddings": True,
            "vocab_size": 134400,
        }

    @patch("transformers.LlamaTokenizer")
    def test_init(self, mock_tokenizer):
        """Test initialization."""
        from mlx_audio.tts.models.outetts.outetts import Model, ModelConfig

        # Mock the tokenizer instance
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.return_value = mock_tokenizer_instance

        # Create a minimal config
        config = ModelConfig(**self._default_config)

        # Initialize model
        model = Model(config)

        # Check that model was created
        self.assertIsInstance(model, Model)

    @patch("transformers.LlamaTokenizer")
    def test_generate(self, mock_tokenizer):
        """Test generate method."""
        from mlx_audio.tts.models.outetts.outetts import Model, ModelConfig

        # Mock tokenizer instance
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.return_value = mock_tokenizer_instance

        config = ModelConfig(**self._default_config)
        model = Model(config)

        input_ids = mx.random.randint(0, config.vocab_size, (2, 9))
        logits = model(input_ids)
        self.assertEqual(logits.shape, (2, 9, config.vocab_size))


class TestDiaModel(unittest.TestCase):
    @property
    def _default_config(self):
        return {
            "version": "0.1",
            "model": {
                "encoder": {
                    "n_layer": 12,
                    "n_embd": 1024,
                    "n_hidden": 4096,
                    "n_head": 16,
                    "head_dim": 128,
                },
                "decoder": {
                    "n_layer": 18,
                    "n_embd": 2048,
                    "n_hidden": 8192,
                    "gqa_query_heads": 16,
                    "cross_query_heads": 16,
                    "kv_heads": 4,
                    "gqa_head_dim": 128,
                    "cross_head_dim": 128,
                },
                "src_vocab_size": 256,
                "tgt_vocab_size": 1028,
                "dropout": 0.0,
            },
            "training": {},
            "data": {
                "text_length": 1024,
                "audio_length": 3072,
                "channels": 9,
                "text_pad_value": 0,
                "audio_eos_value": 1024,
                "audio_pad_value": 1025,
                "audio_bos_value": 1026,
                "delay_pattern": [0, 8, 9, 10, 11, 12, 13, 14, 15],
            },
        }

    def test_init(self):
        """Test DiaModel initialization."""
        from mlx_audio.tts.models.dia.dia import Model

        # Initialize model
        config = self._default_config
        model = Model(config)

        # Check that model was created
        self.assertIsInstance(model, Model)


class TestSparkTTSModel(unittest.TestCase):
    @property
    def _default_config(self):
        return {
            "model_path": "/fake/model/path",
            "sample_rate": 16000,
            "bos_token_id": 151643,
            "eos_token_id": 151645,
            "hidden_act": "silu",
            "hidden_size": 896,
            "initializer_range": 0.02,
            "intermediate_size": 4864,
            "max_position_embeddings": 32768,
            "max_window_layers": 21,
            "model_type": "qwen2",
            "num_attention_heads": 14,
            "num_hidden_layers": 24,
            "num_key_value_heads": 2,
            "rms_norm_eps": 1e-06,
            "rope_theta": 1000000.0,
            "sliding_window": 32768,
            "tie_word_embeddings": True,
            "torch_dtype": "bfloat16",
            "transformers_version": "4.43.1",
            "use_sliding_window": False,
            "vocab_size": 166000,
            "rope_traditional": False,
            "rope_scaling": None,
        }

    @patch("mlx_audio.tts.models.spark.spark.load_tokenizer")
    @patch("mlx_audio.tts.models.spark.spark.BiCodecTokenizer")
    @patch("mlx_audio.tts.models.spark.spark.Qwen2Model")
    def test_init(
        self,
        mock_qwen2_model,
        mock_bicodec_tokenizer,
        mock_load_tokenizer,
    ):
        """Test SparkTTSModel initialization."""
        from pathlib import Path

        from mlx_audio.tts.models.spark.spark import Model, ModelConfig

        # Mock return values for patched functions
        mock_load_tokenizer.return_value = MagicMock()
        mock_bicodec_tokenizer.return_value = MagicMock()
        mock_qwen2_model.return_value = MagicMock()

        # Create a config instance
        config = ModelConfig(**self._default_config)
        config.model_path = Path("/fake/model/path")

        # Initialize the model
        model = Model(config)

        # Check that the model was initialized correctly
        self.assertIsInstance(model, Model)

        # Verify the tokenizer was loaded correctly
        mock_load_tokenizer.assert_called_once_with(
            config.model_path, eos_token_ids=config.eos_token_id
        )
        mock_bicodec_tokenizer.assert_called_once_with(config.model_path)

        # Verify the model was initialized correctly
        mock_qwen2_model.assert_called_once_with(config)


class TestIndexTTS(unittest.TestCase):
    @property
    def _default_config(self):
        return {
            "tokenizer_name": "mlx-community/IndexTTS",
            "bigvgan": {
                "adam_b1": 0.8,
                "adam_b2": 0.99,
                "lr_decay": 0.999998,
                "seed": 1234,
                "resblock": "1",
                "upsample_rates": [4, 4, 4, 4, 2, 2],
                "upsample_kernel_sizes": [8, 8, 4, 4, 4, 4],
                "upsample_initial_channel": 1536,
                "resblock_kernel_sizes": [3, 7, 11],
                "resblock_dilation_sizes": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
                "feat_upsample": False,
                "speaker_embedding_dim": 512,
                "cond_d_vector_in_each_upsampling_layer": True,
                "gpt_dim": 1024,
                "activation": "snakebeta",
                "snake_logscale": True,
                "use_cqtd_instead_of_mrd": True,
                "cqtd_filters": 128,
                "cqtd_max_filters": 1024,
                "cqtd_filters_scale": 1,
                "cqtd_dilations": [1, 2, 4],
                "cqtd_hop_lengths": [512, 256, 256],
                "cqtd_n_octaves": [9, 9, 9],
                "cqtd_bins_per_octaves": [24, 36, 48],
                "resolutions": [[1024, 120, 600], [2048, 240, 1200], [512, 50, 240]],
                "mpd_reshapes": [2, 3, 5, 7, 11],
                "use_spectral_norm": False,
                "discriminator_channel_mult": 1,
                "use_multiscale_melloss": True,
                "lambda_melloss": 15,
                "clip_grad_norm": 1000,
                "segment_size": 16384,
                "num_mels": 100,
                "num_freq": 1025,
                "n_fft": 1024,
                "hop_size": 256,
                "win_size": 1024,
                "sampling_rate": 24000,
                "fmin": 0,
                "fmax": None,
                "fmax_for_loss": None,
                "mel_type": "pytorch",
                "num_workers": 2,
                "dist_config": {
                    "dist_backend": "nccl",
                    "dist_url": "tcp://localhost:54321",
                    "world_size": 1,
                },
            },
            "bigvgan_checkpoint": "bigvgan_generator.pth",
            "dataset": {
                "bpe_model": "checkpoints/bpe.model",
                "sample_rate": 24000,
                "squeeze": False,
                "mel": {
                    "sample_rate": 24000,
                    "n_fft": 1024,
                    "hop_length": 256,
                    "win_length": 1024,
                    "n_mels": 100,
                    "mel_fmin": 0,
                    "normalize": False,
                },
            },
            "dvae_checkpoint": "dvae.pth",
            "gpt": {
                "model_dim": 1024,
                "max_mel_tokens": 605,
                "max_text_tokens": 402,
                "heads": 16,
                "use_mel_codes_as_input": True,
                "mel_length_compression": 1024,
                "layers": 20,
                "number_text_tokens": 12000,
                "number_mel_codes": 8194,
                "start_mel_token": 8192,
                "stop_mel_token": 8193,
                "start_text_token": 0,
                "stop_text_token": 1,
                "train_solo_embeddings": False,
                "condition_type": "conformer_perceiver",
                "condition_module": {
                    "output_size": 512,
                    "linear_units": 2048,
                    "attention_heads": 8,
                    "num_blocks": 6,
                    "input_layer": "conv2d2",
                    "perceiver_mult": 2,
                },
            },
            "gpt_checkpoint": "gpt.pth",
            "vqvae": {
                "channels": 100,
                "num_tokens": 8192,
                "hidden_dim": 512,
                "num_resnet_blocks": 3,
                "codebook_dim": 512,
                "num_layers": 2,
                "positional_dims": 1,
                "kernel_size": 3,
                "smooth_l1_loss": True,
                "use_transposed_convs": False,
            },
        }

    def test_init(self):
        """Test IndexTTS initialization."""
        from mlx_audio.tts.models.indextts.indextts import Model

        # Initialize model
        config = self._default_config
        model = Model(config)  # type: ignore

        # Check that model was created
        self.assertIsInstance(model, Model)


if __name__ == "__main__":
    unittest.main()



================================================
FILE: .github/FUNDING.yml
================================================
# These are supported funding model platforms

github: Blaizzy # Replace with up to 4 GitHub Sponsors-enabled usernames e.g., [user1, user2]
patreon: # Replace with a single Patreon username
open_collective: # Replace with a single Open Collective username
ko_fi: # Replace with a single Ko-fi username
tidelift: # Replace with a single Tidelift platform-name/package-name e.g., npm/babel
community_bridge: # Replace with a single Community Bridge project-name e.g., cloud-foundry
liberapay: # Replace with a single Liberapay username
issuehunt: # Replace with a single IssueHunt username
lfx_crowdfunding: # Replace with a single LFX Crowdfunding project-name e.g., cloud-foundry
polar: # Replace with a single Polar username
buy_me_a_coffee: # Replace with a single Buy Me a Coffee username
thanks_dev: # Replace with a single thanks.dev username
custom: # Replace with up to 4 custom sponsorship URLs e.g., ['link1', 'link2']



================================================
FILE: .github/pull_request_template.md
================================================
## Context
*Gives the reviewer some context about the work and why this change is being made. Focus on the WHY from a product perspective.*

## Description
*Provide a detailed description of how this task will be accomplished. Include technical details, steps, service integration, job logic, implementation, etc.*

## Changes in the codebase
*Describe the functionality you are adding or modifying, as well as any refactoring or improvements to existing code.*

## Changes outside the codebase
*Explain any changes to external services, infrastructure, third-party integrations, or database updates.*

## Additional information
*Provide any extra information that might be useful to the reviewer, such as performance considerations or design choices.*

<!-- Optional: Add a checklist for contributors -->
## Checklist
- [ ] Tests added/updated
- [ ] Documentation updated
- [ ] Issue referenced (e.g., Closes #123)


================================================
FILE: .github/workflows/python-publish.yml
================================================
# This workflow will upload a Python Package using Twine when a release is created
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python#publishing-to-package-registries

name: Upload Python Package

on:
  release:
    types: [published]

permissions:
  contents: read

jobs:
  deploy:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build
    - name: Build package
      run: python -m build
    - name: Publish package
      uses: pypa/gh-action-pypi-publish@27b31702a0e7fc50959f5ad993c78deac1bdfc29
      with:
        user: __token__
        password: ${{ secrets.PYPI_API_TOKEN }}
        packages_dir: dist



================================================
FILE: .github/workflows/swift.yml
================================================
# This workflow will build a Swift project
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-swift

name: Run Swift Tests

on:
  pull_request:
    branches:
      - main

jobs:
  build:
    runs-on: macos-latest

    steps:
      - uses: actions/checkout@v4
      - name: Select Xcode version
        uses: maxim-lobanov/setup-xcode@v1
        with:
          xcode-version: 'latest'
      - name: Clean build folder
        run: cd mlx_audio_swift/tts && xcodebuild clean -scheme Swift-TTS # <-- Add this clean step
      - name: Build and Run tests
        run: cd mlx_audio_swift/tts && xcodebuild test -scheme Swift-TTS -destination 'platform=macOS' MACOSX_DEPLOYMENT_TARGET=14.0 CODE_SIGNING_ALLOWED=NO
      - name: Build Swift Package
        run: swift build


================================================
FILE: .github/workflows/tests.yml
================================================
name: Run Python Tests

on:
  pull_request:
    branches:
      - main

jobs:
  test:
    runs-on: macos-14

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install MLX
        run: |
          pip install mlx>=0.15

      - name: Install pre-commit
        run: |
          python -m pip install pre-commit
          pre-commit run --all
          if ! git diff --quiet; then
            echo 'Style checks failed, please install pre-commit and run pre-commit run --all and push the change'
            exit 1
          fi

      - name: Install package and dependencies
        run: |
          python -m pip install pytest
          python -m pip install -e .

      - name: Run Python tests (TTS)
        run: |
          cd mlx_audio/tts/
          pytest -s ./tests

      - name: Run Python tests (Codec)
        run: |
          cd mlx_audio/codec/
          pytest -s ./tests

      - name: Run Python tests (STS)
        run: |
          cd mlx_audio/sts/
          pytest -s ./tests

      - name: Run Python tests (STT)
        run: |
          cd mlx_audio/stt/
          pytest -s ./tests


