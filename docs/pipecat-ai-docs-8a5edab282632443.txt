Directory structure:
└── pipecat-ai-docs/
    ├── README.md
    ├── examples.mdx
    ├── client/
    │   ├── introduction.mdx
    │   ├── migration-guide.mdx
    │   ├── rtvi-standard.mdx
    │   ├── android/
    │   │   ├── api-reference.mdx
    │   │   ├── introduction.mdx
    │   │   └── transports/
    │   │       ├── daily.mdx
    │   │       ├── gemini-websocket.mdx
    │   │       ├── openai-webrtc.mdx
    │   │       └── small-webrtc.mdx
    │   ├── c++/
    │   │   ├── api-reference.mdx
    │   │   ├── introduction.mdx
    │   │   └── transport.mdx
    │   ├── ios/
    │   │   ├── api-reference.mdx
    │   │   ├── introduction.mdx
    │   │   └── transports/
    │   │       ├── daily.mdx
    │   │       ├── gemini-websocket.mdx
    │   │       └── openai-webrtc.mdx
    │   ├── js/
    │   │   ├── introduction.mdx
    │   │   ├── migration-guide.mdx
    │   │   ├── api-reference/
    │   │   │   ├── callbacks.mdx
    │   │   │   ├── client-constructor.mdx
    │   │   │   ├── client-methods.mdx
    │   │   │   ├── errors.mdx
    │   │   │   └── messages.mdx
    │   │   └── transports/
    │   │       ├── daily.mdx
    │   │       ├── gemini.mdx
    │   │       ├── openai-webrtc.mdx
    │   │       ├── small-webrtc.mdx
    │   │       ├── transport.mdx
    │   │       └── websocket.mdx
    │   ├── react/
    │   │   ├── components.mdx
    │   │   ├── hooks.mdx
    │   │   ├── introduction.mdx
    │   │   └── migration-guide.mdx
    │   └── react-native/
    │       ├── api-reference.mdx
    │       └── introduction.mdx
    ├── getting-started/
    │   ├── client-server.mdx
    │   ├── introduction.mdx
    │   ├── next-steps.mdx
    │   ├── phone-bots.mdx
    │   └── quickstart.mdx
    ├── guides/
    │   ├── introduction.mdx
    │   ├── deployment/
    │   │   ├── cerebrium.mdx
    │   │   ├── fly.mdx
    │   │   ├── modal.mdx
    │   │   ├── overview.mdx
    │   │   ├── pattern.mdx
    │   │   └── pipecat-cloud.mdx
    │   ├── features/
    │   │   ├── gemini-multimodal-live.mdx
    │   │   ├── krisp.mdx
    │   │   ├── metrics.mdx
    │   │   ├── openai-audio-models-and-apis.mdx
    │   │   └── pipecat-flows.mdx
    │   ├── fundamentals/
    │   │   ├── context-management.mdx
    │   │   ├── core-concepts.mdx
    │   │   ├── custom-frame-processor.mdx
    │   │   ├── detecting-user-idle.mdx
    │   │   ├── end-pipeline.mdx
    │   │   ├── function-calling.mdx
    │   │   ├── recording-audio.mdx
    │   │   ├── saving-transcripts.mdx
    │   │   └── user-input-muting.mdx
    │   └── telephony/
    │       ├── daily-webrtc.mdx
    │       ├── dialout.mdx
    │       ├── overview.mdx
    │       ├── twilio-daily-webrtc.mdx
    │       └── twilio-websockets.mdx
    ├── server/
    │   ├── introduction.mdx
    │   ├── frameworks/
    │   │   ├── flows/
    │   │   │   └── pipecat-flows.mdx
    │   │   └── rtvi/
    │   │       ├── google-rtvi-observer.mdx
    │   │       ├── introduction.mdx
    │   │       ├── rtvi-observer.mdx
    │   │       └── rtvi-processor.mdx
    │   ├── links/
    │   │   └── server-reference.mdx
    │   ├── pipeline/
    │   │   ├── heartbeats.mdx
    │   │   ├── parallel-pipeline.mdx
    │   │   ├── pipeline-idle-detection.mdx
    │   │   ├── pipeline-params.mdx
    │   │   └── pipeline-task.mdx
    │   ├── services/
    │   │   ├── supported-services.mdx
    │   │   ├── analytics/
    │   │   │   └── sentry.mdx
    │   │   ├── image-generation/
    │   │   │   ├── fal.mdx
    │   │   │   ├── google-imagen.mdx
    │   │   │   └── openai.mdx
    │   │   ├── llm/
    │   │   │   ├── anthropic.mdx
    │   │   │   ├── aws.mdx
    │   │   │   ├── azure.mdx
    │   │   │   ├── cerebras.mdx
    │   │   │   ├── deepseek.mdx
    │   │   │   ├── fireworks.mdx
    │   │   │   ├── gemini.mdx
    │   │   │   ├── google-vertex.mdx
    │   │   │   ├── grok.mdx
    │   │   │   ├── groq.mdx
    │   │   │   ├── nim.mdx
    │   │   │   ├── ollama.mdx
    │   │   │   ├── openai.mdx
    │   │   │   ├── openpipe.mdx
    │   │   │   ├── openrouter.mdx
    │   │   │   ├── perplexity.mdx
    │   │   │   ├── qwen.mdx
    │   │   │   ├── sambanova.mdx
    │   │   │   └── together.mdx
    │   │   ├── memory/
    │   │   │   └── mem0.mdx
    │   │   ├── s2s/
    │   │   │   ├── aws.mdx
    │   │   │   ├── gemini.mdx
    │   │   │   └── openai.mdx
    │   │   ├── serializers/
    │   │   │   ├── exotel.mdx
    │   │   │   ├── introduction.mdx
    │   │   │   ├── plivo.mdx
    │   │   │   ├── telnyx.mdx
    │   │   │   └── twilio.mdx
    │   │   ├── stt/
    │   │   │   ├── assemblyai.mdx
    │   │   │   ├── aws.mdx
    │   │   │   ├── azure.mdx
    │   │   │   ├── cartesia.mdx
    │   │   │   ├── deepgram.mdx
    │   │   │   ├── fal.mdx
    │   │   │   ├── gladia.mdx
    │   │   │   ├── google.mdx
    │   │   │   ├── groq.mdx
    │   │   │   ├── openai.mdx
    │   │   │   ├── riva.mdx
    │   │   │   ├── sambanova.mdx
    │   │   │   ├── soniox.mdx
    │   │   │   ├── speechmatics.mdx
    │   │   │   ├── ultravox.mdx
    │   │   │   └── whisper.mdx
    │   │   ├── transport/
    │   │   │   ├── daily.mdx
    │   │   │   ├── fastapi-websocket.mdx
    │   │   │   ├── livekit.mdx
    │   │   │   ├── small-webrtc.mdx
    │   │   │   ├── tavus.mdx
    │   │   │   └── websocket-server.mdx
    │   │   ├── tts/
    │   │   │   ├── asyncai.mdx
    │   │   │   ├── aws.mdx
    │   │   │   ├── azure.mdx
    │   │   │   ├── cartesia.mdx
    │   │   │   ├── deepgram.mdx
    │   │   │   ├── elevenlabs.mdx
    │   │   │   ├── fish.mdx
    │   │   │   ├── google.mdx
    │   │   │   ├── groq.mdx
    │   │   │   ├── inworld.mdx
    │   │   │   ├── lmnt.mdx
    │   │   │   ├── minimax.mdx
    │   │   │   ├── neuphonic.mdx
    │   │   │   ├── openai.mdx
    │   │   │   ├── piper.mdx
    │   │   │   ├── playht.mdx
    │   │   │   ├── rime.mdx
    │   │   │   ├── riva.mdx
    │   │   │   ├── sarvam.mdx
    │   │   │   └── xtts.mdx
    │   │   ├── video/
    │   │   │   ├── simli.mdx
    │   │   │   └── tavus.mdx
    │   │   └── vision/
    │   │       └── moondream.mdx
    │   └── utilities/
    │       ├── dtmf-aggregator.mdx
    │       ├── interruption-strategies.mdx
    │       ├── opentelemetry.mdx
    │       ├── transcript-processor.mdx
    │       ├── user-idle-processor.mdx
    │       ├── watchdog-timers.mdx
    │       ├── audio/
    │       │   ├── audio-buffer-processor.mdx
    │       │   ├── koala-filter.mdx
    │       │   ├── krisp-filter.mdx
    │       │   ├── noisereduce-filter.mdx
    │       │   ├── silero-vad-analyzer.mdx
    │       │   └── soundfile-mixer.mdx
    │       ├── daily/
    │       │   └── rest-helper.mdx
    │       ├── filters/
    │       │   ├── frame-filter.mdx
    │       │   ├── function-filter.mdx
    │       │   ├── identify-filter.mdx
    │       │   ├── null-filter.mdx
    │       │   ├── stt-mute.mdx
    │       │   ├── wake-check-filter.mdx
    │       │   └── wake-notifier-filter.mdx
    │       ├── frame/
    │       │   └── producer-consumer.mdx
    │       ├── mcp/
    │       │   └── mcp.mdx
    │       ├── observers/
    │       │   ├── debug-observer.mdx
    │       │   ├── llm-observer.mdx
    │       │   ├── observer-pattern.mdx
    │       │   ├── transcription-observer.mdx
    │       │   └── turn-tracking-observer.mdx
    │       ├── runner/
    │       │   ├── guide.mdx
    │       │   └── transport-utils.mdx
    │       ├── serializers/
    │       │   ├── introduction.mdx
    │       │   ├── plivo.mdx
    │       │   ├── telnyx.mdx
    │       │   └── twilio.mdx
    │       ├── smart-turn/
    │       │   ├── fal-smart-turn.mdx
    │       │   ├── local-coreml-smart-turn.mdx
    │       │   └── smart-turn-overview.mdx
    │       └── text/
    │           ├── markdown-text-filter.mdx
    │           └── pattern-pair-aggregator.mdx
    └── snippets/
        └── snippet-intro.mdx

================================================
FILE: README.md
================================================
<h1><div align="center">
  <img alt="pipecat" width="500px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/docs/main/pipecat-docs.png" />
</div></h1>

Welcome to the Pipecat documentation repository! This project contains the official [documentation](https://docs.pipecat.ai) for the [Pipecat](https://github.com/pipecat-ai/pipecat) open-source project.

This repository is deployed on [docs.pipecat.ai](https://docs.pipecat.ai).

## What is Pipecat

Pipecat is a framework for building voice and multimodal AI agents. Things like personal coaches, meeting assistants, [storytelling toys for kids](https://storytelling-chatbot.fly.dev/), customer support bots, [intake flows](https://www.youtube.com/watch?v=lDevgsp9vn0), and snarky social companions.

## Documentation Structure

This repository is dedicated to maintaining up-to-date, high-quality documentation to support users and contributors. Here you’ll find:

- **User Guides**: Step-by-step instructions to get started with Pipecat.
- **API Documentation**: Detailed API references.
- **Tutorials**: Hands-on tutorials to help you automate your workflows.

## Contributing to the Documentation

We welcome contributions of all kinds! Whether you're fixing a typo, adding a new section, or improving the readability of the existing content, your help is appreciated. Follow these steps to get involved:

1. **Fork this repository**: Start by forking the Pipecat Documentation repository to your GitHub account.

2. **Clone the repository**: Clone your forked repository to your local machine.
   ```bash
   git clone https://github.com/your-username/docs
   ```
3. **Create a branch**: For your contribution, create a new branch.
   ```bash
   git checkout -b your-branch-name
   ```
4. **Make your changes**: Edit or add files as necessary.
5. **Test your changes**: Ensure that your changes look correct and follow the style guide. Refer to **Development** section to test portal locally.
6. **Commit your changes**: Once you're satisfied with your changes, commit them with a meaningful message.

```bash
git commit -m "Description of your changes"
```

7. **Push your changes**: Push your branch to your forked repository.

```bash
git push origin your-branch-name
```

9. **Submit a Pull Request (PR)**: Open a PR from your forked repository to the main branch of this repo. Describe the changes you've made clearly.

Our maintainers will review your PR, and once everything is good, your contributions will be merged!

### Development

The documentation is using [Mintlify](https://mintlify.com/) to render beautifuly. Mintlify comes with a set of [components](https://mintlify.com/docs/content/components/) to help you write better more interactive documentation.

Install the [Mint CLI](https://www.npmjs.com/package/mint) to preview the documentation changes locally. To install, use the following command

```
npm i -g mint
```

Run the following command at the root of your documentation (where mint.json is)

```
mint dev
```

Open `https://localhost:3000` in your browser and check your changes.

#### Troubleshooting

- Mintlify dev isn't running - Run `mint update` to get the latest version.
- Page loads as a 404 - Make sure you are running in a folder with `mint.json`



================================================
FILE: examples.mdx
================================================
[Empty file]


================================================
FILE: client/introduction.mdx
================================================
---
title: "Client SDKs"
description: "Client libraries for building real-time AI applications with Pipecat"
---

<Warning>
  The Client SDKs are currently in transition to a new, simpler API design. The
  js and react libraries have already been deployed with these changes. Their
  corresponding documentation along with this top-level documentation has been
  updated to reflect the latest changes. For transitioning to the new API,
  please refer to the [migration guide](/client/migration-guide). Note that
  React Native, iOS, and Android SDKs are still in the process of being updated
  and their documentation will be updated once the new versions are released. If
  you have any questions or need assistance, please reach out to us on
  [Discord](https://discord.gg/pipecat).
</Warning>

Pipecat provides client SDKs for multiple platforms, all implementing the RTVI (Real-Time Voice and Video Inference) standard. These SDKs make it easy to build real-time AI applications that can handle voice, video, and text interactions.

<CardGroup cols={3}>
  <Card
    title="Javascript"
    icon="JS"
    color="#f7e014"
    href="/client/js/introduction"
  >
    Pipecat JS SDK
  </Card>
  <Card
    title="React"
    icon="react"
    color="#56c4db"
    href="/client/react/introduction"
  >
    Pipecat React SDK
  </Card>
  <Card
    title="React Native"
    icon="react"
    color="#56c4db"
    href="/client/react-native/introduction"
  >
    Pipecat React Native SDK
  </Card>
  <Card
    title="Swift"
    icon="swift"
    color="#F05138"
    vertical="true"
    href="/client/ios/introduction"
  >
    Pipecat iOS SDK
  </Card>
  <Card
    title="Kotlin"
    icon="android"
    color="#78C257"
    href="/client/android/introduction"
  >
    Pipecat Android SDK
  </Card>
  <Card title="C++" icon="C" color="#679cd3" href="/client/c++/introduction">
    Pipecat C++ SDK
  </Card>
</CardGroup>

## Core Functionality

All Pipecat client SDKs provide:

<CardGroup cols={2}>
  <Card title="Media Management" icon="video">
    Handle device inputs and media streams for audio and video
  </Card>
  <Card title="Bot Integration" icon="robot">
    Configure and communicate with your Pipecat bot
  </Card>
  <Card title="Session Management" icon="arrows-rotate">
    Manage connection state and error handling
  </Card>
</CardGroup>

## Core Types

### PipecatClient

The main class for interacting with Pipecat bots. It is the primary type you will interact with.

### Transport

The `PipecatClient` wraps a Transport, which defines and provides the underlying connection mechanism (e.g., WebSocket, WebRTC). Your Pipecat pipeline will contain a corresponding transport.

### RTVIMessage

Represents a message sent to or received from a Pipecat bot.

## Simple Usage Examples

<Tabs>
  <Tab title="Connecting to a Bot">
    Establish ongoing connections via WebSocket or WebRTC for:
    - Live voice conversations
    - Real-time video processing
    - Continuous interactions
    
    <CodeGroup>

    ```javascript javascript
    // Example: Establishing a real-time connection
    import { RTVIEvent, RTVIMessage, PipecatClient } from "@pipecat-ai/client-js";
    import { DailyTransport } from "@pipecat-ai/daily-transport";

    const pcClient = new PipecatClient({
      transport: new DailyTransport(),
      enableMic: true,
      enableCam: false,
      enableScreenShare: false,
      callbacks: {
        onBotConnected: () => {
          console.log("[CALLBACK] Bot connected");
        },
        onBotDisconnected: () => {
          console.log("[CALLBACK] Bot disconnected");
        },
        onBotReady: () => {
          console.log("[CALLBACK] Bot ready to chat!");
        },
      },
    });

    try {
      // Below, we use a REST endpoint to fetch connection credentials for our
      // Daily Transport. Alternatively, you could provide those credentials
      // directly to `connect()`.
      await pcClient.connect({
        endpoint: "https://your-connect-end-point-here/connect",
      });
    } catch (e) {
      console.error(e.message);
    }

    // Events (alternative approach to constructor-provided callbacks)
    pcClient.on(RTVIEvent.Connected, () => {
      console.log("[EVENT] User connected");
    });
    pcClient.on(RTVIEvent.Disconnected, () => {
      console.log("[EVENT] User disconnected");
    });
    ```

    ```jsx react
    // Example: Using PipecatClient in a React component
    import { PipecatClient } from "@pipecat-ai/client-js";
    import {
      PipecatClientProvider,
      PipecatClientAudio,
      usePipecatClient,
      useRTVIClientEvent,
    } from "@pipecat-ai/client-react";
    import { DailyTransport } from "@pipecat-ai/daily-transport";

    // Create the client instance
    const client = new PipecatClient({
      transport: new DailyTransport(),
      enableMic: true,
    });

    // Root component wraps the app with the provider
    function App() {
      return (
        <PipecatClientProvider client={client}>
          <VoiceBot />
          <PipecatClientAudio />
        </PipecatClientProvider>
      );
    }

    // Component using the client
    function VoiceBot() {
      const client = usePipecatClient();

      const handleClick = async () => {
        await client.connect({
          endpoint: `${process.env.PIPECAT_API_URL || "/api"}/connect`
        });
      };

      return (
        <button onClick={handleClick}>Start Conversation</button>;
      );
    }

    function EventListener() {
      useRTVIClientEvent(
        RTVIEvent.Connected,
        useCallback(() => {
          console.log("[EVENT] User connected");
        }, [])
      );
    }
    ```

    </CodeGroup>

</Tab>
<Tab title="Custom Messaging">
  Send custom messages and handle responses from your bot. This is useful for:
  - Running server-side functionality
  - Triggering specific bot actions
  - Querying the server
  - Responding to server requests

  <CodeGroup>

```javascript javascript
import { PipecatClient } from "@pipecat-ai/client-js";

const pcClient = new PipecatClient({
  transport: new DailyTransport(),
  callbacks: {
    onBotConnected: () => {
      pcClient
        .sendClientRequest("get-language")
        .then((response) => {
          console.log("[CALLBACK] Bot using language:", response);
          if (response !== preferredLanguage) {
            pcClient.sendClientMessage("set-language", {
              language: preferredLanguage,
            });
          }
        })
        .catch((error) => {
          console.error("[CALLBACK] Error getting language:", error);
        });
    },
    onServerMessage: (message) => {
      console.log("[CALLBACK] Received message from server:", message);
    },
  },
});
await pcClient.connect({
  url: "https://your-daily-room-url",
  token: "your-daily-token",
});
```

```jsx react
// Example: Messaging in a React application
import { useCallback } from "react";
import { RTVIEvent, TransportState } from "@pipecat-ai/client-js";
import { usePipecatClient, useRTVIClientEvent } from "@pipecat-ai/client-react";

function EventListener() {
  const pcClient = usePipecatClient();
  useRTVIClientEvent(
    RTVIEvent.BotConnected,
    useCallback(() => {
      pcClient
        .sendClientRequest("get-language")
        .then((response) => {
          console.log("[CALLBACK] Bot using language:", response);
          if (response !== preferredLanguage) {
            pcClient.sendClientMessage("set-language", {
              language: preferredLanguage,
            });
          }
        })
        .catch((error) => {
          console.error("[CALLBACK] Error getting language:", error);
        });
    }, [])
  );
  useRTVIClientEvent(
    RTVIEvent.ServerMessage,
    useCallback((data) => {
      console.log("[CALLBACK] Received message from server:", data);
    }, [])
  );
}
```

  </CodeGroup>

</Tab>
</Tabs>

## About RTVI

Pipecat's client SDKs implement the RTVI (Real-Time Voice and Video Inference) standard, an open specification for real-time AI inference. This means:

- Your code can work with any RTVI-compatible inference service
- You get battle-tested tooling for real-time multimedia handling
- You can easily set up development and testing environments

## Next Steps

Get started by trying out examples:

<CardGroup cols={2}>
  <Card
    title="Simple Chatbot Example"
    icon="robot"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    Complete client-server example with both bot backend (Python) and frontend
    implementation (JS, React, React Native, iOS, and Android).
  </Card>
  <Card
    title="More Examples"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-examples"
  >
    Explore our full collection of example applications and implementations
    across different platforms and use cases.
  </Card>
</CardGroup>



================================================
FILE: client/migration-guide.mdx
================================================
---
title: "RTVIClient Migration Guide"
description: "A Guide to migrating from RTVIClient to PipecatClient"
---

This guide will cover the high-level changes between the old `RTVIClient` and the new `PipecatClient`. For specific code updates, refer to the platform-specific migration guides.

## Key changes

- **Client Name**: The class name has changed from `RTVIClient` to `PipecatClient`.
- **Pipeline Connection**: Previously, the client expected a REST endpoint for gathering connection information as part of the constructor and was difficult to update or bipass. The new client expects connection information to be provided directly as part of the `connect()` method and can either be provided as an object with details your Transport requires or as an object with REST endpoint details for acquiring them.
- **Actions and helpers**: These have gone away in favor of some built-in methods for doing common actions like function call handling and appending to the llm context or in the case of custom actions, a simple set of methods for sending messages to the bot and handling responses. See `registerFunctionCallHandler()`, `appendToContext()`, `sendClientMessage()`, and `sendClientRequest()` for more details.
- **Bot Configuration**: This functionality as been removed as a security measure, so that a client cannot inherently have the ability to override a bot configuration and use credentials to its own whims. If you need the client to initialize or update the bot configuration, you will need to do so through an API call to your backend or building on top of the client-server messaging, which has now been made easier.

<Warning>
  The Client SDKs are currently in the process of making these changes. At this time, only the JavaScript and React libraries have been updated and released. Their corresponding documentation along with this top-level documentation has been updated to reflect the latest changes. The React Native, iOS, and Android SDKs are still in the process of being updated and their documentation will be updated and a migration guide provided once the new versions are released. If you have any questions or need assistance, please reach out to us on [Discord](https://discord.gg/pipecat).
</Warning>

## Migration guides

<CardGroup cols={2}>
  <Card
    title="JavaScript"
    icon="code"
    href="/client/js/migration-guide"
  >
    Migrate your JavaScript client code to the new `PipecatClient`
  </Card>
  <Card
    title="React"
    icon="react"
    href="/client/react/migration-guide"
  >
    Update your React components to use the new `PipecatClient`
  </Card>
</CardGroup>


================================================
FILE: client/rtvi-standard.mdx
================================================
---
title: "The RTVI Standard"
description: "An overview of the RTVI standard and its key features"
---

The RTVI (Real-Time Voice and Video Inference) standard defines a set of message types and structures sent between clients and servers. It is designed to facilitate real-time interactions between clients and AI applications that require voice, video, and text communication. It provides a consistent framework for building applications that can communicate with AI models and the backends running those models in real-time.

<Note>
This page documents version 1.0 of the RTVI standard, released in June 2025.
</Note>

## Key Features

<CardGroup cols={2}>
    <Card title="Connection Management" icon="plug">
        RTVI provides a flexible connection model that allows clients to connect to AI services and coordinate state.
    </Card>
    <Card title="Transcriptions" icon="text">
    The standard includes built-in support for real-time transcription of audio streams.
    </Card>
    <Card title="Client-Server Messaging" icon="comments">
    The standard defines a messaging protocol for sending and receiving messages between clients and servers, allowing for efficient communication of requests and responses.
    </Card>
    <Card title="Advanced LLM Interactions" icon="robot">
    The standard supports advanced interactions with large language models (LLMs), including context management, function call handline, and search results.
    </Card>
    <Card title="Service-Specific Insights" icon="lightbulb">
    RTVI supports events to provide insight into the input/output and state for typical services that exist in speech-to-speech workflows.
    </Card>
    <Card title="Metrics and Monitoring" icon="chart-line">
    RTVI provides mechanisms for collecting metrics and monitoring the performance of server-side services.
    </Card>
</CardGroup>

## Terms

- **Client**: The front-end application or user interface that interacts with the RTVI server.
- **Server**: The backend-end service that runs the AI framework and processes requests from the client.
- **User**: The end user interacting with the client application.
- **Bot**: The AI interacting with the user, technically an amalgamation of a large language model (LLM) and a text-to-speech (TTS) service.

## RTVI Message Format

The messages defined as part of the RTVI protocol adhere to the following format:

```json
{
    "id": string,
    "label": "rtvi-ai",
    "type": string,
    "data": unknown
}
```

<ParamField path='id' type='string' required={false}>
    A unique identifier for the message, used to correlate requests and responses.
</ParamField>
<ParamField path='label' type='string' required={true} default='rtvi-ai'>
    A label that identifies this message as an RTVI message. This field is required and should always be set to `'rtvi-ai'`.
</ParamField>
<ParamField path='type' type='string' required={true}>
    The type of message being sent. This field is required and should be set to one of the predefined RTVI message types listed below.
</ParamField>
<ParamField path='data' type='unknown' required={false}>
    The payload of the message, which can be any data structure relevant to the message type.
</ParamField>

## RTVI Message Types

Following the above format, this section describes the various message types defined by the RTVI standard. Each message type has a specific purpose and structure, allowing for clear communication between clients and servers.

<Tip>
Each message type below includes either a 🤖 or 🏄 emoji to denote whether the message is sent from the bot (🤖) or client (🏄).
</Tip>

### Connection Management

#### client-ready 🏄

Indicates that the client is ready to receive messages and interact with the server. Typically sent after the transport media channels have connected.

- **type**: `'client-ready'`
- **data**:
    - **version**: `string`
    
      The version of the RTVI standard being used. This is useful for ensuring compatibility between client and server implementations.

    - **about**: `AboutClient Object`
    
      An object containing information about the client, such as its rtvi-version, client library, and any other relevant metadata. The `AboutClient` object follows this structure:

      <Expandable title="AboutClient">
          <ParamField path="library" type="string" required="true"/>
          <ParamField path="library_version" type="string" />
          <ParamField path="platform" type="string" />
          <ParamField path="platform_version" type="string" />
          <ParamField path="platform_details" type="any">
            Any platform-specific details that may be relevant to the server. This could include information about the browser, operating system, or any other environment-specific data needed by the server. This field is optional and open-ended, so please be mindful of the data you include here and any security concerns that may arise from exposing sensitive or personal-identifiable information.
          </ParamField>
      </Expandable>

#### bot-ready 🤖

Indicates that the bot is ready to receive messages and interact with the client. Typically send after the transport media channels have connected.

- **type**: `'bot-ready'`
- **data**:
    - **version**: `string`
    
        The version of the RTVI standard being used. This is useful for ensuring compatibility between client and server implementations.
    
    - **about**: `any` (Optional)

        An object containing information about the server or bot. It's structure and value are both undefined by default. This provides flexibility to include any relevant metadata your client may need to know about the server at connection time, without any built-in security concerns. Please be mindful of the data you include here and any security concerns that may arise from exposing sensitive information.

#### disconnect-bot 🏄

Indicates that the client wishes to disconnect from the bot. Typically used when the client is shutting down or no longer needs to interact with the bot. Note: Disconnets should happen automatically when either the client or bot disconnects from the transport, so this message is intended for the case where a client may want to remain connected to the transport but no longer wishes to interact with the bot.

- **type**: `'disconnect-bot'`
- **data**: `undefined`

#### error 🤖

Indicates an error occurred during bot initialization or runtime.

- **type**: `'error'`
- **data**:
  - **message**: `string`
    
    Description of the error.

  - **fatal**: `boolean`
    
    Indicates if the error is fatal to the session.

### Transcription

#### user-started-speaking 🤖
Emitted when the user begins speaking
- **type**: `'user-started-speaking'`
- **data**: None

#### user-stopped-speaking 🤖
Emitted when the user stops speaking
- **type**: `'user-stopped-speaking'`
- **data**: None

#### bot-started-speaking 🤖
Emitted when the bot begins speaking
- **type**: `'bot-started-speaking'`
- **data**: None

#### bot-stopped-speaking 🤖
Emitted when the bot stops speaking
- **type**: `'bot-stopped-speaking'`
- **data**: None

#### user-transcription 🤖

Real-time transcription of user speech, including both partial and final results.

- **type**: `'user-transcription'`
- **data**:
  - **text**: `string`
    
    The transcribed text of the user.

  - **final**: `boolean`

    Indicates if this is a final transcription or a partial result.

  - **timestamp**: `string`
    
    The timestamp when the transcription was generated.

  - **user_id**: `string`
    
    Identifier for the user who spoke.

#### bot-transcription 🤖

Transcription of the bot's speech. Note: This protocol currently does not match the user transcription format to support real-time timestamping for bot transcriptions. Rather, the event is typically sent for each sentence of the bot's response. This difference is currently due to limitations in TTS services which mostly do not support (or support well), accurate timing information. If/when this changes, this protocol may be updated to include the necessary timing information. For now, if you want to attempt real-time transcription to match your bot's speaking, you can try using the `bot-tts-text` message type.

- **type**: `'bot-transcription'`
- **data**:
  - **text**: `string`
    
    The transcribed text from the bot, typically aggregated at a per-sentence level.

### Client-Server Messaging

#### server-message 🤖

An arbitrary message sent from the server to the client. This can be used for custom interactions or commands. This message may be coupled with the `client-message` message type to handle responses from the client.

- **type**: `'server-message'`
- **data**: `any`

  The `data` can be any JSON-serializable object, formatted according to your own specifications.

#### client-message 🏄

An arbitrary message sent from the client to the server. This can be used for custom interactions or commands. This message may be coupled with the `server-response` message type to handle responses from the server.

- **type**: `'client-message'`
- **data**:
    - **t**: `string`
    - **d**: `unknown` (optional)

  The data payload should contain a `t` field indicating the type of message and an optional `d` field containing any custom, corresponding data needed for the message.

#### server-response 🤖

An message sent from the server to the client in response to a `client-message`. **IMPORTANT**: The `id` should match the `id` of the original `client-message` to correlate the response with the request.

- **type**: `'client-message'`
- **data**:
    - **t**: `string`
    - **d**: `unknown` (optional)

  The data payload should contain a `t` field indicating the type of message and an optional `d` field containing any custom, corresponding data needed for the message.

#### error-response 🤖

Error response to a specific client message. **IMPORTANT**: The `id` should match the `id` of the original `client-message` to correlate the response with the request.

- **type**: `'error-response'`
- **data**:
    - **error**: `string`


### Advanced LLM Interactions

#### append-to-context 🏄

A message sent from the client to the server to append data to the context of the current llm conversation. This is useful for providing text-based content for the user or augmenting the context for the assistant.

- **type**: `'append-to-context'`
- **data**:
  - **role**: `"user"` | `"assistant"`

    The role the context should be appended to. Currently only supports `"user"` and `"assistant"`.

  - **content**: `unknown`

    The content to append to the context. This can be any data structure the llm understand.

  - **run_immediately**: `boolean` (optional)

    Indicates whether the context should be run immediately after appending. Defaults to `false`. If set to `false`, the context will be appended but not executed until the next llm run.

#### llm-function-call 🤖

A function call request from the LLM, sent from the bot to the client. Note that for most cases, an LLM function call will be handled completely server-side. However, in the event that the call requires input from the client or the client needs to be aware of the function call, this message/response schema is required.

- **type**: `'llm-function-call'`
- **data**:
  - **function_name**: `string`
    
    Name of the function to be called.

  - **tool_call_id**: `string`
    
    Unique identifier for this function call.

  - **args**: `Record<string, unknown>`
    
    Arguments to be passed to the function.

#### llm-function-call-result 🏄

The result of the function call requested by the LLM, returned from the client.

- **type**: `'llm-function-call-result'`
- **data**:
  - **function_name**: `string`
    
    Name of the called function.

  - **tool_call_id**: `string`
    
    Identifier matching the original function call.

  - **args**: `Record<string, unknown>`
    
    Arguments that were passed to the function.

  - **result**: `Record<string, unknown> | string`
    
    The result returned by the function.

#### bot-llm-search-response 🤖

Search results from the LLM's knowledge base.

<Note>
Currently, Google Gemini is the only LLM that supports built-in search. However, we expect other LLMs to follow suite, which is why this message type is defined as part of the RTVI standard. As more LLMs add support for this feature, the format of this message type may evolve to accommodate discrepancies.
</Note>

- **type**: `'bot-llm-search-response'`
- **data**:
  - **search_result**: `string` (optional)
    
    Raw search result text.

  - **rendered_content**: `string` (optional)
    
    Formatted version of the search results.

  - **origins**: `Array<Origin Object>`
    
    Source information and confidence scores for search results. The `Origin Object` follows this structure:
    ```json
    {
        "site_uri": string (optional),
        "site_title": string (optional),
        "results": Array<{
            "text": string,
            "confidence": number[]
        }>
    }
    ```

**Example:**

```json
"id": undefined
"label": "rtvi-ai"
"type": "bot-llm-search-response"
"data": {
    "origins": [
    {
        "results": [
            {
                "confidence": [0.9881149530410768],
                "text": "* Juneteenth: A Freedom Celebration is scheduled for June 18th from 12:00 pm to 2:00 pm."
            },
            {
                "confidence": [0.9692034721374512],
                "ext": "* A Juneteenth celebration at Fort Negley Park will take place on June 19th from 5:00 pm to 9:30 pm."
            }
        ],
        "site_title": "vanderbilt.edu",
        "site_uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHwif83VK9KAzrbMSGSBsKwL8vWfSfC9pgEWYKmStHyqiRoV1oe8j1S0nbwRg_iWgqAr9wUkiegu3ATC8Ll-cuE-vpzwElRHiJ2KgRYcqnOQMoOeokVpWqi"
    },
    {
        "results": [
            {
                "confidence": [0.6554043292999268],
                "text": "In addition to these events, Vanderbilt University is a large research institution with ongoing activities across many fields."
            }
        ],
        "site_title": "wikipedia.org",
        "site_uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESbF-ijx78QbaglrhflHCUWdPTD4M6tYOQigW5hgsHNctRlAHu9ktfPmJx7DfoP5QicE0y-OQY1cRl9w4Id0btiFgLYSKIm2-SPtOHXeNrAlgA7mBnclaGrD7rgnLIbrjl8DgUEJrrvT0CKzuo"
    }],
    "rendered_content": "<style>\n.container ... </div>\n</div>\n",
    "search_result": "Several events are happening at Vanderbilt University:\n\n* Juneteenth: A Freedom Celebration is scheduled for June 18th from 12:00 pm to 2:00 pm.\n* A Juneteenth celebration at Fort Negley Park will take place on June 19th from 5:00 pm to 9:30 pm.\n\n  In addition to these events, Vanderbilt University is a large research institution with ongoing activities across many fields.  For the most recent news, you should check Vanderbilt's official news website.\n"
}
```

### Service-Specific Insights

#### bot-llm-started 🤖
Indicates LLM processing has begun
- **type**: `bot-llm-started`
- **data**: None

#### bot-llm-stopped 🤖
Indicates LLM processing has completed
- **type**: `bot-llm-stopped`
- **data**: None

#### user-llm-text 🤖
Aggregated user input text that is sent to the LLM.
- **type**: `'user-llm-text'`
- **data**:
  - **text**: `string`
    
    The user's input text to be processed by the LLM.

#### bot-llm-text 🤖
Individual tokens streamed from the LLM as they are generated.
- **type**: `'bot-llm-text'`
- **data**:
  - **text**: `string`
    
    The token text from the LLM.

#### bot-tts-started 🤖
Indicates text-to-speech (TTS) processing has begun.
- **type**: `'bot-tts-started'`
- **data**: None

#### bot-tts-stopped 🤖
Indicates text-to-speech (TTS) processing has completed.
- **type**: `'bot-tts-stopped'`
- **data**: None

#### bot-tts-text 🤖
The per-token text output of the text-to-speech (TTS) service (what the TTS actually says).
- **type**: `'bot-tts-text'`
- **data**:
  - **text**: `string`
    
    The text representation of the generated bot speech.

### Metrics and Monitoring

#### metrics 🤖
Performance metrics for various processing stages and services. Each message will contain entries for one or more of the metrics types: `processing`, `ttfb`, `characters`.
- **type**: `'metrics'`
- **data**:
  - **processing**: [See Below] (optional)
    
    Processing time metrics.

  - **ttfb**:  [See Below] (optional)
    
    Time to first byte metrics.

  - **characters**:  [See Below] (optional)
    
    Character processing metrics.

For each metric type, the data structure is an array of objects with the following structure:

- **processor**: `string`

  The name of the processor or service that generated the metric.

- **value**: `number`

  The value of the metric, typically in milliseconds or character count.

- **model**: `string` (optional)

  The model of the service that generated the metric, if applicable.

**Example:**

```json
{
  "type": "metrics",
  "data": {
    "processing": [
      {
        "model": "eleven_flash_v2_5",
        "processor": "ElevenLabsTTSService#0",
        "value": 0.0005140304565429688
      }
    ],
    "ttfb": [
      {
        "model": "eleven_flash_v2_5",
        "processor": "ElevenLabsTTSService#0",
        "value": 0.1573178768157959
      }
    ],
    "characters": [
      {
        "model": "eleven_flash_v2_5",
        "processor": "ElevenLabsTTSService#0",
        "value": 43
      }
    ]
  }
}
```


================================================
FILE: client/android/api-reference.mdx
================================================
---
title: "API Reference"
url: "https://docs-android.rtvi.ai/"
---



================================================
FILE: client/android/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build Android applications with Pipecat's Kotlin client library"
---

The Pipecat Android SDK provides a Kotlin implementation for building voice and multimodal AI applications on Android. It handles:

- Real-time audio and video streaming
- Bot communication and state management
- Media device handling
- Configuration management
- Event handling

## Installation

Add the dependency for your chosen transport to your `build.gradle` file. For example, to use the Daily transport:

```gradle
implementation "ai.pipecat:daily-transport:0.3.3"
```

## Example

Here's a simple example using Daily as the transport layer. Note that the `clientConfig` is optional and depends
on what is required by the bot backend.

```kotlin
val clientConfig = listOf(
    ServiceConfig(
        service = "llm",
        options = listOf(
            Option("model", "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"),
            Option("messages", Value.Array(
                Value.Object(
                    "role" to Value.Str("system"),
                    "content" to Value.Str("You are a helpful assistant.")
                )
            ))
        )
    ),
    ServiceConfig(
        service = "tts",
        options = listOf(
            Option("voice", "79a125e8-cd45-4c13-8a67-188112f4dd22")
        )
    )
)

val callbacks = object : RTVIEventCallbacks() {
    override fun onBackendError(message: String) {
        Log.e(TAG, "Error from backend: $message")
    }
}

val options = RTVIClientOptions(
    services = listOf(ServiceRegistration("llm", "together"), ServiceRegistration("tts", "cartesia")),
    params = RTVIClientParams(baseUrl = "<your API url>", config = clientConfig)
)

val client = RTVIClient(DailyTransport.Factory(context), callbacks, options)
client.connect().await() // Using Coroutines

// Or using callbacks:
// client.start().withCallback { /* handle completion */ }
```

## Documentation

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="book"
    href="https://docs-android.rtvi.ai/"
  >
    Complete SDK API documentation
  </Card>

  <Card title="Daily Transport" icon="network-wired" href="./transports/daily">
    WebRTC implementation using Daily
  </Card>
</CardGroup>



================================================
FILE: client/android/transports/daily.mdx
================================================
---
title: "Daily WebRTC Transport"
description: "WebRTC implementation for Android using Daily"
---

The Daily transport implementation enables real-time audio and video communication in your Pipecat Android applications using [Daily's](https://www.daily.co/) WebRTC infrastructure.

## Installation

Add the Daily transport dependency to your `build.gradle`:

```gradle
implementation "ai.pipecat:daily-transport:0.3.7"
```

## Usage

Create a client using the Daily transport:

```kotlin
val callbacks = object : RTVIEventCallbacks() {
    override fun onBackendError(message: String) {
        Log.e(TAG, "Error from backend: $message")
    }
}

val options = RTVIClientOptions(
    services = listOf(ServiceRegistration("llm", "together"), ServiceRegistration("tts", "cartesia")),
    params = RTVIClientParams(baseUrl = "<your API url>", config = clientConfig)
)

val client = RTVIClient(DailyTransport.Factory(context), callbacks, options)
client.connect().await()
```

## Configuration

Your server endpoint should return Daily-specific configuration:

```json
{
  "url": "https://your-domain.daily.co/room-name",
  "token": "your-daily-token"
}
```

## Resources

<CardGroup cols={2}>
    <Card
        horizontal
        title="Demo"
        icon="play"
        href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot/client/android"
    >
        Simple Chatbot Demo
    </Card>

    <Card
        horizontal
        title="Source"
        icon="github"
        href="https://github.com/pipecat-ai/pipecat-client-android-transports/"
    >
        Client Transports
    </Card>

</CardGroup>

<Card
  title="Daily Transport Reference"
  icon="book-open"
  href="https://docs-android.rtvi.ai/"
>
  Complete API documentation for the Daily transport implementation
</Card>



================================================
FILE: client/android/transports/gemini-websocket.mdx
================================================
---
title: "Gemini Live Websocket Transport"
description: "Websocket implementation for Android using Gemini"
---

The Gemini Live Websocket transport implementation enables real-time audio communication with the Gemini Multimodal Live service, using a direct websocket connection.

<Note>
  Transports of this type are designed primarily for development and testing
  purposes. For production applications, you will need to build a server
  component with a server-friendly transport, like the
  [DailyTransport](./daily), to securely handle API keys.
</Note>

## Installation

Add the transport dependency to your `build.gradle`:

```gradle
implementation "ai.pipecat:gemini-live-websocket-transport:0.3.7"
```

## Usage

Create a client:

```kotlin
val transport = GeminiLiveWebsocketTransport.Factory(context)

val options = RTVIClientOptions(
    params = RTVIClientParams(
        baseUrl = null,
        config = GeminiLiveWebsocketTransport.buildConfig(
            apiKey = "<your Gemini api key>",
            generationConfig = Value.Object(
                "speech_config" to Value.Object(
                    "voice_config" to Value.Object(
                        "prebuilt_voice_config" to Value.Object(
                            "voice_name" to Value.Str("Puck")
                        )
                    )
                )
            ),
            initialUserMessage = "How tall is the Eiffel Tower?"
        )
    )
)

val client = RTVIClient(transport, callbacks, options)

client.start().withCallback {
    // ...
}
```

## Resources

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-android-gemini-live-websocket-demo"
  >
    Simple Chatbot Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-android-transports/"
  >
    Client Transports
  </Card>
</CardGroup>

<Card
  title="Pipecat Android Client Reference"
  icon="book-open"
  href="https://docs-android.rtvi.ai/"
>
  Complete API documentation for the Pipecat Android client.
</Card>



================================================
FILE: client/android/transports/openai-webrtc.mdx
================================================
---
title: "OpenAI Realtime WebRTC Transport"
description: "WebRTC implementation for Android using OpenAI"
---

The OpenAI Realtime WebRTC transport implementation enables real-time audio communication with the OpenAI Realtime service, using a direct WebRTC connection.

## Installation

Add the transport dependency to your `build.gradle`:

```gradle
implementation "ai.pipecat:openai-realtime-webrtc-transport:0.3.7"
```

## Usage

Create a client:

```kotlin
val transport = OpenAIRealtimeWebRTCTransport.Factory(context)

val options = RTVIClientOptions(
    params = RTVIClientParams(
        baseUrl = null,
        config = OpenAIRealtimeWebRTCTransport.buildConfig(
            apiKey = apiKey,
            initialMessages = listOf(
                LLMContextMessage(role = "user", content = "How tall is the Eiffel Tower?")
            ),
            initialConfig = OpenAIRealtimeSessionConfig(
                voice = "ballad",
                turnDetection = Value.Object("type" to Value.Str("semantic_vad")),
                inputAudioNoiseReduction = Value.Object("type" to Value.Str("near_field")),
                inputAudioTranscription = Value.Object("model" to Value.Str("gpt-4o-transcribe"))
            )
        )
    )
)

val client = RTVIClient(transport, callbacks, options)

client.start().withCallback {
    // ...
}
```

## Resources

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-android-openai-realtime-webrtc-demo"
  >
    Simple Chatbot Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-android-transports/"
  >
    Client Transports
  </Card>
</CardGroup>

<Card
  title="Pipecat Android Client Reference"
  icon="book-open"
  href="https://docs-android.rtvi.ai/"
>
  Complete API documentation for the Pipecat Android client.
</Card>



================================================
FILE: client/android/transports/small-webrtc.mdx
================================================
---
title: "Small WebRTC Transport"
description: "WebRTC implementation for Android"
---

The Small WebRTC transport implementation enables real-time audio communication with the Small WebRTC Pipecat transport, using a direct WebRTC connection.

## Installation

Add the transport dependency to your `build.gradle`:

```gradle
implementation "ai.pipecat:small-webrtc-transport:0.3.7"
```

## Usage

Create a client:

```kotlin
val transport = SmallWebRTCTransport.Factory(context, baseUrl)

val options = RTVIClientOptions(
    params = RTVIClientParams(baseUrl = null),
    enableMic = true,
    enableCam = true
)

val client = RTVIClient(transport, callbacks, options)

client.start().withCallback {
    // ...
}
```

## Resources

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/p2p-webrtc/video-transform/client/android"
  >
    Demo App
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-android-transports/"
  >
    Client Transports
  </Card>
</CardGroup>

<Card
  title="Pipecat Android Client Reference"
  icon="book-open"
  href="https://docs-android.rtvi.ai/"
>
  Complete API documentation for the Pipecat Android client.
</Card>



================================================
FILE: client/c++/api-reference.mdx
================================================
---
title: "API Reference"
url: "https://github.com/pipecat-ai/pipecat-client-cxx"
---



================================================
FILE: client/c++/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build native applications with Pipecat’s C++ client library"
---

The Pipecat C++ SDK provides a native implementation for building voice and multimodal AI applications. It supports:

- Linux (`x86_64` and `aarch64`)
- macOS (`aarch64`)
- Windows (`x86_64`)

## Dependencies

### libcurl

The SDK uses [libcurl](https://curl.se/libcurl/) for HTTP requests.

<Tabs>
  <Tab title="Linux">

```bash
sudo apt-get install libcurl4-openssl-dev
```

    </Tab>
  <Tab title="macOS">

On macOS `libcurl` is already included so there is nothing to install.

  </Tab>
  <Tab title="Windows">

On Windows we use [vcpkg](https://vcpkg.io/en/) to install dependencies. You
need to set it up following one of the
[tutorials](https://learn.microsoft.com/en-us/vcpkg/get_started/get-started).

The `libcurl` dependency will be automatically downloaded when building.

  </Tab>
</Tabs>

## Installation

Build the SDK using CMake:

<Tabs>
  <Tab title="Linux/macOS">
    ```bash
    cmake . -G Ninja -Bbuild -DCMAKE_BUILD_TYPE=Release
    ninja -C build
    ```
  </Tab>
  <Tab title="Windows">

    ```bash
    # Initialize Visual Studio environment
    "C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Auxiliary\Build\vcvarsall.bat" amd64
    # Configure and build
    cmake . -Bbuild --preset vcpkg
    cmake --buildbuild --config Release
    ```

  </Tab>
</Tabs>

### Cross-compilation

For Linux aarch64:

```bash
cmake . -G Ninja -Bbuild -DCMAKE_TOOLCHAIN_FILE=aarch64-linux-toolchain.cmake -DCMAKE_BUILD_TYPE=Release
ninja -C build
```

## Documentation

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="book"
    href="https://github.com/pipecat-ai/pipecat-client-cxx"
  >
    Complete SDK API documentation
  </Card>

  <Card title="Daily Transport" icon="network-wired" href="./transport">
    WebRTC implementation using Daily
  </Card>
</CardGroup>



================================================
FILE: client/c++/transport.mdx
================================================
---
title: "Daily WebRTC Transport"
description: "WebRTC implementation for C++ using Daily"
---

The Daily transport implementation enables real-time audio and video communication in your Pipecat C++ applications using [Daily's](https://www.daily.co/) WebRTC infrastructure.

## Dependencies

### Daily Core C++ SDK

Download the [Daily Core C++ SDK](https://github.com/daily-co/daily-core-sdk) from the [available releases](https://github.com/daily-co/daily-core-sdk/releases) for your platform and set:

```bash
export DAILY_CORE_PATH=/path/to/daily-core-sdk
```

### Pipecat C++ SDK

Build the base [Pipecat C++ SDK](https://github.com/pipecat-ai/pipecat-client-cxx-daily) first and set:

```bash
export PIPECAT_SDK_PATH=/path/to/pipecat-client-cxx
```

## Building

First, set a few environment variables:

```bash
PIPECAT_SDK_PATH=/path/to/pipecat-client-cxx
DAILY_CORE_PATH=/path/to/daily-core-sdk
```

Then, build the project:

<Tabs>
  <Tab title="Linux/macOS">

    ```bash
    cmake . -G Ninja -Bbuild -DCMAKE_BUILD_TYPE=Release
    ninja -C build
    ```

  </Tab>
  <Tab title="Windows">

    ```bash
    # Initialize Visual Studio environment
    "C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Auxiliary\Build\vcvarsall.bat" amd64

    # Configure and build
    cmake . -Bbuild --preset vcpkg
    cmake --build build --config Release
    ```

  </Tab>
</Tabs>

## Examples

<CardGroup cols={3}>
  <Card
    title="Basic Client"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-client-cxx-daily/blob/main/examples/c++"
  >
    Simple C++ implementation example
  </Card>

{" "}

<Card
  title="Audio Client"
  icon="waveform"
  href="https://github.com/pipecat-ai/pipecat-client-cxx-daily/blob/main/examples/c++-portaudio"
>
  C++ client with PortAudio support
</Card>

  <Card
    title="Node.js Server"
    icon="server"
    href="https://github.com/pipecat-ai/pipecat-client-cxx-daily/blob/main/examples/server"
  >
    Example Node.js proxy implementation
  </Card>
</CardGroup>



================================================
FILE: client/ios/api-reference.mdx
================================================
---
title: "API Reference"
url: "https://docs-ios.pipecat.ai/"
---



================================================
FILE: client/ios/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build iOS applications with Pipecat’s Swift client library"
---

The Pipecat iOS SDK provides a Swift implementation for building voice and multimodal AI applications on iOS. It handles:

- Real-time audio streaming
- Bot communication and state management
- Media device handling
- Configuration management
- Event handling

## Installation

Add the SDK to your project using Swift Package Manager:

```swift
// Core SDK
.package(url: "https://github.com/pipecat-ai/pipecat-client-ios.git", from: "0.3.0"),

// Daily transport implementation
.package(url: "https://github.com/pipecat-ai/pipecat-client-ios-daily.git", from: "0.3.0"),
```

Then add the dependencies to your target:

```swift
.target(name: "YourApp", dependencies: [
    .product(name: "PipecatClientIOS", package: "pipecat-client-ios")
    .product(name: "PipecatClientIOSDaily", package: "pipecat-client-ios-daily")
]),
```

## Example

Here's a simple example using Daily as the transport layer:

```swift
import PipecatClientIOS
import PipecatClientIOSDaily

let clientConfig = [
    ServiceConfig(
        service: "llm",
        options: [
            Option(name: "model", value: .string("meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo")),
            Option(name: "messages", value: .array([
                .object([
                    "role" : .string("system"),
                    "content": .string("You are a helpful assistant.")
                ])
            ]))
        ]
    ),
    ServiceConfig(
        service: "tts",
        options: [
            Option(name: "voice", value: .string("79a125e8-cd45-4c13-8a67-188112f4dd22"))
        ]
    )
]

let options = RTVIClientOptions.init(
    enableMic: true,
    params: RTVIClientParams(
        baseUrl: $PIPECAT_API_URL,
        config: clientConfig
    )
)

let client = RTVIClient.init(
    transport: DailyTransport.init(options: configOptions),
    options: configOptions
)
try await client.start()
```

## Documentation

<CardGroup cols={2}>
  <Card title="API Reference" icon="book" href="https://docs-ios.pipecat.ai/">
    Complete SDK API documentation
  </Card>
  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-ios/"
  >
    Pipecat Client iOS
  </Card>
</CardGroup>
<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot/client/ios"
  >
    Simple Chatbot Demo
  </Card>
  <Card title="Daily Transport" icon="network-wired" href="./transports/daily">
    WebRTC implementation using Daily
  </Card>
</CardGroup>



================================================
FILE: client/ios/transports/daily.mdx
================================================
---
title: "Daily WebRTC Transport"
description: "WebRTC implementation for iOS using Daily"
---

The Daily transport implementation enables real-time audio and video communication in your Pipecat iOS applications using [Daily's](https://www.daily.co/) WebRTC infrastructure.

## Installation

Add the Daily transport package to your project:

```swift
.package(url: "https://github.com/pipecat-ai/pipecat-client-ios-daily.git", from: "0.3.0")

// Add to your target dependencies
.target(name: "YourApp", dependencies: [
    .product(name: "PipecatClientIOSDaily", package: "pipecat-client-ios-daily")
])
```

## Usage

Create a client using the Daily transport:

```swift
import PipecatClientIOS
import PipecatClientIOSDaily

let configOptions = RTVIClientOptions.init(
    enableMic: true,
    params: RTVIClientParams(
        baseUrl: $PIPECAT_API_URL
    )
)

let client = RTVIClient.init(
    transport: DailyTransport.init(options: configOptions),
    options: configOptions
)

try await client.start()
```

## Configuration

Your server endpoint should return Daily-specific configuration:

```swift
// Example server response
{
    "url": "https://your-domain.daily.co/room-name",
    "token": "your-daily-token"
}
```

## API Reference

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot/client/ios"
  >
    Simple Chatbot Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-ios-daily/"
  >
    Daily Transport
  </Card>
</CardGroup>
<Card
  title="Daily Transport Reference"
  icon="book-open"
  href="https://docs-ios.pipecat.ai/PipecatClientIOSDaily/documentation/pipecatclientiosdaily"
>
  Complete API documentation for the Daily transport implementation
</Card>



================================================
FILE: client/ios/transports/gemini-websocket.mdx
================================================
---
title: "Gemini Live Websocket Transport"
description: "Websocket implementation for iOS using Gemini"
---

The Gemini Live Websocket transport implementation enables real-time audio communication with the Gemini Multimodal Live service, using a direct websocket connection.

<Note>
  Transports of this type are designed primarily for development and testing
  purposes. For production applications, you will need to build a server
  component with a server-friendly transport, like the
  [DailyTransport](./daily), to securely handle API keys.
</Note>

## Installation

Add the Gemini transport package to your project:

```swift
.package(url: "https://github.com/pipecat-ai/pipecat-client-ios-gemini-live-websocket.git", from: "0.3.1"),

// Add to your target dependencies
.target(name: "YourApp", dependencies: [
    .product(name: "PipecatClientIOSGeminiLiveWebSocket", package: "pipecat-client-ios-gemini-live-websocket")
],
```

## Usage

Create a client:

```swift
let options: RTVIClientOptions = .init(
    params: .init(config: [
        .init(
            service: "llm",
            options: [
                .init(name: "api_key", value: .string("<your Gemini api key>")),
                .init(name: "initial_messages", value: .array([
                    .object([
                        "role": .string("user"), // "user" | "system"
                        "content": .string("I need your help planning my next vacation.")
                    ])
                ])),
                .init(name: "generation_config", value: .object([
                    "speech_config": .object([
                        "voice_config": .object([
                            "prebuilt_voice_config": .object([
                                "voice_name": .string("Puck") // "Puck" | "Charon" | "Kore" | "Fenrir" | "Aoede"
                            ])
                        ])
                    ])
                ]))
            ]
        )
    ])
)

let client = GeminiLiveWebSocketVoiceClient(options: options)

try await client.start()
```

## API Reference

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-ios-gemini-live-websocket-demo"
  >
    Simple Chatbot Gemini Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-ios-gemini-live-websocket"
  >
    iOS Gemini Live WebSocket
  </Card>
</CardGroup>
<Card
  title="Pipecat iOS Client Reference"
  icon="book-open"
  href="https://docs-ios.pipecat.ai/PipecatClientIOSGeminiLiveWebSocket/documentation/pipecatclientiosgeminilivewebsocket"
>
  Complete API documentation for the Gemini transport implementation
</Card>



================================================
FILE: client/ios/transports/openai-webrtc.mdx
================================================
---
title: "OpenAIRealTimeWebRTCTransport"
---

## Overview

The OpenAI Realtime WebRTC transport implementation enables real-time audio communication directly with the [OpenAI Realtime API using WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc) voice-to-voice service.
It handles media device management, audio/video streams, and state management for the connection.

## Installation

Add the OpenAI transport package to your project:

```swift
.package(url: "https://github.com/pipecat-ai/pipecat-client-ios-openai-realtime.git", from: "0.0.1"),

// Add to your target dependencies
.target(name: "YourApp", dependencies: [
    .product(name: "PipecatClientIOSOpenAIRealtimeWebrtc", package: "pipecat-client-ios-openai-realtime")
],
```

## Usage

Create a client:

```swift
let rtviClientOptions = RTVIClientOptions.init(
    enableMic: currentSettings.enableMic,
    enableCam: false,
    params: .init(config: [
        .init(
            service: "llm",
            options: [
                .init(name: "api_key", value: .string(openaiAPIKey)),
                .init(name: "initial_messages", value: .array([
                    .object([
                        "role": .string("user"), // "user" | "system"
                        "content": .string("Start by introducing yourself.")
                    ])
                ])),
                .init(name: "session_config", value: .object([
                    "instructions": .string("You are Chatbot, a friendly and helpful assistant who provides useful information, including weather updates."),
                    "voice": .string("echo"),
                    "input_audio_noise_reduction": .object([
                        "type": .string("near_field")
                    ]),
                    "turn_detection": .object([
                        "type": .string("semantic_vad")
                    ])
                ])),
            ]
        )
    ])
)
self.rtviClientIOS = RTVIClient.init(
    transport: OpenAIRealtimeTransport.init(options: rtviClientOptions),
    options: rtviClientOptions
)
try await rtviClientIOS.start()
```

<Note type="warning">
  Currently, invalid session configurations will result in the OpenAI connection
  being failed.
</Note>

## API Reference

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-ios-openai-realtime-webrtc-demo"
  >
    Simple Chatbot OpenAI Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-ios-openai-realtime-webrtc"
  >
    iOS OpenAI Realtime WebRTC
  </Card>
</CardGroup>
<Card
  title="Pipecat iOS Client Reference"
  icon="book-open"
  href="https://docs-ios.pipecat.ai/PipecatClientIOSOpenAIRealtimeWebrtc/documentation/pipecatclientiosopenairealtimewebrtc"
>
  Complete API documentation for the OpenAI transport implementation
</Card>



================================================
FILE: client/js/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build web applications with Pipecat’s JavaScript client library"
---

The Pipecat JavaScript SDK provides a lightweight client implementation that handles:

- Device and media stream management
- Connecting to Pipecat bots
- Messaging with Pipecat bots and handling responses using the RTVI standard
- Managing session state and errors

## Installation

Install the SDK and a transport implementation (e.g. Daily for WebRTC):

```bash
npm install @pipecat-ai/client-js
npm install @pipecat-ai/[daily-transport, small-webrtc-transport, etc.]
```

## Example

Here's a simple example using Daily as the transport layer:

```javascript
import { PipecatClient } from "@pipecat-ai/client-js";
import { DailyTransport } from "@pipecat-ai/daily-transport";

// Handle incoming audio from the bot
function handleBotAudio(track, participant) {
  if (participant.local || track.kind !== "audio") return;

  const audioElement = document.createElement("audio");
  audioElement.srcObject = new MediaStream([track]);
  document.body.appendChild(audioElement);
  audioElement.play();
}

// Create and configure the client
const pcClient = new PipecatClient({
  transport: new DailyTransport(),
  enableMic: true,
  callbacks: {
    onTrackStart: handleBotAudio,
  },
});

// Connect to your bot
pcClient.connect({
    url: "https://your-daily-room-url",
    token: "your-daily-token"
  });
```

## Explore the SDK

<CardGroup cols={2}>
  <Card
    title="Client Constructor"
    icon="cube"
    href="/client/js/api-reference/client-constructor"
  >
    Configure your client instance with transport and callbacks
  </Card>
  <Card
    title="Client Methods"
    icon="code"
    href="/client/js/api-reference/client-methods"
  >
    Core methods for interacting with your bot
  </Card>
  <Card title="API Reference" icon="book" href="/client/js/api-reference">
    Detailed documentation of all available APIs
  </Card>
</CardGroup>

The Pipecat JavaScript SDK implements the [RTVI standard](/client/rtvi-standard) for real-time AI inference, ensuring compatibility with any RTVI-compatible server and transport layer.



================================================
FILE: client/js/migration-guide.mdx
================================================
---
title: "RTVIClient Migration Guide for JavaScript"
description: "A Guide to migrating from a JavaScript RTVIClient to PipecatClient"
---

This guide covers migrating from RTVIClient to the new `PipecatClient` in a JavaScript application. The new client introduces simplified configuration and improved client-server messaging. For an overview of the changes, see the top-level [RTVIClient Migration Guide](https://docs.pipecat.ai/client/migration-guide).

## Key Changes

### 1. Package and Class Names

**Old**
```javascript
import { RTVIClient } from '@pipecat-ai/client-js';
```
**New**
```javascript
// New
import { PipecatClient } from '@pipecat-ai/client-js';
```

### 2. Client and Transport Configuration

The core difference here is that the client no longer accepts a `params` field. The configuration that used to be provided here is no longer supported and endpoint information is now provided directly in the `connect()` method.

**Old**
```javascript
const transport = new DailyTransport();
const client = new RTVIClient({
  transport,
  params: {
    baseUrl: 'http://localhost:7860',
    endpoints: {
      connect: '/connect'
    }
  }
});
```
**New**
```javascript
const client = new PipecatClient({
  transport: new DailyTransport(),
  // Connection params moved to connect() call
});
```

### 3. Connection Method

Previously, `connect()` was called on the client instance without parameters. Now, you provide connection parameters directly in the `connect()` method. This allows for more flexibility and customization of the connection process. For ease of use, you can either provide an object that contains the connection parameters your `Transport` requires or a REST endpoint that returns the connection parameters.

**Old**

```javascript
await client.connect();
```

**New**

```javascript
await client.connect({
  endpoint: 'http://localhost:7860/connect',
  requestData: {
    // Any custom data your /connect endpoint requires
    llm_provider: 'openai',
    initial_prompt: "You are a pirate captain",
    // Any additional data
  }
});
```

### 4. Function Call Handling

Previously, you would use a helper class to handle function calls and provide a single callback to handle any/all function calls. Now, you can register a callback for each function call by name directly on the `PipecatClient` instance.

**Old**
```javascript
let llmHelper = new LLMHelper({});
llmHelper.handleFunctionCall(async (data) => {
  return await this.handleFunctionCall(data.functionName, data.arguments);
});
client.registerHelper('openai', llmHelper);
```
**New**
```javascript
client.registerFunctionCallHandler('functionName', async (data) => {
  // Handle function call
  return result;
});
```

### 5. Pipeline Configuration Initialization

Previously, you could provide a pipeline configuration as part of the `RTVIClient` constructor and it was expected to be in a specific format. Now, if you would like to pass any initial pipeline configurations, you do so as `requestData` added to the endpoint you provide to `connect()`. In both cases, you would need server-side code to parse and apply these settings, but now you can define the structure and what pieces of configuration you want to send.

**Old**
```javascript
const pipelineConfig = [{
  "service": "llm",
  "options": [
    {
      "name": "initial_messages",
      "value": [
        {
          "role": "system",
          "content": `You are a pirate captain.`
        }
      ]
    },
  ],
},
{
  "service": "tts",
  "options": [
    {
      "name": "language",
      "value": "en-US"
    }
  ]
}];

const rtviClient = new RTVIClient({
  ...
  params: {
    ...
    config: pipelineConfig
  },
});
rtviClient.connect();
```
**New**

Check out [this section of docs](./api-reference/messages#connection-time-configuration) for an example, complete with server-side code showing how to initialize the pipeline configuration at connection time.

```javascript
try {
  pcClient.connect({
    endpoint: 'https://your-server/connect',
    requestData: {
      initial_prompt: "You are a pirate captain",
      preferred_language: 'en-US'
    }
  });
} catch (error) {
  console.error("Error connecting to server:", error);
}
```

### 6. Pipeline Configuration Updates

Previously, the client supported updating the pipeline configuration using a specific method that took a configuration object in a generic format. Dynamic and predifined configuration updates, however, are a security concern, allowing clients to override settings and potentially abuse API keys. For this reason, it has been removed and most configuration updates need to be handled custom by your application. To do so, you should take advantage of the client-server messaging system, which allows you to send messages to the server and handle responses. This way, you can implement your own logic for updating configurations securely.

**Old**
```javascript
const updatedConfig = [{
  "service": "tts",
  "options": [
    {
      "name": "voice",
      "value": "Janice"
    }
  ]
}];
try {
  await rtviClient.updateConfig(
    updatedConfig as RTVIClientConfigOption[],
    true
  );
} catch (e) {
  console.error("Failed to update config", e);
}
```
**New**

Check out [this section of docs](./api-reference/messages#sending-custom-messages-to-the-server) for a more complete example, along with an example on making a request (`sendClientRequest()`) to wait for a response.

```javascript
try {
  pcClient.sendClientMessage('set-voice', { voice: 'Janice' });
} catch (error) {
  console.error("Error sending message to server:", error);
}
```

### 7. LLM Context Updates

Previously, you would use a helper class to update the context. This could be a security concern for the same reasons as mentioned above and should now be done using the [client-server messaging system](./api-reference/messages). However, in the case where you simply wish to add to the user's or assistant's context, you can use the [`appendToContext()`](./api-reference/client-methods#appendtocontext) method on the `PipecatClient` instance. This allows for easy, built-in support for text-based context updates without needing to implement custom messaging.

**Old**
```javascript
let llmHelper = new LLMHelper({});
client.registerHelper('llm', llmHelper);
await llmHelper.setContext(
  {
    messages: [
      {
        role: "user",
        content: "Tell me a joke.",
      },
    ],
  },
  true
);
```
**New**
```javascript
client.appendToContext({
  role: "user",\
  content: "Tell me a joke.",
  run_immediately: true
});
```


## Breaking Changes

1. **Configuration Structure**: Connection parameters are now passed to connect() instead of constructor
2. **Helper System**: Removed in favor of direct `PipecatClient` member functions or client-server messaging.

## Migration Steps

1. Update package imports to use new names
2. Move connection configuration from constructor to connect() method
3. Replace any helper classes with corresponding `PipecatClient` methods or custom messaging
4. Update any TypeScript types referencing old names


================================================
FILE: client/js/api-reference/callbacks.mdx
================================================
---
title: "Callbacks and events"
---

The Pipecat JavaScript client listens for messages and events from the bot via the transport layer. This allows you to respond to changes in state, errors, and other events. The client implements the RTVI standard for these communications.

## Event Handling Options

You can handle events in two ways:

### 1. Callbacks

Define handlers in the client constructor:

```typescript
const pcClient = new PipecatClient({
  callbacks: {
    onBotReady: () => console.log("Bot ready via callback"),
    // ... other callbacks
  },
});
```

### 2. Event Listeners

Add handlers using the event emitter pattern:

```typescript
pcClient.on(RTVIEvent.BotReady, () => console.log("Bot ready via event"));
```

<Note>
  Events and callbacks provide the same functionality. Choose the pattern that best fits your application's architecture.
</Note>

## Callbacks

### State and connectivity

<ParamField path="onConnected">
  Local user successfully established a connection to the transport.
</ParamField>

<ParamField path="onDisconnected">
  Local user disconnected from the transport, either intentionally by calling `pcClient.disconnect()` or due to an error.
</ParamField>

<ParamField path="onTransportStateChanged" type="state:TransportState">
  Provides a `TransportState` string representing the connectivity state of the local client. See [transports](./transports) for state explanation.
</ParamField>

<ParamField path="onBotReady" type="botReadyData:BotReadyData">
  The bot has been instantiated, its pipeline is configured, and it is receiving user media and interactions. This method is passed a `BotReadyData` object, which contains the RTVI `version` number. Since the bot is remote and may be using a different version of RTVI than the client, you can use the passed `version` string to check for compatibility.
</ParamField>

<ParamField path="onBotConnected">
  Bot connected to the transport and is configuring. Note: bot connectivity does not infer that its pipeline is yet ready to run. Please use `onBotReady` instead.
</ParamField>

<ParamField path="onBotDisconnected" type="participant: Participant">
  Bot disconnected from the transport. This may occur due to session expiry, a pipeline error or for any reason the server deems the session over.
</ParamField>

<ParamField path="onParticipantJoined" type="participant: Participant">
  A non-bot participant joined the session.
</ParamField>

<ParamField path="onParticipantLeft" type="participant: Participant">
  A non-bot participant left the session. Note: excluded local participant.
</ParamField>

### Messages and errors

<ParamField path="onServerMessage" type="data:any">
  Receives custom messages sent from the server to the client. This provides a generic channel for server-to-client communication. The data structure is flexible and defined by the server implementation.
</ParamField>

<ParamField path="onMessageError" type="message:RTVIMessage">
  Response error when an action fails or an unknown message type is sent from the client.
</ParamField>

<ParamField path="onError" type="message:RTVIMessage">
  Error signalled by the bot. This could be due to a malformed config update or an unknown action dispatch or the inability to complete a client request. The message parameter is of type `error` and matches [the RTVI standard](/client/rtvi-standard#error-%F0%9F%A4%96). Its `data` field includes a `message` string that describes the error and a `fatal` boolean indicating if the error is unrecoverable and resulted in a bot disconnection. If `fatal` is true, the client will automatically disconnect.
</ParamField>

### Media and devices

<ParamField path="onAvailableMicsUpdated" type="mics:MediaDeviceInfo[]">
  Lists available local media microphone devices. Triggered when a new device becomes available, a device is removed, or in response to `pcClient.initDevices()`.
</ParamField>

<ParamField path="onAvailableCamsUpdated" type="cams:MediaDeviceInfo[]">
  Lists available local media camera devices. Triggered when a new device becomes available, a device is removed, or in response to `pcClient.initDevices()`.
</ParamField>

<ParamField path="onAvailableSpeakersUpdated" type="speakers:MediaDeviceInfo[]">
  Lists available local speaker devices. Triggered when a new device becomes available, a device is removed, or in response to `pcClient.initDevices()`.
</ParamField>

<ParamField path="onMicUpdated" type="mic:MediaDeviceInfo">
  User selected a new microphone as their selected/active device.
</ParamField>

<ParamField path="onCamUpdated" type="cam:MediaDeviceInfo">
  User selected a new camera as their selected/active device.
</ParamField>

<ParamField path="onSpeakerUpdated" type="speaker:MediaDeviceInfo">
  User selected a new speaker as their selected/active device.
</ParamField>

<ParamField
  path="onTrackStarted"
  type="track: MediaStreamTrack, participant:Participant"
>
  Media track from a local or remote participant/bot was started and playable. Can be either an audio or video track.
</ParamField>

<ParamField
  path="onTrackStopped"
  type="track: MediaStreamTrack, participant:Participant"
>
  Media track from a local or remote participant/bot was stopped and no longer playable.
</ParamField>

<ParamField
  path="onScreenTrackStarted"
  type="track: MediaStreamTrack, participant:Participant"
>
  Media track from a local or remote participant's screenshare was started and playable. Can be either an audio or video track.
</ParamField>

<ParamField
  path="onScreenTrackStopped"
  type="track: MediaStreamTrack, participant:Participant"
>
  Media track from a local or remote participant's screenshare was stopped and no longer playable.
</ParamField>

### Audio and Voice Activity

<ParamField path="onLocalAudioLevel" type="level:number">
  Local audio gain level (0 to 1).
</ParamField>

<ParamField
  path="onRemoteAudioLevel"
  type="level: number, participant: Participant"
>
  Remote audio gain level (0 to 1). Note: if more than one participant is connected to the transport, the `participant` property details the associated peer/bot.
</ParamField>

<ParamField path="onBotStartedSpeaking">
  The bot started speaking/sending speech audio.
</ParamField>

<ParamField path="onBotStoppedSpeaking">
  The bot stopped speaking/sending speech audio.
</ParamField>

<ParamField path="onUserStartedSpeaking">
  The local user started speaking. This method is more reliable than using audio
  gain and is the result of the bot's VAD (voice activity detection) model. This
  provides a more accurate result in noisy environments.
</ParamField>

<ParamField path="onUserStoppedSpeaking">
  The local user stopped speaking, indicated by the VAD model.
</ParamField>

### Transcription

<ParamField path="onUserTranscript" type="TranscriptData">
  Transcribed local user input (both partial and final).

  Callback receives a `TranscriptData` object:

  <Expandable title="TranscriptData">
    <ParamField path="text" type="string">
      The transcribed text.
    </ParamField>
    <ParamField path="final" type="boolean">
      Indicates if the text is final (true) or partial (false).
    </ParamField>
    <ParamField path="timestamp" type="string">
      The timestamp of the transcription.
    </ParamField>
    <ParamField path="user_id" type="string">
      The ID of the user the transcription is for.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField path="onBotTranscript" type="text:BotLLMTextData">
  Finalized bot output text generated by the LLM. Sentence aggregated.
</ParamField>

### Service-specific Events

<ParamField path="onBotLlmSearchResponse" type="BotLLMSearchResponseData">
  Bot LLM search response text generated by the LLM service. This is typically used for search or retrieval tasks.
  <Warning>
  Search capabilities are currently only supported by Google Gemini. To take advantage of this event, your pipeline must include a [`GoogleLLMService`](/server/services/llm/gemini) and your pipeline task should include the [`GoogleRTVIObserver`](/server/frameworks/rtvi/google-rtvi-observer) in lieu of the typical `RTVIObserver`.
  </Warning>
  <Expandable title="BotLLMSearchResponseData">
    <ParamField path="search_result" type="string">
      The search result text.
    </ParamField>
    <ParamField path="rendered_content" type="string">
      The rendered content of the search result.
    </ParamField>
    <ParamField path="origins" type="LLMSearchOrigin[]">
      The origins of the search result.
      <Expandable title="LLMSearchOrigin">
        <ParamField path="site_uri" type="string">
          The URI of the site where the search result was found.
        </ParamField>
        <ParamField path="site_title" type="string">
          The title of the site where the search result was found.
        </ParamField>
        <ParamField path="results" type="LLMSearchResult[]">
          The individual search results.
          <Expandable title="LLMSearchResult">
            <ParamField path="text" type="string">
              The text of the search result.
            </ParamField>
            <ParamField path="confidence" type="number[]">
              The confidence scores for the search result.
            </ParamField>
          </Expandable>
        </ParamField>
      </Expandable>
    </ParamField>
  </Expandable>
</ParamField>

<ParamField path="onBotLlmText" type="BotLLMTextData">
  Streamed LLM token response text generated by the LLM service.
  <Expandable title="BotLLMTextData">
    <ParamField path="text" type="string">
      The text of the LLM response.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField path="onBotLlmStarted">LLM service inference started.</ParamField>

<ParamField path="onBotLlmStopped">LLM service inference concluded.</ParamField>

<ParamField path="onBotTtsText" type="BotTTSTextData">
  If your TTS service supports streamed responses over sockets, the text parameter contains the words from TTS service as they are spoken. If you are using a HTTP based TTS service, the text parameter will contain the full text of the TTS response.
  <Expandable title="BotTTSTextData" defaultOpen="true">
    <ParamField path="text" type="string">
      The text of the LLM response.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField path="onBotTtsStarted">TTS service started inference.</ParamField>

<ParamField path="onBotTtsStopped">TTS service inference concluded.</ParamField>

### Other

<ParamField path="onMetrics" default="data:PipecatMetricsData">
  Pipeline data provided by Pipecat. Please see [Pipecat
  documentation](https://docs.pipecat.ai) for more information.
</ParamField>

## Events

Each callback described above has a corresponding event that can be listened for using the `.on()` method. This allows you to handle the same functionality using either callbacks or event listeners, depending on your preferred architecture.

Here's the complete reference mapping events to their corresponding callbacks:

### State and connectivity Events

| Event Name              | Callback Name             | Data Type        |
| ----------------------- | ------------------------- | ---------------- |
| `Connected`             | `onConnected`             | -                |
| `Disconnected`          | `onDisconnected`          | -                |
| `TransportStateChanged` | `onTransportStateChanged` | `TransportState` |
| `BotReady`              | `onBotReady`              | `BotReadyData`   |
| `BotConnected`          | `onBotConnected`          | -                |
| `BotDisconnected`       | `onBotDisconnected`       | `Participant`    |
| `ParticipantConnected`  | `onParticipantJoined`     | `Participant`    |
| `ParticipantLeft`       | `onParticipantLeft`       | `Participant`    |

### Message and Error Events

| Event Name         | Callback Name        | Data Type                  |
| ------------------ | -------------------- | -------------------------- |
| `ServerMessage`    | `onServerMessage`    | `any`                      |
| `MessageError`     | `onMessageError`     | `RTVIMessage`              |
| `Error`            | `onError`            | `RTVIMessage`              |

### Media Events

| Event Name             | Callback Name            | Data Type                       |
| ---------------------- | ------------------------ | ------------------------------- |
| `TrackStarted`         | `onTrackStarted`         | `MediaStreamTrack, Participant` |
| `TrackStopped`         | `onTrackStopped`         | `MediaStreamTrack, Participant` |
| `AvailableMicsUpdated` | `onAvailableMicsUpdated` | `MediaDeviceInfo[]`             |
| `AvailableCamsUpdated` | `onAvailableCamsUpdated` | `MediaDeviceInfo[]`             |
| `MicUpdated`           | `onMicUpdated`           | `MediaDeviceInfo`               |
| `CamUpdated`           | `onCamUpdated`           | `MediaDeviceInfo`               |

### Audio Activity Events

| Event Name            | Callback Name           | Data Type             |
| --------------------- | ----------------------- | --------------------- |
| `LocalAudioLevel`     | `onLocalAudioLevel`     | `number`              |
| `RemoteAudioLevel`    | `onRemoteAudioLevel`    | `number, Participant` |
| `BotStartedSpeaking`  | `onBotStartedSpeaking`  | -                     |
| `BotStoppedSpeaking`  | `onBotStoppedSpeaking`  | -                     |
| `UserStartedSpeaking` | `onUserStartedSpeaking` | -                     |
| `UserStoppedSpeaking` | `onUserStoppedSpeaking` | -                     |

### Text and Transcription Events

| Event Name       | Callback Name      | Data Type        |
| ---------------- | ------------------ | ---------------- |
| `UserTranscript` | `onUserTranscript` | `TranscriptData` |
| `BotTranscript`  | `onBotTranscript`  | `BotLLMTextData` |
| `BotLlmText`     | `onBotLlmText`     | `BotLLMTextData` |
| `BotTtsText`     | `onBotTtsText`     | `BotTTSTextData` |

### Service State Events

| Event Name             | Callback Name             | Data Type                       |
| ---------------------- | ------------------------- | ------------------------------- |
| `BotLlmSearchResponse` | `onBotLlmSearchResponse`  | `BotLLMSearchResponseData`      |
| `BotLlmStarted`        | `onBotLlmStarted`         | -                               |
| `BotLlmStopped`        | `onBotLlmStopped`         | -                               |
| `BotTtsStarted`        | `onBotTtsStarted`         | -                               |
| `BotTtsStopped`        | `onBotTtsStopped`         | -                               |

### Other Events

| Event Name          | Callback Name         | Data Type               |
| ------------------- | --------------------- | ----------------------- |
| `Metrics`           | `onMetrics`           | `PipecatMetricsData`    |

## Usage Example

```typescript
import { PipecatClient, RTVIEvent } from "@pipecat-ai/client-js";

// Using callbacks
const pcClient = new PipecatClient({
  callbacks: {
    onBotReady: () => console.log("Bot ready via callback"),
    onUserTranscript: (data) => console.log("Transcript:", data.text),
  },
});

// Alternate approach: Using event listeners
pcClient.on(RTVIEvent.BotReady, () => {
  console.log("Bot ready via event");
});
```



================================================
FILE: client/js/api-reference/client-constructor.mdx
================================================
---
title: "PipecatClient Constructor"
description: "Setting up the PipecatClient"
---

```javascript
import { PipecatClient } from "@pipecat-ai/client-js";
```

`PipecatClient` is the primary component for building the client-side portion of a client-bot interaction. It is designed to work with various transport layers, such as WebRTC, WebSockets, or HTTP, allowing you to pick and choose the communication layer that best suits your application while maintaining a consistent API.

<Note>
  When initializing the `PipecatClient`, you must provide a transport instance to the constructor for your chosen protocol or provider. See [Transport](/client/js/transports) for more information. For the purpose of this guide, we'll demonstrate using the [Daily WebRTC transport](/client/js/transports/daily).
</Note>

## Example

```typescript
import { RTVIEvent, RTVIMessage, PipecatClient } from "@pipecat-ai/client-js";
import { DailyTransport } from "@pipecat-ai/daily-transport";

const PipecatClient = new PipecatClient({
  transport: new DailyTransport(),
  enableMic: true,
  enableCam: false,
  enableScreenShare: false,
  timeout: 15 * 1000,
  callbacks: {
    onConnected: () => {
      console.log("[CALLBACK] User connected");
    },
    onDisconnected: () => {
      console.log("[CALLBACK] User disconnected");
    },
    onTransportStateChanged: (state: string) => {
      console.log("[CALLBACK] State change:", state);
    },
    onBotConnected: () => {
      console.log("[CALLBACK] Bot connected");
    },
    onBotDisconnected: () => {
      console.log("[CALLBACK] Bot disconnected");
    },
    onBotReady: () => {
      console.log("[CALLBACK] Bot ready to chat!");
    },
  },
});
```

---

## API reference

### transport

<ParamField path="transport" type="Class<Transport>" default="undefined" required="true">
An instance of the `Transport` type you will use to connect to your bot service (`PipecatClient.connect()`). Transports implement the underlying device management, connectivity, media transmission, and state logic that manage the lifecycle of your session.

<Note>
As a best practice, we recommend you construct the transport inline in the client constructor, as opposed to holding a reference to it. Access to the transport is typically unnecessary. For advanced use cases that do require access to the transport, we recommend doing so via the `PipecatClient.transport` property, which provides some additional safeguards.
</Note>

```typescript
import { PipecatClient } from "@pipecat-ai/client-js";
import { DailyTransport } from "@pipecat-ai/daily-transport";

const pcClient = new PipecatClient({
  transport: new DailyTransport(),
});
```

</ParamField>

### callbacks

<ParamField path="callbacks" type="RTVIEventCallbacks" required="false">
  Map of callback functions. See [callbacks](./callbacks).
</ParamField>

### Media Initialization

<ParamField path="enableMic" type="boolean" default="true" required="false">
  Enable user's local microphone device.
</ParamField>

<ParamField path="enableCam" type="boolean" default="false"  required="false">
  Enable user's local webcam device. Note: Not all transports support video. Setting this value in that case will have no effect.
</ParamField>

<ParamField path="enableScreenShare" type="boolean" default="false"  required="false">
  Enable user's local screen share. Note: Not all transports support screen sharing. Setting this value in that case will have no effect.
</ParamField>



================================================
FILE: client/js/api-reference/client-methods.mdx
================================================
---
title: "Client Methods"
---

The Pipecat JavaScript client provides a comprehensive set of methods for managing bot interactions and media handling. These core methods are documented below.

## Session connectivity

### connect()

`async connect(connectParams): Promise<unknown>`

This method initiates the connection process, optionally passing parameters that your transport class requires to establish a connection or an endpoint to your server for obtaining those parameters.

<ParamField path="connectParams" type="TransportConnectionParams | ConnectionEndpoint" required="false">

  An object containing either the connection the `TransportConnectionParams` your Transport expects or an endpoint to obtain them.

  Check your transport class documentation for the expected shape of `TransportConnectionParams`. For example, the DailyTransport expects a `url` and `token`.

  When passing an endpoint, the client will make a request to the endpoint to obtain the connection parameters. The endpoint should return a JSON object with the necessary parameters for your transport. For Pipecat driven applications, it is common practice for this endpoint to also initialize the server-side bot process.

  The `ConnectionEndpoint` object should have the following shape:

    <Expandable title="ConnectionEndpoint" defaultOpen="true">
      <ParamField path="endpoint" type="string" required="true">
        The URL of the endpoint to connect to. This should be a valid REST endpoint.
      </ParamField>
        <ParamField path="headers" type="Headers" required="false">
          Optional headers to include in the request to the endpoint. This can be used to pass authentication tokens or other necessary headers.
        </ParamField>
        <ParamField path="requestData" type="object" required="false">
          Optional request data to include in the request to the endpoint. This can be used to pass additional data to your server-side endpoint. Oftentimes, this is used to pass the initial prompt or other configuration data to initialize the bot.
        </ParamField>
        <ParamField path="timeout" type="number" required="false">
          Optional timeout in milliseconds for the request to the endpoint.
        </ParamField>
    </Expandable>
</ParamField>

This method can be try / catched to handle errors at startup:

```typescript
try {
  await pcClient.connect({
    endpoint: "http://my-server/connect",
    requestData: {
      llm_provider: "openai",
      initial_prompt: "You are a pirate captain",
    },
  });
} catch (error) {
  console.error("Error connecting to the bot:", error);
}
```

During the connection process, the transport state will transition through the following states: "authenticating", "authenticated", "connecting", "connected", "ready". If you provide the transport connection parameters directly without an endpoint, the transport will skip the authentication states.

<Note>
  Calling `connect()` asynchronously will resolve when the bot and client signal
  that they are ready. See [messages and events](./messages). If you want to
  call `connect()` without `await`, you can use the `onBotReady` callback or
  `BotReady` event to know when you can interact with the bot.
</Note>

<Warning>
  Attempting to call `connect()` when the transport is already in a 'connected' or 'ready'
  state will throw an error. You should [disconnect](#disconnect) from a session
  first before attempting to connect again.
</Warning>

### disconnect()

`async disconnect(): Promise<void>`

Disconnects from the active session. The transport state will transition to "disconnecting" and then "disconnected".

It is common practice for bots to exit and cleanup when the client disconnects.

```typescript
await pcClient.disconnect();
```

### disconnectBot()

`disconnectBot(): void`

Triggers the bot to disconnect from the session, leaving the client connected.

```typescript
await pcClient.disconnectBot();
```

## Messages

Custom messaging between the client and the bot. This is useful for sending data to the bot, triggering specific actions, reacting to server events, or querying the server.

For more, see: [messages and events](./messages).

### sendClientMessage()

`sendClientMessage(msgType: string, data?: unknown): void`

Sends a custom message to the bot and does not expect a response. This is useful for sending data to the bot or triggering specific actions.

<ParamField path="msgType" type="string" required="true">
  A string identifying the message.
</ParamField>

<ParamField path="data" type="unknown" required="false">
  Optional data to send with the message. This can be any JSON-serializable object.
</ParamField>

### sendClientRequest()

`async sendClientRequest(msgType: string, data: unknown, timeout?: number): Promise<unknown>`

Sends a custom request to the bot and expects a response. This is useful for querying the server or triggering specific actions that require a response. The method returns a Promise that resolves with the data from response.

<ParamField path="msgType" type="string" required="true">
  A string identifying the message.
</ParamField>

<ParamField path="data" type="unknown" required="false">
  Optional data to send with the message. This can be any JSON-serializable object.
</ParamField>

<ParamField path="timeout" type="number" required="false" default="10000">
  Optional timeout in milliseconds for the request. If the request does not receive a response within this time, it will reject with an RTVIMessage of type `'error-response'`.
</ParamField>

## Devices

### initDevices()

`async initDevices(): Promise<void>`

Initializes the media device selection machinery, based on `enableCam`/`enableMic` selections and defaults (i.e. turns on the local cam/mic). This method can be called before `connect()` to test and switch between camera and microphone sources.

```typescript
await pcClient.initDevices();
```

### getAllMics()

`async getAllMics(): Promise<MediaDeviceInfo[]>`

Returns a list of available microphones in the form of [`MediaDeviceInfo[]`](https://developer.mozilla.org/en-US/docs/Web/API/MediaDeviceInfo).

```typescript
mic_device_list = pcClient.getAllMics();
```

### getAllCams()

`async getAllCams(): Promise<MediaDeviceInfo[]>`

Returns a list of available cameras in the form of [`MediaDeviceInfo[]`](https://developer.mozilla.org/en-US/docs/Web/API/MediaDeviceInfo).

```typescript
cam_device_list = pcClient.getAllCams();
```

### getAllSpeakers()

`async getAllSpeakers(): Promise<MediaDeviceInfo[]>`

Returns a list of available speakers in the form of [`MediaDeviceInfo[]`](https://developer.mozilla.org/en-US/docs/Web/API/MediaDeviceInfo).

```typescript
speaker_device_list = pcClient.getAllSpeakers();
```

### selectedMic

`selectedMic: MediaDeviceInfo | {}`

The currently selected microphone, represented as a `MediaDeviceInfo` object. If no microphone is selected, it returns an empty object.

```typescript
current_mic = pcClient.selectedMic;
```

### selectedCam

`selectedCam: MediaDeviceInfo | {}`

The currently selected camera, represented as a `MediaDeviceInfo` object. If no camera is selected, it returns an empty object.

```typescript
current_cam = pcClient.selectedCam;
```

### selectedSpeaker

`selectedSpeaker: MediaDeviceInfo | {}`

The currently selected speaker, represented as a `MediaDeviceInfo` object. If no speaker is selected, it returns an empty object.

```typescript
current_speaker = pcClient.selectedSpeaker;
```

### updateMic()

`updateMic(micId: string): void`

Switches to the microphone identified by the provided `micId`, which should match a `deviceId` in the list returned from [`getAllMics()`](#getAllMics).

<ParamField path="micId" type="string">
  deviceId
</ParamField>

```typescript
pcClient.updateMic(deviceId);
```

### updateCam()

`updateCam(camId: string): void`

Switches to the camera identified by the provided `camId`, which should match a `deviceId` in the list returned from [`getAllCams()`](#getAllCams).

<ParamField path="camId" type="string">
  deviceId
</ParamField>

```typescript
pcClient.updateCam(deviceId);
```

### updateSpeaker()

`updateSpeaker(speakerId: string): void`

Switches to the speaker identified by the provided `speakerId`, which should match a `deviceId` in the list returned from [`getAllSpeakers()`](#getAllSpeakers).

<ParamField path="speakerId" type="string">
  deviceId
</ParamField>

```typescript
pcClient.updateSpeaker(deviceId);
```

### enableMic(enable: boolean)

`enableMic(enable: boolean): void`

Turn on or off (unmute or mute) the client mic input.

<ParamField path="enable" type="boolean" required="true">
  A boolean indicating whether to enable (`true`) or disable (`false`) the microphone.
</ParamField>

```typescript
pcClient.enableMic(true);
```

### enableCam(enable: boolean)

`enableCam(enable: boolean): void`

Turn on or off the client cam input.

<ParamField path="enable" type="boolean" required="true">
  A boolean indicating whether to enable (`true`) or disable (`false`) the camera.
</ParamField>

```typescript
pcClient.enableCam(true);
```

### enableScreenShare(enable: boolean)

`enableScreenShare(enable: boolean): void`

Start a screen share from the client's device.

<ParamField path="enable" type="boolean" required="true">
  A boolean indicating whether to enable (`true`) or disable (`false`) screen sharing.
</ParamField>

```typescript
pcClient.enableScreenShare(true);
```

### isMicEnabled

`isMicEnabled: boolean`

An accessor to determine if the client's microphone is enabled.

```typescript
mic_enabled = pcClient.isMicEnabled;
```

### isCamEnabled

`isCamEnabled: boolean`

An accessor to determine if the client's camera is enabled.

```typescript
cam_enabled = pcClient.isCamEnabled;
```

### isSharingScreen

An accessor to determine if the client is sharing their screen.

```typescript
screen_sharing = pcClient.isSharingScreen;
```

## Tracks (audio and video)

### tracks()

`tracks(): Tracks`

Returns a `Tracks` object with available `MediaStreamTrack` objects for both the client and the bot.

```typescript
live_tracks_list = pcClient.tracks()
```

**Tracks Type**
```typescript
{
  local: {
    audio?: MediaStreamTrack;
    video?: MediaStreamTrack;
  },
  bot?: {
    audio?: MediaStreamTrack;
    video?: MediaStreamTrack;
  }
}
```

## Advanced LLM Interactions

### appendToContext()

`async appendToContext(context: LLMContextMessage): boolean`

A method to append data to the bot's context. This is useful for providing additional information or context to the bot during the conversation.

<ParamField path="context" type="LLMContextMessage" required="true">
  The context to append. This should be an object with the following shape:
  <Expandable title="LLMContextMessage" defaultOpen="true">
    <ParamField path="role" type="string" required="true">
      The role to append the context to. Currently only "user" or "assistant" are supported.
    </ParamField>
    <ParamField path="content" type="unknown" required="true">
      The content to append to the context. This can be any JSON-serializable object.
    </ParamField>
    <ParamField path="run_immediately" type="boolean" required="false" default="false">
      Whether to immediately run the bot with the updated context. If `false`, the context will be updated but the bot will not be run until the next message or action that triggers the bot to run (like the user speaking).
    </ParamField>
  </Expandable>
</ParamField>

### registerFunctionCallHandler()

`registerFunctionCallHandler(functionName: string, callback: FunctionCallCallback): void`

Registers a function call handler that will be called when the bot requests a function call. This is useful for when the server-side function handler needs information from the client to execute the function call or when the client needs to perform some action based on the running of function call.

<ParamField path="functionName" type="string" required="true">
  The name of the function to handle. This should match the function name in the bot's context.
</ParamField>
<ParamField path="callback" type="FunctionCallCallback" required="true">
  `type FunctionCallCallback = (fn: FunctionCallParams) => Promise<LLMFunctionCallResult | void>`

  The callback function to call when the bot sends a function call request. This function should accept the following parameters:

  <Expandable title="FunctionCallParams" defaultOpen="true">
    <ParamField path="functionName" type="string" required="true">
      The name of the function being called. It should always match the name you registered the handler under.
    </ParamField>
    <ParamField path="arguments" type="Record<string, unknown>" required="true">
      The arguments passed to the function call. This is a key-value object where the keys are the argument names and the values are the argument values.
    </ParamField>
  </Expandable>

  The callback should return a Promise that resolves with the result of the function call or void if no result is needed. If returning a result, it should be a `string` or `Record<string, unknown>`.
</ParamField>

## Other

### transport

`transport: Transport`

A safe accessor for the transport instance used by the client. This is useful for accessing transport-specific methods or properties that are not exposed directly on the client.

```typescript
const transport = pcClient.transport as DailyTransport;
transport.getSessionInfo();
```

### setLogLevel()

`setLogLevel(level: LogLevel): void`

Sets the log level for the client. This is useful for debugging and controlling the verbosity of logs. The log levels are defined in the `LogLevel` enum:

```typescript
export enum LogLevel {
  NONE = 0,
  ERROR = 1,
  WARN = 2,
  INFO = 3,
  DEBUG = 4,
}
```

By default, the log level is set to `LogLevel.DEBUG`.

```typescript
pcClient.setLogLevel(LogLevel.INFO);
```


================================================
FILE: client/js/api-reference/errors.mdx
================================================
---
title: "Errors"
---

## RTVIError Type

Base `PipecatClient` error type, extends [`Error`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error) and primarily introduces the `status` field. Most methods will try to throw an error of this type when something goes wrong. This is different from the RTVI error event and its corresponding [`onError` callback](./callbacks#param-on-error), which are used for communicating errors that are sent by the bot.

<ParamField path="status" type="number" required="false">
A unique identifier (or HTTP code if applicable) for the error.
</ParamField>

<ParamField path="message" type="string" required="false">
A human-readable message describing the error.
</ParamField>

## Pre-defined RTVIErrors

### ConnectionTimeoutError

Emitted when the bot does not enter a ready state within the specified timeout period during the `connect()` method call.

### StartBotError

Emitted when the `connect()` method is called with an endpoint, and the endpoint responds with an error or the `fetch` itself fails. This may be due to the endpoint being unavailable, or the server failing to parse the provided data.

<ParamField path="error" type="string" required="true">
All `StartBotError` instances will have an `error` field set to `invalid-request-error`.
</ParamField>

<ParamField path="status" type="number" required="false">
HTTP status code returned by the endpoint, if applicable.
</ParamField>

<ParamField path="message" type="string" required="false">
Verbose error message returned by the endpoint, if provided. To take advantage of this, the endpoint should return an error response with a JSON object with an `info` field containing the error message.
</ParamField>

### TransportStartError

Emitted when the Transport is not able to connect. You may need to check the connection parameters provided or returned from you endpoint.

### BotNotReadyError

Emitted when the client attempts to perform an action or method that requires the bot to be in a ready state, but the bot is not ready. You must call `connect()` first and wait for the bot to be ready before performing such actions.

### UnsupportedFeatureError

Not all Transports are created equal, and some may not support certain features. This error is thrown when a feature is requested that the current Transport does not support.

<ParamField path="feature" type="string" required="true">
This custom field will contain the name of the unsupported feature.
</ParamField>



================================================
FILE: client/js/api-reference/messages.mdx
================================================
---
title: "Custom Messaging"
---

The Pipecat JavaScript client can send and receive arbitrary messages to/from the server running the bot. This page outlines and demonstrates both client and server code for passing and responding to custom messages as well as providing arbitrary data at connection time.

## Connection-Time Configuration

Oftentimes clients need to provide configuration data to the server when connecting. This can include things like preferred language, user preferences, initial messages, or any other data that the server needs to know about the client. This must occur before the bot is started and therefore not part of the RTVI standard, but rather a custom implementation. That said, the `PipecatClient` makes it easy to send this data as part of the `connect()` method, by passing an object with the `requestData` property. Your server endpoint can then handle this data as needed. In the example below, we demonstrate sending an initial prompt and preferred language to the server when connecting.

<CodeGroup>
```javascript client
try {
    pcClient.connect({
        endpoint: 'https://your-server/connect',
        requestData: {
            initial_prompt: "You are a pirate captain",
            preferred_language: 'en-US'
        }
    });
} catch (error) {
    console.error("Error connecting to server:", error);
}
```

```python FastAPI endpoint

def validate_request_data(body: Dict[str, Any]) -> Tuple[str, str]:
    """Validate and extract prompt and language from request data."""
    if not isinstance(body, dict):
        raise ValueError("Request body must be a dictionary")
    
    prompt = body.get("initial_prompt", "You are a pirate captain")
    lang = body.get("preferred_language", "en-US")
    
    if not isinstance(prompt, str) or not isinstance(lang, str):
        raise ValueError("Both initial_prompt and preferred_language must be strings")
    
    return prompt, lang

@app.post("/connect")
async def rtvi_connect(request: Request) -> Dict[Any, Any]:
    """RTVI connect endpoint that creates a room and returns connection credentials.

    This endpoint is called by RTVI clients to establish a connection.

    Returns:
        Dict[Any, Any]: Authentication bundle containing room_url and token

    Raises:
        HTTPException: If room creation, token generation, or bot startup fails
    """
    body = await request.json()
    try:
        prompt, lang = validate_request_data(body)
    except ValueError as e:
        raise HTTPException(status_code=400, info=f"Invalid request data: {e}")

    print("Creating room for RTVI connection", body)
    room_url, token = await create_room_and_token()
    print(f"Room URL: {room_url}")

    # Start the bot process
    try:
        proc = subprocess.Popen(
            [f"python3 -m bot -u {room_url} -t {token} -p {prompt} -l {lang}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
        bot_procs[proc.pid] = (proc, room_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Return the authentication bundle in format expected by DailyTransport
    return {"url": room_url, "token": token}
```

```python bot
import argparse
import asyncio

def extract_arguments():
    parser = argparse.ArgumentParser(description="Example")
    parser.add_argument(
        "-u", "--room-url", type=str, default=os.getenv("DAILY_SAMPLE_ROOM_URL", "")
    )
    parser.add_argument(
        "-t", "--token", type=str, default=os.getenv("DAILY_SAMPLE_ROOM_TOKEN", None)
    )
    parser.add_argument(
        "-p", "--prompt", type=str, default="You are a pirate captain"
    )
    parser.add_argument("-l", "--language", type=str, default="en-US")
    return parser.parse_args()

async def main():
    args = extract_arguments()
    print(f"room_url: {args.room_url}")

    daily_transport = DailyTransport(
        args.room_url,
        args.token,
        "Chatbot",
        DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    llm = GeminiMultimodalLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",
        transcribe_user_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
        params=InputParams(
            language=args.language,  # Pass preferred language to LLM
        ),
    )

    messages = [ { role: "system", content: args.prompt } ]
    context = OpenAILLMContext(messages)
    context_aggregator = llm.create_context_aggregator(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]), transport=daily_transport)

    pipeline = Pipeline(
        [
            daily_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,
            daily_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(allow_interruptions=True),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.debug(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)

if __name__ == "__main__":
    asyncio.run(main())
```

</CodeGroup>

## Sending Custom Messages to the Server

Once connected, you can send custom messages to the server using the `sendClientMessage` method. This is useful for triggering specific actions or sending data that the server needs to process.

<CodeGroup>
```javascript client
try {
    pcClient.sendClientMessage('set-language', { language: 'en-US' });
} catch (error) {
    console.error("Error sending message to server:", error);
}
```

```python bot
    task = PipelineTask(
        pipeline,
        params,
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_message")
    async def on_client_message(rtvi, msg):
        print("RTVI client message:", msg.type, msg.data)
        if msg.type == "set-language":
            language = msg.data.get("language", "en-US")
            await task.queue_frames([context_aggregator.user().get_context_frame()])
        # Kick off the conversation
        await task.queue_frames([context_aggregator.user().get_context_frame()])

### Alternatively, if your message requires asynchronous processing or storing of
### state, you may want to handle it from inside a FrameProcessor, listen for a
### RTVIClientMessageFrame and push a RTVIServerResponseFrame
class CustomFrameProcessor(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        if isinstance(frame, RTVIClientMessageFrame):
            print("RTVI client message:", frame.msg_id, frame.type, frame.data)
            if frame.type == "set-language":
                language = frame.data.get("language", "en-US")
                await self.push_frame(STTUpdateSettingsFrame(language=language))
                return
        await self.push_frame(frame, direction)
```
</CodeGroup>

## Requesting Information from the Server

You can also request information from the server using the `sendClientRequest` method. This is useful for querying the server for specific data or triggering and action and getting a success/failure response.

<CodeGroup>
```javascript client
try {
    const response = await pcClient.sendClientRequest('get-language');
    console.log("Current language:", response.language);
} catch (error) {
    console.error("Error requesting data from server:", error);
}
```

```python bot
@rtvi.event_handler("on_client_message")
async def on_client_message(rtvi, msg):
    print("RTVI client message:", msg.type, msg.data)
    if msg.type == "get-language":
        await rtvi.send_server_response(msg, {"language": get_current_language()})
    else:
        await rtvi.send_error_response(msg, "Unknown request type")

### Alternatively, if your message requires asynchronous processing or storing of
### state, you may want to handle it from inside a FrameProcessor, listen for a
### RTVIClientMessageFrame and push a RTVIServerResponseFrame
class CustomFrameProcessor(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        if isinstance(frame, RTVIClientMessageFrame):
            print("RTVI client message:", frame.msg_id, frame.type, frame.data)
            if frame.type == "get-language":
                data = {"language": get_current_language()}
                await self.push_frame(
                    RTVIServerResponseFrame(
                        client_msg=frame,
                        data=data,
                    ),
                )
                return
            else:
                await self.push_frame(
                    RTVIServerResponseFrame(
                        client_msg=frame,
                        error="Unknown request type"
                    )
                )
        await self.push_frame(frame, direction)
```
</CodeGroup>

## Handling Custom Messages from the Server

You can handle custom messages sent from the server using the `onServerMessage` callback. This allows you to process messages that the server sends back to the client, such as notifications or updates.

<CodeGroup>
```javascript client
pcClient.onServerMessage((message) => {
    console.log("Received message from server:", message);
    if (message.data.msg === 'language-updated') {
        console.log("Language updated to:", message.data.language);
    }
});
```

```python bot
## From inside an Observer, call `send_server_message` directly on your rtvi instance
class CustomObserver(BaseObserver):
    async def on_push_frame(self, data: FramePushed):
        if isinstance(frame, STTUpdateSettingsFrame):
            for key, value in settings.items():
                if key == "language":
                    await rtvi.send_server_message({
                        "msg": "language-updated",
                        "language": value
                    })

### Alternatively, from inside a FrameProcessor, push a RTVIServerMessageFrame
class CustomFrameProcessor(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        if isinstance(frame, STTUpdateSettingsFrame):
            for key, value in settings.items():
                if key == "language":
                    await self.push_frame(
                        RTVIServerMessageFrame(
                            data={
                                "msg": "language-updated",
                                "language": value
                            }
                        )
                    )
        await self.push_frame(frame, direction)
```
</CodeGroup>


================================================
FILE: client/js/transports/daily.mdx
================================================
---
title: "Daily WebRTC Transport"
---

The DailyTransport class provides a WebRTC transport layer using [Daily.co's](https://daily.co) infrastructure. It wraps a Daily-JS call client to handle audio/video device management, WebRTC connections, and real-time communication between clients and bots. For complete documentation on Daily's API, see the [Daily API Reference](https://docs.daily.co/reference/daily-js).

This transport is designed for production use cases, leveraging Daily's global infrastructure for low-latency, high-quality audio and video streaming. It expects your Pipecat server to include the corresponding [`DailyTransport` server-side](/server/services/transport/daily) implementation.

## Usage

### Basic Setup

```javascript
import { PipecatClient } from "@pipecat-ai/client-js";
import { DailyTransport } from "@pipecat-ai/daily-transport";

const pcClient = new PipecatClient({
  transport: new DailyTransport({
    // DailyTransport constructor options
    bufferLocalAudioUntilBotReady: true, // Optional, defaults to false
    inputSettings: { video: { processor: { type: "background-blur" } } },
  }),
  enableCam: false, // Default camera off
  enableMic: true, // Default microphone on
  callbacks: {
    // Event handlers
  },
  // ...
});

await pcClient.connect({
  endpoint: "https://your-server/connect",
});
```

## API Reference

### Constructor Options

```typescript
interface DailyTransportConstructorOptions extends DailyFactoryOptions {
  bufferLocalAudioUntilBotReady?: boolean;
}
```

<ParamField path="bufferLocalAudioUntilBotReady" type="boolean" default="false" required="false">

If set to `true`, the transport will buffer local audio until the bot is ready. This is useful for ensuring that bot gets any audio from the user that started before the bot is ready to process it.

</ParamField>

<ParamField path="DailyFactoryOptions">

The `DailyTransportConstructorOptions` extends the `DailyFactoryOptions` type that is accepted by the underlying Daily instance. These options are passed directly through to the Daily constructor. See the [Daily API Reference](https://docs.daily.co/reference/daily-js/daily-call-client/properties) for a complete list of options.

<Note>
  While you can provide the room url and optional token as part of your
  constructor options, the typical pattern is to provide them via a connection
  endpoint or directly as part of `connect()`. See below.
</Note>

</ParamField>

### TransportConnectionParams

On `connect()`, the `DailyTransport` optionally takes a set of [`DailyCallOptions`](https://docs.daily.co/reference/daily-js/daily-call-client/methods#dailycalloptions) to connect to a Daily room. This can be provided directly or via a connection endpoint passed to the `PipecatClient`'s connect method. If using an endpoint, your endpoint should return a JSON object matching the `DailyCallOptions` type. See the [client connect()](https://docs.pipecat.ai/client/js/api-reference/client-methods#connect) documentation for more information.

<CodeGroup>
```typescript client
pcClient.connect({
  url: 'https://your.daily.co/room'
});
// OR...
pcClient.connect({
  endpoint: 'https://your-server/connect',
});
```

```python server
@app.post("/connect")
async def rtvi_connect(request: Request) -> Dict[Any, Any]:
    print("Creating room and token for RTVI connection")
    room_url, token = await create_room_and_token()

    # Start the bot process
    try:
        subprocess.Popen(
            [f"python3 -m bot.py -u {room_url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Return the Daily call options in format expected by DailyTransport/Daily Call Object
    return {"url": room_url, "token": token}
```

</CodeGroup>

### Methods

For most operations, you will not interact with the transport directly. Most methods have an equivalent in the `PipecatClient` and should be called from the `PipecatClient`. However, there are a few transport-specific methods that you may need to call directly. When doing so, be sure to access your transport via the `transport` property of the `PipecatClient` instance.

- `preAuth()`

  This is the one method meant to be called directly, which is used to allow you to gather information about the Daily room prior to connecting. As a Daily-specific action, it is not exposed through the `PipecatClient`. This method must be called prior to `connect()` and use the same `room_url` and `token` (optional) as what will be returned by your connection endpoint/eventually used on `connect()`.

  ```typescript
  pcClient.transport.preAuth({
    url: "https://your.daily.co/room",
    token: "your_token",
  });
  const roomInfo = pcClient.transport.dailyCallClient.room();
  ```

## Events

The transport implements the various [`PipecatClient` event handlers](https://docs.pipecat.ai/client/js/api-reference/callbacks). For Daily-specific events, you can attach listeners to the underlying Daily call client. For a list of available events, see the [Daily API Reference](https://docs.daily.co/reference/daily-js/events).

```typescript
pcClient.transport.dailyCallClient.on('recording-started', (ev) => {...});
```

## Advanced

### Accessing the Daily Call

For advanced use cases, where you may need to work with the Daily call client directly, you can access it via the `dailyCallClient` property.

```javascript
const dailyCall = pcClient.transport.dailyCallClient;
```

<Note>
  The Daily call client returned is safe-guarded to not allow you to call
  functions which affect the call's lifecycle and will redirect you to use
  either a Transport method or the `PipecatClient` to perform the equivalent
  action.
</Note>

## More Information

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    Simple Chatbot Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/transports/daily"
  >
    `OpenAIRealTimeWebRTCTransport`
  </Card>
</CardGroup>
<Card
  horizontal
  title="Package"
  icon="browser"
  href="https://www.npmjs.com/package/@pipecat-ai/daily-transport"
>
  `@pipecat-ai/daily-transport`
</Card>



================================================
FILE: client/js/transports/gemini.mdx
================================================
---
title: "GeminiLiveWebSocketTransport"
---

## Overview

The `GeminiLiveWebsocketTransport` class implements a fully functional [Pipecat `Transport`](./transport), providing a framework for implementing real-time communication directly with the [Gemini Multimodal Live](https://ai.google.dev/api/multimodal-live) service. Like all transports, it handles media device management, audio/video streams, and state management for the connection.

<Note>
  Transports of this type are designed primarily for development and testing purposes. For production applications, you will need to build a server component with a server-friendly transport, like the  [DailyTransport](./daily), to securely handle API keys.
</Note>

## Usage

### Basic Setup

```javascript
import { GeminiLiveWebsocketTransport, GeminiLLMServiceOptions } from '@pipecat-ai/gemini-live-websocket-transport';
import { PipecatClient } from '@pipecat-ai/client-js';

const options: GeminiLLMServiceOptions = {
  api_key: 'YOUR_API_KEY',
  initial_messages: [
    // Set up initial system and user messages.
    // Without the user message, the bot will not respond immediately
    // and wait for the user to speak first.
    {
      role: "model",
      content: "You are a confused jellyfish.",
    },
    { role: "user", content: "Blub blub!" },
  ],
  generation_config: {
    temperature: 0.7,
    maxOutput_tokens: 1000
  }
};

const transport = new GeminiLiveWebsocketTransport(options);
let pcClient = new PipecatClient({
  transport: new GeminiLiveWebsocketTransport (options),
  callbacks: {
    // Event handlers
  },
});
pcClient.connect();
```

## API Reference

### Constructor Options

#### `GeminiLLMServiceOptions`

```typescript
interface GeminiLLMServiceOptions {
  api_key: string; // Required: Your Gemini API key
  initial_messages?: Array<{
    // Optional: Initial conversation context
    content: string;
    role: string;
  }>;
  generation_config?: {
    // Optional: Generation parameters
    candidate_count?: number;
    max_output_tokens?: number;
    temperature?: number;
    top_p?: number;
    top_k?: number;
    presence_penalty?: number;
    frequency_penalty?: number;
    response_modalities?: string;
    speech_config?: {
      voice_config?: {
        prebuilt_voice_config?: {
          voice_name: "Puck" | "Charon" | "Kore" | "Fenrir" | "Aoede";
        };
      };
    };
  };
}
```

### TransportConnectionParams

The `GeminiLiveWebsocketTransport` does not take connection parameters. It connects directly to the Gemini Multimodal Live service using the API key provided as part of the  initial configuration.

### Events

The GeminiLiveWebSocketTransport implements the various [PipecatClient event handlers](https://docs.pipecat.ai/client/js/api-reference/callbacks). Check out the docs or samples for more info.

## More Information

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/examples/directToLLMTransports"
  >
    Gemini MultiModal Live Basic Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/transports/gemini-live-websocket-transport"
  >
    `GeminiLiveWebsocketTransport`
  </Card>
</CardGroup>
<Card
  horizontal
  title="Package"
  icon="browser"
  href="https://www.npmjs.com/package/@pipecat-ai/gemini-live-websocket-transport"
>
  `@pipecat-ai/realtime-websocket-transport`
</Card>



================================================
FILE: client/js/transports/openai-webrtc.mdx
================================================
---
title: "OpenAIRealTimeWebRTCTransport"
---

## Overview

The `OpenAIRealTimeWebRTCTransport` is a fully functional [Pipecat `Transport`](https://docs.pipecat.ai/client/js/transports/transport). It provides a framework for implementing real-time communication directly with the [OpenAI Realtime API using WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc) voice-to-voice service. It handles media device management, audio/video streams, and state management for the connection.

<Note>
  Transports of this type are designed primarily for development and testing purposes. For production applications, you will need to build a server component with a server-friendly transport, like the  [DailyTransport](./daily), to securely handle API keys.
</Note>

## Usage

### Basic Setup

```javascript
import { OpenAIRealTimeWebRTCTransport, OpenAIServiceOptions } from '@pipecat-ai/openai-realtime-webrtc-transport';
import { PipecatClient } from '@pipecat-ai/client-js';

const options: OpenAIServiceOptions = {
  api_key: 'YOUR_API_KEY',
  session_config: {
    instructions: 'You are a confused jellyfish.',
  },
  initial_messages: [{ role: "user", content: "Blub blub!" }],
};

let pcClient = new PipecatClient({
  transport: new OpenAIRealTimeWebRTCTransport (options),
  ...
});
pcClient.connect();
```

## API Reference

### Constructor Options

Below is the transport's type definition for the OpenAI Session configuration you need to pass in to the `create()` method. See the [OpenAI Realtime API documentation](https://platform.openai.com/docs/api-reference/realtime-client-events/session/update) for more details on each of the options and their defaults.

```typescript
export type OpenAIFunctionTool = {
  type: "function";
  name: string;
  description: string;
  parameters: JSONSchema;
};

export type OpenAIServerVad = {
  type: "server_vad";
  create_response?: boolean;
  interrupt_response?: boolean;
  prefix_padding_ms?: number;
  silence_duration_ms?: number;
  threshold?: number;
};

export type OpenAISemanticVAD = {
  type: "semantic_vad";
  eagerness?: "low" | "medium" | "high" | "auto";
  create_response?: boolean; // defaults to true
  interrupt_response?: boolean; // defaults to true
};

export type OpenAISessionConfig = Partial<{
  modalities?: string;
  instructions?: string;
  voice?:
    | "alloy"
    | "ash"
    | "ballad"
    | "coral"
    | "echo"
    | "sage"
    | "shimmer"
    | "verse";
  input_audio_noise_reduction?: {
    type: "near_field" | "far_field";
  } | null; // defaults to null/off
  input_audio_transcription?: {
    model: "whisper-1" | "gpt-4o-transcribe" | "gpt-4o-mini-transcribe";
    language?: string;
    prompt?: string[] | string; // gpt-4o models take a string
  } | null; // we default this to gpt-4o-transcribe
  turn_detection?: OpenAIServerVad | OpenAISemanticVAD | null; // defaults to server_vad
  temperature?: number;
  max_tokens?: number | "inf";
  tools?: Array<OpenAIFunctionTool>;
}>;

export interface OpenAIServiceOptions {
  api_key: string;
  model?: string;
  initial_messages?: LLMContextMessage[];
  settings?: OpenAISessionConfig;
}
```

### TransportConnectionParams

The `OpenAIRealTimeWebRTCTransport` does not take connection parameters. It connects directly to the OpenAI Realtime API using the API key provided as part of the initial configuration.

### Events

The transport implements the various [`PipecatClient` event handlers](https://docs.pipecat.ai/client/js/api-reference/callbacks). Check out the docs or samples for more info.

## More Information

<CardGroup cols={2}>
  <Card
    horizontal
    title="Demo"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/examples/directToLLMTransports"
  >
    OpenAI Realtime Basic Demo
  </Card>

  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/transports/openai-realtime-webrtc-transport"
  >
    `OpenAIRealTimeWebRTCTransport`
  </Card>
</CardGroup>
<Card
  horizontal
  title="Package"
  icon="browser"
  href="https://www.npmjs.com/package/@pipecat-ai/openai-realtime-webrtc-transport"
>
  `@pipecat-ai/openai-realtime-webrtc-transport`
</Card>



================================================
FILE: client/js/transports/small-webrtc.mdx
================================================
---
title: "SmallWebRTCTransport"
description: "A lightweight WebRTC transport for peer-to-peer connections with Pipecat"
---

`SmallWebRTCTransport` enables peer-to-peer WebRTC connections between clients and your Pipecat application. It implements bidirectional audio and video streaming using WebRTC for real-time communication.

This transport is intended for lightweight implementations, particularly for local development and testing. It expects your Pipecat server to include the corresponding [`SmallWebRTCTransport` server-side](/server/services/transport/small-webrtc) implementation.

## Usage

### Basic Setup

```javascript
import { PipecatClient } from "@pipecat-ai/client-js";
import { SmallWebRTCTransport } from "@pipecat-ai/small-webrtc-transport";

const pcClient = new PipecatClient({
  transport: new SmallWebRTCTransport({
    // Optional configuration for the transport
    iceServers: ["stun:stun.l.google.com:19302"],
  }),
  enableCam: false, // Default camera off
  enableMic: true, // Default microphone on
  callbacks: {
    // Event handlers
  },
});

await pcClient.connect({
  endpoint: "https://your-server/connect",
});
```

## API Reference

### Constructor Options

```typescript
interface SmallWebRTCTransportConstructorOptions {
  iceServers?: RTCIceServer[];
  waitForICEGathering?: boolean;
  connectionUrl?: string;
  audioCodec?: string;
  videoCodec?: string;
}
```

#### Properties

<ParamField name="iceServers" type="string[]">
  Array of STUN/TURN server URLs for ICE connection establishment. Default is `["stun:stun.l.google.com:19302"]`.

```javascript
// Set custom ICE servers
transport.iceServers = [
  "stun:stun.l.google.com:19302",
  "stun:stun1.l.google.com:19302",
];
```

</ParamField>

<ParamField name="waitForICEGathering" type="boolean" default="false">
  If `true`, the transport will wait for ICE gathering to complete before being
  considered `'connected'`.
</ParamField>

<ParamField name="connectionUrl" type="string">
  URL of the WebRTC signaling server's offer endpoint. This endpoint may also be
  provided as part of the `connect()` either directly or as the response from a
  server-side endpoint you control. It should return an `answer` from your
  corresponding Pipecat server's `SmallWebRTCConnection`.
</ParamField>

<ParamField name="audioCodec" type="string">
  Preferred audio codec to use. If not specified, your browser default will be
  used.
</ParamField>

<ParamField name="videoCodec" type="string">
  Preferred video codec to use. If not specified, your browser default will be
  used.
</ParamField>

### TransportConnectionParams

```typescript
export type SmallWebRTCTransportConnectionOptions = {
  connectionUrl?: string;
};
```

On `connect()`, the `SmallWebRTCTransport` optionally takes a set of connection parameters. This can be provided directly or via a connection endpoint passed to the `PipecatClient`'s connect method. If using an endpoint, your endpoint should return a JSON object matching the `SmallWebRTCTransportConnectionOptions` type, which currently expects a single `connectionUrl` property.

<CodeGroup>
```typescript client
pcClient.connect({
  connectionUrl: 'https://your-pipecat-webrtc-server'
});
// OR...
pcClient.connect({
  endpoint: 'https://your-server/api/offer',
});
```

```python server
# See
# https://github.com/pipecat-ai/pipecat-examples/blob/main/p2p-webrtc/video-transform/server/server.py
# for a complete example of how to implement the server-side endpoint.
@app.post("/api/offer")
async def offer(request: dict, background_tasks: BackgroundTasks):
    pipecat_connection = SmallWebRTCConnection(ice_servers)
    await pipecat_connection.initialize(sdp=request["sdp"], type=request["type"])

    @pipecat_connection.event_handler("closed")
    async def handle_disconnected(webrtc_connection: SmallWebRTCConnection):
        logger.info(f"Discarding peer connection for pc_id: {webrtc_connection.pc_id}")
        pcs_map.pop(webrtc_connection.pc_id, None)

    background_tasks.add_task(run_bot, pipecat_connection)

    answer = pipecat_connection.get_answer()

    return answer
```

</CodeGroup>

### Methods

For most operations, you will not interact with the transport directly. Most methods have an equivalent in the `PipecatClient` and should be called from the `PipecatClient`. However, there are a few transport-specific methods that you may need to call directly. When doing so, be sure to access your transport via the `transport` property of the `PipecatClient` instance.

<ResponseField name="setAudioCodec" type="method">
  Sets the preferred audio codec.

```javascript
transport.setAudioCodec("opus");
```

</ResponseField>

<ResponseField name="setVideoCodec" type="method">
  Sets the preferred video codec.

```javascript
transport.setVideoCodec("VP8");
```

</ResponseField>

## Events

The transport implements the various [`PipecatClient` event handlers](https://docs.pipecat.ai/client/js/api-reference/callbacks).

## Connection Process

The connection process follows these steps:

1. The transport negotiates a WebRTC connection with the corresponding pipecat transport, complete with transceivers for the media and a data channel for messaging.
2. The transport sends a message to the pipecat transport to let it know it's ready.
3. The Pipecat transport sends a message letting the client know it is ready.

## Reconnection Handling

The transport includes automatic reconnection logic:

- Up to 3 reconnection attempts after connection failures
- Detection of ICE connection state changes
- Graceful recovery from temporary disconnections
- Graceful disconnect when reconnection attempts fail

## More Information

<CardGroup cols={2}>
  <Card
    horizontal
    title="Video Transform Demo"
    icon="video"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/p2p-webrtc/video-transform"
  >
    Real-time video transformation example
  </Card>
  <Card
    horizontal
    title="Package"
    icon="browser"
    href="https://www.npmjs.com/package/@pipecat-ai/small-webrtc-transport"
  >
    `@pipecat-ai/small-webrtc-transport`
  </Card>
</CardGroup>



================================================
FILE: client/js/transports/transport.mdx
================================================
---
title: "Transport Overview"
sidebarTitle: "Overview"
---

Transports are the means by which `PipecatClient`s communicate with their bot services. Transports implement the underlying device management, connectivity, media transmission, and state logic that manage the lifecycle of your session.

All transport packages (such as `DailyTransport`) extend from the `Transport` base class defined in the `client-js` library. You can extend this class if you are looking to implement your own or add additional functionality.

## Transport lifecycle

Each Pipecat client instance is associated with a transport instance. The instance will re-use the transport instance across multiple calls to `connect()`, allowing you to connect to different bot services without needing to create a new transport or client each time.

```typescript
import { PipecatClient } from "@pipecat-ai/client-js";
import { DailyTransport } from "@pipecat-ai/daily-transport";

const pcClient = new PipecatClient({
  transport: new DailyTransport(),
  ...
});

await pcClient.connect({endpoint: "https://your-server-url/connect"});
await pcClient.disconnect();
await pcClient.connect(); // re-uses url returned from previous connect call, skipping the endpoint
```

## Transport states

`TransportState`

Your transport instance goes through a series of states during its lifecycle. These states are:

<Steps>
  <Step title="Disconnected">
    Transport is idle and has not yet been initialized (default state).
  </Step>
  <Step title="Initializing">
    Transport is being initialized. This occurs in response to a `pcClient.initDevices()` call, where the transport is being set up in order to enumerate local media devices. If you call `connect()` and bypass `initDevices()`, the transport will skip this state and go directly to `Authenticating` or `Connecting`.
  </Step>
  <Step title="Initialized">
    Transport has been initialized and is ready to connect. This state is reached after a successful `pcClient.initDevices()` call and skipped if `initDevices()` is not used.
  </Step>
  <Step title="Authenticating">
    Your client has called `pcClient.connect()` with a `ConnectionEndpoint` and is waiting for a response from your server containing connection details for your transport (such as a session URL and token). Note: If you provide the `TransportConnectionParams` directly, the transport will skip this state and go directly to `Connecting`.
  </Step>
  <Step title="Authenticated">
    Your client has called `pcClient.connect()` with a `ConnectionEndpoint` and has successfully received a response. It will quickly move into the `Connecting` state.
  </Step>
  <Step title="Connecting">
    The transport is connecting to the server.
  </Step>
  <Step title="Connected">
    The transport has successfully connected to the session and is awaiting a client-ready signal (indicated audio and video tracks are ready to be sent and received).
  </Step>
  <Step title="Ready">Transport is ready and the session can begin.</Step>
  <Step title="Disconnecting">
    Transport is disconnecting from the session.
  </Step>
  <Step title="Error">
  An error occurred during the transport lifecycle. This indicates a fatal error and the transport should move quickly into the `Disconnected` state.
  </Step>
</Steps>

You can access the current transport state via `pcClient.state`, or by defining a callback or event:

```typescript
// Callback
const pcClient = new PipecatClient({
  transport: new DailyTransport(),
  callbacks: {
    onTransportStateChange: (state) => {
      console.log(state);
    }
  //...
});

// Event
pcClient.on(RTVIEvent.TransportStateChanged, (e) => console.log(e));

// Client getter
console.log(pcClient.state); // Disconnected <TransportState>
```



================================================
FILE: client/js/transports/websocket.mdx
================================================
---
title: "WebSocketTransport"
description: "A lightweight transport for WebSocket based connections with Pipecat"
---

`WebSocketTransport` enables a purely WebSocket based connection between clients and your Pipecat application. It implements bidirectional audio and video streaming using a WebSocket for real-time communication.

This transport is intended for lightweight implementations, particularly for local development and testing. It expects your Pipecat server to include the corresponding [`WebSocketTransport` server-side](/server/services/transport/websocket-server) implementation.

<Warning>
The `WebSocketTransport` is best suited for server-server applications and prototyping client/server apps.

For client/server production applications, we strongly recommend using a WebRTC-based transport for robust network and media handling. For more on WebRTC vs. Websocket communication, check out [this article](https://voiceaiandvoiceagents.com/#websockets-webrtc).

</Warning>

## Usage

### Basic Setup

```javascript
import { PipecatClient } from "@pipecat-ai/client-js";
import {
  WebSocketTransport,
  ProtobufFrameSerializer,
} from "@pipecat-ai/websocket-transport";

const pcClient = new PipecatClient({
  transport: new WebSocketTransport({
    serializer: new ProtobufFrameSerializer(),
    recorderSampleRate: 8000,
    playerSampleRate: 8000,
  }),
  enableCam: false, // Default camera off
  enableMic: true, // Default microphone on
  callbacks: {
    // Event handlers
  },
});

await pcClient.connect({
  endpoint: "https://your-server/connect",
});
```

## API Reference

### Constructor Options

```typescript
type WebSocketTransportOptions = {
  ws_url?: string;
  serializer?: WebSocketSerializer;
  recorderSampleRate?: number;
  playerSampleRate?: number;
};
```

#### Properties

<ParamField name="ws_url" type="string">
  URL of the WebSocket server. This is the endpoint your client will connect to
  for WebSocket communication.
</ParamField>

<ParamField
  name="serializer"
  type="WebSocketSerializer"
  default="ProtobufFrameSerializer"
>
  The serializer to use for encoding/decoding messages sent over the WebSocket
  connection. The websocket-transport package provides two serializer options: -
  `ProtobufFrameSerializer`: Uses Protocol Buffers for serialization. -
  `TwilioSerializer`: Uses Twilio's serialization format. The main purpose of
  the TwilioSerializer is to allow testing the bots built to work with Twilio
  without having to make phone calls.
</ParamField>

<ParamField name="recorderSampleRate" type="number">
  Sample rate for which to encode the audio input. Default is `16000`.
</ParamField>

<ParamField name="playerSampleRate" type="number">
  Sample rate for which to decode the incoming audio for output. Default is
  `24000`.
</ParamField>

### TransportConnectionParams

The `WebSocketTransport` takes the same options as the constructor; `WebSocketTransportOptions`. Anything provided here will override the defaults set in the constructor. The `ws_url` is required to establish a connection.

<CodeGroup>
```typescript client
pcClient.connect({
  ws_url: 'http://localhost:7860/ws'
});
// OR...
pcClient.connect({
  endpoint: 'https://your-server/connect', // returns { ws_url }
});
```

```python server
# See
# https://github.com/pipecat-ai/pipecat-examples/blob/main/websocket/server/server.py
# for a complete example of how to implement the server-side endpoint.
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("WebSocket connection accepted")
    try:
        await run_bot(websocket)
    except Exception as e:
        print(f"Exception in run_bot: {e}")


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    ws_url = "ws://localhost:7860/ws"
    return {"ws_url": ws_url}
```

```python bot
# See
# https://github.com/pipecat-ai/pipecat-examples/blob/main/websocket/server/bot_websocket_server.py
# for a complete example of a bot script using the WebSocketTransport.
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.transports.network.websocket_server import (
    WebsocketServerParams,
    WebsocketServerTransport,
)

async def run_bot(websocket_client):
    ws_transport = FastAPIWebsocketTransport(
        websocket=websocket_client,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=ProtobufFrameSerializer(),
        ),
    )

    llm = ... # Initialize your LLM here, e.g., OpenAI, HuggingFace, etc.
    context_aggregator = llm.create_context_aggregator(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            ws_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,  # LLM
            ws_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params,
        observers=[RTVIObserver(rtvi)],
    )
    ...
```

</CodeGroup>

### Methods

For most operations, you will not interact with the transport directly. Most methods have an equivalent in the `PipecatClient` and should be called from the `PipecatClient`. However, there is one transport-specific methods that you may need to call directly. When doing so, be sure to access your transport via the `transport` property of the `PipecatClient` instance.

<ResponseField name="handleUserAudioStream" type="method">
  If implementing your own serializer, you will need to pass the user audio
  stream to the transport via this method, which takes an `ArrayBuffer` of audio
  data.

```javascript
transport.handleUserAudioStream(chunk.data);
```

</ResponseField>
## Events

The transport implements the various [`PipecatClient` event handlers](https://docs.pipecat.ai/client/js/api-reference/callbacks).

## Reconnection Handling

The WebSocketTransport does provide reconnection handling. If the WebSocket connection is lost, it will attempt to reconnect twice. If all reconnection attempts fail, the transport will gracefully disconnect.

## More Information

<CardGroup cols={2}>
  <Card
    horizontal
    title="WebSocket Demo"
    icon="video"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/websocket"
  >
    Basic Agent example using a WebSocket transport
  </Card>
  <Card
    horizontal
    title="Twilio Demo"
    icon="video"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot"
  >
    Example using a WebSocket transport to simulate a Twilio connection to a bot
  </Card>
  <Card
    horizontal
    title="Source"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-client-web-transports/tree/main/transports/websocket-transport"
  >
    `WebSocketTransport`
  </Card>
  <Card
    horizontal
    title="Package"
    icon="browser"
    href="https://www.npmjs.com/package/@pipecat-ai/websocket-transport"
  >
    `@pipecat-ai/websocket-transport`
  </Card>
</CardGroup>



================================================
FILE: client/react/components.mdx
================================================
---
title: "Components"
description: "Ready-to-use React components for Pipecat applications"
---

The Pipecat React SDK provides several components for handling audio, video, and visualization in your application.

## PipecatClientProvider

The root component for providing Pipecat client context to your application.

```jsx
<PipecatClientProvider client={pcClient}>
  {/* Child components */}
</PipecatClientProvider>
```

**Props**

<ParamField path="client" type="PipecatClient" required="true">
A singleton instance of `PipecatClient`
</ParamField>

## PipecatClientAudio

Creates a new `<audio>` element that mounts the bot's audio track.

```jsx
<PipecatClientAudio />
```

**Props**

No props required

## PipecatClientVideo

Creates a new `<video>` element that renders either the bot or local participant's video track.

```jsx
<PipecatClientVideo
  participant="local"
  fit="cover"
  mirror
  onResize={({ aspectRatio, height, width }) => {
    console.log("Video dimensions changed:", { aspectRatio, height, width });
  }}
/>
```

**Props**

<ParamField path="participant" type="('local' | 'bot')" required="true">
Defines which participant's video track is rendered
</ParamField>
<ParamField path="fit" type="('contain' | 'cover')">
Defines whether the video should be fully contained or cover the box. Default: 'contain'
</ParamField>
<ParamField path="mirror" type="boolean">
Forces the video to be mirrored, if set
</ParamField>
<ParamField path="onResize(dimensions: object)" type="function">
Triggered whenever the video's rendered width or height changes
</ParamField>

## PipecatClientCamToggle

A headless component to read and set the local participant's camera state.

```jsx
<PipecatClientCamToggle
  onCamEnabledChanged={(enabled) => console.log("Camera enabled:", enabled)}
  disabled={false}
>
  {({ disabled, isCamEnabled, onClick }) => (
    <button disabled={disabled} onClick={onClick}>
      {isCamEnabled ? "Disable Camera" : "Enable Camera"}
    </button>
  )}
</PipecatClientCamToggle>
```

**Props**

<ParamField path="onCamEnabledChanged(enabled: boolean)" type="function">
Triggered whenever the local participant's camera state changes
</ParamField>
<ParamField path="disabled" type="boolean">
If true, the component will not allow toggling the camera state. Default: false
</ParamField>
<ParamField path="children" type="function">
A render prop that provides state and handlers to the children
</ParamField>

## PipecatClientMicToggle

A headless component to read and set the local participant's microphone state.

```jsx
<PipecatClientMicToggle
  onMicEnabledChanged={(enabled) => console.log("Microphone enabled:", enabled)}
  disabled={false}
>
  {({ disabled, isMicEnabled, onClick }) => (
    <button disabled={disabled} onClick={onClick}>
      {isMicEnabled ? "Disable Microphone" : "Enable Microphone"}
    </button>
  )}
</PipecatClientMicToggle>
```

**Props**

<ParamField path="onMicEnabledChanged(enabled: boolean)" type="function">
Triggered whenever the local participant's microphone state changes
</ParamField>
<ParamField path="disabled" type="boolean">
If true, the component will not allow toggling the microphone state. Default: false
</ParamField>
<ParamField path="children" type="function">
A render prop that provides state and handlers to the children
</ParamField>

## VoiceVisualizer

Renders a visual representation of audio input levels on a `<canvas>` element.

```jsx
<VoiceVisualizer
  participantType="local"
  backgroundColor="white"
  barColor="black"
  barGap={1}
  barWidth={4}
  barMaxHeight={24}
/>
```

**Props**

<ParamField path="participantType" type="string" required="true">
The participant type to visualize audio for
</ParamField>
<ParamField path="backgroundColor" type="string">
The background color of the canvas. Default: 'transparent'
</ParamField>
<ParamField path="barColor" type="string">
The color of the audio level bars. Default: 'black'
</ParamField>
<ParamField path="barCount" type="number">
The number of bars to display. Default: 5
</ParamField>
<ParamField path="barGap" type="number">
The gap between bars in pixels. Default: 12
</ParamField>
<ParamField path="barWidth" type="number">
The width of each bar in pixels. Default: 30
</ParamField>
<ParamField path="barMaxHeight" type="number">
The maximum height at full volume of each bar in pixels. Default: 120
</ParamField>


================================================
FILE: client/react/hooks.mdx
================================================
---
title: "Hooks"
description: "React hooks for accessing Pipecat client functionality"
---

The Pipecat React SDK provides hooks for accessing client functionality, managing media devices, and handling events.

## usePipecatClient

Provides access to the `PipecatClient` instance originally passed to `PipecatClientProvider`.

```jsx
import { usePipecatClient } from "@pipecat-ai/client-react";

function MyComponent() {
  const pcClient = usePipecatClient();

  await pcClient.connect({
    endpoint: 'https://your-pipecat-api-url/connect',
    requestData: {
      // Any custom data your /connect endpoint requires
    }
  });
}
```

## useRTVIClientEvent

Allows subscribing to RTVI client events. It is advised to wrap handlers with `useCallback`.

```jsx
import { useCallback } from "react";
import { RTVIEvent, TransportState } from "@pipecat-ai/client-js";
import { useRTVIClientEvent } from "@pipecat-ai/client-react";

function EventListener() {
  useRTVIClientEvent(
    RTVIEvent.TransportStateChanged,
    useCallback((transportState: TransportState) => {
      console.log("Transport state changed to", transportState);
    }, [])
  );
}
```

**Arguments**

<ParamField path="event" type="RTVIEvent" required="true" />
<ParamField path="handler" type="function" required="true" />

## usePipecatClientMediaDevices

Manage and list available media devices.

```jsx
import { usePipecatClientMediaDevices } from "@pipecat-ai/client-react";

function DeviceSelector() {
  const {
    availableCams,
    availableMics,
    selectedCam,
    selectedMic,
    updateCam,
    updateMic,
  } = usePipecatClientMediaDevices();

  return (
    <>
      <select
        name="cam"
        onChange={(ev) => updateCam(ev.target.value)}
        value={selectedCam?.deviceId}
      >
        {availableCams.map((cam) => (
          <option key={cam.deviceId} value={cam.deviceId}>
            {cam.label}
          </option>
        ))}
      </select>
      <select
        name="mic"
        onChange={(ev) => updateMic(ev.target.value)}
        value={selectedMic?.deviceId}
      >
        {availableMics.map((mic) => (
          <option key={mic.deviceId} value={mic.deviceId}>
            {mic.label}
          </option>
        ))}
      </select>
    </>
  );
}
```

## usePipecatClientMediaTrack

Access audio and video tracks.

```jsx
import { usePipecatClientMediaTrack } from "@pipecat-ai/client-react";

function MyTracks() {
  const localAudioTrack = usePipecatClientMediaTrack("audio", "local");
  const botAudioTrack = usePipecatClientMediaTrack("audio", "bot");
}
```

**Arguments**

<ParamField path="trackType" type="'audio' | 'video'" required="true" />
<ParamField path="participantType" type="'bot' | 'local'" required="true" />

## usePipecatClientTransportState

Returns the current transport state.

```jsx
import { usePipecatClientTransportState } from "@pipecat-ai/client-react";

function ConnectionStatus() {
  const transportState = usePipecatClientTransportState();
}
```

## usePipecatClientCamControl

Controls the local participant's camera state.

```jsx
import { usePipecatClientCamControl } from "@pipecat-ai/client-react";
function CamToggle() {
  const { enableCam, isCamEnabled } = usePipecatClientCamControl();

  return (
    <button onClick={() => enableCam(!isCamEnabled)}>
      {isCamEnabled ? "Disable Camera" : "Enable Camera"}
    </button>
  );
}
```

## usePipecatClientMicControl

Controls the local participant's microphone state.

```jsx
import { usePipecatClientMicControl } from "@pipecat-ai/client-react";
function MicToggle() {
  const { enableMic, isMicEnabled } = usePipecatClientMicControl();

  return (
    <button onClick={() => enableMic(!isMicEnabled)}>
      {isMicEnabled ? "Disable Microphone" : "Enable Microphone"}
    </button>
  );
}
```



================================================
FILE: client/react/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build React applications with Pipecat's React client library"
---

The Pipecat React SDK provides React-specific components and hooks for building voice and multimodal AI applications. It wraps the core JavaScript SDK functionality in an idiomatic React interface that handles:

- React context for client state management
- Components for audio and video rendering
- Hooks for accessing client functionality
- Media device management
- Event handling through hooks

## Installation

Install the SDK, core client, and a transport implementation (e.g. Daily for WebRTC):

```bash
npm install @pipecat-ai/client-js
npm install @pipecat-ai/client-react
npm install @pipecat-ai/daily-transport
```

## Example

Here's a simple example using Daily as the transport layer:

```tsx
import { PipecatClient } from "@pipecat-ai/client-js";
import {
  PipecatClientProvider,
  PipecatClientAudio,
  usePipecatClient,
} from "@pipecat-ai/client-react";
import { DailyTransport } from "@pipecat-ai/daily-transport";

// Create the client instance
const client = new PipecatClient({
  transport: new DailyTransport(),
  enableMic: true,
});

// Root component wraps the app with the provider
function App() {
  return (
    <PipecatClientProvider client={client}>
      <VoiceBot />
      <PipecatClientAudio />
    </PipecatClientProvider>
  );
}

// Component using the client
function VoiceBot() {
  const client = usePipecatClient();

  const handleClick = async () => {
    await client.connect({
      endpoint: `${process.env.PIPECAT_API_URL || "/api"}/connect`
    });
  };

  return (
    <button onClick={handleClick}>Start Conversation</button>;
  );
}
```

## Explore the SDK

<CardGroup cols={2}>
  <Card title="Components" icon="puzzle-piece" href="/client/react/components">
    Ready-to-use components for audio, video, and visualization
  </Card>
  <Card title="Hooks" icon="code" href="/client/react/hooks">
    React hooks for accessing client functionality
  </Card>
</CardGroup>

The Pipecat React SDK builds on top of the [JavaScript SDK](/client/js/introduction) to provide an idiomatic React interface while maintaining compatibility with the RTVI standard.



================================================
FILE: client/react/migration-guide.mdx
================================================
---
title: "RTVIClient Migration Guide for React"
description: "A Guide to migrating from an RTVIClient to PipecatClient in React"
---

This guide covers migrating from RTVIClient to the new `PipecatClient` in a React application. The new client introduces simplified configuration and improved client-server messaging. For an overview of the changes, see the top-level [RTVIClient Migration Guide](https://docs.pipecat.ai/client/migration-guide).

## Key Changes

1. Package and Class Names

```javascript
// Old
import { RTVIClient } from '@pipecat-ai/client-js';

// New
import { PipecatClient } from '@pipecat-ai/client-js';
```

2. React Components and Hooks

```javascript
// Old
import { 
  RTVIClientProvider,
  RTVIClientAudio,
  RTVIClientVideo,
  useRTVIClient,
  useRTVIClientTransportState
} from '@pipecat-ai/client-react';

// New
import {
  PipecatClientProvider,
  PipecatClientAudio,
  PipecatClientVideo,
  usePipecatClient,
  usePipecatClientTransportState
} from '@pipecat-ai/client-react';
```

3. Client and Transport Configuration

```javascript
// Old
const transport = new DailyTransport();
const client = new RTVIClient({
  transport,
  params: {
    baseUrl: 'http://localhost:7860',
    endpoints: {
      connect: '/connect'
    }
  }
});

// New
const client = new PipecatClient({
  transport: new DailyTransport(),
  // Connection params moved to connect() call
});
```

4. Connection Method

```javascript
// Old
await client.connect();

// New
await client.connect({
  endpoint: 'http://localhost:7860/connect',
  requestData: {
    // Any custom data your /connect endpoint requires
    llm_provider: 'openai',
    initial_prompt: "You are a pirate captain",
    // Any additional data
  }
});
```

4. Function Call Handling

```javascript
// Old
let llmHelper = new LLMHelper({});
llmHelper.handleFunctionCall(async (data) => {
  return await this.handleFunctionCall(data.functionName, data.arguments);
});
client.registerHelper('openai', llmHelper);

// New
client.registerFunctionCallHandler('functionName', async (data) => {
  // Handle function call
  return result;
});
```

## Breaking Changes

1. **Configuration Structure**: Connection parameters are now passed to connect() instead of constructor
2. **Helper System**: Removed in favor of direct `PipecatClient` member functions or client-server messaging.
3. **Component Names**: All React components renamed from RTVI prefix to Pipecat prefix
4. **Hook Names**: All React hooks renamed from useRTVI prefix to usePipecat prefix

## Migration Steps

1. Update package imports to use new names
2. Move connection configuration from constructor to connect() method
3. Replace any helper classes with corresponding `PipecatClient` methods or custom messaging
4. Update React component and hook names
5. Update any TypeScript types referencing old names



================================================
FILE: client/react-native/api-reference.mdx
================================================
---
title: "API Reference"
description: "API reference for the Pipecat React Native SDK"
---

<Note>
  The Pipecat React Native SDK leverages the Pipecat JavaScript SDK for seamless integration with React Native applications.
  For detailed information, please reference to the [Javascript SDK docs](/client/js/api-reference/client-constructor).

  **Just ensure you use the appropriate transport layer for React Native.**
</Note>



================================================
FILE: client/react-native/introduction.mdx
================================================
---
title: "SDK Introduction"
description: "Build React Native applications with Pipecat's React Native client library"
---

The Pipecat React Native SDK leverages the [Pipecat JavaScript SDK](/client/js/introduction) to provide seamless integration for React Native applications.
Since the JavaScript SDK is designed to work across both web and React Native platforms, the core functionalities remain the same:
- Device and media stream management
- Managing bot configuration
- Sending actions to the bot
- Handling bot messages and responses
- Managing session state and errors

The primary difference lies in the transport layer, which is tailored to support the unique requirements of the React Native environment.

For example, when using the SDK with React Native, you would install `RNDailyTransport` instead of `DailyTransport`.

## Installation

Install the SDK and a transport implementation (e.g. Daily for WebRTC):

```bash
npm i @pipecat-ai/react-native-daily-transport
npm i @daily-co/react-native-daily-js@^0.70.0
npm i @daily-co/react-native-webrtc@^118.0.3-daily.2
npm i @react-native-async-storage/async-storage@^1.23.1
npm i react-native-background-timer@^2.4.1
npm i react-native-get-random-values@^1.11.0
```

<Note>Installing `@pipecat-ai/react-native-daily-transport` automatically includes the corresponding version of the JavaScript SDK.</Note>

If you are using Expo, you will also need to add the following dependencies:

```bash
npm i @config-plugins/react-native-webrtc@^10.0.0
npm i @daily-co/config-plugin-rn-daily-js@0.0.7
```

## Requirements

This package introduces some constraints on what OS/SDK versions your project can support:

- iOS: Deployment target >= 13
- Android: `minSdkVersion` >= 24

## Quick start

Here's a simple example using Daily as the transport layer:

```tsx
import { RNDailyTransport } from '@pipecat-ai/react-native-daily-transport';
import { RTVIClient } from '@pipecat-ai/client-js';

// Create and configure the client
let voiceClient = new RTVIClient({
  params: {
    baseUrl: process.env.PIPECAT_API_URL || "/api",
  },
  transport: new RNDailyTransport(),
  enableMic: true
});

// Connect to your bot
await voiceClient.connect();
```

> You can find a basic working example [here](https://github.com/pipecat-ai/pipecat-client-react-native-daily-transport/tree/main/example)
> and a more comprehensive example [here](https://github.com/daily-demos/daily-bots-react-native-demo/).

## Explore the SDK

The Pipecat React Native SDK leverages the Pipecat JavaScript SDK for seamless integration with React Native applications. For detailed information, refer to our JavaScript documentation.

> Just ensure you use the appropriate transport layer for React Native.

<CardGroup cols={2}>
  <Card
    title="Client Constructor"
    icon="cube"
    href="/client/js/api-reference/client-constructor"
  >
    Configure your client instance with transport and callbacks
  </Card>
  <Card
    title="Client Methods"
    icon="code"
    href="/client/js/api-reference/client-methods"
  >
    Core methods for interacting with your bot
  </Card>
  <Card title="API Reference" icon="book" href="/client/js/api-reference">
    Detailed documentation of all available APIs
  </Card>
  <Card title="Helpers" icon="wand-magic-sparkles" href="/client/js/helpers">
    Utility functions for common operations
  </Card>
</CardGroup>




================================================
FILE: getting-started/client-server.mdx
================================================
---
title: "Client/Server Bots"
description: "Build web and mobile applications with custom interfaces using Pipecat's client SDKs"
---

## Overview

Learn how to build web and mobile applications using Pipecat's client/server architecture. This approach separates your bot logic from your user interface, giving you full control over the client experience while maintaining real-time voice communication.

This guide introduces two key technologies that power client/server applications:

### Pipecat Client SDKs

Pipecat provides client SDKs for multiple platforms, all implementing the [RTVI (Real-Time Voice and Video Inference) standard](/client/rtvi-standard). These SDKs handle the complex real-time communication between your custom interface and your Pipecat bot.

<CardGroup cols={3}>
  <Card
    title="JavaScript"
    icon="JS"
    color="#f7e014"
    href="/client/js/introduction"
  >
    Build web applications with vanilla JavaScript
  </Card>
  <Card
    title="React"
    icon="react"
    color="#56c4db"
    href="/client/react/introduction"
  >
    Create React applications with hooks and components
  </Card>
  <Card
    title="React Native"
    icon="react"
    color="#56c4db"
    href="/client/react-native/introduction"
  >
    Build mobile apps with React Native
  </Card>
  <Card
    title="Swift"
    icon="swift"
    color="#F05138"
    href="/client/ios/introduction"
  >
    Native iOS applications
  </Card>
  <Card
    title="Kotlin"
    icon="android"
    color="#78C257"
    href="/client/android/introduction"
  >
    Native Android applications
  </Card>
  <Card title="C++" icon="C" color="#679cd3" href="/client/c++/introduction">
    High-performance native applications
  </Card>
</CardGroup>

**Key Benefits:**

- **Real-time communication**: Handle voice, video, and messaging in real-time
- **RTVI standard**: Open specification ensuring compatibility and consistency
- **Cross-platform**: Consistent API across all supported platforms
- **Production-ready**: Battle-tested for multimedia handling

### Voice UI Kit

The [Voice UI Kit](https://github.com/pipecat-ai/voice-ui-kit) provides React components and templates for quickly building voice AI interfaces. It's designed specifically to accelerate Pipecat application development.

**Key Features:**

- **Debug console**: Flexible UI for testing and benchmarking your bots
- **Headless components**: Building blocks for custom voice interfaces
- **Drop-in templates**: Fully-featured UIs ready for development and demos
- **Fully customizable**: Built on Tailwind with CSS variables for easy theming
- **Responsive design**: Optimized for desktop, tablet, and mobile

## React + SmallWebRTC Example

This example demonstrates a React client using the Voice UI Kit's console template, connected to a Pipecat bot server via SmallWebRTC transport.

## Prerequisites

### Python 3.10+

Pipecat requires Python 3.10 or newer. Check your version with:

```bash
python --version
```

If you need to upgrade Python, we recommend using a version manager like `uv` or `pyenv`.

### Node.js 16+

The React client requires Node.js 16 or newer. Check your version with:

```bash
node --version
```

### AI Service API Keys

<CardGroup cols={3}>
  <Card title="Deepgram (STT)" icon="microphone" href="https://console.deepgram.com/signup">
    Create an account and generate your API key for real-time speech recognition.
  </Card>

<Card
  title="OpenAI (LLM)"
  icon="brain"
  href="https://auth.openai.com/create-account"
>
  Create an account and generate an API key for intelligent conversation
  responses.
</Card>

  <Card title="Cartesia (TTS)" icon="volume-high" href="https://play.cartesia.ai/sign-up">
    Sign up and generate your API key for natural voice synthesis.
  </Card>
</CardGroup>

## Setup

This example requires running both a server and client in **two separate terminal windows**.

### 1. Clone the quickstart repository

```bash
git clone https://github.com/pipecat-ai/pipecat-quickstart-client-server.git
cd pipecat-quickstart-client-server
```

### 2. Set up the server (Terminal 1)

Navigate to the server directory and set up your environment:

```bash
cd server
```

Create your environment file:

```bash
cp env.example .env
```

Open the `.env` file and add your API keys:

```bash
DEEPGRAM_API_KEY=your_deepgram_api_key
OPENAI_API_KEY=your_openai_api_key
CARTESIA_API_KEY=your_cartesia_api_key
```

Set up your virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

<Tip>**Using `uv`?** Create a venv and get dependencies with: `uv sync`.</Tip>

Run your bot server:

```bash
python bot.py
```

<Tip>**Using `uv`?** Run with: `uv run bot.py`</Tip>

You should see output confirming your server is running and ready to receive connections.

### 3. Set up the client (Terminal 2)

In a new terminal window, navigate to the client directory:

```bash
cd client
```

Install client dependencies:

```bash
npm install
```

Start the development server:

```bash
npm run dev
```

You should see output like:

```

  VITE v6.2.5  ready in 5957 ms

  ➜  Local:   http://localhost:5173/
  ➜  Network: use --host to expose
  ➜  press h + enter to show help

```

### 4. Connect and test

**Open http://localhost:5173 in your browser**. You'll see the Voice UI Kit console interface with a connect button.

Click **Connect** and allow microphone access when prompted. The console will establish a connection to your bot server and you can start having a voice conversation!

## Understanding the Architecture

### Transport Layer

Both the client and server use the **SmallWebRTC transport**, which enables:

- **Bidirectional audio communication**: Real-time voice data flows between client and server
- **Bidirectional events and messaging**: RTVI protocol messages for coordination and control
- **Synchronized state**: Client and server stay in sync throughout the conversation

### Client Implementation

The React client uses the Voice UI Kit's `ConsoleTemplate` component:

```tsx
import {
  ConsoleTemplate,
  FullScreenContainer,
  ThemeProvider,
} from "@pipecat-ai/voice-ui-kit";

createRoot(document.getElementById("root")!).render(
  <StrictMode>
    <ThemeProvider>
      <FullScreenContainer>
        <ConsoleTemplate
          transportType="smallwebrtc"
          connectParams={{
            connectionUrl: "/api/offer",
          }}
        />
      </FullScreenContainer>
    </ThemeProvider>
  </StrictMode>
);
```

**Key Points:**

- **Full-page experience**: The console template provides a complete interface for voice interaction
- **Custom UI building**: The Voice UI Kit also offers individual components for building custom interfaces
- **Transport configuration**: The client connects to the `/api/offer` endpoint provided by the server

### Server Implementation

The server uses Pipecat's [development runner](/server/utilities/runner/guide), which automatically provides:

- **FastAPI server**: Handles HTTP requests and WebRTC connections
- **WebRTC endpoint**: The `/api/offer` endpoint that clients connect to
- **Connection management**: Automatic setup and teardown of client connections

<Note>
  This is the same server-side Pipecat bot from the [Quickstart
  example](/getting-started/quickstart).
</Note>

## Building for Other Platforms

This example uses React and SmallWebRTC, but Pipecat's client SDKs work across platforms with the same server-side bot code. The RTVI standard ensures consistent communication regardless of which client SDK you use.

<Card
  title="Multi-Platform Example"
  icon="laptop-mobile"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
>
  See the same bot working with 6 different client implementations: JavaScript,
  React, iOS (Swift), Android (Kotlin), React Native, and Daily Prebuilt
</Card>

**Key advantages:**

- **Write once, deploy everywhere**: Your Pipecat server works with any client platform
- **Consistent experience**: RTVI protocol ensures uniform behavior across platforms
- **Easy migration**: Switch between client platforms without changing server code

## Deployment Considerations

### Local Development

- **Client connects to local server**: The development runner provides all necessary endpoints
- **Automatic setup**: No additional configuration needed beyond API keys

### Pipecat Cloud Deployment

- **Server**: Deploy your bot code as-is to Pipecat Cloud
- **Client**: Update connection endpoint to your web server, which then connects to Pipecat Cloud's `/{agent}/start` endpoint
- **API key security**: Your web server keeps API keys secret from the client

### Self-Hosting

- **Custom server setup**: Implement your own FastAPI or similar server
- **Connection handling**: Handle bot startup and WebRTC connection management
- **Similar pattern**: Web server receives client connections, then starts bot instances

<Note>
  A web server layer is required in production to keep your AI service API keys
  secure and never expose them to client-side code.
</Note>

## Next Steps

Now that you have a working client/server application:

- **Customize the interface**: Explore Voice UI Kit components to build custom interfaces beyond the console template
- **Try other platforms**: Check out the iOS, Android, and React Native client SDKs
- **Join the community**: Connect with other developers on [Discord](https://discord.gg/pipecat) to share your projects and get help



================================================
FILE: getting-started/introduction.mdx
================================================
---
title: Introduction
description: "Learn about Pipecat and how to get started."
---

Pipecat is an open source Python framework for building voice and multimodal AI bots that can see, hear, and speak in real-time.

The framework orchestrates AI services, network transports, and audio processing to enable ultra-low latency conversations that feel natural and responsive. Build everything from simple voice assistants to complex multimodal applications that combine audio, video, images, and text.

Want to dive right in? Check out the Quickstart example to run your first Pipecat application.

<Card title="Quickstart" icon="rocket" href="/getting-started/quickstart">
  Build and run your first Pipecat application
</Card>

## What You Can Build

<CardGroup cols={2}>
  <Card
    title="Voice Assistants"
    icon="microphone"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot">
    Natural, real-time conversations with AI using speech recognition and
    synthesis
  </Card>

<Card
  title="Phone Agents"
  icon="phone"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/patient-intake"
>
  Connect to your agent via phone for support, intake, and customer service
  interactions
</Card>

<Card
  title="Multimodal Apps"
  icon="layer-group"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/moondream-chatbot"
>
  Applications that combine voice, video, images, and text for rich interactions
</Card>

<Card
  title="Creative Experiences"
  icon="wand-magic-sparkles"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/storytelling-chatbot"
>
  Storytelling experiences and social companions that engage users
</Card>

<Card
  title="Interactive Games"
  icon="gamepad"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/word-wrangler-gemini-live"
>
  Voice-controlled games and interactive experiences with real-time AI responses
</Card>

  <Card
    title="Conversation Flows"
    icon="diagram-project"
    href="https://github.com/pipecat-ai/pipecat-flows">
    Build structured conversations with Pipecat Flows to complete tasks and improve LLM accuracy
  </Card>
</CardGroup>

## How It Works

Pipecat orchestrates AI services in a **pipeline**, which is a series of processors that handle real-time audio, text, and video frames with ultra-low latency.

Here's what happens in a typical voice conversation:

1. **Transport** receives audio from the user (browser, phone, etc.)
2. **Speech Recognition** converts speech to text in real-time
3. **LLM** generates intelligent responses based on context
4. **Speech Synthesis** converts responses back to natural speech
5. **Transport** streams audio back to the user

In most cases, the entire round-trip interaction happens between 500-800ms, creating a natural conversation experience for the user.

The diagram below shows a typical voice assistant pipeline, where each step happens in real-time:

<img
  src="/images/pipecat-overview.png"
  alt="Pipecat Overview"
  className="rounded-lg"
/>

## Ready to Build?

The best way to understand Pipecat is to build with it. Start with our 5-minute quickstart to create your first voice AI bot.

<Card title="Quickstart" icon="rocket" href="/getting-started/quickstart">
  Build and run your first Pipecat application
</Card>

## Get Involved

<CardGroup cols={2}>

<Card
  title="Discord Community"
  icon="discord"
  iconType="duotone"
  href="https://discord.gg/pipecat"
>
  Connect with other developers, share your projects, and get support from the
  Pipecat team.
</Card>

<Card
  title="Pipecat GitHub repo"
  icon="github"
  iconType="duotone"
  href="https://github.com/pipecat-ai/pipecat"
>
  Explore the source code, open issues, and contribute to the project.
</Card>

</CardGroup>



================================================
FILE: getting-started/next-steps.mdx
================================================
---
title: "Next Steps"
description: "Continue your Pipecat journey with advanced examples and comprehensive guides"
---

## You've Built Your First Bot! 🙌

Congrats! You've successfully built and run your first Pipecat application. Now you're ready to explore more advanced concepts and build production applications.

## Choose Your Path

<CardGroup cols={2}>
  <Card
    title="Explore Examples"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-examples"
  >
    **Production-ready applications** Dive into complete examples including
    multimodal bots, creative applications, and enterprise integrations.
  </Card>

  <Card title="Learn Core Concepts" icon="book-open" href="/guides/fundamentals/core-concepts">
    **Master Pipecat fundamentals** Understand pipelines, processors,
    transports, and how to build custom AI applications from the ground up.
  </Card>
</CardGroup>

## Popular Next Steps

### 🚀 Ready to Build?

- **[Explore Examples](https://github.com/pipecat-ai/pipecat-examples)**: 30+ production examples
- **[Deploy to Production](/guides/deployment/overview)**: Scale your applications

### 🧠 Want to Learn More?

Learn about the most common features:

- **[Context Management](/guides/fundamentals/context-management)**: Manage user context and state
- **[Custom Frame Processors](/guides/fundamentals/custom-frame-processor)**: Build custom processing logic for your frames
- **[Function Calling](/guides/fundamentals/function-calling)**: Integrate external APIs and services
- **[Recording Audio](/guides/fundamentals/recording-audio)**: Capture and process audio streams
- **[Saving Transcripts](/guides/fundamentals/saving-transcripts)**: Store and manage conversation transcripts

### 💬 Need Help?

- **[Discord Community](https://discord.gg/pipecat)**: Get support and share projects
- **[GitHub Issues](https://github.com/pipecat-ai/pipecat/issues)**: Report bugs or request features



================================================
FILE: getting-started/phone-bots.mdx
================================================
---
title: "Phone Bots"
description: "Connect your Pipecat bot to a phone number for voice conversations"
---

## Overview

Learn how to connect your Pipecat bot to a phone number so users can call and have voice conversations. This guide covers different telephony integration options and shows you how to implement telephone-based AI interactions.

## Telephony Integration Options

Pipecat provides flexible integration with telephony providers through multiple connection mechanisms. Choose the approach that best fits your use case:

### WebSocket + Media Streams

**Best for:** Quick prototypes and production applications that need simple call handling

- **Simplicity**: Fast setup with minimal configuration
- **Supported providers**: Twilio, Telnyx, Plivo, Exotel
- **Limitations**: Basic call control (no transfers, reconnects, or advanced call center features)

**Examples:**

<CardGroup cols={3}>
  <Card
    title="Twilio WebSocket"
    icon="phone"
    href="#websocket-example-with-twilio"
  >
    Complete setup guide using Twilio's Media Streams (shown below)
  </Card>

<Card
  title="Telnyx WebSocket"
  icon="phone"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/telnyx-chatbot"
>
  WebSocket integration with Telnyx telephony services
</Card>

  <Card
    title="Plivo WebSocket"
    icon="phone"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/plivo-chatbot"
  >
    WebSocket integration with Plivo telephony platform
  </Card>
</CardGroup>

### Daily PSTN Calling

**Best for:** Simple telephony needs without external provider setup

- **Ease of use**: Built-in PSTN connectivity through Daily
- **Quick setup**: No additional telephony account required
- **Limitations**: Fewer customization options compared to dedicated providers

**Examples:**

<CardGroup cols={2}>
  <Card
    title="Daily PSTN: Dial-in"
    icon="phone-arrow-down-left"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-pstn-dial-in"
  >
    Simple inbound calling using Daily's built-in PSTN connectivity
  </Card>

  <Card
    title="Daily PSTN: Dial-out"
    icon="phone-arrow-up-right"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-pstn-dial-out"
  >
    Make outbound calls using Daily's PSTN service
  </Card>
</CardGroup>

### SIP + Daily WebRTC

**Best for:** Advanced call flows and call center integration

- **Advanced features**: Call transfers, reconnection, sophisticated routing
- **Audio quality**: Superior audio quality and reliability
- **Call center integration**: Connect to existing telephony infrastructure
- **Complexity**: More complex setup and configuration

**Examples:**

<CardGroup cols={2}>
  <Card
    title="Daily + Twilio SIP: Dial-in"
    icon="phone-arrow-down-left"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-twilio-sip-dial-in"
  >
    Advanced dial-in calling with SIP integration for better call control
  </Card>

  <Card
    title="Daily + Twilio SIP: Dial-out"
    icon="phone-arrow-up-right"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-twilio-sip-dial-out"
  >
    Make outbound calls with advanced telephony features
  </Card>
</CardGroup>

## WebSocket Example with Twilio

This example demonstrates the WebSocket approach using Twilio's Media Streams. It's the fastest way to get your bot answering phone calls.

## Prerequisites

### Python 3.10+

Pipecat requires Python 3.10 or newer. Check your version with:

```bash
python --version
```

If you need to upgrade Python, we recommend using a version manager like `uv` or `pyenv`.

### Required Services

You'll need accounts and API keys for the following services:

<CardGroup cols={2}>
  <Card title="Twilio Account" icon="phone" href="https://www.twilio.com/login">
    **Telephony Provider** Create an account and purchase a phone number for
    your bot
  </Card>

  <Card
    title="ngrok"
    icon="globe"
    href="https://ngrok.com/docs/getting-started/"
  >
    **Local Tunneling** Create secure tunnels to your local development server
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Deepgram (STT)" icon="microphone" href="https://console.deepgram.com/signup">
    Create an account and generate your API key for real-time speech recognition.
  </Card>

<Card
  title="OpenAI (LLM)"
  icon="brain"
  href="https://auth.openai.com/create-account"
>
  Create an account and generate an API key for intelligent conversation
  responses.
</Card>

  <Card title="Cartesia (TTS)" icon="volume-high" href="https://play.cartesia.ai/sign-up">
    Sign up and generate your API key for natural voice synthesis.
  </Card>
</CardGroup>

## Setup

This example requires running both a server and ngrok tunnel in **two separate terminal windows**.

### 1. Clone the quickstart repository

```bash
git clone https://github.com/pipecat-ai/quickstart-phone-bot.git
cd quickstart-phone-bot
```

### 2. Set up your environment

Create your environment file:

```bash
cp env.example .env
```

Open the `.env` file in your text editor and add your API keys:

```bash
DEEPGRAM_API_KEY=your_deepgram_api_key
OPENAI_API_KEY=your_openai_api_key
CARTESIA_API_KEY=your_cartesia_api_key
```

<Tip>
  **Optional**: Add your `TWILIO_ACCOUNT_SID` and `TWILIO_AUTH_TOKEN` to enable
  automatic call hangup features.
</Tip>

### 3. Install dependencies

Set up your virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

<Tip>**Using `uv`?** Create a venv and get dependencies with: `uv sync`.</Tip>

### 4. Start ngrok tunnel (Terminal 1)

In your first terminal window, start ngrok to create a tunnel to your local server:

```bash
ngrok http 7860
```

<Tip>
  **Want a consistent URL?** Use the `--subdomain` flag: `ngrok http
  --subdomain=your-bot-name 7860`
</Tip>

Copy your ngrok URL (e.g., `https://abc123.ngrok.io`). You'll need it for the next step.

### 5. Configure Twilio webhook

1. Go to your [Twilio phone number's configuration page](https://console.twilio.com/us1/develop/phone-numbers/manage/incoming)
2. Under "Voice Configuration", in the "A call comes in" section:
   - Select "Webhook" from the dropdown
   - Enter your ngrok URL: `https://your-ngrok-url.ngrok.io`
   - Ensure "HTTP POST" is selected
3. Click "Save" at the bottom of the page

### 6. Run your phone bot (Terminal 2)

In your second terminal window, start your bot server:

```bash
python bot.py --transport twilio --proxy your-ngrok-url.ngrok.io
```

Replace `your-ngrok-url.ngrok.io` with your actual ngrok domain (without `https://`).

<Tip>
  **Using `uv`?** Run with: `uv run bot.py --transport twilio --proxy
  your-ngrok-url.ngrok.io`
</Tip>

You should see output confirming your server is running and ready to receive calls.

### 7. Test your phone bot

**Call your Twilio phone number** to start talking with your AI bot! 🚀

The bot will answer, introduce itself, and engage in conversation based on what you say.

## Understanding the Call Flow

When someone calls your Twilio number, here's what happens:

1. **Incoming Call**: User dials your Twilio phone number
2. **Webhook**: Twilio sends call data to your ngrok URL
3. **WebSocket**: Your server establishes a real-time audio connection via WebSocket and exchanges Media Streams with Twilio
4. **Processing**: Audio flows through your Pipecat pipeline (STT → LLM → TTS)
5. **Response**: Synthesized speech streams back to the caller in real-time

This WebSocket approach provides low-latency, real-time audio processing perfect for natural conversation flow.

## Deployment Patterns

This example uses Pipecat's [development runner](/development/runner/guide) for local testing. The runner automatically handles server setup and connection management.

**For Pipecat Cloud deployment**: Your code can remain this simple. Pipecat Cloud handles all server infrastructure, requiring no additional setup in your code.

**For self-hosting**: You'll need to implement your own server to handle webhooks and WebSocket connections. See the WebSocket examples above for full implementation references in the pipecat-examples repository.

## Troubleshooting

- **Call doesn't connect**: Verify your ngrok URL is correctly set in the Twilio webhook configuration
- **No audio or bot doesn't respond**: Check that all API keys are correctly set in your `.env` file
- **Webhook errors**: Ensure your server is running and ngrok tunnel is active before making calls
- **ngrok tunnel issues**: Free ngrok URLs change on each restart. Remember to update your Twilio webhook URL

## Next Steps

Now that you have a working phone bot, you can explore more advanced telephony features:

- **Deploy to production**: Replace ngrok with a proper server deployment and update your Twilio webhook
- **Try advanced call flows**: Explore the Daily + SIP examples for call transfers, reconnection, and call center integration
- **Advanced features**: Check out [pipecat-examples](https://github.com/pipecat-ai/pipecat-examples) for call recording, analytics, and more
- **Join the community**: Connect with other developers on [Discord](https://discord.gg/pipecat) to share your projects and get help



================================================
FILE: getting-started/quickstart.mdx
================================================
---
title: "Quickstart"
description: "Run your first Pipecat bot in under 5 minutes"
---

This quickstart guide will help you set up and run your first Pipecat application. You'll create a simple voice AI bot that you can talk to in real-time using your browser.

## Prerequisites

### Python 3.10+

Pipecat requires Python 3.10 or newer. Check your version with:

```bash
python --version
```

If you need to upgrade Python, we recommend using a version manager like `uv` or `pyenv`.

### AI Service API Keys

This quickstart uses three AI services working together in a pipeline. You'll need API keys from each service:

<CardGroup cols={3}>
  <Card
    title="Deepgram (STT)"
    icon="microphone"
    href="https://console.deepgram.com/signup"
  >
    Create an account and generate your API key for real-time
    speech recognition.
  </Card>

<Card
  title="OpenAI (LLM)"
  icon="brain"
  href="https://auth.openai.com/create-account"
>
  Create an account and generate an API key for intelligent conversation
  responses.
</Card>

  <Card
    title="Cartesia (TTS)"
    icon="volume-high"
    href="https://play.cartesia.ai/sign-up"
  >
    Sign up and generate your API key for natural voice
    synthesis.
  </Card>
</CardGroup>

Have these API keys ready. You'll add them to your environment file in the next section.

## Setup

### 1. Clone the quickstart repository

```bash
git clone https://github.com/pipecat-ai/pipecat-quickstart.git
cd pipecat-quickstart
```

### 2. Set up your environment

Create your environment file:

```bash
cp env.example .env
```

Open the `.env` file in your text editor and add your API keys:

```bash
DEEPGRAM_API_KEY=your_deepgram_api_key
OPENAI_API_KEY=your_openai_api_key
CARTESIA_API_KEY=your_cartesia_api_key
```

Replace each placeholder with your actual API key from the respective service.

### 3. Install dependencies

Set up your virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

<Tip>**Using `uv`?** Create a venv and get dependencies with: `uv sync`.</Tip>

### 4. Run your bot

Now you're ready to run your bot! Start it with:

```bash
python bot.py
```

<Tip>**Using `uv`?** Run with: `uv run bot.py`</Tip>

You should see output similar to this:

```
🚀 WebRTC server starting at http://localhost:7860/client
   Open this URL in your browser to connect!
```

<Note>
  **First run timing**: The initial startup may take around 15 seconds as
  Pipecat downloads required models like the Silero VAD (Voice Activity
  Detection) model. Subsequent runs will be much faster.
</Note>

### 5. Connect and test

**Open http://localhost:7860/client in your browser**. You'll see the Voice UI Kit console interface with a connect button in the upper right corner.

Click **Connect** and allow microphone access when prompted. The console will establish a connection to your bot server and you can start having a voice conversation!

When you're finished, click **Disconnect** or close the browser tab to end the session. You can also stop the bot by pressing **Ctrl+C** in your terminal.

## Understanding the Quickstart Bot

When you speak to your bot, here's the real-time pipeline that processes your conversation:

1. **Audio Capture**: Your browser captures microphone audio and sends it via WebRTC
2. **Voice Activity Detection**: Silero VAD detects when you start and stop speaking
3. **Speech Recognition**: Deepgram converts your speech to text in real-time
4. **Language Processing**: OpenAI's GPT model generates an intelligent response
5. **Speech Synthesis**: Cartesia converts the response text back to natural speech
6. **Audio Playback**: The generated audio streams back to your browser

Each step happens with minimal latency, typically completing the full round-trip in under one second.

### AI Services

Your bot uses three AI services, each configured with API keys from your `.env` file:

```python
# Create AI Services
stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",
)
llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
```

Pipecat supports many different AI services. You can swap out Deepgram for Azure Speech, OpenAI for Anthropic, or Cartesia for ElevenLabs without changing the rest of your code.

See the [supported services documentation](https://docs.pipecat.ai/server/services/supported-services) for all available options.

### Context and Messages

Your bot maintains conversation history using a context object, enabling multi-turn interactions where the bot remembers what was said earlier.

The context is initialized with a system message that defines the bot's personality:

```python
# All messages use the OpenAI message format
messages = [
    {
        "role": "system",
        "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational.",
    },
]

context = OpenAILLMContext(messages)
context_aggregator = llm.create_context_aggregator(context)
```

The context aggregator automatically collects user messages (after speech-to-text) and assistant responses (after text-to-speech), maintaining the conversation flow without manual intervention.

### RTVI Protocol

When building web or mobile clients, you can use [Pipecat's client SDKs](https://docs.pipecat.ai/client/introduction) that communicate with your bot via the RTVI (Real-Time Voice Interaction) protocol. In our quickstart example, we initialize the RTVI processor to handle client-server messaging and events:

```python
rtvi = RTVIProcessor(config=RTVIConfig(config=[]))
```

### Pipeline Configuration

The core of your bot is a Pipeline that processes data through a series of processors:

```python
# Create the pipeline with the processors
pipeline = Pipeline([
    transport.input(),              # Receive audio from browser
    rtvi,                           # Protocol for client/server messaging and events
    stt,                            # Speech-to-text (Deepgram)
    context_aggregator.user(),      # Add user message to context
    llm,                            # Language model (OpenAI)
    tts,                            # Text-to-speech (Cartesia)
    transport.output(),             # Send audio back to browser
    context_aggregator.assistant(), # Add bot response to context
])
```

Data flows through the pipeline as "frames", objects containing audio, text, or other data types. The ordering is crucial: audio must be transcribed before it can be processed by the LLM, and text must be synthesized before it can be played back.

The pipeline is managed by a PipelineTask:

```python
# Create a PipelineTask to manage the pipeline execution
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
    observers=[RTVIObserver(rtvi)],
)
```

The task handles pipeline execution, collects metrics, and manages RTVI events through observers.

### Event Handlers

Event handlers manage the bot's lifecycle and user interactions:

```python
# Event handler for when a client connects
@transport.event_handler("on_client_connected")
async def on_client_connected(transport, client):
    logger.info(f"Client connected")
    # Add a greeting message to the context
    messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
    # Get a context frame and queue it for the task
    # This is what prompts the bot to start talking when the client connects
    await task.queue_frames([context_aggregator.user().get_context_frame()])

# Event handler for when a client disconnects
@transport.event_handler("on_client_disconnected")
async def on_client_disconnected(transport, client):
    logger.info(f"Client disconnected")
    # Cancel the task when the client disconnects
    # This stops the pipeline and all processors, cleaning up resources
    await task.cancel()
```

When a client connects, the bot adds a greeting instruction and queues a context frame to initiate the conversation. When disconnecting, it properly cancels the task to clean up resources.

### Running the Pipeline

Finally, the pipeline is executed by a PipelineRunner:

```python
# Create a PipelineRunner to run the task
runner = PipelineRunner(handle_sigint=False)

# Finally, run the task using the runner
# This will start the pipeline and begin processing frames
await runner.run(task)
```

The runner manages the pipeline's execution lifecycle. Note that `handle_sigint=False` because the main runner handles system signals.

### Bot Entry Point

The quickstart uses Pipecat's runner system:

```python
async def bot(session_args: SmallWebRTCSessionArguments):
    """Main bot entry point for the bot starter."""

    transport = SmallWebRTCTransport(
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
        webrtc_connection=session_args.webrtc_connection,
    )

    await run_bot(transport)

if __name__ == "__main__":
    from pipecat.runner.run import main
    main()
```

This runner automatically handles WebRTC connection setup and management, making it easy to get started with minimal configuration.

<Tip>
  **Ready for production?** This bot pattern is compatible with Pipecat Cloud,
  meaning you can deploy your bot without any code changes.
</Tip>

## Troubleshooting

- **Browser permissions**: Make sure to allow microphone access when prompted by your browser.
- **Connection issues**: If the WebRTC connection fails, first try a different browser. If that fails, make sure you don't have a VPN or firewall rules blocking traffic. WebRTC uses UDP to communicate.
- **Audio issues**: Check that your microphone and speakers are working and not muted.

## Next Steps

Now that you have your first bot working, learn how to build applications for different platforms:

<CardGroup cols={2}>
  <Card
    title="Client/Server Web App"
    icon="laptop-mobile"
    href="https://github.com/pipecat-ai/pipecat-quickstart-client-server"
  >
    Build custom web and mobile interfaces using Pipecat's client SDKs and Voice UI Kit
  </Card>

  <Card
    title="Phone Bot with Twilio"
    icon="phone"
    href="https://github.com/pipecat-ai/pipecat-quickstart-phone-bot"
  >
    Connect your bot to a phone number so users can call and have voice conversations
  </Card>
</CardGroup>

You can also:

- **Customize your bot**: Edit the system prompt in `bot.py` to change your agent's personality and behavior. Try different roles like a helpful assistant, creative writer, or domain expert.
- **Join the community**: Connect with other Pipecat developers on [Discord](https://discord.gg/pipecat) to share your projects, get help, and see what others are building.



================================================
FILE: guides/introduction.mdx
================================================
---
title: Guides
description: "Learn how to deploy, scale, and extend your Pipecat applications"
---

These guides cover key aspects of building and deploying Pipecat applications. Choose a guide based on what you want to accomplish:

## Features

<CardGroup cols={2}>
  <Card
    title="OpenAI Audio Models and APIs"
    icon="microphone"
    href="/guides/features/openai-audio-models-and-apis"
  >
    Build voice agents with OpenAI audio models
  </Card>
  <Card
    title="Gemini Multimodal Live"
    icon="robot"
    href="/guides/features/gemini-multimodal-live"
  >
    Build real-time AI chatbots with Gemini
  </Card>
  <Card
    title="Function Calling"
    icon="function"
    href="/guides/fundamentals/function-calling"
  >
    Implement custom functions in your bot
  </Card>
  <Card
    title="Metrics & Monitoring"
    icon="chart-line"
    href="/guides/features/metrics"
  >
    Track and monitor your application
  </Card>
  <Card title="Noise Reduction" icon="waveform" href="/guides/features/krisp">
    Improve audio quality with Krisp
  </Card>
  <Card
    title="Pipecat Flows"
    icon="diagram-project"
    href="/guides/features/pipecat-flows"
  >
    Build structured conversation flows
  </Card>
</CardGroup>

## Telephony

<CardGroup cols={2}>
  <Card title="Overview" icon="book-open" href="/guides/telephony/overview">
    Introduction to voice and telephony features
  </Card>
  <Card
    title="WebRTC with Daily"
    icon="phone"
    href="/guides/telephony/daily-webrtc"
  >
    Implement dial-in using Daily's WebRTC
  </Card>
  <Card
    title="Twilio + Daily Integration"
    icon="phone-volume"
    href="/guides/telephony/twilio-daily-webrtc"
  >
    Combine Twilio and Daily for advanced telephony
  </Card>
  <Card
    title="WebSockets with Twilio"
    icon="plug"
    href="/guides/telephony/twilio-websockets"
  >
    Using WebSockets for Twilio integration
  </Card>
  <Card
    title="Dialout Capabilities"
    icon="phone-arrow-up-right"
    href="/guides/telephony/dialout"
  >
    Enable outbound calling with Daily
  </Card>
</CardGroup>

## Deployment

<CardGroup cols={2}>
  <Card title="Overview" icon="book-open" href="/guides/deployment/overview">
    Learn the basics of deploying Pipecat applications
  </Card>
  <Card
    title="Deployment Patterns"
    icon="sitemap"
    href="/guides/deployment/pattern"
  >
    Common architectures and deployment strategies
  </Card>
  <Card
    title="Deploying to Pipecat Cloud"
    icon="cat"
    href="/guides/deployment/pipecat-cloud"
  >
    Step-by-step guide for deploying to Pipecat Cloud
  </Card>
  <Card title="Deploying to Fly.io" icon="cloud" href="/guides/deployment/fly">
    Step-by-step guide for deploying to Fly.io
  </Card>
  <Card
    title="Deploying to Cerebrium"
    icon="server"
    href="/guides/deployment/cerebrium"
  >
    Deploy your application on Cerebrium
  </Card>
</CardGroup>

## New to Pipecat?

<Card title="Get started" icon="rocket" href="/getting-started/installation">
  Check out our Getting Started guide to build your first application
</Card>



================================================
FILE: guides/deployment/cerebrium.mdx
================================================
---
title: "Example: Cerebrium"
description: "Deploy Pipecat applications to Cerebrium"
---

[Cerebrium](https://www.cerebrium.ai) is a serverless Infrastructure platform that makes it easy for companies to build, deploy and scale AI applications.
Cerebrium offers both CPUs and GPUs (H100s, A100s etc) with extremely low cold start times allowing us to create highly performant applications in the most cost efficient manner.

### Install the Cerebrium CLI

To get started, let us run the following commands:

1. Run `pip install cerebrium` to install the Python package.
2. Run `cerebrium login` to authenticate yourself.

If you don't have a Cerebrium account, you can create one and get started with $30 in free credits.

### Create a Cerebrium project

1. Create a new Cerebrium project:

```bash
cerebrium init pipecat-agent
```

2. This will create two key files:

- `main.py` - Your application entrypoint
- `cerebrium.toml` - Configuration for build and environment settings

Update your `cerebrium.toml` with the necessary configuration:

```toml
[cerebrium.hardware]
region = "us-east-1"
provider = "aws"
compute = "CPU"
cpu = 4
memory = 18.0

[cerebrium.dependencies.pip]
torch = ">=2.0.0"
"pipecat-ai[silero, daily, openai, cartesia]" = "latest"
aiohttp = "latest"
torchaudio = "latest"
```

In order for our application to work, we need to copy our API keys from the various platforms. Navigate to the Secrets section in your Cerebrium dashboard to store your API keys:

- `OPENAI_API_KEY` - We use OpenAI For the LLM. You can get your API key from [here](https://platform.openai.com/api-keys)
- `DAILY_TOKEN` - For WebRTC communication. You can get your token from [here](https://dashboard.daily.co/developers)
- `CARTERSIA_API_KEY` - For text-to-speech services. You can get your API key from [here](https://play.cartesia.ai/keys)

We access these secrets in our code as if they are normal ENV vars. For the above, You can swap in any LLM or TTS service you wish to use.

### Agent setup

We create a basic pipeline setup in our `main.py` that combines our LLM, TTS and Daily WebRTC transport layer.

```python
import os
import sys
import time

import aiohttp
from loguru import logger
from pipecat.frames.frames import EndFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.processors.aggregators.openai_llm_context import (
    OpenAILLMContext,
)
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.transports.services.daily import DailyParams, DailyTransport
from pipecat.vad.silero import SileroVADAnalyzer
from pipecat.vad.vad_analyzer import VADParams


logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

async def main(room_url: str, token: str):
    async with aiohttp.ClientSession() as session:
        transport = DailyTransport(
            room_url,
            token,
            "Friendly bot",
            DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                transcription_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.6)),
            ),
        )


        messages = [
            {
                "role": "system",
                "content": "You are a helpful AI assistant that can switch between two services to showcase the difference in performance and cost: 'openai_realtime' and 'custom'. Respond to user queries and switch services when asked.",
            },
        ]

        llm = OpenAILLMService(
            name="LLM",
            api_key=os.environ.get("OPENAI_API_KEY"),
            model="gpt-4",
        )

        tts = CartesiaTTSService(
            api_key=os.getenv("CARTESIA_API_KEY"),
            voice_id="79a125e8-cd45-4c13-8a67-188112f4dd22",  # British Lady
        )

        custom_context = OpenAILLMContext(messages=messages)
        context_aggregator_custom = llm.create_context_aggregator(custom_context)

        pipeline = Pipeline(
            [
                transport.input(),  # Transport user input
                context_aggregator_custom.user(),
                llm,
                tts,
                context_aggregator_custom.assistant(),
                transport.output(),  # Transport bot output
            ]
        )

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                allow_interruptions=True,
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
        )

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            transport.capture_participant_transcription(participant["id"])
            time.sleep(1.5)
            messages.append(
                {
                    "role": "system",
                    "content": "Introduce yourself.",
                }
            )
            await task.queue_frame(LLMMessagesFrame(messages))

        @transport.event_handler("on_participant_left")
        async def on_participant_left(transport, participant, reason):
            await task.queue_frame(EndFrame())

        @transport.event_handler("on_call_state_updated")
        async def on_call_state_updated(transport, state):
            if state == "left":
                await task.queue_frame(EndFrame())

        runner = PipelineRunner()
        await runner.run(task)
        await session.close()
```

First, in our main function, we initialize the daily transport layer to receive/send the audio/video data from the Daily room we will connect to. You can see we pass the room_url we would like to join as well as a token to authenticate us programmatically joining. We also set our VAD stop seconds which is the amount of time we wait for a pause before our bot will respond - in this example, we set it to 600 milliseconds.

Next we connect to our LLM (OpenAI) as well as our TTS model (Cartesia). By setting 'transcription_enabled=true' we are using the STT from Daily itself. This is where the Pipecat framework helps convert audio data to text and vice versa.

We then put this all together as a PipelineTask which is what Pipecat runs all together. The make up of a task is completely customizable and has support for Image and Vision use cases. Lastly, we have some event handlers for when a user joins/leaves the room.

### Deploy bot

Deploy your application to Cerebrium:

```bash
cerebrium deploy
```

You will then see that an endpoint is created for your bot at `POST \<BASE_URL\>/main` that you can call with your room_url and token. Let us test it.

### Test it out

```python
def create_room():
    url = "https://api.daily.co/v1/rooms/"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.environ.get('DAILY_TOKEN')}",
    }
    data = {
        "properties": {
            "exp": int(time.time()) + 60 * 5,  ##5 mins
            "eject_at_room_exp": True,
        }
    }

    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        room_info = response.json()
        token = create_token(room_info["name"])
        if token and "token" in token:
            room_info["token"] = token["token"]
        else:
            logger.error("Failed to create token")
            return {
                "message": "There was an error creating your room",
                "status_code": 500,
            }
        return room_info
    else:
        logger.error(f"Failed to create room: {response.status_code}")
        return {"message": "There was an error creating your room", "status_code": 500}


def create_token(room_name: str):
    url = "https://api.daily.co/v1/meeting-tokens"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.environ.get('DAILY_TOKEN')}",
    }
    data = {"properties": {"room_name": room_name, "is_owner": True}}

    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        token_info = response.json()
        return token_info
    else:
        logger.error(f"Failed to create token: {response.status_code}")
        return None

if __name__ == "__main__":
    room_info = create_room()
    print(f"Join room: {room_info["url"]}")
    asyncio.run(main(room_info["url"], room_info["token"]))

```

## Future Considerations

Since Cerebrium supports both CPU and GPU workloads if you would like to lower the latency of your application then the best would be to get model weights from various providers and run them locally.
You can do this for:

- LLM: Run any OpenSource model using a framework such as vLLM
- TTS: Both PlayHt and Deepgram offer TTS models that can be run locally
- STT: Deepgram offers a local model that can be run locally

If you implement all three models locally, you should have much better performance. We have been able to get ~300ms voice-to-voice responses.

## Examples

- [Fastest voice agent](https://docs.cerebrium.ai/v4/examples/realtime-voice-agents): Local only implementation
- [RAG voice agent](https://www.cerebrium.ai/blog/creating-a-realtime-rag-voice-agent): Create a voice agent that can do RAG using Cerebrium + OpenAI + Pinecone
- [Twilio voice agent](https://docs.cerebrium.ai/v4/examples/twilio-voice-agent): Create a voice agent that can receive phone calls via Twilio
- [OpenAI Realtime API implementation](https://www.cerebrium.ai/blog/an-alternative-to-openai-realtime-api-for-voice-capabilities): Create a voice agent that can receive phone calls via OpenAI Realtime API



================================================
FILE: guides/deployment/fly.mdx
================================================
---
title: "Example: Fly.io"
description: "Deploy Pipecat bots to Fly.io machines"
---

## Project setup

Let's explore how we can use [fly.io](www.fly.io) to make our app scalable for production by spawning our Pipecat bots on virtual machines with their own resources.

We mentioned [before](/guides/deployment/pattern) that you would ideally containerize the `bot_runner.py` web service and the `bot.py` separately. To keep this example simple, we'll use the same container image for both services.

<Frame>
  ![Fly.io Pipecat deployment](/deployment/images/deployment-fly.png)
</Frame>

### Install the Fly CLI

You can find instructions for creating and setting up your fly account [here](https://fly.io/docs/getting-started/).

## Creating the Pipecat project

<Note>
  We have created a template project
  [here](https://github.com/pipecat-ai/pipecat-examples/tree/main/deployment/flyio-example)
  which you can clone. Since we're targeting production use-cases, this example
  uses Daily (WebRTC) as a transport, but you can configure your bot however you
  like.
</Note>

### Adding a fly.toml

Add a `fly.toml` to the root of your project directory. Here is a basic example:

```toml fly.toml
app = 'some-unique-app-name'
primary_region = 'sjc'

[build]

[env]
FLY_APP_NAME = 'some-unique-app-name'

[http_service]
internal_port = 7860
force_https = true
auto_stop_machines = true
auto_start_machines = true
min_machines_running = 0
processes = ['app']

[[vm]]
memory = 512
cpu_kind = 'shared'
cpus = 1
```

For apps with lots of users, consider what resources your HTTP service will require to meet load. We'll define our `bot.py` resources later, so you can set and scale these as you like (`fly scale ...`)

### Environment setup

Our bot requires some API keys and configuration, so create a `.env` in your project root:

```shell .env
DAILY_API_KEY=
OPENAI_API_KEY=
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=
FLY_API_KEY=
FLY_APP_NAME=
```

Of course, the exact keys you need will depend on which services you are using within your `bot.py`.

<Warning>
  **Important:** your `FLY_APP_NAME` should match the name of your fly instance,
  such as that declared in your fly.toml.
</Warning>

The `.env` will allow us to test in local development, but is not included in the deployment. You'll need to set them as Fly app secrets, which you can do via the Fly dashboard or cli.

`fly secrets set ...`

## Containerize our app

Our Fly deployment will need a container image; let's create a simple `Dockerfile` in the root of the project:

<CodeGroup>
```shell Dockerfile
FROM python:3.11-slim-bookworm

# Open port 7860 for http service

ENV FAST_API_PORT=7860
EXPOSE 7860

# Install Python dependencies

COPY \*.py .
COPY ./requirements.txt requirements.txt
RUN pip3 install --no-cache-dir --upgrade -r requirements.txt

# Install models

RUN python3 install_deps.py

# Start the FastAPI server

CMD python3 bot_runner.py --port ${FAST_API_PORT}

````

```shell .dockerignore
**/.DS_Store
.env
.env.*
fly.toml
````

</CodeGroup>

<Note>You can use any base image as long as Python is available</Note>

Our container does the following:

- Opens port `7860` to serve our `bot_runner.py` FastAPI service.
- Downloads the necessary python dependencies.
- Download / cache the model dependencies the `bot.py` requires.
- Runs the `bot_runner.py` and listens for web requests.

### What models are we downloading?

To support voice activity detection, we're using Silero VAD. Whilst the filesize is not huge, having each new machine download the Silero model at runtime will impact bootup time. Instead, we include the model as part of the Docker image so it's cached and available.

You could, of course, also attach a network volume to each instance if you plan to include larger files as part of your deployment and don't want to bloat the size of your image.

## Launching new machines in `bot_runner.py`

When a user starts a session with our Pipecat bot, we want to launch a new machine on fly.io with it's own system resources.

Let's grab the bot_runner.py from the example repo [here](https://github.com/pipecat-ai/pipecat/blob/jpt/deployment-examples/examples/deployment/flyio-example/bot_runner.py).

This runner differs from others in the Pipecat repo; we've added a new method that sends a REST request to Fly to provision a new machine for the session.

This method is invoked as part of the `/start_bot` endpoint:

```python bot_runner.py
FLY_API_HOST = os.getenv("FLY_API_HOST", "https://api.machines.dev/v1")
FLY_APP_NAME = os.getenv("FLY_APP_NAME", "your-fly-app-name")
FLY_API_KEY = os.getenv("FLY_API_KEY", "")
FLY_HEADERS = {
    'Authorization': f"Bearer {FLY_API_KEY}",
    'Content-Type': 'application/json'
}

def spawn_fly_machine(room_url: str, token: str):
    # Use the same image as the bot runner
    res = requests.get(f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines", headers=FLY_HEADERS)
    if res.status_code != 200:
        raise Exception(f"Unable to get machine info from Fly: {res.text}")
    image = res.json()[0]['config']['image']

    # Machine configuration
    cmd = f"python3 bot.py -u {room_url} -t {token}"
    cmd = cmd.split()
    worker_props = {
        "config": {
            "image": image,
            "auto_destroy": True,
            "init": {
                "cmd": cmd
            },
            "restart": {
                "policy": "no"
            },
            "guest": {
                "cpu_kind": "shared",
                "cpus": 1,
                "memory_mb": 1024 # Note: 512 is just enough to run VAD, but 1gb is better
            }
        },

    }

    # Spawn a new machine instance
    res = requests.post(
        f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines",
        headers=FLY_HEADERS,
        json=worker_props)

    if res.status_code != 200:
        raise Exception(f"Problem starting a bot worker: {res.text}")

    # Wait for the machine to enter the started state
    vm_id = res.json()['id']

    res = requests.get(
        f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines/{vm_id}/wait?state=started",
        headers=FLY_HEADERS)

    if res.status_code != 200:
        raise Exception(f"Bot was unable to enter started state: {res.text}")
```

We want to make sure the machine started ok before returning any data to the user. Fly launches machines pretty fast, but will timeout if things take longer than they should. Depending on your transport method, you may want to optimistically return a response to the user, so they can join the room and poll for the status of their bot.

## Launch the Fly project

Getting your bot on Fly is as simple as:

`fly launch` or `fly launch --org orgname` if you're part of a team.

This will step you through some configuration, and build and deploy your Docker image.

Be sure to configure your app secrets with the necessary environment variables once the deployment has complete.

Assuming all goes well, you can update with any changes with `fly deploy`.

### Test it out

Start a new bot instance by sending a `POST` request to `https://your-fly-url.fly.dev/start_bot`. All being well, this will return a room URL and token.

A nice feature of Fly is the ability to monitor your machines (with live logs) via their dashboard:

https://fly.io/apps/YOUR-APP_NAME/machines

This is really helpful for monitoring the status of your spawned machine, and debugging if things do not work as expected.

<Note>
  This example is configured to expire after 5 minutes. The bot process is also
  configured to exit after the user leaves the room. This is a good way to
  ensure we don't have any hanging VMs, although you'll likely need to configure
  this behaviour this to meet your own needs.

You'll also notice that we set `restart policy` to `no`. This prevents the machine attempting to restart after the session has concluded and the process exits.

</Note>

---

## Important considerations

This example does little in the way of load balancing or app security. Indeed, a user can spawn a new machine on your account simply by sending a `POST` request to the `bot_runner.py`. Be sure to configure a maximum number of instances, or authenticate requests to avoid costs getting out of control.

We also deployed our `bot.py` on a machine with the same image as our `bot_runner.py`. To optimize container file sizes and increase security, consider individual images that only deploy resources they require.



================================================
FILE: guides/deployment/modal.mdx
================================================
---
title: "Example: Modal"
description: "Deploy Pipecat applications to Modal"
---

[Modal](https://www.modal.com) is well-suited for Pipecat deployments because it handles container orchestration, scaling, and cold starts efficiently. This makes it a good choice for production Pipecat bots that need reliable performance.

This guide walks through the Modal example included in the Pipecat repository, which follows the same [deployment pattern](/guides/deployment/pattern).

<Card
  title="Modal example"
  icon="github"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/deployment/modal-example"
>
  View the complete Modal deployment example in our GitHub repository
</Card>

<Frame>
  ![Diagram of the deployment architecture of Modal and Pipecat
  example](images/modal.jpg)
</Frame>

## Install the Modal CLI

<Card
  title="Set up Modal"
  icon="rocket"
  href="https://modal.com/docs/guide#getting-started"
>
  Follow Modal's official instructions for creating an account and setting up
  the CLI
</Card>

## Deploy a self-serve LLM

1. Deploy Modal's OpenAI-compatible LLM service:

   ```bash
   git clone https://github.com/modal-labs/modal-examples
   cd modal-examples
   modal deploy 06_gpu_and_ml/llm-serving/vllm_inference.py
   ```

<Info>
  Refer to Modal's guide and example for [Deploying an OpenAI-compatible LLM
  service with vLLM](https://modal.com/docs/examples/vllm_inference) for more
  details.
</Info>

2. Take note of the endpoint URL from the previous step, which will look like:
   ```
   https://{your-workspace}--example-vllm-openai-compatible-serve.modal.run
   ```
   You'll need this for the `bot_vllm.py` file in the next section.

<Note>
  The default Modal LLM example uses Llama-3.1 and will shut down after 15
  minutes of inactivity. Cold starts take 5-10 minutes. To prepare the service,
  we recommend visiting the `/docs` endpoint (`https://<Modal workspace>--example-vllm-openai-compatible-serve.modal.run/docs`)
  for your deployed LLM and wait for it to fully load before connecting your client.
</Note>

## Deploy FastAPI App and Pipecat pipeline to Modal

1. Setup environment variables:

```bash
cd server
cp env.example .env
# Modify .env to provide your service API Keys
```

<Info>
  Alternatively, you can configure your Modal app to use
  [secrets](https://modal.com/docs/guide/secrets).
</Info>

2. Update the `modal_url` in `server/src/bot_vllm.py` to point to the URL you received from the self-serve LLM deployment in the previous step.

3. From within the `server` directory, test the app locally:

```bash
modal serve app.py
```

4. Deploy to production:

```bash
modal deploy app.py
```

5. Note the endpoint URL produced from this deployment. It will look like:

```bash
https://{your-workspace}--pipecat-modal-fastapi-app.modal.run
```

You'll need this URL for the client's `app.js` configuration mentioned in its README.

## Launch your bots on Modal

### Option 1: Direct Link

Simply click on the URL displayed after running the server or deploy step to launch an agent and be redirected to a Daily room to talk with the launched bot. This will use the OpenAI pipeline.

### Option 2: Connect via an RTVI Client

Follow the instructions provided in the [client folder's README](https://github.com/pipecat-ai/pipecat-examples/tree/main/deployment/modal-example/client/javascript) for building and running a custom client that connects to your Modal endpoint. The provided client includes a dropdown for choosing which bot pipeline to run.

## Navigating your LLM, server, and Pipecat logs

On your [Modal dashboard](https://modal.com/apps), you should have two Apps listed under Live Apps:

1. `example-vllm-openai-compatible`: This App contains the containers and logs used to run your self-hosted LLM. There will be just one App Function listed: `serve`. Click on this function to view logs for your LLM.
2. `pipecat-modal`: This App contains the containers and logs used to run your `connect` endpoints and Pipecat pipelines. It will list two App Functions:
   1. `fastapi_app`: This function is running the endpoints that your client will interact with and initiate starting a new pipeline (`/`, `/connect`, `/status`). Click on this function to see logs for each endpoint hit.
   2. `bot_runner`: This function handles launching and running a bot pipeline. Click on this function to get a list of all pipeline runs and access each run's logs.

## Modal & Pipecat Tips

- In most other Pipecat examples, we use `Popen` to launch the pipeline process from the `/connect` endpoint. In this example, we use a Modal function instead. This allows us to run the pipelines using a separately defined Modal image as well as run each pipeline in an isolated container.
- For the FastAPI and most common Pipecat Pipeline containers, a default `debian_slim` CPU-only should be all that's required to run. GPU containers are needed for self-hosted services.
- To minimize cold starts of the pipeline and reduce latency for users, set `min_containers=1` on the Modal Function that launches the pipeline to ensure at least one warm instance of your function is always available.

## Next steps

<Card
  title="Explore Modal's LLM Examples"
  icon="layer-group"
  href="https://modal.com/docs/examples/vllm_inference"
>
  For next steps on running a self-hosted LLM and reducing latency, check out
  all of Modal's LLM examples
</Card>



================================================
FILE: guides/deployment/overview.mdx
================================================
---
title: "Overview"
description: "How to deploy your Pipecat bot online"
---

## Introduction to Pipecat deployment

You've created your Pipecat bot, had a good chat with it locally, and are eager to share it with the world. Let’s explore how to approach deployment.

<Note>
  We're continually adding further deployment example projects to the Pipecat
  repo, which you can find
  [here](https://github.com/pipecat-ai/pipecat-examples/tree/main/deployment).
</Note>

### Deployment options

You have several options for deploying your Pipecat bot:

1. **Pipecat Cloud** - A purpose-built managed service designed specifically for Pipecat deployments
2. **Self-managed cloud deployment** - Deploy to providers like Fly.io, AWS, Google Cloud Run, etc.
3. **Custom infrastructure** - Run on your own servers or specialized AI infrastructure

The best choice depends on your specific requirements, scale, and expertise.

### Things you'll need

- **Transport service** - Pipecat has existing services for various different [media transport](/server/services/supported-services#transports) modes, such as WebRTC or WebSockets. If you're not using a third-party service for handling media transport, you'll want to make sure that infrastructure is hosted and ready to receive connections.

- **Deployment target** - You can deploy and run Pipecat bots anywhere that can run Python code - Google Cloud Run, AWS, Fly.io etc. We recommend providers that offer APIs, so you can programmatically spawn new bot agents on-demand.

- **Docker** - If you're targeting cloud architecture / VMs, they will most often expect a containerized app. It's worth having Docker installed and setup to run builds. We'll step through creating a `Dockerfile` in this documentation.

## Production-ready bots

In local development, things often work great as you're testing on controlled, stable network conditions. In real-world use-cases, however, your users will likely interact with your bot across a variety of different devices and network conditions.

WebSockets are fine for server-to-server communication or for initial development. But for production use, you'll likely want client-server audio that uses a protocol designed for real-time media transport. For an explanation of the difference between WebSockets and WebRTC, [see this post](https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/#webrtc).

If you're targeting scalable, client-server interactions, we recommend you use WebRTC for the best results.

## Supporting models

Most chatbots require very little in the way of system resources, but if you are making use of custom models or require GPU-powered infrastructure, it's important to consider how to pre-cache local resources so that they are not downloaded at runtime. Your bot processes / VMs should aim to launch and connect as quickly as possible, so the user is not left waiting.

Designing and operating a pool of workers is out of scope for our documentation, but we'll highlight best practices in all of our examples.

As an example of a supporting model, most Pipecat [examples](https://github.com/pipecat-ai/pipecat/tree/main/examples) make use of Silero VAD which we recommend including as part of your Docker image (so it's cached and readily available when your bot runs.) Since the Silero model is quite small, this doesn't inflate the size of the container too much. You may, however, want to consider making large models availabile via a network volume and ensuring your bot knows where to find it.

For Silero specifically, you can read more about how to do download it directly [here](https://pypi.org/project/silero/#:~:text=Models%20are%20downloaded%20on%20demand,downloaded%20to%20a%20cache%20folder).

```python
# Run at buildtime
torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=True
    )
```

---

## Getting started

<Card
  title="Basic deployment pattern"
  icon="draw-square"
  iconType="duotone"
  href="/deployment/pattern"
>
  Introduction to a model for deploying Pipecat bots
</Card>

---

## Provider guides

Once you've familiarized yourself with the Pipecat [deployment pattern](/guides/deployment/pattern), here are some guides that walk you through the process for various deployment options. Remember, your Pipecat bots are simply Python processes, so you can host them on whichever infrastructure or service best suits your project.

<CardGroup cols={3}>
  <Card title="Pipecat Cloud" icon="cloud" href="/deployment/pipecat-cloud">
    Managed service purpose-built for Pipecat deployments
  </Card>
  <Card title="Fly.io" icon="paper-plane" href="/deployment/fly">
    For service-driven / CPU bots
  </Card>
  <Card title="Cerebrium" icon="brain" href="/deployment/cerebrium">
    For GPU-accelerated models & specialized AI infrastructure
  </Card>
</CardGroup>



================================================
FILE: guides/deployment/pattern.mdx
================================================
---
title: "Deployment pattern"
description: "Basic deployment pattern for Pipecat bots"
---

## Project structure

A Pipecat project will often consist of the following:

<Card title="1. Bot file">
  E.g. `bot.py`. Your Pipecat bot / agent, containing all the pipelines that you
  want to run in order to communicate with an end-user. A bot file may take some
  command line arguments, such as a transport URL and configuration.
</Card>

<Card title="2. Bot runner">
  E.g. `bot_runner.py`. Typically a basic HTTP service that listens for incoming
  user requests and spawns the relevant bot file in response.
</Card>

<Note>
  You can call these files whatever you like! We use `bot.py` and
  `bot_runner.py` for simplicity.
</Note>

### Typical user / bot flow

There are many ways to approach connecting users to bots. Pipecat is unopinionated about _how_ exactly you should do this, but it's helpful to put an idea forward.

At a very basic level, it may look something like this:

<Steps>
  <Step title="User requests to join session via client / app">
    Client initiates a HTTP request to a hosted bot runner service.
  </Step>
  <Step title="Bot runner handles the request">
    Authenticates, configures and instantiates everything necessary for the
    session to commence (e.g. a new WebSocket channel, or WebRTC room, etc.)
  </Step>
  <Step title="Bot runner spawns bot / agent">
    A new bot process / VM is created for the user to connect with (passing
    across any necessary configuration.) Your project may load just one bot
    file, contextually swap between multiple, or launch many at once.
  </Step>
  <Step title="Bot instantiates and joins session via specified transport credentials">
    Bot initializes, connects to the session (e.g. locally or via WebSockets,
    WebRTC etc) and runs your bot code.
  </Step>
  <Step title="Bot runner returns status to client">
    Once the bot is ready, the runner resolves the HTTP request with details for
    the client to connect.
  </Step>
</Steps>

<Frame>![Basic pipeline image](/deployment/images/deployment-1.png)</Frame>

---

## Bot runner

The majority of use-cases require a way to trigger and manage a bot session over the internet. We call these bot runners; a HTTP service that provides a gateway for spawning bots on-demand.

The anatomy of a bot runner service is entirery arbitrary, but at very least will have a method that spawns a new bot process, for example:

```python
import uvicorn

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse

app = FastAPI()

@app.post("/start_bot")
async def start_bot(request: Request) -> JSONResponse:
    # ... handle / authenticate the request
    # ... setup the transport session

    # Spawn a new bot process
    try:
       #... create a new bot instance
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to start bot: {e}")

    # Return a URL for the user to join
    return JSONResponse({...})

if __name__ == "__main__":
    uvicorn.run(
        "bot_runner:app",
        host="0.0.0.0",
        port=7860
    )

```

This pseudo code defines a `/start_bot/` endpoint which listens for incoming user `POST` requests or webhooks, then configures the session (such as creating rooms on your transport provider) and instantiates a new bot process.

A client will typically require some information regarding the newly spawned bot, such as a web address, so we also return some JSON with the necessary details.

### Data transport

Your [transport layer](/server/services/transport/daily) is responsible for sending and receiving audio and video data over the internet.

You will have implemented a transport layer as part of your `bot.py` pipeline. This may be a service that you want to host and include in your deployment, or it may be a third-party service waiting for peers to connect (such as [Daily](https://www.daily.co), or a websocket.)

For this example, we will make use of Daily’s WebRTC transport. This will mean that our `bot_runner.py` will need to do some configuration when it spawns a new bot:

1. Create and configure a new Daily room for the session to take place in.
2. Issue both the bot and the user an authentication token to join the session.

Whatever you use for your transport layer, you’ll likely need to setup some environmental variables and run some custom code before spawning the agent.

### Best practice for bot files

A good pattern to work to is the assumption that your `bot.py` is an encapsulated entity and does not have any knowledge of the `bot_runner.py`. You should provide the bot everything it needs to operate during instantiation.

Sticking to this approach helps keep things simple and makes it easier to step through debugging (if the bot launched and something goes wrong, you know to look for errors in your bot file.)

---

## Example

Let's assume we have a fully service-driven `bot.py` that connects to a WebRTC session, passes audio transcription to GPT4 and returns audio text-to-speech with ElevenLabs.

We'll also use Silero voice activity detection, to better know when the user has stopped talking.

```python bot.py
import asyncio
import aiohttp
import os
import sys
import argparse

from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_response import LLMAssistantResponseAggregator, LLMUserResponseAggregator
from pipecat.frames.frames import LLMMessagesFrame, EndFrame
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.transports.services.daily import DailyParams, DailyTransport
from pipecat.vad.silero import SileroVADAnalyzer

from loguru import logger

from dotenv import load_dotenv
load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

daily_api_key = os.getenv("DAILY_API_KEY", "")
daily_api_url = os.getenv("DAILY_API_URL", "https://api.daily.co/v1")


async def main(room_url: str, token: str):
    async with aiohttp.ClientSession() as session:
        transport = DailyTransport(
            room_url,
            token,
            "Chatbot",
            DailyParams(
                api_url=daily_api_url,
                api_key=daily_api_key,
                audio_in_enabled=True,
                audio_out_enabled=True,
                video_out_enabled=False,
                vad_analyzer=SileroVADAnalyzer(),
                transcription_enabled=True,
            )
        )

        tts = ElevenLabsTTSService(
            aiohttp_session=session,
            api_key=os.getenv("ELEVENLABS_API_KEY", ""),
            voice_id=os.getenv("ELEVENLABS_VOICE_ID", ""),
        )

        llm = OpenAILLMService(
            api_key=os.getenv("OPENAI_API_KEY"),
            model="gpt-4o")

        messages = [
            {
                "role": "system",
                "content": "You are Chatbot, a friendly, helpful robot. Your output will be converted to audio so don't include special characters other than '!' or '?' in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by saying hello.",
            },
        ]

        tma_in = LLMUserResponseAggregator(messages)
        tma_out = LLMAssistantResponseAggregator(messages)

        pipeline = Pipeline([
            transport.input(),
            tma_in,
            llm,
            tts,
            transport.output(),
            tma_out,
        ])

        task = PipelineTask(pipeline, params=PipelineParams(allow_interruptions=True))

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            transport.capture_participant_transcription(participant["id"])
            await task.queue_frames([LLMMessagesFrame(messages)])

        @transport.event_handler("on_participant_left")
        async def on_participant_left(transport, participant, reason):
            await task.queue_frame(EndFrame())

        @transport.event_handler("on_call_state_updated")
        async def on_call_state_updated(transport, state):
            if state == "left":
                await task.queue_frame(EndFrame())

        runner = PipelineRunner()

        await runner.run(task)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Bot")
    parser.add_argument("-u", type=str, help="Room URL")
    parser.add_argument("-t", type=str, help="Token")
    config = parser.parse_args()

    asyncio.run(main(config.u, config.t))

```

### HTTP API

To launch this bot, let's create a `bot_runner.py` that:

- Creates an API for users to send requests to.
- Launches a bot as a subprocess.

```python bot_runner.py
import os
import argparse
import subprocess

from pipecat.transports.services.helpers.daily_rest import DailyRESTHelper, DailyRoomObject, DailyRoomProperties, DailyRoomParams

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Load API keys from env
from dotenv import load_dotenv
load_dotenv(override=True)


# ------------ Configuration ------------ #

MAX_SESSION_TIME = 5 * 60  # 5 minutes
# List of require env vars our bot requires
REQUIRED_ENV_VARS = [
    'DAILY_API_KEY',
    'OPENAI_API_KEY',
    'ELEVENLABS_API_KEY',
    'ELEVENLABS_VOICE_ID']

daily_rest_helper = DailyRESTHelper(
    os.getenv("DAILY_API_KEY", ""),
    os.getenv("DAILY_API_URL", 'https://api.daily.co/v1'))


# ----------------- API ----------------- #

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ----------------- Main ----------------- #


@app.post("/start_bot")
async def start_bot(request: Request) -> JSONResponse:
    try:
        # Grab any data included in the post request
        data = await request.json()
    except Exception as e:
        pass

    # Create a new Daily WebRTC room for the session to take place in
    try:
        params = DailyRoomParams(
            properties=DailyRoomProperties()
        )
        room: DailyRoomObject = daily_rest_helper.create_room(params=params)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Unable to provision room {e}")

    # Give the agent a token to join the session
    token = daily_rest_helper.get_token(room.url, MAX_SESSION_TIME)

    # Return an error if we were unable to create a room or a token
    if not room or not token:
        raise HTTPException(
            status_code=500, detail=f"Failed to get token for room: {room_url}")

    try:
        # Start a new subprocess, passing the room and token to the bot file
        subprocess.Popen(
            [f"python3 -m bot -u {room.url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)))
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to start subprocess: {e}")

    # Grab a token for the user to join with
    user_token = daily_rest_helper.get_token(room.url, MAX_SESSION_TIME)

    # Return the room url and user token back to the user
    return JSONResponse({
        "room_url": room.url,
        "token": user_token,
    })


if __name__ == "__main__":
    # Check for required environment variables
    for env_var in REQUIRED_ENV_VARS:
        if env_var not in os.environ:
            raise Exception(f"Missing environment variable: {env_var}.")

    parser = argparse.ArgumentParser(description="Pipecat Bot Runner")
    parser.add_argument("--host", type=str,
                        default=os.getenv("HOST", "0.0.0.0"), help="Host address")
    parser.add_argument("--port", type=int,
                        default=os.getenv("PORT", 7860), help="Port number")
    parser.add_argument("--reload", action="store_true",
                        default=False, help="Reload code on change")

    config = parser.parse_args()

    try:
        import uvicorn

        uvicorn.run(
            "bot_runner:app",
            host=config.host,
            port=config.port,
            reload=config.reload
        )

    except KeyboardInterrupt:
        print("Pipecat runner shutting down...")

```

### Dockerfile

Since our bot is just using Python, our Dockerfile can be quite simple:

<CodeGroup>
```shell Dockerfile
FROM python:3.11-bullseye

# Open port 7860 for http service

ENV FAST_API_PORT=7860
EXPOSE 7860

# Install Python dependencies

COPY \*.py .
COPY ./requirements.txt requirements.txt
RUN pip3 install --no-cache-dir --upgrade -r requirements.txt

# Install models

RUN python3 install_deps.py

# Start the FastAPI server

CMD python3 bot_runner.py --port ${FAST_API_PORT}

````

```python install_deps.py
import torch

# Download (cache) the Silero VAD model
torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)
````

</CodeGroup>

The bot runner and bot `requirements.txt`:

```shell requirements.txt
pipecat-ai[daily,openai,silero]
fastapi
uvicorn
python-dotenv
```

And finally, let's create a `.env` file with our service keys

```shell .env
DAILY_API_KEY=...
OPENAI_API_KEY=...
ELEVENLABS_API_KEY=...
ELEVENLABS_VOICE_ID=...
```

---

### How it works

Right now, this runner is spawning `bot.py` as a subprocess. When spawning the process, we pass through the transport room and token as system arguments to our bot, so it knows where to connect.

Subprocesses serve as a great way to test out your bot in the cloud without too much hassle, but depending on the size of the host machine, it will likely not hold up well under load.

Whilst some bots are just simple operators between the transport and third-party AI services (such as OpenAI), others have somewhat CPU-intensive operations, such as running and loading VAD models, so you may find you’re only able to scale this to support up to 5-10 concurrent bots.

Scaling your setup would require virtualizing your bot with it's own set of system resources, the process of which depends on your cloud provider.

### Best practices

In an ideal world, we'd recommend containerizing your bot and bot runner independently so you can deploy each without any unnecessary dependencies or models.

Most cloud providers will offer a way to deploy various images programmatically, which we explore in the various provider examples in these docs.

For the sake of simplicity defining this pattern, we're just using one container for everything.

### Build and run

We should now have a project that contains the following files:

- `bot.py`
- `bot_runner.py`
- `requirements.txt`
- `.env`
- `Dockerfile`

You can now `docker build ...` and deploy your container. Of course, you can still work with your bot in local development too:

```shell
# Install and activate a virtual env
python -m venv venv
source venv/bin/activate # or OS equivalent

pip install -r requirements.txt

python bot_runner.py --host localhost --reload
```



================================================
FILE: guides/deployment/pipecat-cloud.mdx
================================================
---
title: "Example: Pipecat Cloud"
description: "Deploy Pipecat agents with managed infrastructure"
---

[Pipecat Cloud](https://pipecat.daily.co) is a managed platform for hosting and scaling Pipecat agents in production.

## Prerequisites

Before you begin, you'll need:

- A [Pipecat Cloud account](https://pipecat.daily.co)
- [Docker](https://www.docker.com) installed
- Python 3.10+
- The Pipecat Cloud CLI: `pip install pipecatcloud`

<Card
  title="Quickstart Guide"
  icon="play"
  href="https://docs.pipecat.daily.co/quickstart"
>
  Follow a step-by-step guided experience to deploy your first agent
</Card>

## Choosing a starting point

Pipecat Cloud offers several ways to get started:

1. **Use a starter template** - Pre-built agent configurations for common use cases
2. **Build from the base image** - Create a custom agent using the official base image
3. **Clone the starter project** - A bare-bones project template to customize

### Starter templates

Pipecat Cloud provides several ready-made templates for common agent types:

| Template                                                                                                                     | Description                                                           |
| ---------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| [voice](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/voice)                                   | Voice conversation agent with STT, LLM and TTS                        |
| [twilio](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/twilio)                                 | Telephony agent that works with Twilio                                |
| [natural_conversation](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/natural_conversation)     | Agent focused on natural dialogue flow, allowing a user time to think |
| [openai_realtime](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/openai_realtime)               | Agent using OpenAI's Realtime API                                     |
| [gemini_multimodal_live](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/gemini_multimodal_live) | Multimodal agent using Google's Gemini Multimodal Live API            |
| [vision](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/vision)                                 | Computer vision agent that can analyze images                         |

These templates include a functioning implementation and Dockerfile. You can use them directly:

```bash
# Clone the repository
git clone https://github.com/daily-co/pipecat-cloud-images.git

# Navigate to a starter template
cd pipecat-cloud-images/pipecat-starters/voice

# Customize the agent for your needs
```

## Project structure

Whether using a starter template or building from scratch, a basic Pipecat Cloud project typically includes:

```
my-agent/
├── bot.py             # Your Pipecat pipeline
├── Dockerfile         # Container definition
├── requirements.txt   # Python dependencies
└── pcc-deploy.toml    # Deployment config (optional)
```

### Agent implementation with bot.py

Your agent's `bot.py` code must include a specific `bot()` function that serves as the entry point for Pipecat Cloud. This function has different signatures depending on the transport method:

#### For WebRTC/Daily transports

```python
async def bot(args: DailySessionArguments):
    """Main bot entry point compatible with the FastAPI route handler.

    Args:
        config: The configuration object from the request body
        room_url: The Daily room URL
        token: The Daily room token
        session_id: The session ID for logging
    """
    logger.info(f"Bot process initialized {args.room_url} {args.token}")

    try:
        await main(args.room_url, args.token)
        logger.info("Bot process completed")
    except Exception as e:
        logger.exception(f"Error in bot process: {str(e)}")
        raise
```

#### For WebSocket transports (e.g., Twilio)

```python
async def bot(args: WebSocketSessionArguments):
    """Main bot entry point for WebSocket connections.

    Args:
        ws: The WebSocket connection
        session_id: The session ID for logging
    """
    logger.info("WebSocket bot process initialized")

    try:
        await main(args.websocket)
        logger.info("WebSocket bot process completed")
    except Exception as e:
        logger.exception(f"Error in WebSocket bot process: {str(e)}")
        raise
```

<CardGroup cols={2}>
  <Card
    title="Complete Example: Voice Bot"
    icon="code"
    href="https://github.com/daily-co/pipecat-cloud-images/blob/main/pipecat-starters/voice/bot.py"
  >
    View a complete WebRTC voice bot implementation
  </Card>

  <Card
    title="Complete Example: Twilio Bot"
    icon="phone"
    href="https://github.com/daily-co/pipecat-cloud-images/blob/main/pipecat-starters/twilio/bot.py"
  >
    View a complete Twilio WebSocket bot implementation
  </Card>
</CardGroup>

### Example Dockerfile

Pipecat Cloud provides base images that include common dependencies for Pipecat agents:

```Dockerfile
FROM dailyco/pipecat-base:latest

COPY ./requirements.txt requirements.txt
RUN pip install --no-cache-dir --upgrade -r requirements.txt

COPY ./bot.py bot.py
```

This Dockerfile:

1. Uses the official Pipecat base image
2. Installs Python dependencies from requirements.txt
3. Copies your bot.py file to the container

<Note>
  The base image (dailyco/pipecat-base) includes the HTTP API server, session
  management, and platform integration required to run on Pipecat Cloud. See the
  [base image source
  code](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-base)
  for details.
</Note>

## Building and pushing your Docker image

With your project structure in place, build and push your Docker image:

```bash
# Build the image
docker build --platform=linux/arm64 -t my-first-agent:latest .

# Tag the image for your repository
docker tag my-first-agent:latest your-username/my-first-agent:0.1

# Push the tagged image to the repository
docker push your-username/my-first-agent:0.1
```

<Note>
  Pipecat Cloud requires ARM64 images. Make sure to specify the
  `--platform=linux/arm64` flag when building.
</Note>

## Managing secrets

Your agent likely requires API keys and other credentials. Create a secret set to store them securely:

```bash
# Create an .env file with your credentials
touch .env

# Create a secret set from the file
pcc secrets set my-first-agent-secrets --file .env
```

## Deploying your agent

Deploy your agent with the CLI:

```bash
pcc deploy my-first-agent your-username/my-first-agent:0.1 --secret-set my-first-agent-secrets
```

For a more maintainable approach, create a `pcc-deploy.toml` file:

```toml
agent_name = "my-first-agent"
image = "your-username/my-first-agent:0.1"
secret_set = "my-first-agent-secrets"
image_credentials = "my-first-agent-image-credentials"  # For private repos

[scaling]
    min_instances = 0
```

Then deploy using:

```bash
pcc deploy
```

## Starting a session

Once deployed, you can start a session with your agent:

```bash
# Create and set a public access key if needed
pcc organizations keys create
pcc organizations keys use

# Start a session using Daily for WebRTC
pcc agent start my-first-agent --use-daily
```

This will open a Daily room where you can interact with your agent.

## Checking deployment status

Monitor your agent deployment:

```bash
# Check deployment status
pcc agent status my-first-agent

# View deployment logs
pcc agent logs my-first-agent
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Agent Images"
    icon="box"
    href="https://docs.pipecat.daily.co/agents/agent-images"
  >
    Learn about containerizing your agent
  </Card>
  <Card
    title="Secrets"
    icon="key"
    href="https://docs.pipecat.daily.co/agents/secrets"
  >
    Managing sensitive information
  </Card>
  <Card
    title="Scaling"
    icon="arrows-up-down-left-right"
    href="https://docs.pipecat.daily.co/agents/scaling"
  >
    Configure scaling for production workloads
  </Card>
  <Card
    title="Active Sessions"
    icon="phone"
    href="https://docs.pipecat.daily.co/agents/active-sessions"
  >
    Understand how sessions work
  </Card>
</CardGroup>



================================================
FILE: guides/features/gemini-multimodal-live.mdx
================================================
---
title: "Building with Gemini Multimodal Live"
sidebarTitle: "Gemini Multimodal Live"
description: "Create a real-time AI chatbot using Gemini Multimodal Live and Pipecat"
---

This guide will walk you through building a real-time AI chatbot using Gemini Multimodal Live and Pipecat. We'll create a complete application with a Pipecat server and a Pipecat React client that enables natural conversations with an AI assistant.

<Frame>
  ![Pipecat + Gemini Multimodal Live](/images/gemini-client-final.png)
</Frame>

<CardGroup cols={3}>
  <Card title="API Reference" icon="book" href="/server/services/s2s/gemini">
    Gemini Multimodal Live API documentation
  </Card>
  <Card
    title="Example Code"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    Find the complete client and server code in Github
  </Card>
  <Card title="Client SDK" icon="react" href="/client/react/introduction">
    Pipecat React SDK documentation
  </Card>
</CardGroup>

## What We'll Build

In this guide, you'll create:

- A FastAPI server that manages bot instances
- A Gemini-powered conversational AI bot
- A React client with real-time audio/video
- A complete pipeline for speech-to-speech interaction

## Key Concepts

Before we dive into implementation, let's cover some important concepts that will help you understand how Pipecat and Gemini work together.

### Understanding Pipelines

At the heart of Pipecat is the pipeline system. A pipeline is a sequence of processors that handle different aspects of the conversation flow. Think of it like an assembly line where each station (processor) performs a specific task.

For our chatbot, the pipeline looks like this:

```python
pipeline = Pipeline([
    transport.input(),              # Receives audio/video from the user via WebRTC
    rtvi,                           # Handles client/server messaging and events
    context_aggregator.user(),      # Manages user message history
    llm,                            # Processes speech through Gemini
    talking_animation,              # Controls bot's avatar
    transport.output(),             # Sends audio/video back to the user via WebRTC
    context_aggregator.assistant(), # Manages bot message history
])
```

### Processors

Each processor in the pipeline handles a specific task:

<CardGroup cols={2}>
  <Card title="Transport" icon="arrow-right-arrow-left">
    `transport.input()` and `transport.output()` handle media streaming with
    Daily
  </Card>

{" "}

<Card title="Context" icon="brain">
  `context_aggregator` maintains conversation history for natural dialogue
</Card>

{" "}

<Card title="Speech Processing" icon="waveform-lines">
  `rtvi_user_transcription` and `rtvi_bot_transcription` handle speech-to-text
</Card>

  <Card title="Animation" icon="robot">
    `talking_animation` controls the bot's visual state based on speaking
    activity
  </Card>
</CardGroup>

<Note>
The order of processors matters! Data flows through the pipeline in sequence,
so each processor should receive the data it needs from previous processors.

Learn more about the [Core Concepts](/getting-started/core-concepts) to Pipecat server.

</Note>

### Gemini Integration

The `GeminiMultimodalLiveLLMService` is a speech-to-speech LLM service that interfaces with the Gemini Multimodal Live API.

It provides:

- Real-time speech-to-speech conversation
- Context management
- Voice activity detection
- Tool use

<Note>
  Pipecat manages two types of connections:
  1. A WebRTC connection between the Pipecat client and server for reliable audio/video streaming
  2. A WebSocket connection between the Pipecat server and Gemini for real-time AI processing

This architecture ensures stable media streaming while maintaining responsive AI interactions.

</Note>

## Prerequisites

Before we begin, you'll need:

- Python 3.10 or higher
- Node.js 16 or higher
- A [Daily API key](https://dashboard.daily.co/u/signup?pipecat=1)
- A Google API key with Gemini Multimodal Live access
- Clone the Pipecat repo:

```bash
git clone git@github.com:pipecat-ai/pipecat.git
```

## Server Implementation

Let's start by setting up the server components. Our server will handle bot management, room creation, and client connections.

### Environment Setup

1. Navigate to the simple-chatbot's server directory:

```bash
cd examples/simple-chatbot/server
```

2. Set up a python virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install requirements:

```bash
pip install -r requirements.txt
```

4. Copy env.example to .env and make a few changes:

```bash
# Remove the hard-coded example room URL
DAILY_SAMPLE_ROOM_URL=

# Add your Daily and Gemini API keys
DAILY_API_KEY=[your key here]
GEMINI_API_KEY=[your key here]

# Use Gemini implementation
BOT_IMPLEMENTATION=gemini
```

### Server Setup (server.py)

`server.py` is a FastAPI server that creates the meeting room where clients and bots interact, manages bot instances, and handles client connections. It's the orchestrator that brings everything on the server-side together.

#### Creating Meeting Room

The server uses Daily's API via a [REST API helper](/server/utilities/daily/rest-helper#create-room) to create rooms where clients and bots can meet. Each room is a secure space for audio/video communication:

```python server/server.py
async def create_room_and_token():
    """Create a Daily room and generate access credentials."""
    room = await daily_helpers["rest"].create_room(DailyRoomParams())
    token = await daily_helpers["rest"].get_token(room.url)
    return room.url, token
```

#### Managing Bot Instances

When a client connects, the server starts a new bot instance configured specifically for that room. It keeps track of running bots and ensures there's only one bot per room:

```python server/server.py
# Start the bot process for a specific room
bot_file = "bot-gemini.py"
proc = subprocess.Popen([f"python3 -m {bot_file} -u {room_url} -t {token}"])
bot_procs[proc.pid] = (proc, room_url)
```

#### Connection Endpoints

The server provides two ways to connect:

<CardGroup cols={2}>
  <Card title="Browser Access (/)" icon="browser">
    Creates a room, starts a bot, and redirects the browser to the Daily meeting
    URL. Perfect for quick testing and development.
  </Card>

  <Card title="RTVI Client (/connect)" icon="plug">
    Creates a room, starts a bot, and returns connection credentials. Used by
    RTVI clients for custom implementations.
  </Card>
</CardGroup>

### Bot Implementation (bot-gemini.py)

The bot implementation connects all the pieces: Daily transport, Gemini service, conversation context, and processors.

Let's break down each component:

#### Transport Setup

First, we configure the Daily transport, which handles WebRTC communication between the client and server.

```python server/bot-gemini.py
transport = DailyTransport(
    room_url,
    token,
    "Chatbot",
    DailyParams(
        audio_in_enabled=True,        # Enable audio input
        audio_out_enabled=True,       # Enable audio output
        video_out_enabled=True,      # Enable video output
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.5)),
    ),
)
```

<Info>
Gemini Multimodal Live audio requirements:

- Input: 16 kHz sample rate
- Output: 24 kHz sample rate

</Info>

#### Gemini Service Configuration

Next, we initialize the Gemini service which will provide speech-to-speech inference and communication:

```python server/bot-gemini.py
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GEMINI_API_KEY"),
    voice_id="Puck",                     # Choose your bot's voice
    params=InputParams(temperature=0.7)  # Set model input params
)
```

#### Conversation Context

We give our bot its personality and initial instructions:

```python server/bot-gemini.py
messages = [{
    "role": "user",
    "content": """You are Chatbot, a friendly, helpful robot.
                 Keep responses brief and avoid special characters
                 since output will be converted to audio."""
}]

context = OpenAILLMContext(messages)
context_aggregator = llm.create_context_aggregator(context)
```

`OpenAILLMContext` is used as a common LLM base service for context management. In the future, we may add a specific context manager for Gemini.

<Note>
  The context aggregator automatically maintains conversation history, helping
  the bot remember previous interactions.
</Note>

#### Processor Setup

We initialize two additional processors in our pipeline to handle different aspects of the interaction:

<CardGroup cols={2}>
  <Card title="RTVI Processors" icon="message-bot">
    `RTVIProcessor`: Handles all client communication events including transcriptions,
    speaking states, and performance metrics
  </Card>

  <Card title="Animation" icon="robot">
    `TalkingAnimation`: Controls the bot's visual state, switching between
    static and animated frames based on speaking status
  </Card>
</CardGroup>

<Info>
  [Learn more](/server/frameworks/rtvi/introduction) about the RTVI framework
  and available processors.
</Info>

#### Pipeline Assembly

Finally, we bring everything together in a pipeline:

```python server/bot-gemini.py
pipeline = Pipeline([
    transport.input(),             # Receive media
    rtvi,                          # Client UI events
    context_aggregator.user(),     # Process user context
    llm,                           # Gemini processing
    ta,                            # Animation (talking/quiet states)
    transport.output(),            # Send media
    context_aggregator.assistant() # Process bot context
])

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
    observers=[RTVIObserver(rtvi)],
)
```

<Tip>
  The order of processors is crucial! For example, the RTVI processor should be 
  early in the pipeline to capture all relevant events.

The `RTVIObserver` monitors the entire pipeline and automatically collects relevant events to send to the client.

</Tip>

## Client Implementation

Our React client uses the [Pipecat React SDK](/client/react/introduction) to communicate with the bot. Let's explore how the client connects and interacts with our Pipecat server.

### Connection Setup

The client needs to connect to our bot server using the same transport type (Daily WebRTC) that we configured on the server:

```typescript examples/react/src/providers/PipecatProvider.tsx
const client = new PipecatClient({
  transport: new DailyTransport(),
  enableMic: true, // Enable audio input
  enableCam: false, // Disable video input
  enableScreenShare: false, // Disable screen sharing
});
client.connect({
  endpoint: "http://localhost:7860/connect", // Your bot connection endpoint
});
```

The connection configuration must match your server:

- `DailyTransport`: Matches the WebRTC transport used in `bot-gemini.py`
- `connect` endpoint: Matches the `/connect` route in `server.py`
- Media settings: Controls which devices are enabled on join

### Media Handling

Pipecat's React components handle all the complex media stream management for you:

```tsx
function App() {
  return (
    <PipecatClientProvider client={client}>
      <div className="app">
        <PipecatClientVideo participant="bot" /> {/* Bot's video feed */}
        <PipecatClientAudio /> {/* Audio input/output */}
      </div>
    </PipecatClientProvider>
  );
}
```

The `PipecatClientProvider` is the root component for providing Pipecat client context to your application. By wrapping your `PipecatClientAudio` and `PipecatClientVideo` components in this provider, they can access the client instance and receive and process the streams received from the Pipecat server.

### Real-time Events

The RTVI processors we configured in the pipeline emit events that we can handle in our client:

```typescript
// Listen for transcription events
useRTVIClientEvent(RTVIEvent.UserTranscript, (data: TranscriptData) => {
  if (data.final) {
    console.log(`User said: ${data.text}`);
  }
});

// Listen for bot responses
useRTVIClientEvent(RTVIEvent.BotTranscript, (data: BotLLMTextData) => {
  console.log(`Bot responded: ${data.text}`);
});
```

<CardGroup cols={2}>
  <Card title="Available Events" icon="bell">
  - Speaking state changes
  - Transcription updates
  - Bot responses
  - Connection status
  - Performance metrics

  </Card>

  <Card title="Event Usage" icon="code">
  
  Use these events to:
  - Show speaking indicators
  - Display transcripts
  - Update UI state
  - Monitor performance

  </Card>

</CardGroup>

<Tip>
  Optionally, uses callbacks to handle events in your application. [Learn
  more](/client/js/api-reference/callbacks#callbacks) in the Pipecat client
  docs.
</Tip>

### Complete Example

Here's a basic client implementation with connection status and transcription display:

```tsx
function ChatApp() {
  return (
    <PipecatClientProvider client={client}>
      <div className="app">
        {/* Connection UI */}
        <StatusDisplay />
        <ConnectButton />

        {/* Media Components */}
        <BotVideo />
        <PipecatClientAudio />

        {/* Debug/Transcript Display */}
        <DebugDisplay />
      </div>
    </PipecatClientProvider>
  );
}
```

<Note>
  Check out the [example
  repository](https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot)
  for a complete client implementation with styling and error handling.
</Note>

## Running the Application

From the `simple-chatbot` directory, start the server and client to test the chatbot:

### 1. Start the Server

In one terminal:

```bash
python server/server.py
```

### 2. Start the Client

In another terminal:

```bash
cd examples/react
npm install
npm run dev
```

### 3. Testing the Connection

1. Open `http://localhost:5173` in your browser
2. Click "Connect" to join a room
3. Allow microphone access when prompted
4. Start talking with your AI assistant

<Warning>
Troubleshooting:
- Check that all API keys are properly configured in .env
- Grant your browser access to your microphone, so it can receive your audio input
- Verify WebRTC ports aren't blocked by firewalls

</Warning>

## Next Steps

Now that you have a working chatbot, consider these enhancements:

- Add custom avatar animations
- Implement [function calling](/guides/fundamentals/function-calling) for external integrations
- Add support for multiple languages
- Enhance error recovery and reconnection logic

### Examples

<CardGroup cols={2}>
  <Card
    title="Foundational Example"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/26a-gemini-multimodal-live-transcription.py"
  >
    A basic implementation demonstrating core Gemini Multimodal Live features and transcription capabilities
  </Card>

  <Card
    title="Simple Chatbot"
    icon="robot"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    A complete client/server implementation showing how to build a Pipecat JS or React client that connects to a Gemini Live Pipecat bot
  </Card>
</CardGroup>

### Learn More

- [Gemini Multimodal Live API Reference](/server/services/s2s/gemini)
- [React Client SDK Documentation](https://docs.pipecat.ai/client/react/introduction)



================================================
FILE: guides/features/krisp.mdx
================================================
---
title: "Noise cancellation with Krisp"
description: "Learn how to integrate Krisp noise cancellation into your Pipecat application"
---

## Overview

This guide will walk you through setting up and using Krisp's noise reduction capabilities in your Pipecat application. Krisp provides professional-grade noise cancellation that can significantly improve audio quality in real-time communications.

To use Krisp's noise cancellation, you'll need to obtain their SDK and models through a Krisp developer account. Our Pipecat Krisp module simplifies the integration process, which we'll cover in this guide.

## Walkthrough

### Get Access to Krisp SDK and Models

1. Create a Krisp developers account at [krisp.ai/developers](https://krisp.ai/developers)
2. Download the [Krisp Desktop SDK (v7)](https://sdk.krisp.ai/sdk/desktop) that matches your platform:
   - Linux: `Desktop SDK v7.0.2: Linux`
   - macOS (ARM): `Desktop SDK v7.0.1: Mac ARM`
   - macOS (Intel): `Desktop SDK v7.0.1: Mac Intel`
   - Windows(ARM): `Desktop SDK v7.0.2: Windows ARM`
   - Windows (x64): `Desktop SDK v7.0.2: Windows`
3. Download the corresponding models. We recommend trying the `Background Voice Cancellation` model. Recommended model for each platform:
   - Linux (ARM): `hs.c6.f.s.de56df.kw`
   - Mac (ARM): `hs.c6.f.s.de56df.kw`
   - Linux (x86_64): `hs.c6.f.s.de56df.kw`
   - Mac (x86_64): `hs.c6.f.s.de56df.kw`
   - Windows (x86_64): `hs.c6.f.s.de56df.kw`

<Frame>![Krisp SDK Portal](/images/krisp-portal.png)</Frame>

### Install build dependencies

The `pipecat-ai-krisp` module is a python wrapper around Krisp's C++ SDK. To build the module, you'll need to install the following dependencies:

<Tabs>
  <Tab title="macOS">
    ```bash
    # Using Homebrew
    brew install cmake pybind11
    ```
  </Tab>
  <Tab title="Ubuntu/Debian">
    ```bash
    # Using apt
    sudo apt-get update
    sudo apt-get install cmake python3-dev pybind11-dev g++
    ```
  </Tab>
  <Tab title="Windows">
    ```powershell
    # Using Chocolatey
    choco install cmake
    # pybind11 will be installed via pip during the build process

    # OR using Visual Studio
    # 1. Install Visual Studio with C++ development tools
    # 2. Install CMake from https://cmake.org/download/
    # pybind11 will be installed via pip during the build process
    ```

  </Tab>
</Tabs>

<Note>
  For Windows users: Make sure you have Visual Studio installed with C++
  development tools, or alternatively, have the Visual C++ Build Tools
  installed.
</Note>

### Install the pipecat-ai-krisp module

In your Pipecat repo, activate your virtual environment:

<Tabs>
  <Tab title="macOS/Linux">
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
  </Tab>
  <Tab title="Windows">
    ```powershell
    # Using PowerShell
    python -m venv venv
    .\venv\Scripts\Activate.ps1

    # OR using Command Prompt
    python -m venv venv
    .\venv\Scripts\activate.bat

    # OR using Git Bash
    python -m venv venv
    source venv/Scripts/activate
    ```

  </Tab>
</Tabs>

Export the path to your Krisp SDK and model files:

<Tabs>
  <Tab title="macOS/Linux">
    ```bash
    # Add to your .env file or shell configuration
    export KRISP_SDK_PATH=/PATH/TO/KRISP/SDK
    export KRISP_MODEL_PATH=/PATH/TO/KRISP/MODEL.kef
    ```
  </Tab>
  <Tab title="Windows">
    ```powershell
    # Using PowerShell
    $env:KRISP_SDK_PATH = "C:\PATH\TO\KRISP\SDK"
    $env:KRISP_MODEL_PATH = "C:\PATH\TO\KRISP\MODEL.kef"

    # OR using Command Prompt
    set KRISP_SDK_PATH=C:\PATH\TO\KRISP\SDK
    set KRISP_MODEL_PATH=C:\PATH\TO\KRISP\MODEL.kef
    ```

  </Tab>
</Tabs>

<Tip>
  When selecting a `KRISP_MODEL_PATH`, ensure that you're selecting the actual
  model file, not just the directory. The path should look something like this:
  <Tabs>
    <Tab title="ARM Linux">
      ```bash
      export KRISP_MODEL_PATH=./krisp/hs.c6.f.s.de56df.kw
      ```
    </Tab>
    <Tab title="Mac ARM">
      ```bash export
      KRISP_MODEL_PATH=./krisp/outbound-bvc-models-fp16/hs.c6.f.s.de56df.bucharest.kef
      ```
    </Tab>
    <Tab title="Windows">
      ```powershell
      $env:KRISP_MODEL_PATH="C:\krisp\outbound-bvc-models-fp32\hs.c6.f.s.de56df.bucharest.kef"
      ```
    </Tab>

  </Tabs>
</Tip>

Next, install the `pipecat-ai[krisp]` module, which will automatically build the `pipecat-ai-krisp` python wrapper module:

```bash
pip install "pipecat-ai[krisp]"
```

### Test the integration

You can now test the Krisp integration. The easiest way to do this is to run the foundational example: [07p-interruptible-krisp.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07p-interruptible-krisp.py).

<Tip>
You can run a foundational example by running the following command:

```bash
python examples/foundational/07p-interruptible-krisp.py -u YOUR_DAILY_ROOM_URL
```

</Tip>

#### Important for macOS users

If you're running on macOS you may receive a security warning about running the script. This is expected. You can allow access by going to `System Settings > Privacy & Security` then click `Allow Anyway` to permit the example to run.

<Frame>![Security & Privacy](/images/allow-krisp.png)</Frame>

After allowing and re-running, you may get a pop-up asking for permission. Select `Open Anyway` to run the script.

<Frame>![Open Anyway](/images/open-anyway-krisp.png)</Frame>

## Usage Example

Here's a basic example of how to add Krisp noise reduction to your Pipecat pipeline:

```python
from pipecat.audio.filters.krisp_filter import KrispFilter
from pipecat.transports.services.daily import DailyParams, DailyTransport

# Add to transport configuration
transport = DailyTransport(
    room_url,
    token,
    "Audio Bot",
    DailyParams(
        audio_in_filter=KrispFilter(),  # Enable Krisp noise reduction
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)
```

## Troubleshooting

Common issues and solutions:

1. **Missing Dependencies**

```bash
Error: Missing module: pipecat_ai_krisp
```

Solution: Ensure you've installed with the krisp extra: `pip install "pipecat-ai[krisp]"`

2. **Model Path Not Found**

```bash
Error: Model path for KrispAudioProcessor must be provided
```

Solution: Set the `KRISP_MODEL_PATH` environment variable or provide it in the constructor

3. **SDK Path Issues**

```bash
Error: Cannot find Krisp SDK
```

Solution: Verify `KRISP_SDK_PATH` points to a valid Krisp SDK installation

## Additional Resources

- [KrispFilter Reference Documentation](/api-reference/utilities/audio/krisp-filter)
- [Example Application](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07p-interruptible-krisp.py)



================================================
FILE: guides/features/metrics.mdx
================================================
---
title: "Metrics"
description: "Learn how to monitor performance and LLM/TTS usage with Pipecat."
---

When developing real-time, multimodal AI applications, monitoring two key
factors is crucial: performance (latency) and LLM/TTS usage. Performance impacts
user experience, while usage can affect operational costs. Pipecat offers
built-in metrics for both, which can be enabled with straightforward
configuration options.

## Enabling performance metrics

Set `enable_metrics=True` in `PipelineParams` when creating a task:

```python Example config
task = PipelineTask(
            pipeline,
            params=PipelineParams(
                ...
                enable_metrics=True,
                ...
            ),
        )
```

Once enabled, Pipecat logs the following metrics:

| Metric          | Description                                     |
| --------------- | ----------------------------------------------- |
| TTFB            | Time To First Byte in seconds                   |
| Processing Time | Time taken by the service to respond in seconds |

```console Sample output
AnthropicLLMService#0 TTFB: 0.8378312587738037
CartesiaTTSService#0 processing time: 0.0005071163177490234
CartesiaTTSService#0 TTFB: 0.17177796363830566
AnthropicLLMService#0 processing time: 2.4927797317504883
```

### Limiting TTFB responses

If you only want the **first** TTFB measurement for each service, you can
optionally pass `report_only_initial_ttfb=True` in `PipelineParams`:

```python Example config
task = PipelineTask(
            pipeline,
            params=PipelineParams(
                ...
                enable_metrics=True,
                report_only_initial_ttfb=True,
                ...
            ),
        )
```

> **Note:** `enable_metrics=True` is required for this setting to have an
> effect.

# Enabling LLM/TTS Usage Metrics

Set `enable_usage_metrics=True` in PipelineParams when creating a task:

```python Example config
task = PipelineTask(
            pipeline,
            params=PipelineParams(
                ...
                enable_usage_metrics=True,
                ...
            ),
        )
```

Pipecat will log the following as applicable:

| Metric    | Description                                 |
| --------- | ------------------------------------------- |
| LLM Usage | Number of prompt and completion tokens used |
| TTS Usage | Number of characters processed              |

```console Sample output
CartesiaTTSService#0 usage characters: 65
AnthropicLLMService#0 prompt tokens: 104, completion tokens: 53
```

> **Note:** Usage metrics are recorded per interaction and do not represent
> running totals.



================================================
FILE: guides/features/openai-audio-models-and-apis.mdx
================================================
---
title: "Building With OpenAI Audio Models and APIs"
sidebarTitle: "OpenAI Audio Models and APIs"
description: "Create voice agents with OpenAI audio models and Pipecat"
---

This guide provides an overview of the audio capabilities OpenAI offers via their APIs. We'll also link to Pipecat sample code.

## Two Ways To Build Voice-to-voice

You can build voice-to-voice applications in two ways:

1. The cascaded models approach, using separate models for transcription, the LLM, and voice generation.

<Frame>![OpenAI Cascaded Pipeline](/images/openai-cascade.jpg)</Frame>

A cascaded pipeline looks like this, in Pipecat code. Here's a [single-file example that uses a cascaded pipeline](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py). (See below for an overview of Pipecat core concepts.)

```python
pipeline = Pipeline(
    [
        transport.input(),
        speech_to_text,
        context_aggregator.user(),
        llm,
        text_to_speech,
        context_aggregator.assistant(),
        transport.output(),
    ]
)
```

2. Using a single, speech-to-speech model. This is conceptually much simpler. Though note that most applications also need to implement things like function calling, retrieval-augmented search, context management, and integration with existing systems. So the core pipeline is only part of an app's complexity.

<Frame>![OpenAI S2S Pipeline](/images/openai-s2s.jpg)</Frame>

Here's a speech-to-speech pipeline in Pipecat code. And here's a [single-file example that uses the OpenAI Realtime API](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/19-openai-realtime-beta.py).

```python
pipeline = Pipeline(
    [
        transport.input(),
        context_aggregator.user(),
        speech_to_speech_llm,
        context_aggregator.assistant(),
        transport.output(),
    ]
)
```

Which approach should you choose?

- The cascaded models approach is preferable if you are implementing a complex workflow and need the best possible instruction following performance and function calling reliability. The `gpt-4o` model operating in text-to-text mode has the strongest instruction following and function calling performance.
- The speech-to-speech approach offers better audio understanding and human-like voice output. If your application is primarily free-form, open-ended conversation, these attributes might be more important than instruction following and function calling performance. Note also that `gpt-4o-audio-preview` and the OpenAI Realtime API are currently beta products.

## OpenAI Audio Models and APIs

### Transcription API

- Models: `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`
- Pipecat service: `OpenAISTTService` ([reference docs](https://docs.pipecat.ai/server/services/stt/openai))
- OpenAI endpoint: `/v1/audio/transcriptions` ([docs](https://platform.openai.com/docs/api-reference/audio/createTranscription))

### Chat Completions API

- Models: `gpt-4o`, `gpt-4o-mini`, `gpt-4o-audio-preview`
- Pipecat service: `OpenAILLMService` ([reference docs](https://docs.pipecat.ai/server/services/llm/openai))
- OpenAI endpoint: `/v1/chat/completions` ([docs](https://platform.openai.com/docs/api-reference/chat))

### Realtime API

- Models: `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`
- Pipecat service: `OpenAIRealtimeBetaLLMService` ([reference docs](https://docs.pipecat.ai/server/services/s2s/openai))
- OpenAI docs ([overview](https://platform.openai.com/docs/guides/realtime))

### Speech API

- Models: `gpt-4o-mini-tts`
- Pipecat service: `OpenAITTSService` ([reference docs](https://docs.pipecat.ai/server/services/tts/openai))
- OpenAI endpoint: `/v1/audio/speech` ([docs](https://platform.openai.com/docs/api-reference/audio/createSpeech))

## Sample code and starter kits

_If you have a code example or starter kit you would like this doc to link to, please let us know. We can add examples that help people get started with the OpenAI audio models and APIs._

### Single-file examples

<CardGroup cols={2}>
  <Card
    title="OpenAI STT → LLM → TTS"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py"
  >
    A complete implementation demonstrating the cascaded approach with OpenAI services
  </Card>

  <Card
    title="OpenAI Realtime API"
    icon="tower-broadcast"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/19-openai-realtime-beta.py"
  >
    A speech-to-speech implementation using OpenAI's Realtime API
  </Card>
</CardGroup>

### OpenAI + Twilio + Pipecat Cloud

[This starter kit](https://github.com/daily-co/pcc-openai-twilio/) is a complete telephone voice agent that can talk about the NCAA March Madness basketball tournaments and look up realtime game information using function calls.

<Frame>
![OpenAI Twilio](/images/openai-twilio.png)

</Frame>

The starter kit includes two bot configurations: cascaded model and speech-to-speech. The code can be packaged for deployment to Pipecat Cloud, a commercial platform for Pipecat agent hosting.



================================================
FILE: guides/features/pipecat-flows.mdx
================================================
---
title: "Pipecat Flows"
description: "Learn how to create structured conversations using Pipecat's flow system"
---

## What is Pipecat Flows?

Pipecat Flows is an add-on framework for Pipecat that allows you to build structured conversations in your AI applications. It enables you to define conversation paths while handling the complexities of state management and LLM interactions.

Want to dive right in? Check out these examples:

<CardGroup cols={2}>
  <Card
    title="Hello World"
    icon="rocket"
    href="https://github.com/pipecat-ai/pipecat-flows/tree/main/examples/quickstart"
  >
    A great first Flow to show you the ropes
  </Card>
  <Card
    title="Restaurant Reservation"
    icon="utensils"
    href="https://github.com/pipecat-ai/pipecat-flows/blob/main/examples/dynamic/restaurant_reservation.py"
  >
    A simple, practical Flow example
  </Card>
</CardGroup>

## How do Pipecat and Pipecat Flows work together?

**Pipecat** defines the core capabilities of your bot. This includes the pipeline and processors which, at a minimum, enable your bot to:

- Receive audio from a user
- Transcribe the user's input
- Run an LLM completion
- Convert the LLM response to audio
- Send audio back to the user

**Pipecat Flows** complements Pipecat's core functionality by providing structure to a conversation, managing context and tools as the conversation progresses from one state to another. This is separate from the core pipeline, allowing you to separate conversation logic from core pipeline mechanics.

## When to Use Pipecat Flows?

Pipecat Flows is best suited for use cases where:

- **You need precise control** over how a conversation progresses through specific steps
- **Your bot handles complex tasks** that can be broken down into smaller, manageable pieces
- **You want to improve LLM accuracy** by focusing the model on one specific task at a time instead of managing multiple responsibilities simultaneously

This approach addresses a common problem: traditional methods often use large, monolithic prompts with many tools available at once, leading to hallucinations and lower accuracy.

Pipecat Flows solves this by:

- **Breaking complex tasks into focused steps** - Each node has a clear, single purpose
- **Providing relevant tools only when needed** - Functions are available only in the appropriate context
- **Giving clear, specific instructions** - Task messages focus the LLM on exactly what to do next

## Selecting a Flows Pattern

Pipecat Flows provides two general patterns for how to build your application: Dynamic Flows and Static Flows.

- **Dynamic Flows (recommended)**: When conversation paths need to be determined at runtime based on user input, external data, or business logic.
- **Static Flows**: When your conversation structure is known upfront and follows predefined paths.

<Note>
  Dynamic Flows can handle both simple and complex use cases. Selecting it
  provides you with an option that can grow with the complexity of your
  application. For these reasons, we strongly recommend it as the API pattern to
  follow.
</Note>

## Technical Overview

Pipecat Flows represents a conversation as a graph, where each step of the conversation is represented by a node. Nodes are of type `NodeConfig`, and may contain the following properties:

- `name`: The name of the node; used as a reference to transition to the node.
- `role_messages`: A list of message `dicts` defining the bot's role/personality. Typically set once in the initial node.
- `task_messages`: A list of message `dicts` defining the current node's objectives.
- `functions`: A list of function call definitions and their corresponding handlers.
- `pre_actions`: Actions to execute before LLM inference. Actions run once upon transitioning to a node.
- `post_actions`: Actions to execute after LLM inference. Actions run once after the node's initial LLM inference.
- `context_strategy`: Strategy for updating context during transitions. The default behavior is to append messages to the context.
- `respond_immediately`: Whether to run LLM inference as soon as the node is set. The default is True.

<Info>
  The only required field is `task_messages`, as your bot always needs a prompt
  to advance the conversation.
</Info>

Now that we've defined the structure of a node, let's look at the components that make up a node.

### Messages

Messages define what your bot should do and how it should behave at each node in your conversation flow.

#### Message Types

There are two types of messages you can configure:

**Role Messages** (Optional)
Define your bot's personality, tone, and overall behavior. These are typically set once at the beginning of your flow and establish the consistent persona your bot maintains throughout the conversation.

**Task Messages** (Required)
Define the specific objective your bot should accomplish in the current node. These messages focus the LLM on the immediate task at hand, such as asking a specific question or processing particular information.

#### Message Format

Messages are specified in OpenAI format as a list of `dicts`:

```python
"role_messages": [
    {
        "role": "system",
        "content": "You are an inquisitive child.",
    }
],
"task_messages": [
    {
        "role": "system",
        "content": "Say 'Hello world' and ask what is the user's favorite color.",
    }
],
```

#### Cross-Provider Compatibility

Pipecat's default message format uses the OpenAI message spec. With this message format, messages are automatically translated by Pipecat to work with your chosen LLM provider, making it easy to switch between OpenAI, Anthropic, Google, and other providers without changing your code.

<Warning>
  Some LLMs, like Anthropic and Gemini, can only set the system instruction at
  initialization time. This means you will only be able to set the
  `role_messages` at initialization for those LLMs.
</Warning>

### Functions

Functions in Pipecat Flows serve two key purposes:

1. **Process data** by interfacing with external systems and APIs to read or write information
2. **Progress the conversation** by transitioning between nodes in your flow

#### How Functions Work

When designing your nodes, clearly define the task in the `task_messages` and reference the available functions. The LLM will use these functions to complete the task and signal when it's ready to move forward.

For example, if your node's job is to collect a user's favorite color:

1. The LLM asks the question
2. The user provides their answer
3. The LLM calls the function with the answer
4. The function processes the data and determines the next node

#### Function Definition

Flows provides a universal `FlowsFunctionSchema` that works across all LLM providers:

```python
from pipecat_flows import FlowsFunctionSchema

record_favorite_color_func = FlowsFunctionSchema(
    name="record_favorite_color_func",
    description="Record the color the user said is their favorite.",
    required=["color"],
    handler=record_favorite_color_and_set_next_node,
    properties={"color": {"type": "string"}},
)
```

#### Function Handlers

Each function has a corresponding `handler` where you implement your application logic and specify the next node:

```python
async def record_favorite_color_and_set_next_node(
    args: FlowArgs, flow_manager: FlowManager
) -> tuple[str, NodeConfig]:
    """Function handler that records the color then sets the next node.

    Here "record" means print to the console, but any logic could go here:
    Write to a database, make an API call, etc.
    """
    print(f"Your favorite color is: {args['color']}")
    return args["color"], create_end_node()
```

#### Handler Return Values

Function handlers must return a tuple containing:

- **Result**: Data provided to the LLM for context in subsequent completions
- **Next Node**: The `NodeConfig` for Flows to transition to next

Some handlers may not want to transition conversational state, in which case you can return `None` for the next node. Other handlers may _only_ want to transition conversational state without doing other work, in which case you can return `None` for the result.

#### Direct Functions

For more concise code, you can optionally use Direct Functions where the function definition and handler are combined in a single function. The function signature and docstring are automatically used to generate the function schema:

```python
async def record_favorite_color(
    flow_manager: FlowManager,
    color: str
) -> tuple[FlowResult, NodeConfig]:
    """
    Record the color the user said is their favorite.

    Args:
        color (str): The user's favorite color
    """
    print(f"Your favorite color is: {args["color"]}")
    return args["color"], create_end_node()

# Use directly in NodeConfig
node_config = {
    "functions": [record_favorite_color]
}
```

This approach eliminates the need for separate `FlowsFunctionSchema` definitions while maintaining the same functionality.

### Actions

Actions allow you to execute custom functionality at specific points in your conversation flow, giving you precise control over timing and sequencing.

#### Action Types

- `pre_actions` execute immediately when transitioning to a new node, _before_ the LLM inference begins.
- `post_actions` execute after the LLM inference completes and any TTS has finished speaking.

#### Built-in Actions

Pipecat Flows includes several ready-to-use actions for common scenarios:

- **`tts_say`**: Speak a phrase immediately (useful for "please wait" messages)

```python
"pre_actions": [
    {
        "type": "tts_say",
        "text": "Please hold while I process your request..."
    }
]
```

- **`end_conversation`**: Gracefully terminate the conversation

```python
"post_actions": [
    {
        "type": "end_conversation",
        "text": "Thank you for your time!"
    }
]
```

- **`function`**: Execute a custom function at the specified timing

```python
"post_actions": [
    {
        "type": "function",
        "handler": end_conversation_handler
    }
]
```

#### Custom Actions

You can define your own actions to handle specific business logic or integrations. In most cases, consider using a **function action** first, as it executes at the expected time in the pipeline.

Custom actions give you complete flexibility to execute any functionality your application needs, but require careful timing considerations.

#### Action Timing

The execution order ensures predictable behavior:

1. **Pre-actions** run first upon node entry (in the order they are defined)
2. **LLM inference** processes the node's messages and functions
3. **TTS** speaks the LLM's response
4. **Post-actions** run after TTS completes (in the order they are defined)

This timing guarantees that actions execute in the correct sequence, such as ensuring the bot finishes speaking before ending the conversation. Note that custom actions may not follow this predictable timing, which is another reason to prefer function actions when possible.

### Context Strategy

Flows provides three built-in ways to manage conversation context as you move between nodes:

#### Strategy Types

1. **APPEND** (Default): New node messages are added to the existing context, preserving the full conversation history. The context grows as the conversation progresses.

2. **RESET**: The context is cleared and replaced with only the new node's messages. Useful when previous conversation history is no longer relevant or to reduce context window size.

3. **RESET_WITH_SUMMARY**: The context is cleared but includes an AI-generated summary of the previous conversation along with the new node's messages. Helps reduce context size while preserving key information.

#### When to Use Each Strategy

- Use **APPEND** when full conversation history is important for context
- Use **RESET** when starting a new topic or when previous context might confuse the current task
- Use **RESET_WITH_SUMMARY** for long conversations where you need to preserve key points but reduce context size

#### Configuration Examples

Context strategies can be defined globally in the FlowManager constructor:

```python
from pipecat_flows import ContextStrategy, ContextStrategyConfig

# Global strategy configuration
flow_manager = FlowManager(
    task=task,
    llm=llm,
    context_aggregator=context_aggregator,
    context_strategy=ContextStrategyConfig(
        strategy=ContextStrategy.APPEND,
    )
)
```

Or on a per-node basis:

```python
# Per-node strategy configuration
node_config = {
    "task_messages": [...],
    "functions": [...],
    "context_strategy": ContextStrategyConfig(
        strategy=ContextStrategy.RESET_WITH_SUMMARY,
        summary_prompt="Provide a concise summary of the customer's order details and preferences."
    )
}
```

### Respond Immediately

For each node in the conversation, you can decide whether the LLM should respond immediately upon entering the node (the default behavior) or whether the LLM should wait for the user to speak first before responding. You do this using the `respond_immediately` field.

<Tip>
  `respond_immediately=False` may be particularly useful in the very first node,
  especially in outbound-calling cases where the user has to first answer the
  phone to trigger the conversation.
</Tip>

```python
NodeConfig(
    task_messages=[
        {
            "role": "system",
            "content": "Warmly greet the customer and ask how many people are in their party. This is your only job for now; if the customer asks for something else, politely remind them you can't do it.",
        }
    ],
    respond_immediately=False,
    # ... other fields
)
```

<Warning>
  Keep in mind that if you specify `respond_immediately=False`, the user may not
  be aware of the conversational task at hand when entering the node (the bot
  hasn't told them yet). While it's always important to have guardrails in your
  node messages to keep the conversation on topic, letting the user speak first
  makes it even more so.
</Warning>

## Initialization

Initialize your flow by creating a `FlowManager` instance and calling `initialize()` to start the conversation.

### Dynamic Flow

First, create the FlowManager:

```python
flow_manager = FlowManager(
    task=task,                              # PipelineTask
    llm=llm,                                # LLMService
    context_aggregator=context_aggregator,  # Context aggregator
    transport=transport,                    # Transport
)
```

Then, initialize by passing the first `NodeConfig` into the `initialize()` method:

```python
@transport.event_handler("on_client_connected")
async def on_client_connected(transport, client):
    logger.info(f"Client connected")
    # Kick off the conversation.
    await flow_manager.initialize(create_initial_node())
```

### Static Flow

First, create the FlowManager:

```python
flow_manager = FlowManager(
    task=task,                              # PipelineTask
    llm=llm,                                # LLMService
    context_aggregator=context_aggregator,  # Context aggregator
    flow_config=flow_config,                # FlowConfig
)
```

Then initialize:

```python
@transport.event_handler("on_first_participant_joined")
async def on_first_participant_joined(transport, participant):
    await transport.capture_participant_transcription(participant["id"])
    logger.debug("Initializing flow")
    await flow_manager.initialize()
```

<Note>For Static Flows, `initialize()` does not take any args.</Note>

## Cross-Node State

Pipecat Flows supports cross-node state through the `flow_manager.state` dictionary. This persistent storage lets you share data across nodes throughout the entire conversation.

**Basic usage**

```python
async def record_favorite_color_and_set_next_node(
    args: FlowArgs, flow_manager: FlowManager
) -> tuple[str, NodeConfig]:
    """Function handler that records the color then sets the next node.

    Here "record" means print to the console, but any logic could go here;
    Write to a database, make an API call, etc.
    """
    flow_manager.state["color"] = args["color"]
    print(f"Your favorite color is: {args['color']}")
    return args["color"], create_end_node()
```

## Usage Examples:

### Dynamic Flow (Recommended)

Here's an example that ties together all the concepts we've covered:

```python
def create_initial_node() -> NodeConfig:
    """Create the initial node of the flow.

    Define the bot's role and task for the node as well as the function for it to call.
    The function call includes a handler which provides the function call result to
    Pipecat and then transitions to the next node.
    """
    record_favorite_color_func = FlowsFunctionSchema(
        name="record_favorite_color_func",
        description="Record the color the user said is their favorite.",
        required=["color"],
        handler=record_favorite_color_and_set_next_node,
        properties={"color": {"type": "string"}},
    )

    return {
        "name": "initial",
        "role_messages": [
            {
                "role": "system",
                "content": "You are an inquisitive child. Use very simple language. Ask simple questions. You must ALWAYS use one of the available functions to progress the conversation. Your responses will be converted to audio. Avoid outputting special characters and emojis.",
            }
        ],
        "task_messages": [
            {
                "role": "system",
                "content": "Say 'Hello world' and ask what is the user's favorite color.",
            }
        ],
        "functions": [record_favorite_color_func],
    }
```

### Static Flow

For static flows, you define your entire flow structure upfront using a `FlowConfig`. This follows the same rules as the NodeConfig, but is assembled as a single JSON object that defines the entire state of the program.

See the [food_ordering example](https://github.com/pipecat-ai/pipecat-flows/blob/main/examples/static/food_ordering.py) for a complete FlowConfig implementation.

## Next Steps

Now that you understand the basics of Pipecat Flows, explore the reference docs and more examples:

<CardGroup cols={2}>
  <Card
    title="Pipecat Flows Reference"
    icon="book"
    href="/server/frameworks/flows/pipecat-flows"
  >
    Complete API reference and technical details
  </Card>
  <Card
    title="Pipecat Flows Examples"
    icon="rocket"
    href="https://github.com/pipecat-ai/pipecat-flows/tree/main/examples"
  >
    Explore more complex examples and use cases
  </Card>
</CardGroup>



================================================
FILE: guides/fundamentals/context-management.mdx
================================================
---
title: "Context Management"
description: "A guide to working with Pipecat's Context and Context Aggregators"
---

## What is Context in Pipecat?

In Pipecat, **context** refers to the text that the LLM uses to perform an inference. Commonly, this is the text inputted to the LLM and outputted from the LLM. The context consists of a list of alternating user/assistant messages that represents the information you want an LLM to respond to. Since Pipecat is a real-time voice (and multimodal) AI framework, the context serves as the collective history of the entire conversation.

## How Context Updates During Conversations

After every user and bot turn in the conversation, processors in the pipeline push frames to update the context:

- **STT Service**: Pushes `TranscriptionFrame` objects that represent what the user says.
- **LLM and TTS Services**: Work together to represent what the bot says. The LLM streams tokens (as `LLMTextFrame`s) to the TTS service, which outputs `TTSTextFrame`s representing the bot's spoken words.

## Setting Up Context Management

Pipecat includes a context aggregator class that creates and manages context for both user and assistant messages. Here's how to set it up:

### 1. Create the Context and Context Aggregator

```python
# Create LLM service
llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

# Create context
context = OpenAILLMContext(messages, tools)

# Create context aggregator instance
context_aggregator = llm.create_context_aggregator(context)
```

The context (which represents the conversation) is passed to the context aggregator. This ensures that both user and assistant instances of the context aggregators have access to the shared conversation context.

### 2. Add Context Aggregators to Your Pipeline

```python
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),      # User context aggregator
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant(), # Assistant context aggregator
])
```

## Context Aggregator Placement

The placement of context aggregator instances in your pipeline is **crucial** for proper operation:

### User Context Aggregator

Place the user context aggregator **downstream from the STT service**. Since the user's speech results in `TranscriptionFrame` objects pushed by the STT service, the user aggregator needs to be positioned to collect these frames.

### Assistant Context Aggregator

Place the assistant context aggregator **after `transport.output()`**. This positioning is important because:

- The TTS service outputs spoken words in addition to audio
- The assistant aggregator must be downstream to collect those frames
- It ensures context updates happen word-by-word for specific services (e.g. Cartesia, ElevenLabs, and Rime)
- Your context stays updated at the word level in case an interruption occurs

<Tip>
  Always place the assistant context aggregator **after** `transport.output()`
  to ensure proper word-level context updates during interruptions.
</Tip>

## Manually Managing Context

You can programmatically add new messages to the context by pushing or queueing specific frames:

### Adding Messages

- **`LLMMessagesAppendFrame`**: Appends a new message to the existing context
- **`LLMMessagesUpdateFrame`**: Completely replaces the existing context with new context provided in the frame

### Retrieving Current Context

The context aggregator provides a `get_context_frame()` method to obtain the latest context:

```python
await task.queue_frames([context_aggregator.user().get_context_frame()])
```

## Triggering Bot Responses

You'll commonly use this manual mechanism—obtaining the current context and pushing/queueing it—to trigger the bot to speak in two scenarios:

1. **Starting a pipeline** where the bot should speak first
2. **After pushing new context frames** using `LLMMessagesAppendFrame` or `LLMMessagesUpdateFrame`

This gives you fine-grained control over when and how the bot responds during the conversation flow.



================================================
FILE: guides/fundamentals/core-concepts.mdx
================================================
---
title: Core Concepts
description: "Understanding how Pipecat works: frames, processors, and pipelines."
---

Pipecat uses a pipeline-based architecture to handle real-time AI processing. Let's look at how this works in practice, then break down the key components.

## Real-time Processing in Action

Consider how a voice assistant processes a user's question and generates a response:

<Frame>![Real-time processing pipeline](/images/architecture-1.png)</Frame>

Instead of waiting for complete responses at each step, Pipecat processes data in small units called frames that flow through the pipeline:

1. Speech is transcribed in real-time as the user speaks
2. Transcription is sent to the LLM as it becomes available
3. LLM responses are processed as they stream in
4. Text-to-speech begins generating audio for early sentences while later ones are still being generated
5. Audio playback starts as soon as the first sentence is ready
6. LLM context is aggregated and updated continuously and in real-time

This streaming approach creates natural, responsive interactions.

## Architecture Overview

Here's how Pipecat organizes these processes:

<Frame>
  <img
    src="/images/architecture-2.png"
    alt="Example Pipeline"
    style={{
      maxHeight: "1000px",
    }}
  />
</Frame>

The architecture consists of three key components:

### 1. Frames

Frames are containers for data moving through your application. Think of them like packages on a conveyor belt - each contains a specific type of cargo. For example:

- Audio data from a microphone
- Text from transcription
- LLM responses
- Generated speech audio
- Images or video
- Control signals and system messages

Frames can flow in two directions:

- Downstream (normal processing flow)
- Upstream (for errors and control signals)

### 2. Processors (Services)

Processors are workers along our conveyor belt. Each one:

- Receives frames as inputs
- Processes specific frame types
- Passes through frames it doesn't handle
- Generates new frames as output

Frame processors can do anything, but for real-time, multimodal AI applications, they commonly include:

- A speech-to-text processor that receives raw audio input frames and outputs transcription frames
- An LLM processor takes context frames and produces text frames
- A text-to-speech processor that receives text frames and generates raw audio output frames
- An image generation processor that takes in text frames and outputs an image URL frame
- A logging processor might watch all frames but not modify them

### 3. Pipelines

Pipelines connect processors together, creating a path for frames to flow through your application. They can be:

```python
# Simple linear pipeline
pipeline = Pipeline([
    transport.input()    # Speech   -> Audio
    stt,                 # Audio    -> Text
    llm,                 # Text     -> Response
    tts,                 # Response -> Audio
    transport.output()   # Audio    -> Playback
])

# Complex parallel pipeline
pipeline = Pipeline([
    input_source,
    ParallelPipeline([
        [image_processor, image_output],  # Handle images
        [audio_processor, audio_output]   # Handle audio
    ])
])
```

The pipeline also contains the transport, which is the connection to the real world (e.g., microphone, speakers).

## How It All Works Together

Let's see how these components handle a simple voice interaction:

1. **Input**

   - User speaks into their microphone
   - Transport converts audio into frames
   - Frames enter the pipeline

2. **Processing**

   - Transcription processor converts speech to text frames
   - LLM processor takes text frames, generates response frames
   - TTS processor converts response frames to audio frames
   - Error frames flow upstream if issues occur
   - System frames can bypass normal processing for immediate handling

3. **Output**
   - Audio frames reach the transport
   - Transport plays the audio for the user

This happens continuously and in parallel, creating smooth, real-time interactions.

## Next Steps

Now that you understand the big picture, the next step is to install and run your first Pipecat application. Check out the [Installation & Setup](/getting-started/installation) guide to get started.

Need help? Join our [Discord community](https://discord.gg/pipecat) for support and discussions.



================================================
FILE: guides/fundamentals/custom-frame-processor.mdx
================================================
---
title: Custom FrameProcessor
description: Learn how to write your own custom FrameProcessor
---

Pipecat's architecture is made up of a Pipeline, FrameProcessors, and Frames. See the [Core Concepts](/getting-started/core-concepts#architecture-overview) for a full review. From that architecture, recall that FrameProcessors are the workers in the pipeline that receive frames and complete actions based on the frames received.

Pipecat comes with many FrameProcessors built in. These consist of services, like `OpenAILLMService` or `CartesiaTTSService`, utilities, like `UserIdleProcessor`, and other things. Largely, you can build most of your application with these built-in FrameProcessors, but commonly, your application code may require custom frame processing logic. For example, you may want to perform an action as a result of a frame that's pushed in the pipeline.

## Example: ImageSyncAggregator

Let's look at an example custom FrameProcessor that synchronizes images with bot speech:

```python
class ImageSyncAggregator(FrameProcessor):
    def __init__(self, speaking_path: str, waiting_path: str):
        super().__init__()
        self._speaking_image = Image.open(speaking_path)
        self._speaking_image_format = self._speaking_image.format
        self._speaking_image_bytes = self._speaking_image.tobytes()

        self._waiting_image = Image.open(waiting_path)
        self._waiting_image_format = self._waiting_image.format
        self._waiting_image_bytes = self._waiting_image.tobytes()

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, BotStartedSpeakingFrame):
            await self.push_frame(
                OutputImageRawFrame(
                    image=self._speaking_image_bytes,
                    size=(1024, 1024),
                    format=self._speaking_image_format,
                )
            )

        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(
                OutputImageRawFrame(
                    image=self._waiting_image_bytes,
                    size=(1024, 1024),
                    format=self._waiting_image_format,
                )
            )

        await self.push_frame(frame)
```

This example custom FrameProcessor looks for `BotStartedSpeakingFrame` and `BotStoppedSpeakingFrame`. When it sees a `BotStartedSpeakingFrame`, it will show an image that says the bot is speaking. When it sees a `BotStoppedSpeakingFrame`, it will show an image that says the bot is not speaking.

<Note>
  See this [working
  example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/06a-image-sync.py)
  using the `ImageSyncAggregator` FrameProcessor
</Note>

## Adding to a Pipeline

This custom FrameProcessor can be added to a Pipeline just before the transport output:

```python
# Create and initialize the custom FrameProcessor
image_sync_aggregator = ImageSyncAggregator(
    os.path.join(os.path.dirname(__file__), "assets", "speaking.png"),
    os.path.join(os.path.dirname(__file__), "assets", "waiting.png"),
)

pipeline = Pipeline(
    [
        transport.input(),
        stt,
        context_aggregator.user(),
        llm,
        tts,
        image_sync_aggregator,  # Our custom FrameProcessor
        transport.output(),
        context_aggregator.assistant(),
    ]
)
```

With this positioning, the `ImageSyncAggregator` FrameProcessor will receive the `BotStartedSpeakingFrame` and `BotStoppedSpeakingFrame` outputted by the TTS processor and then push its own frame—`OutputImageRawFrame`—to the output transport.

## Key Requirements

FrameProcessors must inherit from the base `FrameProcessor` class. This ensures that your custom FrameProcessor will correctly handle frames like `StartFrame`, `EndFrame`, `StartInterruptionFrame` without having to write custom logic for those frames. This inheritance also provides it with the ability to `process_frame()` and `push_frame()`:

- **`process_frame()`** is what allows the FrameProcessor to receive frames and add custom conditional logic based on the frames that are received.
- **`push_frame()`** allows the FrameProcessor to push frames to the pipeline. Normally, frames are pushed DOWNSTREAM, but based on which processors need the output, you can also push UPSTREAM or in both directions.

### Essential Implementation Details

To ensure proper base class inheritance, it's critical to include:

1. **`super().__init__()`** in your `__init__` method
2. **`await super().process_frame(frame, direction)`** in your `process_frame()` method

```python
class MyCustomProcessor(FrameProcessor):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)  # ✅ Required
        # Your initialization code here

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)  # ✅ Required

        # Your custom frame processing logic here
        if isinstance(frame, SomeSpecificFrame):
            # Handle the frame
            pass

        await self.push_frame(frame)  # ✅ Required - pass frame through
```

## Critical Responsibility: Frame Forwarding

FrameProcessors receive **all** frames that are pushed through the pipeline. This gives them a lot of power, but also a great responsibility. Critically, they must push all frames through the pipeline; if they don't, they block frames from moving through the Pipeline, which will cause issues in how your application functions.

You can see this at work in the `ImageSyncAggregator`'s `process_frame()` method. It handles both bot speaking frames and also has an `await self.push_frame(frame)` which pushes the frame through to the next processor in the pipeline.

## Frame Direction

When pushing frames, you can specify the direction:

```python
# Push downstream (default)
await self.push_frame(frame)
await self.push_frame(frame, FrameDirection.DOWNSTREAM)

# Push upstream
await self.push_frame(frame, FrameDirection.UPSTREAM)
```

Most custom FrameProcessors will push frames downstream, but upstream can be useful for sending control frames or error notifications back up the pipeline.

## Best Practices

1. **Always call the parent methods**: Use `super().__init__()` and `await super().process_frame()`
2. **Forward all frames**: Make sure every frame is pushed through with `await self.push_frame(frame)`
3. **Handle frames conditionally**: Use `isinstance()` checks to handle specific frame types
4. **Use proper error handling**: Wrap risky operations in try/catch blocks
5. **Position carefully in pipeline**: Consider where in the pipeline your processor needs to be to receive the right frames

With these patterns, you can create powerful custom FrameProcessors that extend Pipecat's capabilities for your specific use case.



================================================
FILE: guides/fundamentals/detecting-user-idle.mdx
================================================
---
title: "Detecting Idle Users"
description: "Learn how to detect and respond when users are inactive in conversations"
---

## Overview

In conversational applications, it's important to handle situations where users go silent or inactive. Pipecat's `UserIdleProcessor` helps you detect when users haven't spoken for a defined period, allowing your bot to respond appropriately.

## How It Works

The `UserIdleProcessor` monitors user activity and:

1. Starts tracking after the first interaction (user or bot speaking)
2. Resets a timer whenever the user speaks
3. Calls your custom callback function when the user is idle for longer than the timeout period
4. Provides a retry counter to track consecutive idle periods
5. Allows you to implement escalating responses or gracefully end the conversation

<Note>
  The processor uses speech events (not audio frames) to detect activity, so it
  requires an active speech-to-text service or a transport with built-in speech
  detection.
</Note>

## Basic Implementation

### Step 1: Create a Handler Function

First, create a callback function that will be triggered when the user is idle:

```python
# Simple handler that doesn't use retry count
async def handle_user_idle(processor):
    # Send a reminder to the user
    await processor.push_frame(TTSSpeakFrame("Are you still there?"))

# OR

# Advanced handler with retry logic
async def handle_user_idle(processor, retry_count):
    if retry_count == 1:
        # First attempt - gentle reminder
        await processor.push_frame(TTSSpeakFrame("Are you still there?"))
        return True  # Continue monitoring
    elif retry_count == 2:
        # Second attempt - more direct prompt
        await processor.push_frame(TTSSpeakFrame("Would you like to continue our conversation?"))
        return True  # Continue monitoring
    else:
        # Third attempt - end conversation
        await processor.push_frame(TTSSpeakFrame("I'll leave you for now. Have a nice day!"))
        await processor.push_frame(EndFrame(), FrameDirection.UPSTREAM)
        return False  # Stop monitoring
```

### Step 2: Create the Idle Processor

Initialize the processor with your callback and desired timeout:

```python
from pipecat.processors.user_idle_processor import UserIdleProcessor

user_idle = UserIdleProcessor(
    callback=handle_user_idle,  # Your callback function
    timeout=5.0,               # Seconds of inactivity before triggering
)
```

### Step 3: Add to Your Pipeline

Place the processor after speech detection but before context handling:

```python
pipeline = Pipeline(
    [
        transport.input(),
        stt,                         # Speech-to-text
        user_idle,                   # Add idle detection here
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ]
)
```

## Best Practices

- **Set appropriate timeouts**: Shorter timeouts (5-10 seconds) work well for voice conversations
- **Use escalating responses**: Start with gentle reminders and gradually become more direct
- **Limit retry attempts**: After 2-3 unsuccessful attempts, consider [ending the conversation](/guides/fundamentals/end-pipeline) gracefully by pushing an `EndFrame`

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Try the User Idle Example"
    icon="code"
    iconType="duotone"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/17-detect-user-idle.py"
  >
    Explore a complete working example that demonstrates how to detect and
    respond to user inactivity in Pipecat.
  </Card>

  <Card
    title="UserIdleProcessor Reference"
    icon="book"
    iconType="duotone"
    href="/server/utilities/user-idle-processor"
  >
    Read the complete API reference documentation for advanced configuration
    options and callback patterns.
  </Card>
</CardGroup>

Implementing idle user detection improves the conversational experience by ensuring your bot can handle periods of user inactivity gracefully, either by prompting for re-engagement or politely ending the conversation when appropriate.



================================================
FILE: guides/fundamentals/end-pipeline.mdx
================================================
---
title: "Ending a Pipeline"
description: "Best practices for properly terminating Pipecat pipelines"
---

## Overview

Properly ending a Pipecat pipeline is essential to prevent hanging processes and ensure clean shutdown of your session and related infrastructure. This guide covers different approaches to pipeline termination and provides best practices for each scenario.

## Shutdown Approaches

Pipecat provides two primary methods for shutting down a pipeline:

1. **Graceful Shutdown**: Allows completion of in-progress processing before termination
2. **Immediate Shutdown**: Cancels all tasks immediately

Each approach is designed for different use cases, as detailed below.

## Graceful Shutdown

A graceful shutdown is ideal when you want the bot to properly end a conversation. For example, you might want to terminate a session after the bot has completed a specific task or reached a natural conclusion.

This approach ensures that any final messages from the bot are processed and delivered before the pipeline terminates.

### Implementation

To implement a graceful shutdown, there are two options:

- Push an `EndFrame` from outside your pipeline using the pipeline task:

```python
await task.queue_frame(EndFrame())
```

- Push an `EndTaskFrame` upstream from inside your pipeline. For example, inside a function call:

```python
async def end_conversation(params: FunctionCallParams):
    await params.llm.push_frame(TTSSpeakFrame("Have a nice day!"))

    # Signal that the task should end after processing this frame
    await params.llm.push_frame(EndTaskFrame(), FrameDirection.UPSTREAM)
```

### How Graceful Shutdown Works

In both cases, an `EndFrame` is pushed downstream from the beginning of the pipeline:

1. `EndFrame`s are queued, so they'll process after any pending frames (like goodbye messages)
2. All processors in the pipeline will shutdown when processing the `EndFrame`
3. Once the `EndFrame` reaches the sink of the `PipelineTask`, the Pipeline is ready to shut down
4. The Pipecat processor terminates and related resources are released

<Tip>
  Graceful shutdowns allow your bot to say goodbye and complete any final
  actions before terminating.
</Tip>

## Immediate Shutdown

An immediate shutdown is appropriate when the human participant is no longer active in the conversation. For example:

- In a client/server app, when the user closes the browser tab or ends the session
- In a phone call, when the user hangs up
- When an error occurs that requires immediate termination

In these scenarios, there's no value in having the bot complete its current turn.

### Implementation

To implement an immediate shutdown, you can use event handlers to, for example, detect disconnections and then push a `CancelFrame`:

```python
@transport.event_handler("on_client_closed")
async def on_client_closed(transport, client):
    logger.info(f"Client closed connection")
    await task.cancel()
```

### How Immediate Shutdown Works

1. An event triggers the cancellation (like a client disconnection)
2. `task.cancel()` is called, which pushes a `CancelFrame` downstream from the `PipelineTask`
3. `CancelFrame`s are `SystemFrame`s and are not queued
4. Processors that handle the `CancelFrame` immediate shutdown and push the frame downstream
5. Once the `CancelFrame` reaches the sink of the `PipelineTask`, the Pipeline is ready to shut down

<Warning>
  Immediate shutdowns will discard any pending frames in the pipeline. Use this
  approach when completing the conversation is no longer necessary.
</Warning>

## Pipeline Idle Detection

In addition to the two explicit shutdown mechanisms, Pipecat includes a backup mechanism to prevent hanging pipelines—Pipeline Idle Detection.

This feature monitors activity in your pipeline and can automatically cancel tasks when no meaningful bot interactions are occurring for an extended period. It serves as a safety net to conditionally terminate the pipeline if anomalous behavior occurs.

<Tip>
  Pipeline Idle Detection is enabled by default and helps prevent resources from
  being wasted on inactive conversations.
</Tip>

For more information on configuring and customizing this feature, see the [Pipeline Idle Detection](/server/pipeline/pipeline-idle-detection) documentation.

## Best Practices

- **Use graceful shutdowns** when you want to let the bot complete its conversation
- **Use immediate shutdowns** when the human participant has already disconnected
- **Implement error handling** to ensure pipelines can terminate even when exceptions occur
- **Configure idle detection timeouts** appropriate for your use case

By following these practices, you'll ensure that your Pipecat pipelines terminate properly and efficiently, preventing resource leaks and improving overall system reliability.



================================================
FILE: guides/fundamentals/function-calling.mdx
================================================
---
title: "Function Calling"
description: "Enable LLMs to interact with external services and APIs"
---

## Understanding Function Calling

Function calling (also known as tool calling) allows LLMs to request information from external services and APIs. This enables your bot to access real-time data and perform actions that aren't part of its training data.

For example, you could give your bot the ability to:

- Check current weather conditions
- Look up stock prices
- Query a database
- Control smart home devices
- Schedule appointments

Here's how it works:

1. You define functions the LLM can use and register them to the LLM service used in your pipeline
2. When needed, the LLM requests a function call
3. Your application executes any corresponding functions
4. The result is sent back to the LLM
5. The LLM uses this information in its response

## Implementation

### 1. Define Functions

Pipecat provides a standardized `FunctionSchema` that works across all supported LLM providers. This makes it easy to define functions once and use them with any provider.

As a shorthand, you could also bypass specifying a function configuration at all and instead use "direct" functions. Under the hood, these are converted to `FunctionSchema`s.

#### Using the Standard Schema (Recommended)

```python
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Define a function using the standard schema
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get the current weather in a location",
    properties={
        "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA",
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "The temperature unit to use.",
        },
    },
    required=["location", "format"]
)

# Create a tools schema with your functions
tools = ToolsSchema(standard_tools=[weather_function])

# Pass this to your LLM context
context = OpenAILLMContext(
    messages=[{"role": "system", "content": "You are a helpful assistant."}],
    tools=tools
)
```

The `ToolsSchema` will be automatically converted to the correct format for your LLM provider through adapters.

#### Using Direct Functions (Shorthand)

You can bypass specifying a function configuration (as a `FunctionSchema` or in a provider-specific format) and instead pass the function directly to your `ToolsSchema`. Pipecat will auto-configure the function, gathering relevant metadata from its signature and docstring. Metadata includes:

- name
- description
- properties (including individual property descriptions)
- list of required properties

Note that the function signature is a bit different when using direct functions. The first parameter is `FunctionCallParams`, followed by any others necessary for the function.

```python
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.llm_service import FunctionCallParams

# Define a direct function
async def get_current_weather(params: FunctionCallParams, location: str, format: str):
    """Get the current weather.

    Args:
        location: The city and state, e.g. "San Francisco, CA".
        format: The temperature unit to use. Must be either "celsius" or "fahrenheit".
    """
    weather_data = {"conditions": "sunny", "temperature": "75"}
    await params.result_callback(weather_data)

# Create a tools schema, passing your function directly to it
tools = ToolsSchema(standard_tools=[get_current_weather])

# Pass this to your LLM context
context = OpenAILLMContext(
    messages=[{"role": "system", "content": "You are a helpful assistant."}],
    tools=tools
)
```

#### Using Provider-Specific Formats (Alternative)

You can also define functions in the provider-specific format if needed:

<CodeGroup>

```python OpenAI
from openai.types.chat import ChatCompletionToolParam

# OpenAI native format
tools = [
    ChatCompletionToolParam(
        type="function",
        function={
            "name": "get_current_weather",
            "description": "Get the current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "format": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The temperature unit to use.",
                    },
                },
                "required": ["location", "format"],
            },
        },
    )
]
```

```python Anthropic
# Anthropic native format
tools = [
    {
        "name": "get_weather",
        "description": "Get the weather in a given location",
        "input_schema": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city, e.g. San Francisco",
                },
                "format": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "The temperature unit to use.",
                },
            },
            "required": ["location", "format"],
        },
    }
]
```

```python Gemini
# Gemini native format
tools = [
    {
        "function_declarations": [
            {
                "name": "get_weather_current",
                "description": "Get the current weather",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "Location as 'city,state,country'",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use.",
                        },
                    },
                    "required": ["location", "format"],
                },
            }
        ]
    }
]
```

</CodeGroup>

#### Provider-Specific Custom Tools

Some providers support unique tools that don't fit the standard function schema. For these cases, you can add custom tools:

```python
from pipecat.adapters.schemas.tools_schema import AdapterType, ToolsSchema

# Standard functions
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get the current weather",
    properties={"location": {"type": "string"}},
    required=["location"]
)

# Custom Gemini search tool
gemini_search_tool = {
    "web_search": {
        "description": "Search the web for information"
    }
}

# Create a tools schema with both standard and custom tools
tools = ToolsSchema(
    standard_tools=[weather_function],
    custom_tools={
        AdapterType.GEMINI: [gemini_search_tool]
    }
)
```

<Tip>
  See the provider-specific documentation for details on custom tools and their
  formats.
</Tip>

### 2. Register Function Handlers

Register handlers for your functions using one of these [LLM service methods](https://reference-server.pipecat.ai/en/latest/api/pipecat.services.llm_service.html#llm-service):

- `register_function`
- `register_direct_function`

Which one you use depends on whether your function is a ["direct" function](#using-direct-functions-shorthand).

<CodeGroup>

```python Non-Direct Function
from pipecat.services.llm_service import FunctionCallParams

llm = OpenAILLMService(api_key="your-api-key")

# Main function handler - called to execute the function
async def fetch_weather_from_api(params: FunctionCallParams):
    # Fetch weather data from your API
    weather_data = {"conditions": "sunny", "temperature": "75"}
    await params.result_callback(weather_data)

# Register the function
llm.register_function(
    "get_current_weather",
    fetch_weather_from_api,
)
```

```python Direct Function
from pipecat.services.llm_service import FunctionCallParams

llm = OpenAILLMService(api_key="your-api-key")

# Direct function
async def get_current_weather(params: FunctionCallParams, location: str, format: str):
    """Get the current weather.

    Args:
        location: The city and state, e.g. "San Francisco, CA".
        format: The temperature unit to use. Must be either "celsius" or "fahrenheit".
    """
    weather_data = {"conditions": "sunny", "temperature": "75"}
    await params.result_callback(weather_data)

# Register the function
llm.register_direct_function(get_current_weather)
```

</CodeGroup>

### 3. Create the Pipeline

Include your LLM service in your pipeline with the registered functions:

```python
# Initialize the LLM context with your function schemas
context = OpenAILLMContext(
    messages=[{"role": "system", "content": "You are a helpful assistant."}],
    tools=tools
)

# Create the context aggregator to collect the user and assistant context
context_aggregator = llm.create_context_aggregator(context)

# Create the pipeline
pipeline = Pipeline([
    transport.input(),               # Input from the transport
    stt,                             # STT processing
    context_aggregator.user(),       # User context aggregation
    llm,                             # LLM processing
    tts,                             # TTS processing
    transport.output(),              # Output to the transport
    context_aggregator.assistant(),  # Assistant context aggregation
])
```

## Function Handler Details

### FunctionCallParams

The `FunctionCallParams` object contains all the information needed for handling function calls:

- `params`: FunctionCallParams
  - `function_name`: Name of the called function
  - `arguments`: Arguments passed by the LLM
  - `tool_call_id`: Unique identifier for the function call
  - `llm`: Reference to the LLM service
  - `context`: Current conversation context
  - `result_callback`: Async function to return results

<ParamField path="function_name" type="str">
  Name of the function being called
</ParamField>

<ParamField path="tool_call_id" type="str">
  Unique identifier for the function call
</ParamField>

<ParamField path="arguments" type="Mapping[str, Any]">
  Arguments passed by the LLM to the function
</ParamField>

<ParamField path="llm" type="LLMService">
  Reference to the LLM service that initiated the call
</ParamField>

<ParamField path="context" type="OpenAILLMContext">
  Current conversation context
</ParamField>

<ParamField path="result_callback" type="FunctionCallResultCallback">
  Async callback function to return results
</ParamField>

### Handler Structure

Your function handler should:

1. Receive necessary arguments, either:
   - From `params.arguments`
   - Directly From function arguments, if using [direct functions](#using-direct-functions-shorthand)
2. Process data or call external services
3. Return results via `params.result_callback(result)`

<CodeGroup>

```python Non-Direct Function
async def fetch_weather_from_api(params: FunctionCallParams):
    try:
        # Extract arguments
        location = params.arguments.get("location")
        format_type = params.arguments.get("format", "celsius")

        # Call external API
        api_result = await weather_api.get_weather(location, format_type)

        # Return formatted result
        await params.result_callback({
            "location": location,
            "temperature": api_result["temp"],
            "conditions": api_result["conditions"],
            "unit": format_type
        })
    except Exception as e:
        # Handle errors
        await params.result_callback({
            "error": f"Failed to get weather: {str(e)}"
        })
```

```python Direct Function
async def get_current_weather(params: FunctionCallParams, location: str, format: str):
    """Get the current weather.

    Args:
        location: The city and state, e.g. "San Francisco, CA".
        format: The temperature unit to use. Must be either "celsius" or "fahrenheit".
    """
    try:
        # Call external API
        api_result = await weather_api.get_weather(location, format)

        # Return formatted result
        await params.result_callback({
            "location": location,
            "temperature": api_result["temp"],
            "conditions": api_result["conditions"],
            "unit": format_type
        })
    except Exception as e:
        # Handle errors
        await params.result_callback({
            "error": f"Failed to get weather: {str(e)}"
        })
```

</CodeGroup>

## Controlling Function Call Behavior (Advanced)

When returning results from a function handler, you can control how the LLM processes those results using a `FunctionCallResultProperties` object passed to the result callback.

<Tip>
  It can be handy to skip a completion when you have back-to-back function
  calls. Note, if you skip a completion, you must manually trigger one from the
  context.
</Tip>

### Properties

<ParamField path="run_llm" type="Optional[bool]">
Controls whether the LLM should generate a response after the function call:

- `True`: Run LLM after function call (default if no other function calls in progress)
- `False`: Don't run LLM after function call
- `None`: Use default behavior

</ParamField>

<ParamField
  path="on_context_updated"
  type="Optional[Callable[[], Awaitable[None]]]"
>
  Optional callback that runs after the function result is added to the context
</ParamField>

### Example Usage

```python
from pipecat.frames.frames import FunctionCallResultProperties
from pipecat.services.llm_service import FunctionCallParams

async def fetch_weather_from_api(params: FunctionCallParams):
    # Fetch weather data
    weather_data = {"conditions": "sunny", "temperature": "75"}

    # Don't run LLM after this function call
    properties = FunctionCallResultProperties(run_llm=False)

    await params.result_callback(weather_data, properties=properties)

async def query_database(params: FunctionCallParams):
    # Query database
    results = await db.query(params.arguments["query"])

    async def on_update():
        await notify_system("Database query complete")

    # Run LLM after function call and notify when context is updated
    properties = FunctionCallResultProperties(
        run_llm=True,
        on_context_updated=on_update
    )

    await params.result_callback(results, properties=properties)
```

## Next steps

- Check out the [function calling examples](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14-function-calling.py) to see a complete example for specific LLM providers.
- Refer to your LLM provider's documentation to learn more about their function calling capabilities.



================================================
FILE: guides/fundamentals/recording-audio.mdx
================================================
---
title: "Recording Conversation Audio"
sidebarTitle: "Recording Audio"
description: "Learn how to record and save audio from conversations between users and your bot"
---

## Overview

Recording audio from conversations provides valuable data for analysis, debugging, and quality control. You have two options for how to record with Pipecat:

### Option 1: Record using your transport service provider

Record without writing custom code by using your [transport](/server/services/supported-services#transports) provider's recording capabilities. In addition to saving you development time, some providers offer unique recording capabilities.

<Info>Refer to your service provider's documentation to learn more.</Info>

### Option 2: Create your own recording pipeline

Pipecat's `AudioBufferProcessor` makes it easy to capture high-quality audio recordings of both the user and bot during interactions. Opt for this approach if you want more control over your recording.

This guide focuses on how to recording using the `AudioBufferProcessor`, including high-level guidance for how to set up post-processing jobs for longer recordings.

## How the AudioBufferProcessor Works

The `AudioBufferProcessor` captures audio by:

1. Collecting audio frames from both the user (input) and bot (output)
2. Emitting events with recorded audio data
3. Providing options for composite or separate track recordings

<Note>
  Add the processor to your pipeline after the `transport.output()` to capture
  both the user audio and the bot audio as it's spoken.
</Note>

## Audio Recording Options

The `AudioBufferProcessor` offers several configuration options:

- **Composite recording**: Combined audio from both user and bot
- **Track-level recording**: Separate audio files for user and bot
- **Turn-based recording**: Individual audio clips for each speaking turn
- **Mono or stereo output**: Single channel mixing or two-channel separation

## Basic Implementation

### Step 1: Create an Audio Buffer Processor

Initialize the audio buffer processor with your desired configuration:

```python
from pipecat.processors.audio.audio_buffer_processor import AudioBufferProcessor

# Create audio buffer processor with default settings
audiobuffer = AudioBufferProcessor(
    num_channels=1,               # 1 for mono, 2 for stereo (user left, bot right)
    enable_turn_audio=False,      # Enable per-turn audio recording
    user_continuous_stream=True,  # User has continuous audio stream
)
```

### Step 2: Add to Your Pipeline

Place the processor in your pipeline after all audio-producing components:

```python
pipeline = Pipeline(
    [
        transport.input(),
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        audiobuffer,          # Add after all audio components
        context_aggregator.assistant(),
    ]
)
```

### Step 3: Start Recording

Explicitly start recording when needed, typically when a session begins:

```python
@transport.event_handler("on_client_connected")
async def on_client_connected(transport, client):
    logger.info(f"Client connected")
    # Important: Start recording explicitly
    await audiobuffer.start_recording()
    # Continue with session initialization...
```

<Warning>
  You must call `start_recording()` explicitly to begin capturing audio. The
  processor won't record automatically when initialized.
</Warning>

### Step 4: Handle Audio Data

Register an event handler to process audio data:

```python
@audiobuffer.event_handler("on_audio_data")
async def on_audio_data(buffer, audio, sample_rate, num_channels):
    # Save or process the composite audio
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"recordings/conversation_{timestamp}.wav"

    # Create the WAV file
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(num_channels)
        wf.setsampwidth(2)  # 16-bit audio
        wf.setframerate(sample_rate)
        wf.writeframes(audio)

    logger.info(f"Saved recording to {filename}")
```

<Tip>
  If recording separate tracks, you can use the `on_track_audio_data` event
  handler to save user and bot audio separately.
</Tip>

## Recording Longer Conversations

For conversations that last a few minutes, it may be sufficient to just buffer the audio in memory. However, for longer sessions, storing audio in memory poses two challenges:

1. **Memory Usage**: Long recordings can consume significant memory, leading to potential crashes or performance issues.
2. **Conversation Loss**: If the application crashes or the connection drops, you may lose all recorded audio.

Instead, consider using a chunked approach to record audio in manageable segments. This allows you to periodically save audio data to disk or upload it to cloud storage, reducing memory usage and ensuring data persistence.

### Chunked Recording

Set a reasonable `buffer_size` to trigger periodic uploads:

```python
# 30-second chunks (recommended for most use cases)
SAMPLE_RATE = 24000
CHUNK_DURATION = 30  # seconds
audiobuffer = AudioBufferProcessor(
    sample_rate=SAMPLE_RATE,
    buffer_size=SAMPLE_RATE * 2 * CHUNK_DURATION  # 2 bytes per sample (16-bit)
)

chunk_counter = 0

@audiobuffer.event_handler("on_track_audio_data")
async def on_chunk_ready(buffer, user_audio, bot_audio, sample_rate, num_channels):
    global chunk_counter

    # Upload or save individual chunks
    await upload_audio_chunk(f"user_chunk_{chunk_counter:03d}.wav", user_audio, sample_rate, 1)
    await upload_audio_chunk(f"bot_chunk_{chunk_counter:03d}.wav", bot_audio, sample_rate, 1)

    chunk_counter += 1
```

### Multipart Upload Strategy

For cloud storage, consider using multipart uploads to stream audio chunks:

**Conceptual Approach:**

1. **Initialize multipart upload** when recording starts
2. **Upload chunks as parts** when buffers fill (every ~30 seconds)
3. **Complete multipart upload** when recording ends
4. **Post-process** to create final WAV file(s)

**Benefits:**

- Memory efficient for long sessions
- Fault tolerant (no data loss if connection drops)
- Enables real-time processing and analysis
- Parallel upload of multiple tracks

### Post-Processing Pipeline

After uploading chunks, create final audio files using tools like FFmpeg:

**Concatenating Audio Files:**

```bash
# Method 1: Simple concatenation (same format)
ffmpeg -i "concat:chunk_001.wav|chunk_002.wav|chunk_003.wav" -acodec copy final.wav

# Method 2: Using file list (recommended for many chunks)
# Create filelist.txt with format:
# file 'chunk_001.wav'
# file 'chunk_002.wav'
# ...
ffmpeg -f concat -safe 0 -i filelist.txt -c copy final_recording.wav
```

**Automation Considerations:**

- Use sequence numbers in chunk filenames for proper ordering
- Include metadata (sample rate, channels, duration) with each chunk
- Implement retry logic for failed uploads
- Consider using cloud functions/lambdas for automatic post-processing

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Try the Audio Recording Example"
    icon="code"
    iconType="duotone"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/34-audio-recording.py"
  >
    Explore a complete working example that demonstrates how to record and save
    both composite and track-level audio with Pipecat.
  </Card>

  <Card
    title="AudioBufferProcessor Reference"
    icon="book"
    iconType="duotone"
    href="/server/utilities/audio/audio-buffer-processor"
  >
    Read the complete API reference documentation for advanced configuration
    options and event handlers.
  </Card>
</CardGroup>

Consider implementing audio recording in your application for quality assurance, training data collection, or creating conversation archives. The recorded audio can be stored locally, uploaded to cloud storage, or processed in real-time for further analysis.



================================================
FILE: guides/fundamentals/saving-transcripts.mdx
================================================
---
title: "Saving Conversation Transcripts"
sidebarTitle: "Saving Transcripts"
description: "Learn how to collect and save conversation transcripts between users and your bot"
---

## Overview

Recording transcripts of conversations between users and your bot is useful for debugging, analysis, and creating a record of interactions. Pipecat's `TranscriptProcessor` makes it easy to collect both user and bot messages as they occur.

## How It Works

The `TranscriptProcessor` collects transcripts by:

1. Capturing what the user says (from `TranscriptionFrame`s)
2. Capturing what the bot says (from `TTSTextFrame`s)
3. Emitting events with transcript updates in real-time
4. Allowing you to handle these events with custom logic

<Note>
  The `TranscriptProcessor` provides two separate processors: one for user
  speech and one for assistant speech. Both emit the same event type when new
  transcript content is available.
</Note>

## Basic Implementation

### Step 1: Create a Transcript Processor

First, initialize the transcript processor:

```python
from pipecat.processors.transcript_processor import TranscriptProcessor

# Create a single transcript processor instance
transcript = TranscriptProcessor()
```

### Step 2: Add to Your Pipeline

Place the processors in your pipeline at the appropriate positions:

```python
pipeline = Pipeline(
    [
        transport.input(),
        stt,                            # Speech-to-text
        transcript.user(),              # Captures user transcripts
        context_aggregator.user(),
        llm,
        tts,                            # Text-to-speech
        transport.output(),
        transcript.assistant(),         # Captures assistant transcripts
        context_aggregator.assistant(),
    ]
)
```

<Note>
  Place `transcript.user()` after the STT processor and `transcript.assistant()`
  after `transport.output()` to ensure accurate transcript collection.
</Note>

### Step 3: Handle Transcript Updates

Register an event handler to process transcript updates:

```python
@transcript.event_handler("on_transcript_update")
async def handle_transcript_update(processor, frame):
    # Each message contains role (user/assistant), content, and timestamp
    for message in frame.messages:
        print(f"[{message.timestamp}] {message.role}: {message.content}")
```

<Tip>
  In addition to console logging, you can save transcripts to a database or file
  for later analysis.
</Tip>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Try the Transcript Example"
    icon="code"
    iconType="duotone"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/28-transcription-processor.py"
  >
    Explore a complete working example that demonstrates how to collect and save
    conversation transcripts with Pipecat.
  </Card>

  <Card
    title="TranscriptProcessor Reference"
    icon="book"
    iconType="duotone"
    href="/server/utilities/transcript-processor"
  >
    Read the complete API reference documentation for advanced configuration
    options and event handlers.
  </Card>
</CardGroup>

Consider implementing transcript recording in your application for debugging during development and preserving important conversations in production. The transcript data can also be useful for analyzing conversation patterns and improving your bot's responses over time.



================================================
FILE: guides/fundamentals/user-input-muting.mdx
================================================
---
title: "User Input Muting with STTMuteFilter"
sidebarTitle: "Muting User Input"
description: "Learn how to control when user speech is processed in your conversational bot"
---

## Overview

In conversational applications, there are moments when you don't want to process user speech, such as during bot introductions or while executing function calls. Pipecat's `STTMuteFilter` lets you selectively "mute" user input based on different conversation states.

## When to Use STTMuteFilter

Common scenarios for muting user input include:

- **During introductions**: Prevent the bot from being interrupted during its initial greeting
- **While processing functions**: Block input while the bot is retrieving external data
- **During bot speech**: Reduce false transcriptions while the bot is speaking
- **For guided conversations**: Create more structured interactions with clear turn-taking

## How It Works

The `STTMuteFilter` works by blocking specific user-related frames from flowing through your pipeline. When muted, it filters:

- Voice activity detection (VAD) events
- Interruption signals
- Raw audio input frames
- Transcription frames

This prevents the Speech-to-Text service from receiving and processing the user's speech during muted periods.

<Note>
  The filter must be placed between your STT service and context aggregator in
  the pipeline to work correctly.
</Note>

## Mute Strategies

The `STTMuteFilter` supports several strategies for determining when to mute user input:

<CardGroup cols={2}>
  <Card title="FIRST_SPEECH" icon="microphone-slash" iconType="duotone">
    Mute only during the bot's first speech utterance. Useful for introductions
    when you want the bot to complete its greeting before the user can speak.
  </Card>
  <Card
    title="MUTE_UNTIL_FIRST_BOT_COMPLETE"
    icon="hourglass-half"
    iconType="duotone"
  >
    Start muted and remain muted until the first bot utterance completes.
    Ensures the bot's initial instructions are fully delivered.
  </Card>
  <Card title="FUNCTION_CALL" icon="gear" iconType="duotone">
    Mute during function calls. Prevents users from speaking while the bot is
    processing external data requests.
  </Card>
  <Card title="ALWAYS" icon="volume-xmark" iconType="duotone">
    Mute whenever the bot is speaking. Creates a strict turn-taking conversation
    pattern.
  </Card>
  <Card title="CUSTOM" icon="sliders" iconType="duotone">
    Use custom logic via callback to determine when to mute. Provides maximum
    flexibility for complex muting rules.
  </Card>
</CardGroup>

<Warning>
  The `FIRST_SPEECH` and `MUTE_UNTIL_FIRST_BOT_COMPLETE` strategies should not
  be used together as they handle the first bot speech differently.
</Warning>

## Basic Implementation

### Step 1: Configure the Filter

First, create a configuration for the `STTMuteFilter`:

```python
from pipecat.processors.filters.stt_mute_filter import STTMuteConfig, STTMuteFilter, STTMuteStrategy

# Configure with one or more strategies
stt_mute_processor = STTMuteFilter(
    config=STTMuteConfig(
        strategies={
            STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE,
            STTMuteStrategy.FUNCTION_CALL,
        }
    ),
)
```

### Step 2: Add to Your Pipeline

Place the filter between the STT service and context aggregator:

```python
pipeline = Pipeline(
    [
        transport.input(),           # Transport user input
        stt,                         # Speech-to-text service
        stt_mute_processor,          # Add between STT and context aggregator
        context_aggregator.user(),   # User responses
        llm,                         # LLM
        tts,                         # Text-to-speech
        transport.output(),          # Transport bot output
        context_aggregator.assistant(),  # Assistant spoken responses
    ]
)
```

## Best Practices

- **Place the filter correctly**: Always position `STTMuteFilter` between the STT service and context aggregator
- **Choose strategies wisely**: Select the minimal set of strategies needed for your use case
- **Test user experience**: Excessive muting can frustrate users; balance control with usability
- **Consider feedback**: Provide visual cues when the user is muted to improve the experience

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Try the STTMuteFilter Example"
    icon="code"
    iconType="duotone"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/24-stt-mute-filter.py"
  >
    Explore a complete working example that demonstrates how to use
    STTMuteFilter to control user input during bot speech and function calls.
  </Card>

  <Card
    title="STTMuteFilter Reference"
    icon="book"
    iconType="duotone"
    href="/server/utilities/filters/stt-mute"
  >
    Read the complete API reference documentation for advanced configuration
    options and muting strategies.
  </Card>
</CardGroup>

Experiment with different muting strategies to find the right balance for your application. For advanced scenarios, try implementing custom muting logic based on specific conversation states or content.



================================================
FILE: guides/telephony/daily-webrtc.mdx
================================================
---
title: "Dial-in: WebRTC (Daily)"
description: "Call your Pipecat bot using Daily WebRTC"
---

## Things you'll need

- An active [Daily](https://www.daily.co) developer key.
- One or more Daily provisioned phone numbers (covered below).

<Card
  title="Prefer to look at code? See the example project!"
  icon="code"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot"
>
  We have a complete dialin-ready project using Daily as both a transport and
  PSTN/SIP provider in the Pipecat repo. This guide referencse the project and
  steps through the important parts that make dial-in work.
</Card>

### Do I need to provision my phone numbers through Daily?

You can use Daily solely as a transport if you prefer. This is particularly useful if you already have Twilio-provisioned numbers and workflows. In that case, you can configure Twilio to forward calls to your Pipecat agents and join a Daily WebRTC call. More details on using Twilio with Daily as a transport can be found [here](/telephony/twilio-daily-webrtc).

If you’re starting from scratch, using everything on one platform offers some convenience. By provisioning your phone numbers through Daily and using Daily as the transport layer, you won’t need to worry about initial call routing.

## Purchasing a phone number

You can purchase a number via the Daily REST API

<CodeGroup>
```shell Purchase a random number
curl --request POST \ 
--url 'https://api.daily.co/v1/buy-phone-number' \ 
--header 'Authorization: Bearer [YOUR_DAILY_API_KEY]' \ 
--header 'Content-Type: application/json'
```

```shell Purchase specific number
curl --request POST \
--url 'https://api.daily.co/v1/buy-phone-number' \
--header 'Authorization: Bearer [YOUR_DAILY_API_KEY]' \
--header 'Content-Type: application/json' \
--data '{
    "number": "..."
}'
```

```shell List numbers
curl --request GET \
--url 'https://api.daily.co/v1/purchased-phone-numbers' \
--header 'Authorization: Bearer [YOUR_DAILY_API_KEY]'
```

</CodeGroup>

## Configuring your bot runner

You'll need a HTTP service that can receive incoming call hooks and trigger a new agent session. We discussed the concept of a [bot runner](/deployment/pattern) in the deployment section, which we'll build on here to add support for incoming phone calls.

Within the `start_bot` method, we'll need to grab both `callId` and `callDomain` from the incoming web request that is triggered by Daily when someone dials the number:

```python bot_daily.py
# Get the dial-in properties from the request
try:
    data = await request.json()
    callId = data.get("callId")
    callDomain = data.get("callDomain")
except Exception:
    raise HTTPException(
        status_code=500,
        detail="Missing properties 'callId' or 'callDomain'")
```

<Note>
  Full bot source code
  [here](https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-pstn-dial-in)
</Note>

### Orchestrating incoming calls

Daily needs a URL / webhook endpoint it can trigger when a user dials the phone number.

We can configure this by assigning the number to an endpoint via their REST API.

Here is an example:

```shell
curl --location 'https://api.daily.co/v1' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer [DAILY API TOKEN HERE]' \
--data '{
    "properties": {
        "pinless_dialin": [
            {
                "phone_number": "[DAILY PROVISIONED NUMBER HERE]",
                "room_creation_api": "[BOT RUNNER URL]/start_bot"
            }
        ]
    }
}'
```

If you want to test locally, you can expose your web method using a service such as [ngrok](https://ngrok.com/).

```shell Example ngrok tunnel
python bot_runner.py --host localhost --port 7860 --reload
ngrok http localhost:7860

# E.g: https://123.ngrok.app/start_bot
```

### Creating a new SIP-enabled room

We'll need to configure the Daily room to be setup to receive SIP connections. `daily-helpers.py` included in Pipecat has some useful imports that make this easy. We just need to pass through new SIP parameters as part of room creation:

```python bot_runner.py

from pipecat.transports.services.helpers.daily_rest import DailyRoomParams, DailyRoomProperties, DailyRoomSipParams

params = DailyRoomParams(
    properties=DailyRoomProperties(
        sip=DailyRoomSipParams(
            display_name = "sip-dialin"
            video = False
            sip_mode = "dial-in"
            num_endpoints = 1
        )
    )
)

# Create sip-enabled Daily room via REST
try:
    room: DailyRoomObject = daily_rest_helper.create_room(params=params)
except Exception as e:
    raise HTTPException(
        status_code=500,
        detail=f"Unable to provision room {e}")

print (f"Daily room returned {room.url} {room.config.sip_endpoint}")
```

Incoming calls will include both `callId` and `callDomain` properties in the body of the request; we'll need to pass to the Pipecat agent.

For simplicity, our agents are spawned as sub-processes of the bot runner, so we'll pass the callId and callDomain through as command line arguments:

```python bot_runner.py
proc = subprocess.Popen(
    [
        f"python3 -m bot_daily -u {room.url} -t {token} -i {callId} -d {callDomain}"
    ],
    shell=True,
    bufsize=1,
    cwd=os.path.dirname(os.path.abspath(__file__))
)
```

That's all the configuration we need in our `bot_runner.py`.

## Configuring your Pipecat bot

Let's take a look at `bot_daily.py` and step through the differences from other examples.

First, it's setup to receive additional command line parameters which are passed through to the `DailyTransport` object:

```python bot_daily.py
# ...

async def main(room_url: str, token: str, callId: str, callDomain: str):
    async with aiohttp.ClientSession() as session:
        diallin_settings = DailyDialinSettings(
            call_id=callId,
            call_domain=callDomain
        )
        transport = DailyTransport(
            room_url,
            token,
            "Chatbot",
            DailyParams(
                api_url=daily_api_url,
                api_key=daily_api_key,
                dialin_settings=diallin_settings,
                audio_in_enabled=True,
                audio_out_enabled=True,
                video_out_enabled=False,
                vad_analyzer=SileroVADAnalyzer(),
                transcription_enabled=True,
            )
        )

        # ... your bot code

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Simple ChatBot")
    parser.add_argument("-u", type=str, help="Room URL")
    parser.add_argument("-t", type=str, help="Token")
    parser.add_argument("-i", type=str, help="Call ID")
    parser.add_argument("-d", type=str, help="Call Domain")
    config = parser.parse_args()

    asyncio.run(main(config.u, config.t, config.i, config.d))
```

Optionally, we can listen and respond to the `on_dialin_ready` event manually. This is useful if you have specific scenarios in whih you want to indicates that the SIP worker and is ready to be forwarded to the call.

This would stop any hold music and connect the end-user to our Pipecat bot.

```python
@transport.event_handler("on_dialin_ready")
async def on_dialin_ready(transport, cdata):
    print(f"on_dialin_ready", cdata)

```

Since we're using Daily as a phone vendor, this method is handled internally by the Pipecat Daily service. It can, however, be useful to override this default behaviour if you want to configure your bot in a certain way as soon as the call is ready. Typically, however, initial setup is done in the `on_first_participant_joined` event after the user has joined the session.

```python
@transport.event_handler("on_first_participant_joined")
async def on_first_participant_joined(transport, participant):
    transport.capture_participant_transcription(participant["id"])
    await task.queue_frames([LLMMessagesFrame(messages)])
```



================================================
FILE: guides/telephony/dialout.mdx
================================================
---
title: "Dialout: WebRTC (Daily)"
description: "Learn how to preview changes locally"
---

<Note>
  We're working on this page and will update it soon. In the meantime, you can
  follow the instructions in the [Quickstart](/getting-started/quickstart) guide
  to get started.
</Note>



================================================
FILE: guides/telephony/overview.mdx
================================================
---
title: "Overview"
description: "How to call your bots (or have them call you)"
---

## Introduction

You can dial-in to your Pipecat bots, and have them dial-out too, across both PSTN and SIP. The technical implementation will depend on your chosen transport and phone number vendor; each will likely have their own methods and events to consider.

## Which transport should I use?

This really depends on your project. We have examples that cover both WebRTC (Daily) and Twilio (WebSockets), and Pipecat supports multiple different types of media transport: local, WebSockets, WebRTC etc.

- Use Pipecat's native Twilio WebSockets integration for simple workflows that are only telephony-based.

  - The call is managed by Twilio (or similar telephony provider), which means that the bot is not able to perform any form of complex call control. Typically Twilio-specific APIs need to be implemented (for example, when you’re already using Twilio Studio, Twilio Flex, etc).
  - We strongly recommend against using WebSockets for non-telephony use cases (mobile apps, web browsers, etc.). See below.

- You must use SIP for use cases like the below. These require SIP-based call control:

  - Multi-agents or multi-party calls
  - Connect to legacy call centers powered by open source or cloud
  - Forwarding calls, agent assist/co-pilot, warm transfers, etc.
  - Supporting different telephony vendors without having telephony platform-specific code

- We strongly recommend using WebRTC for non telephony use cases — ie, mobile apps, web-based experiences. WebRTC is designed to support users on devices with varying network conditions at scale. Learn more in the Voice AI & Voice Agents Illustrated Primer [here](https://voiceaiandvoiceagents.com/#websockets-webrtc)

<Note>
  **Please note:** you can configure your Pipecat bots to handle multiple
  vendors. You could,for example, use both Daily and Twilio as phone number
  vendors concurrently.
</Note>

## What are PSTN and SIP? What are the differences?

PSTN is an abbreviation for traditional phone networks, consisting of the physical phone lines, cables and transmission links. One of the main differences to consider between these two forms of telephony is PSTN operates on a one user per line basis while SIP can have multiple users per line.

Depending on your use-case, you may or may not want to have a single phone number that routes users to a specific bot session, something we'll cover in the following guides.



================================================
FILE: guides/telephony/twilio-daily-webrtc.mdx
================================================
---
title: "Dial-in: WebRTC (Twilio + Daily)"
description: "Connect phone calls to your Pipecat bot using Twilio and Daily's SIP integration"
---

## Things you'll need

- An active [Twilio](https://www.twilio.com) developer key
- One or more Twilio provisioned phone numbers (covered below)
- The Twilio Python client library (`pip install twilio`)

<Card
  title="Prefer to look at code? See the example project!"
  icon="code"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot-daily-twilio-sip"
>
  We have a complete example project using Daily as a transport with Twilio as a
  phone provider. This guide walks through the important components and best
  practices when integrating voice calls.
</Card>

## How It Works

Here's the sequence of events when someone calls your Twilio number:

1. Twilio receives an incoming call to your phone number
2. Twilio calls your webhook server (`/call` endpoint)
3. Your server creates a Daily room with SIP capabilities
4. Your server starts the bot process with the room details
5. Your server responds to Twilio with TwiML that puts the caller on hold with music
6. Upon receiving the `on_dialin_ready` event, the bot forwards the call to the Daily SIP endpoint
7. The caller and bot are connected, and the bot handles the conversation

## Getting a phone number

- Visit [console.twilio.com](https://console.twilio.com) and purchase a new phone number (or via the API)
- Ensure your purchased number supports Voice capabilities
- Ensure your purchased number appears in your 'active numbers' list

## Project setup

You'll need to set two environment variables for your project: `TWILIO_ACCOUNT_SID` and `TWILIO_AUTH_TOKEN`, which you can obtain from the Twilio console.

```shell .env
DAILY_API_KEY=...
DAILY_API_URL=https://api.daily.co/v1
TWILIO_ACCOUNT_SID=...
TWILIO_AUTH_TOKEN=...
OPENAI_API_KEY=...
CARTESIA_API_KEY=...
```

## Configuring your bot runner

You'll need a HTTP service that can receive incoming call hooks and trigger a new agent session. We discussed the concept of a [bot runner](/guides/deployment/pattern) in the deployment section, which we'll build on here to add support for incoming phone calls.

Here's how to implement this with FastAPI:

```python server.py
@app.post("/call", response_class=PlainTextResponse)
async def handle_call(request: Request):
    """Handle incoming Twilio call webhook."""
    print("Received call webhook from Twilio")

    try:
        # Get form data from Twilio webhook
        form_data = await request.form()
        data = dict(form_data)

        # Extract call ID (required to forward the call later)
        call_sid = data.get("CallSid")
        if not call_sid:
            raise HTTPException(status_code=400, detail="Missing CallSid in request")

        print(f"Processing call with ID: {call_sid}")

        # 1. Create a Daily room with SIP capabilities
        # More on this in the next section
        room_details = await create_sip_room(request.app.session)
        room_url = room_details["room_url"]
        token = room_details["token"]
        sip_endpoint = room_details["sip_endpoint"]

        # 2. Start the bot process with the necessary parameters
        bot_cmd = f"python bot.py -u {room_url} -t {token} -i {call_sid} -s {sip_endpoint}"
        subprocess.Popen(shlex.split(bot_cmd))
        print(f"Started bot process with command: {bot_cmd}")

        # 3. IMPORTANT: Put the caller on hold with music while the bot initializes
        # This is critical to avoid timing issues with Daily SIP initialization
        resp = VoiceResponse()
        resp.play(
            url="https://your-hold-music.mp3",  # Your custom hold music URL
            loop=10,  # Loop the music until the bot is ready
        )

        return str(resp)

    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")
```

### Creating a SIP-enabled room

We'll need to configure the Daily room to be setup to receive SIP connections. In our example project, `utils/daily-helpers.py` demonstrates how to set up the Daily room using the [Daily REST helpers](/server/utilities/daily/rest-helper) available in Pipecat. We just need to pass through new SIP parameters as part of room creation:

```python utils/daily_helpers.py
async def create_sip_room(session: Optional[aiohttp.ClientSession] = None) -> Dict[str, str]:
    """Create a Daily room with SIP capabilities for phone calls."""
    daily_helper = await get_daily_helper(session)

    # Configure SIP parameters
    sip_params = DailyRoomSipParams(
        display_name="phone-user",  # This can be customized with caller info
        video=False,                # Audio-only call
        sip_mode="dial-in",         # For receiving calls
        num_endpoints=1,            # Number of SIP endpoints needed
    )

    # Create room properties with SIP enabled
    properties = DailyRoomProperties(
        sip=sip_params,
        enable_dialout=True,  # For future expansion if needed
        start_video_off=True, # Voice only
    )

    # Create and return the room
    room = await daily_helper.create_room(DailyRoomParams(properties=properties))
    token = await daily_helper.get_token(room.url, 24 * 60 * 60)  # 24 hours validity

    return {
        "room_url": room.url,
        "token": token,
        "sip_endpoint": room.config.sip_endpoint
    }
```

## Configuring your bot

Your bot needs to handle the `on_dialin_ready` event to forward the call at the right time:

```python bot.py
async def run_bot(room_url: str, token: str, call_id: str, sip_uri: str) -> None:
    """Run the voice bot with the given parameters."""
    logger.info(f"Starting bot with room: {room_url}")
    logger.info(f"SIP endpoint: {sip_uri}")

    # IMPORTANT: Track if call has been forwarded to avoid multiple forwards
    call_already_forwarded = False

    # Setup the Daily transport
    transport = DailyTransport(
        room_url,
        token,
        "Phone Bot",
        DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            transcription_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    # ... rest of your bot setup code ...

    # Handle call ready to forward
    @transport.event_handler("on_dialin_ready")
    async def on_dialin_ready(transport, cdata):
        nonlocal call_already_forwarded

        # We only want to forward the call once
        # The on_dialin_ready event will be triggered for each SIP endpoint
        if call_already_forwarded:
            logger.warning("Call already forwarded, ignoring this event.")
            return

        logger.info(f"Forwarding call {call_id} to {sip_uri}")

        try:
            # Update the Twilio call with TwiML to forward to the Daily SIP endpoint
            twilio_client.calls(call_id).update(
                twiml=f"<Response><Dial timeout=\"30\"><Sip>{sip_uri}</Sip></Dial></Response>"
            )
            logger.info("Call forwarded successfully")
            call_already_forwarded = True
        except Exception as e:
            logger.error(f"Failed to forward call: {str(e)}")
            raise
```

## Setting up the Twilio webhook

Configure your Twilio phone number to use your server's webhook URL:

1. Go to the [Twilio Console](https://console.twilio.com)
2. Navigate to Phone Numbers → Manage → Active Numbers
3. Click on your phone number
4. Under "Configure", set "A Call Comes In" to:
   - Webhook: `https://your-server.com/call` (your server's URL)
   - HTTP Method: POST

## Testing locally

For local development, you can use [ngrok](https://ngrok.com/) to expose your local server:

```shell
# Start your server
python server.py

# In another terminal, start ngrok
ngrok http 8000

# Use the ngrok URL (e.g., https://a1b2c3.ngrok.io/call) as your webhook
```

## Best Practices and Common Pitfalls

### ✅ Best Practice: Put the call on hold

Always respond to Twilio's initial webhook with hold music. This gives your bot time to initialize and the Daily SIP endpoint to become ready.

```python
resp = VoiceResponse()
resp.play(url="https://your-hold-music.mp3", loop=10)
return str(resp)
```

### ❌ Pitfall: Using `<Pause>` instead of hold music

Don't use the TwiML `<Pause>` element to wait for the bot to initialize:

```python
# DON'T DO THIS
resp = VoiceResponse()
resp.pause(length=10)  # This can cause connection issues
```

Twilio can only pause a call for a short duration (~5 seconds), which may not be enough time for the Daily SIP setup to complete.

### ✅ Best Practice: Handle multiple `on_dialin_ready` events

If your Daily room has multiple SIP endpoints, use a flag to ensure you only forward the call once:

```python
call_already_forwarded = False

@transport.event_handler("on_dialin_ready")
async def on_dialin_ready(transport, cdata):
    nonlocal call_already_forwarded

    if call_already_forwarded:
        logger.info("Call already forwarded, ignoring this event.")
        return

    # Forward the call...
    call_already_forwarded = True
```

<Note>
  A single SIP endpoint is sufficient for the initial connection. A second SIP
  endpoint is needed only if you plan to forward the call.
</Note>



================================================
FILE: guides/telephony/twilio-websockets.mdx
================================================
---
title: "Dial-in: Twilio (Media Streams)"
description: "Call your Pipecat bot over websockets using Twilio"
---

This guide walks through creating a voice AI agent that users can reach by dialing a Twilio phone number. We'll integrate Twilio Media Streams via WebSockets with a Pipecat pipeline to create a fully functional voice AI experience.

## Things you'll need

- An active [Twilio](https://www.twilio.com) account with at least one phone number
- A public-facing server or a tunneling service like [ngrok](https://ngrok.com/)
- API keys for speech-to-text, text-to-speech, and LLM services

<Card
  title="Complete example code on GitHub"
  icon="code"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot"
>
  The full source code for this example is available in the Pipecat repository.
</Card>

## Architecture Overview

When a user dials your Twilio phone number:

1. Twilio calls your server's endpoint, which returns TwiML that establishes a WebSocket connection
2. Twilio sends real-time audio data over the WebSocket, and your server processes it using the Pipecat pipeline
3. Your Pipecat agent processes the audio which is sent to the pipeline, which then outputs audio data back to Twilio
4. Twilio plays this audio to the caller in real-time

## Server Implementation

The server needs to:

1. Serve TwiML in response to Twilio's HTTP requests
2. Handle WebSocket connections
3. Process audio with the Pipecat pipeline

Let's look at the key components:

### FastAPI Server Setup

```python
from fastapi import FastAPI, WebSocket
from starlette.responses import HTMLResponse

app = FastAPI()

@app.post("/")
async def start_call():
    return HTMLResponse(
        content=open("templates/streams.xml").read(),
        media_type="application/xml"
    )

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial WebSocket messages
    start_data = ws.iter_text()
    await start_data.__anext__()

    # Second message contains the call details
    call_data = json.loads(await start_data.__anext__())

    # Extract both StreamSid and CallSid
    stream_sid = call_data["start"]["streamSid"]
    call_sid = call_data["start"]["callSid"]

    # Run your Pipecat bot
    await run_bot(websocket, stream_sid, call_sid, app.state.testing)
```

This server has two main endpoints:

- `POST /` - Returns TwiML instructions to Twilio
- `WebSocket /ws` - Handles the WebSocket connection for real-time audio

### TwiML Configuration

The TwiML tells Twilio to establish a WebSocket connection with your server:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Connect>
    <Stream url="wss://<your server url>/ws"></Stream>
  </Connect>
  <Pause length="40"/>
</Response>
```

<Note>
  Replace `<your server url>` with your server's publicly accessible domain. The `Pause` element keeps the call alive for a maximum of 40 seconds. Adjust this value based on your expected conversation length.
</Note>

### Pipecat Bot Implementation

The `run_bot` function creates and connects all the components in the Pipecat pipeline:

```python
async def run_bot(websocket_client: WebSocket, stream_sid: str, call_sid: str, testing: bool):
    # Initialize the Twilio serializer with stream and call SIDs
    serializer = TwilioFrameSerializer(
        stream_sid=stream_sid,
        call_sid=call_sid,
        account_sid=os.getenv("TWILIO_ACCOUNT_SID", ""),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN", ""),
    )

    # Create the WebSocket transport
    transport = FastAPIWebsocketTransport(
        websocket=websocket_client,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    # Initialize AI services
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"), audio_passthrough=True)
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        push_silence_after_stop=testing,
    )

    # Create the initial conversation prompt
    messages = [
        {
            "role": "system",
            "content": "You are an elementary teacher in an audio call. Your output will be converted to audio so don't include special characters in your answers. Respond to what the student said in a short short sentence.",
        },
    ]

    # Setup the context aggregator
    context = OpenAILLMContext(messages)
    context_aggregator = llm.create_context_aggregator(context)

    # Create an audio buffer processor to capture conversation audio
    audiobuffer = AudioBufferProcessor(user_continuous_stream=not testing)

    # Build the pipeline
    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            audiobuffer,  # Used to buffer the audio in the pipeline
            context_aggregator.assistant(),
        ]
    )

    # Create a pipeline task with appropriate parameters
    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,  # Twilio uses 8kHz audio
            audio_out_sample_rate=8000,
            allow_interruptions=True,
        ),
    )

    # Setup event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Start recording
        await audiobuffer.start_recording()
        # Kickstart the conversation
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    @audiobuffer.event_handler("on_audio_data")
    async def on_audio_data(buffer, audio, sample_rate, num_channels):
        server_name = f"server_{websocket_client.client.port}"
        await save_audio(server_name, audio, sample_rate, num_channels)

    # Run the pipeline
    runner = PipelineRunner(handle_sigint=False, force_gc=True)
    await runner.run(task)
```

## Key Technical Considerations

### Audio Format and Sample Rate

Twilio Media Streams uses 8kHz mono audio with 16-bit PCM encoding. Make sure your pipeline is configured correctly:

```python
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        audio_in_sample_rate=8000,  # Twilio's audio format
        audio_out_sample_rate=8000,
        allow_interruptions=True,
    ),
)
```

### Serialization and Call Control

The `TwilioFrameSerializer` handles the protocol specifics for communicating with Twilio's Media Streams:

```python
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,
    account_sid=os.getenv("TWILIO_ACCOUNT_SID", ""),
    auth_token=os.getenv("TWILIO_AUTH_TOKEN", ""),
)
```

<Tip>
  When you provide the `account_sid` and `auth_token` to the
  `TwilioFrameSerializer`, it will automatically end the call via Twilio's REST
  API when the pipeline ends. This ensures clean call termination when your bot
  finishes its conversation.
</Tip>

### Voice Activity Detection

The SileroVAD analyzer helps determine when a user has finished speaking:

```python
params=FastAPIWebsocketParams(
    audio_in_enabled=True,
    audio_out_enabled=True,
    add_wav_header=False,
    vad_analyzer=SileroVADAnalyzer(),
    serializer=serializer,
)
```

## Configuring Twilio

To set up your Twilio phone number to use your server:

1. Purchase a phone number in your Twilio account
2. Navigate to the Phone Numbers section and select your number
3. Under "Voice & Fax", set the webhook for "A Call Comes In" to your server's URL (e.g., `https://your-server.com/`)
4. Make sure the request type is set to HTTP POST
5. Save your changes

<Note>
  If you're using ngrok for local development, your webhook URL will look like
  `https://abc123.ngrok.io/`. Remember to update your TwiML template with the
  correct WebSocket URL as well.
</Note>

## Testing Your Implementation

### Local Testing Without Phone Calls

The example includes a test client that can simulate phone calls without actually using Twilio:

```python
python server.py -t  # Start server in testing mode
python client.py -u http://localhost:8765 -c 2  # Start 2 test clients
```

The `-t` flag puts the server in testing mode, and the test client creates virtual clients that communicate with your server as if they were Twilio Media Streams.

### Using the Phone

To test with an actual phone call:

1. Make sure your server is running and accessible via the internet
2. Configure Twilio as described above
3. Dial your Twilio phone number from any phone
4. You should hear your AI agent respond!

## Ending a Conversation

There are two primary ways to end a conversation:

1. **Automatic termination**: If you provided Twilio credentials to the `TwilioFrameSerializer`, the call will be ended automatically when your pipeline ends:

```python
# End the conversation programmatically
await task.queue_frame(EndFrame())
```

2. **Manual termination**: You can also end the call explicitly through Twilio's REST API. This is useful for implementing custom hang-up logic:

```python
from twilio.rest import Client

client = Client(account_sid, auth_token)
client.calls(call_sid).update(status="completed")
```

## Scaling Considerations

For production deployments, consider:

1. **Multiple concurrent calls**: Each bot instance should run in its own process to handle concurrent calls efficiently. The example server can spawn individual bot processes for each call:

```python
proc = subprocess.Popen(
    [
        f"python3 -m bot_twilio --stream-sid {stream_sid} --call-sid {call_sid}"
    ],
    shell=True,
    bufsize=1,
    cwd=os.path.dirname(os.path.abspath(__file__))
)
```

2. **Error handling**: Add robust error handling for network issues, service outages, etc.

3. **Logging and monitoring**: Implement detailed logging to track call quality and agent performance.

4. **Security**: Add authentication to your endpoints and use environment variables for all sensitive credentials.

## Next Steps

<CardGroup>
  <Card
    title="Try the complete example"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot"
  >
    Clone the full working Twilio chatbot example from GitHub and run it
    yourself.
  </Card>
  <Card
    title="Deploy to Pipecat Cloud"
    icon="cloud"
    href="https://docs.pipecat.daily.co/pipecat-in-production/telephony/twilio-mediastreams"
  >
    Learn how to deploy your Twilio voice AI agent to production with Pipecat
    Cloud's managed infrastructure.
  </Card>
</CardGroup>



================================================
FILE: server/introduction.mdx
================================================
---
title: "Server API Reference"
description: "Complete reference for Pipecat’s Python server APIs and services"
---

## Overview

Pipecat's server-side functionality is organized into three main categories:

<CardGroup cols={3}>
  <Card
    title="Services"
    icon="puzzle-piece"
    href="/server/services/supported-services"
  >
    AI service integrations for speech, language, vision, and more
  </Card>
  <Card title="Frameworks" icon="puzzle" href="/server/frameworks">
    > User RTVI for client/server communication or Pipecat Flows for building structured conversations
  </Card>

  <Card title="Utilities" icon="wrench" href="/server/utilities">
    Helper functions and tools for audio, filtering, and flows
  </Card>
</CardGroup>

## Service Categories

Pipecat integrates with various AI services across different categories:

<CardGroup cols={2}>
  <Card title="Transport" icon="network-wired">
    WebRTC and WebSocket implementations for real-time communication
  </Card>
  <Card title="Speech Processing" icon="waveform-lines">
    Speech-to-text, text-to-speech, and speech-to-speech services
  </Card>
  <Card title="Language Models" icon="brain">
    Integration with various LLM providers
  </Card>
  <Card title="Vision & Media" icon="image">
    Image generation, video processing, and computer vision
  </Card>
</CardGroup>

## Getting Started

1. Browse our [Supported Services](/server/services/supported-services) to see available integrations
2. Install required dependencies for your chosen services
3. Reference individual service docs for detailed configuration options

## Example Usage

```python
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline import Pipeline
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.services.daily import DailyParams, DailyTransport

transport = DailyTransport(
    room_url,
    token,
    "Respond bot",
    DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)

# Configure services
stt = DeepgramSTTService(api_key=KEY)
llm = OpenAILLMService(api_key=KEY, model="gpt-4o")
tts = CartesiaTTSService(api_key=KEY, voice_id=ID)

# Create context and aggregators
context = OpenAILLMContext(
    messages=[{"role": "system", "content": "You are a helpful assistant."}]
)
context_aggregator = llm.create_context_aggregator(context)

# Create pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator,assistant()
])
```



================================================
FILE: server/frameworks/flows/pipecat-flows.mdx
================================================
---
title: "Pipecat Flows"
description: "Reference docs for Pipecat’s conversation flow system"
---

<Tip>
  New to building conversational flows? Check out our [Pipecat Flows
  guide](/guides/features/pipecat-flows) first.
</Tip>

Pipecat Flows is an add-on framework for Pipecat that allows you to build structured conversations in your AI applications. It enables you to define conversation paths while handling the complexities of state management and LLM interactions.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-flows.pipecat.ai"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="GitHub Repository"
    icon="github"
    href="https://github.com/pipecat-ai/pipecat-flows"
  >
    Source code, examples, and issue tracking
  </Card>
  <Card
    title="Hello World Example"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-flows/blob/main/examples/quickstart/hello_world.py"
  >
    Working example with basic conversation flow
  </Card>
</CardGroup>

## Installation

### Pipecat Flows

To use Pipecat Flows, install the required dependency:

```bash
pip install pipecat-ai-flows
```

### Pipecat Dependencies

For fresh installations, you'll need to install Pipecat with depdencies for your Transport, STT, LLM, and TTS providers.

For example, to use Daily, OpenAI, Deepgram, Cartesia, and Silero:

```bash
pip install "pipecat-ai[daily,openai,deepgram,cartesia,silero]"
```

## Flow Types

### Dynamic Flows (Recommended)

Create conversation paths at runtime based on user input, external data, or business logic. Enables more flexible and adaptive interactions. Additionally, can handle simple scenarios with predefined paths.

<Note>
  Dynamic Flows are the recommended approach for most applications, as they
  allow for greater flexibility and adaptability in conversation design.
</Note>

### Static Flows

Use predefined conversation paths configured upfront. Ideal for predefined interactions where the graph navigation can be predetermined.

## Key Components

### Core Classes

- **FlowManager** - Main orchestration class for managing conversation flows
- **FlowsFunctionSchema** - Standardized function call schema that works with any LLM provider

### Type Definitions

- **FlowArgs** - Function handler arguments
- **FlowResult** - Function return type with status and optional data
- **NodeConfig** - Complete node configuration including messages, functions, and actions
- **FlowConfig** - Static flow configuration with nodes and initial state

### Context Management

- **ContextStrategyConfig** - Configuration for context management during transitions
- **ContextStrategy** - Enum for APPEND, RESET, and RESET_WITH_SUMMARY strategies

### Actions

- **ActionConfig** - Configuration for custom pre- and post-actions during conversation flow

## Function Types

### Node Functions

Execute operations within a single conversation state without switching nodes. Return `(FlowResult, None)`.

### Edge Functions

Create transitions between conversation states, optionally processing data first. Return `(FlowResult, NodeConfig)` or `(FlowResult, str)`.

### Direct Functions

Functions passed directly to NodeConfig with automatic metadata extraction from signatures and docstrings.

## LLM Provider Support

Pipecat Flows automatically handles format differences between providers:

| Provider      | Format Support        | Installation                          |
| ------------- | --------------------- | ------------------------------------- |
| OpenAI        | Function calling      | `pip install "pipecat-ai[openai]"`    |
| Anthropic     | Native tools          | `pip install "pipecat-ai[anthropic]"` |
| Google Gemini | Function declarations | `pip install "pipecat-ai[google]"`    |
| AWS Bedrock   | Anthropic-compatible  | `pip install "pipecat-ai[aws]"`       |

## Error Handling

Comprehensive exception hierarchy for flow management:

- **FlowError** - Base exception for all flow-related errors
- **FlowInitializationError** - Flow manager setup failures
- **FlowTransitionError** - Node transition failures
- **InvalidFunctionError** - Function registration or execution errors
- **ActionError** - Action execution failures

## Additional Notes

- **State Management**: Use `flow_manager.state` dictionary for persistent conversation data
- **Automatic Function Call Registration and Validation**: All functions are automatically registered and validated at run-time
- **Provider Compatibility**: Format differences handled automatically via adapter system
- **Deprecation**: Some legacy patterns (`transition_to`, `transition_callback`) are deprecated in favor of consolidated function handlers



================================================
FILE: server/frameworks/rtvi/google-rtvi-observer.mdx
================================================
---
title: "Google RTVI Observer"
description: "Adding support for sending search responses to RTVI clients"
---

## Overview

The `GoogleRTVIObserver` extends the [base `RTVIObserver` type](./rtvi-observer), to add support for the `bot-llm-search-response` message type and providing clients with the search results from the `GoogleLLMService`. See [this section on Search Grounding](/server/services/llm/gemini#search-grounding) for more details.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.google.rtvi.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Gemini Docs"
    icon="book"
    href="https://ai.google.dev/gemini-api/docs/google-search"
  >
    Official Google Gemini API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/news-chatbot"
  >
    Working example using Google's search grounding to ask about current events
  </Card>
</CardGroup>

## Installation

To use `GoogleRTVIObserver`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You'll also need to follow setup instructions for the [Google LLM Service](/server/services/llm/gemini#installation) to ensure the `GoogleLLMService` is properly configured.

## Frame Translation

The observer maps the `LLMSearchResponseFrame` to the `RTVIBotLLMSearchResponseMessage`. Check out the [RTVI Standard Reference](/client/rtvi-standard#bot-llm-search-response-🤖) for details on the message format.

## Usage Example

This observer should replace the base `RTVIObserver` and be set up exactly the same otherwise:

```python
from pipecat.services.google.rtvi import GoogleRTVIObserver

# Create the RTVIProcessor
rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

# Add to pipeline
pipeline = Pipeline([
    transport.input(),
    rtvi,
    # Other processors...
])

# Create pipeline task with observer
task = PipelineTask(
    pipeline,
    params=PipelineParams(allow_interruptions=True),
    observers=[GoogleRTVIObserver(rtvi)],  # Add the observer here
)
```



================================================
FILE: server/frameworks/rtvi/introduction.mdx
================================================
---
title: "RTVI (Real-Time Voice Interaction)"
sidebarTitle: "Overview"
description: "Build real-time voice and multimodal applications with Pipecat’s RTVI protocol"
---

Pipecat's RTVI (Real-Time Voice Interaction) protocol provides a standardized communication layer between clients and servers for building real-time voice and multimodal applications. It handles the synchronization of user and bot interactions, transcriptions, LLM processing, and text-to-speech delivery.

<CardGroup cols={2}>
  <Card title="Speaking States" icon="microphone">
    Track when users and bots start/stop speaking for natural turn-taking
  </Card>

<Card title="Transcription" icon="closed-captioning">
  Handle real-time transcriptions from both users and bots
</Card>

<Card title="LLM Processing" icon="brain">
  Manage LLM responses and function calls with proper client notifications
</Card>

  <Card title="TTS Management" icon="waveform-lines">
    Control text-to-speech state and audio delivery
  </Card>
</CardGroup>

## Architecture

RTVI operates with two primary components:

1. **RTVIProcessor** - A frame processor residing in the pipeline that serves as the entry point for sending and receiving messages to/from the client.

2. **RTVIObserver** - An observer that monitors pipeline events and translates them into client-compatible messages, handling:
   - Speaking state changes
   - Transcription updates
   - LLM responses
   - TTS events
   - Performance metrics

## Basic Example

Here's how to set up RTVI in your Pipecat application:

```python
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor

# Create the RTVI processor
rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

# Include the RTVIProcessor in your pipeline
pipeline = Pipeline(
    [
        transport.input(),
        rtvi,
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ]
)

# Add the RTVIObserver to your pipeline task
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
    ),
    observers=[RTVIObserver(rtvi)],
)

# Handle client connection
@rtvi.event_handler("on_client_ready")
async def on_client_ready(rtvi):
    # Signal bot is ready to receive messages
    await rtvi.set_bot_ready()
    # Initialize the conversation
    await task.queue_frames([context_aggregator.user().get_context_frame()])

# Handle participant disconnection
@transport.event_handler("on_participant_left")
async def on_participant_left(transport, participant, reason):
    await task.cancel()

# Run the pipeline
runner = PipelineRunner()
await runner.run(task)
```

## Protocol Flow

1. Client connects and sends a `client-ready` message
2. Server responds with `bot-ready` and initial configuration
3. Client and server exchange real-time events:
   - Speaking state changes (`user/bot-started/stopped-speaking`)
   - Transcriptions (`user/bot-transcription`)
   - LLM processing (`bot-llm-text`, `llm-function-call`)
   - TTS events (`bot-tts-text`, `bot-tts-audio`)

## Key Components

<CardGroup cols={2}>
  <Card 
    title="RTVIProcessor" 
    icon="gear"
    href="/server/frameworks/rtvi/rtvi-processor">
    Configure and manage RTVI services, actions, and client communication
  </Card>

  <Card 
    title="RTVIObserver" 
    icon="magnifying-glass"
    href="/server/frameworks/rtvi/rtvi-observer">
    Translate internal pipeline events to standardized client messages
  </Card>
</CardGroup>

## Client Integration

RTVI is implemented in Pipecat client SDKs, providing a high-level API to interact with the protocol. Visit the Pipecat Client SDKs documentation:

<Card title="Client SDKs" icon="mobile-screen" href="/client/introduction">
  Learn how to implement RTVI on the client-side with our JavaScript, React, and
  mobile SDKs
</Card>



================================================
FILE: server/frameworks/rtvi/rtvi-observer.mdx
================================================
---
title: "RTVI Observer"
description: "Converting pipeline frames to RTVI protocol messages"
---

The `RTVIObserver` translates Pipecat's internal pipeline events into standardized RTVI protocol messages. It monitors frame flow through the pipeline and generates corresponding client messages based on event types.

## Purpose

The `RTVIObserver` serves two main functions:

1. Converting internal pipeline frames to client-compatible RTVI messages
2. Managing aggregated state for multi-frame events (like bot transcriptions)

## Adding to a Pipeline

The observer is attached to a pipeline task along with the RTVI processor:

```python
# Create the RTVIProcessor
rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

# Add to pipeline
pipeline = Pipeline([
    transport.input(),
    rtvi,
    # Other processors...
])

# Create pipeline task with observer
task = PipelineTask(
    pipeline,
    params=PipelineParams(allow_interruptions=True),
    observers=[RTVIObserver(rtvi)],  # Add the observer here
)
```

## Frame Translation

The observer maps Pipecat's internal frames to RTVI protocol messages:

| Pipeline Frame              | RTVI Message                                |
| --------------------------- | ------------------------------------------- |
| **Speech Events**           |
| `UserStartedSpeakingFrame`  | `RTVIUserStartedSpeakingMessage`            |
| `UserStoppedSpeakingFrame`  | `RTVIUserStoppedSpeakingMessage`            |
| `BotStartedSpeakingFrame`   | `RTVIBotStartedSpeakingMessage`             |
| `BotStoppedSpeakingFrame`   | `RTVIBotStoppedSpeakingMessage`             |
| **Transcription**           |
| `TranscriptionFrame`        | `RTVIUserTranscriptionMessage(final=true)`  |
| `InterimTranscriptionFrame` | `RTVIUserTranscriptionMessage(final=false)` |
| **LLM Processing**          |
| `LLMFullResponseStartFrame` | `RTVIBotLLMStartedMessage`                  |
| `LLMFullResponseEndFrame`   | `RTVIBotLLMStoppedMessage`                  |
| `LLMTextFrame`              | `RTVIBotLLMTextMessage`                     |
| **TTS Events**              |
| `TTSStartedFrame`           | `RTVIBotTTSStartedMessage`                  |
| `TTSStoppedFrame`           | `RTVIBotTTSStoppedMessage`                  |
| `TTSTextFrame`              | `RTVIBotTTSTextMessage`                     |
| **Context/Metrics**         |
| `OpenAILLMContextFrame`     | `RTVIUserLLMTextMessage`                    |
| `MetricsFrame`              | `RTVIMetricsMessage`                        |
| `RTVIServerMessageFrame`    | `RTVIServerMessage`                         |



================================================
FILE: server/frameworks/rtvi/rtvi-processor.mdx
================================================
---
title: "RTVIProcessor"
description: "Core coordinator for RTVI protocol communication"
---

The `RTVIProcessor` manages bidirectional communication between clients and your Pipecat application. It processes client messages, handles service configuration, executes actions, and coordinates function calls.

## Initialization

Add the `RTVIProcessor` to your pipeline:

```python
from pipecat.processors.rtvi import RTVIProcessor, RTVIConfig, RTVIServiceConfig

# Create the RTVIProcessor
rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

# Add to pipeline
pipeline = Pipeline([
    transport.input(),
    rtvi,
    stt,
    # ... other processors ...
    transport.output()
])
```

## Readiness Protocol

### Client Ready State

Clients indicate readiness by sending a `client-ready` message, triggering the `on_client_ready` event in the processor:

```python
@rtvi.event_handler("on_client_ready")
async def on_client_ready(rtvi):
    # Handle client ready state
    await rtvi.set_bot_ready()
    # Initialize conversation
    await task.queue_frames([...])
```

### Bot Ready State

The server must mark the bot as ready before it can process client messages:

```python
await rtvi.set_bot_ready()
```

When marked ready, the bot sends a response containing:

- RTVI protocol version
- Current service configuration
- Available actions

## Services

Services represent configurable components of your application that clients can interact with.

### Registering Services

```python
# 1. Define option handler
async def handle_voice_option(processor, service, option):
    voice_id = option.value
    # Apply configuration change
    logger.info(f"Voice ID updated to: {voice_id}")

# 2. Create RTVIService
voice_service = RTVIService(
    name="voice",
    options=[
        RTVIServiceOption(
            name="voice_id",
            type="string",
            handler=handle_voice_option
        )
    ]
)

# 3. Register with processor
rtvi.register_service(voice_service)
```

### Option Types

Services support multiple data types for configuration:

```python
RTVIServiceOption(
    name="temperature",
    type="number",  # number, string, bool, array, object
    handler=handle_temperature
)
```

Option handlers receive:

- The processor instance
- The service name
- The option configuration with new value

## Actions

Actions are server-side functions that clients can trigger with arguments.

### Registering Actions

```python
# 1. Define handler function
async def handle_print_message(processor, service, arguments):
    message = arguments.get("message", "Default message")
    logger.info(f"Print action triggered with message: {message}")
    return True

# 2. Create and register RTVIAction
print_action = RTVIAction(
    service="conversation",
    action="print_message",
    arguments=[
        RTVIActionArgument(name="message", type="string")
    ],
    result="bool",
    handler=handle_print_message
)
rtvi.register_action(print_action)
```

### Action Arguments

Actions can accept typed arguments from clients:

```python
search_action = RTVIAction(
    service="knowledge",
    action="search",
    arguments=[
        RTVIActionArgument(name="query", type="string"),
        RTVIActionArgument(name="limit", type="number")
    ],
    result="array",
    handler=handle_search
)
```

## Function Calls

Handle LLM function calls with client interaction:

```python

await processor.handle_function_call(
    function_name=function_name,
    tool_call_id=tool_call_id,
    arguments=arguments,
)

await processor.handle_function_call(params)
```

The function call process:

1. LLM requests a function call
2. Processor notifies client with `llm-function-call` message
3. Client executes function and returns result
4. Result is passed back to LLM via `FunctionCallResultFrame`
5. Conversation continues

## Error Handling

Send error messages to clients:

```python
# General error
await processor.send_error("Invalid configuration")

# Request-specific error
await processor._send_error_response(request_id, "Invalid action arguments")
```

Error categories:

- Configuration errors
- Action execution errors
- Function call errors
- Protocol errors
- Fatal and non-fatal errors

## Bot Control

Manage bot state and handle interruptions:

```python
# Set bot as ready
await processor.set_bot_ready()

# Handle interruptions
await processor.interrupt_bot()
```

## Custom Messaging

Send custom messages from server to client:

```python
from pipecat.processors.frameworks.rtvi import RTVIServerMessageFrame

# Send a custom message
frame = RTVIServerMessageFrame(
    data={
        "type": "custom-event",
        "payload": {"key": "value"}
    }
)
await rtvi.push_frame(frame)
```

The client receives these messages through the `onServerMessage` callback or `serverMessage` event.



================================================
FILE: server/links/server-reference.mdx
================================================
---
title: "Reference docs"
url: "https://reference-server.pipecat.ai/"
icon: "book"
---



================================================
FILE: server/pipeline/heartbeats.mdx
================================================
---
title: "Pipeline Heartbeats"
description: "Monitor pipeline health with heartbeat frames"
---

## Overview

Pipeline heartbeats provide a way to monitor the health of your pipeline by sending periodic heartbeat frames through the system. When enabled, the pipeline will send heartbeat frames every second and monitor their progress through the pipeline.

## Enabling Heartbeats

Heartbeats can be enabled by setting `enable_heartbeats` to `True` in the `PipelineParams`:

```python
from pipecat.pipeline.task import PipelineParams, PipelineTask

pipeline = Pipeline([...])
params = params=PipelineParams(enable_heartbeats=True)
task = PipelineTask(pipeline, params)
```

## How It Works

When heartbeats are enabled:

1. The pipeline sends a `HeartbeatFrame` every second
2. The frame traverses through all processors in the pipeline, from source to sink
3. The pipeline monitors how long it takes for heartbeat frames to complete their journey
4. If a heartbeat frame isn't received within 5 seconds, a warning is logged

## Monitoring Output

The system will log:

- Trace-level logs showing heartbeat processing time
- Warning messages if heartbeats aren't received within the monitoring window

Example warning message:

```
WARNING    PipelineTask#1: heartbeat frame not received for more than 5.0 seconds
```

## Use Cases

Heartbeat monitoring is useful for:

- Detecting pipeline stalls or blockages
- Monitoring processing latency through the pipeline
- Identifying performance issues in specific processors
- Ensuring the pipeline remains responsive

## Configuration

The heartbeat system uses two key timing constants:

- `HEARTBEAT_SECONDS = 1.0` - Interval between heartbeat frames
- `HEARTBEAT_MONITOR_SECONDS = 10.0` - Time before warning if no heartbeat received

<Note>
  These values are currently fixed but may be configurable in future versions.
</Note>



================================================
FILE: server/pipeline/parallel-pipeline.mdx
================================================
---
title: "ParallelPipeline"
description: "Run multiple pipeline branches in parallel, with synchronized inputs and outputs for complex flows"
---

## Overview

`ParallelPipeline` allows you to create multiple independent processing branches that run simultaneously, sharing input and coordinating output. It's particularly useful for multi-agent systems, parallel stream processing, and creating redundant service paths.

Each branch receives the same downstream frames, processes them independently, and the results are merged back into a single stream. System frames (like `StartFrame` and `EndFrame`) are synchronized across all branches.

## Constructor Parameters

<ParamField path="*args" type="List[List[FrameProcessor]]" required>
  Multiple lists of processors, where each list defines a parallel branch. All
  branches execute simultaneously when frames flow through the pipeline.
</ParamField>

## Usage Examples

### Multi-Agent Conversation

Create a conversation with two AI agents that can interact with the user independently:

```python
pipeline = Pipeline([
    transport.input(),
    ParallelPipeline(
        # Agent 1: Customer service representative
        [
            stt_1,
            context_aggregator.user_a(),
            llm_agent_1,
            tts_agent_1,
        ],
        # Agent 2: Technical specialist
        [   stt_2,
            context_aggregator.user_b(),
            llm_agent_2,
            tts_agent_2,
        ]
    ),
    transport.output(),
])
```

### Redundant Services with Failover

Set up redundant services with automatic failover:

```python
pipeline = Pipeline([
    transport.input(),
    stt,
    ParallelPipeline(
        # Primary LLM service
        [   gate_primary,
            primary_llm,
            error_detector,
        ],
        # Backup LLM service (used only if primary fails)
        [   gate_backup,
            backup_llm,
            fallback_processor,
        ]
    ),
    tts,
    transport.output(),
])
```

### Cross-Branch Communication

Using Producer/Consumer processors to share data between branches:

```python
# Create producer/consumer pair for cross-branch communication
frame_producer = ProducerProcessor(filter=is_important_frame)
frame_consumer = ConsumerProcessor(producer=frame_producer)

pipeline = Pipeline([
    transport.input(),
    ParallelPipeline(
        # Branch that generates important frames
        [
            stt,
            llm,
            tts,
            frame_producer,  # Share frames with other branch
        ],
        # Branch that consumes those frames
        [
            frame_consumer,  # Receive frames from other branch
            llm,             # Speech to Speech LLM (audio in)
        ]
    ),
    transport.output(),
])
```

## How It Works

1. `ParallelPipeline` adds special source and sink processors to each branch
2. System frames (like `StartFrame` and `EndFrame`) are sent to all branches
3. Other frames flow downstream to all branch sources
4. Results from each branch are collected at the sinks
5. The pipeline ensures `EndFrame`s are only passed through after all branches complete



================================================
FILE: server/pipeline/pipeline-idle-detection.mdx
================================================
---
title: "Pipeline Idle Detection"
description: "Automatically detect and handle idle pipelines with no bot activity"
---

## Overview

Pipeline idle detection is a feature that monitors activity in your pipeline and can automatically cancel tasks when no meaningful bot interactions are occurring. This helps prevent pipelines from running indefinitely when a conversation has naturally ended but wasn't properly terminated.

## How It Works

The system monitors specific "activity frames" that indicate the bot is actively engaged in the conversation. By default, these are:

- `BotSpeakingFrame` - When the bot is speaking
- `LLMFullResponseEndFrame` - When the LLM has completed a response

If no activity frames are detected within the configured timeout period (5 minutes by default), the system considers the pipeline idle and can automatically terminate it.

<Note>
  Idle detection only starts after the pipeline has begun processing frames. The
  idle timer resets whenever an activity frame (as specified in
  `idle_timeout_frames`) is received.
</Note>

## Configuration

You can configure idle detection behavior when creating a `PipelineTask`:

```python
from pipecat.pipeline.task import PipelineParams, PipelineTask

# Default configuration - cancel after 5 minutes of inactivity
task = PipelineTask(pipeline)

# Custom configuration
task = PipelineTask(
    pipeline,
    params=PipelineParams(allow_interruptions=True),
    idle_timeout_secs=600,  # 10 minute timeout
    idle_timeout_frames=(BotSpeakingFrame,),  # Only monitor bot speaking
    cancel_on_idle_timeout=False,  # Don't auto-cancel, just notify
)
```

## Configuration Parameters

<ParamField path="idle_timeout_secs" type="Optional[float]" default="300">
  Timeout in seconds before considering the pipeline idle. Set to `None` to
  disable idle detection.
</ParamField>

<ParamField
  path="idle_timeout_frames"
  type="Tuple[Type[Frame], ...]"
  default="(BotSpeakingFrame, LLMFullResponseEndFrame)"
>
  Frame types that should prevent the pipeline from being considered idle.
</ParamField>

<ParamField path="cancel_on_idle_timeout" type="bool" default="True">
  Whether to automatically cancel the pipeline task when idle timeout is
  reached.
</ParamField>

## Handling Idle Timeouts

You can respond to idle timeout events by adding an event handler:

```python
@task.event_handler("on_idle_timeout")
async def on_idle_timeout(task):
    logger.info("Pipeline has been idle for too long")
    # Perform any custom cleanup or logging
    # Note: If cancel_on_idle_timeout=True, the pipeline will be cancelled after this handler runs
```

## Example Implementation

Here's a complete example showing how to configure idle detection with custom handling:

```python
from pipecat.frames.frames import BotSpeakingFrame, LLMFullResponseEndFrame, TTSSpeakFrame
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask

# Create pipeline
pipeline = Pipeline([...])

# Configure task with custom idle settings
task = PipelineTask(
    pipeline,
    params=PipelineParams(allow_interruptions=True),
    idle_timeout_secs=180,  # 3 minutes
    cancel_on_idle_timeout=False  # Don't auto-cancel
)

# Add event handler for idle timeout
@task.event_handler("on_idle_timeout")
async def on_idle_timeout(task):
    logger.info("Conversation has been idle for 3 minutes")

    # Add a farewell message
    await task.queue_frame(TTSSpeakFrame("I haven't heard from you in a while. Goodbye!"))

    # Then end the conversation gracefully
    await task.stop_when_done()

runner = PipelineRunner()

await runner.run(task)
```



================================================
FILE: server/pipeline/pipeline-params.mdx
================================================
---
title: "PipelineParams"
description: "Configure pipeline execution with PipelineParams"
---

## Overview

The `PipelineParams` class provides a structured way to configure various aspects of pipeline execution. These parameters control behaviors like audio settings, metrics collection, heartbeat monitoring, and interruption handling.

## Basic Usage

```python
from pipecat.pipeline.task import PipelineParams, PipelineTask

# Create with default parameters
params = PipelineParams()

# Or customize specific parameters
params = PipelineParams(
    allow_interruptions=True,
    audio_in_sample_rate=16000,
    enable_metrics=True
)

# Pass to PipelineTask
pipeline = Pipeline([...])
task = PipelineTask(pipeline, params=params)
```

## Available Parameters

<ParamField path="allow_interruptions" type="bool" default="False">
  Whether to allow pipeline interruptions. When enabled, a user's speech will
  immediately interrupt the bot's response.
</ParamField>

<ParamField path="audio_in_sample_rate" type="int" default="16000">
  Input audio sample rate in Hz.

<Info>
  Setting the `audio_in_sample_rate` as a `PipelineParam` sets the input sample
  rate for all corresponding services in the pipeline.
</Info>

</ParamField>

<ParamField path="audio_out_sample_rate" type="int" default="24000">
  Output audio sample rate in Hz.

<Info>
  Setting the `audio_out_sample_rate` as a `PipelineParam` sets the output
  sample rate for all corresponding services in the pipeline.
</Info>

</ParamField>

<ParamField path="enable_heartbeats" type="bool" default="False">
  Whether to enable heartbeat monitoring to detect pipeline stalls. See
  [Heartbeats](/server/pipeline/heartbeats) for details.
</ParamField>

<ParamField path="heartbeats_period_secs" type="float" default="1.0">
  Period between heartbeats in seconds (when heartbeats are enabled).
</ParamField>

<ParamField path="enable_metrics" type="bool" default="False">
  Whether to enable metrics collection for pipeline performance.
</ParamField>

<ParamField path="enable_usage_metrics" type="bool" default="False">
  Whether to enable usage metrics tracking.
</ParamField>

<ParamField path="report_only_initial_ttfb" type="bool" default="False">
  Whether to report only initial time to first byte metric.
</ParamField>

<ParamField path="send_initial_empty_metrics" type="bool" default="True">
  Whether to send initial empty metrics frame at pipeline start.
</ParamField>

<ParamField path="start_metadata" type="Dict[str, Any]" default="{}">
  Additional metadata to include in the StartFrame.
</ParamField>

## Common Configurations

### Audio Processing Configuration

You can set the audio input and output sample rates in the `PipelineParams` to set the sample rate for all input and output services in the pipeline. This acts as a convenience to avoid setting the sample rate for each service individually. Note, if services are set individually, they will supersede the values set in `PipelineParams`.

```python
params = PipelineParams(
    audio_in_sample_rate=8000,   # Lower quality input audio
    audio_out_sample_rate=8000  # High quality output audio
)
```

### Performance Monitoring Configuration

Pipeline heartbeats provide a way to monitor the health of your pipeline by sending periodic heartbeat frames through the system. When enabled, the pipeline will send heartbeat frames every second and monitor their progress through the pipeline.

```python
params = PipelineParams(
    enable_heartbeats=True,
    heartbeats_period_secs=2.0,  # Send heartbeats every 2 seconds
    enable_metrics=True
)
```

## How Parameters Are Used

The parameters you set in `PipelineParams` are passed to various components of the pipeline:

1. **StartFrame**: Many parameters are included in the StartFrame that initializes the pipeline
2. **Metrics Collection**: Metrics settings configure what performance data is gathered
3. **Heartbeat Monitoring**: Controls the pipeline's health monitoring system
4. **Audio Processing**: Sample rates affect how audio is processed throughout the pipeline

## Complete Example

```python
from pipecat.frames.frames import TTSSpeakFrame
from pipecat.observers.file_observer import FileObserver
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.pipeline.runner import PipelineRunner

# Create comprehensive parameters
params = PipelineParams(
    allow_interruptions=True,
    audio_in_sample_rate=8000,
    audio_out_sample_rate=8000,
    enable_heartbeats=True,
    enable_metrics=True,
    enable_usage_metrics=True,
    heartbeats_period_secs=1.0,
    report_only_initial_ttfb=False,
    start_metadata={
        "conversation_id": "conv-123",
        "session_data": {
            "user_id": "user-456",
            "start_time": "2023-10-25T14:30:00Z"
        }
    }
)

# Create pipeline and task
pipeline = Pipeline([...])
task = PipelineTask(
    pipeline,
    params=params,
    observers=[FileObserver("pipeline_logs.jsonl")]
)

# Run the pipeline
runner = PipelineRunner()
await runner.run(task)
```

## Additional Information

- Parameters are immutable once the pipeline starts
- The `start_metadata` dictionary can contain any serializable data
- For metrics collection to work properly, `enable_metrics` must be set to `True`



================================================
FILE: server/pipeline/pipeline-task.mdx
================================================
---
title: "PipelineTask"
description: "Manage pipeline execution and lifecycle with PipelineTask"
---

## Overview

`PipelineTask` is the central class for managing pipeline execution. It handles the lifecycle of the pipeline, processes frames in both directions, manages task cancellation, and provides event handlers for monitoring pipeline activity.

## Basic Usage

```python
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask

# Create a pipeline
pipeline = Pipeline([...])

# Create a task with the pipeline
task = PipelineTask(pipeline)

# Queue frames for processing
await task.queue_frame(TTSSpeakFrame("Hello, how can I help you today?"))

# Run the pipeline
runner = PipelineRunner()
await runner.run(task)
```

## Constructor Parameters

<ParamField path="pipeline" type="BasePipeline" required>
  The pipeline to execute.
</ParamField>

<ParamField path="params" type="PipelineParams" default="PipelineParams()">
  Configuration parameters for the pipeline. See
  [PipelineParams](/server/pipeline/pipeline-params) for details.
</ParamField>

<ParamField path="observers" type="List[BaseObserver]" default="[]">
  List of observers for monitoring pipeline execution. See
  [Observers](/server/utilities/observers/observer-pattern) for details.
</ParamField>

<ParamField path="clock" type="BaseClock" default="SystemClock()">
  Clock implementation for timing operations.
</ParamField>

<ParamField path="task_manager" type="Optional[BaseTaskManager]" default="None">
  Custom task manager for handling asyncio tasks. If None, a default TaskManager
  is used.
</ParamField>

<ParamField path="check_dangling_tasks" type="bool" default="True">
  Whether to check for processors' tasks finishing properly.
</ParamField>

<ParamField path="idle_timeout_secs" type="Optional[float]" default="300">
  Timeout in seconds before considering the pipeline idle. Set to None to
  disable idle detection. See [Pipeline Idle
  Detection](/server/pipeline/pipeline-idle-detection) for details.
</ParamField>

<ParamField
  path="idle_timeout_frames"
  type="Tuple[Type[Frame], ...]"
  default="(BotSpeakingFrame, LLMFullResponseEndFrame)"
>
  Frame types that should prevent the pipeline from being considered idle. See
  [Pipeline Idle Detection](/server/pipeline/pipeline-idle-detection) for
  details.
</ParamField>

<ParamField path="cancel_on_idle_timeout" type="bool" default="True">
  Whether to automatically cancel the pipeline task when idle timeout is
  reached. See [Pipeline Idle
  Detection](/server/pipeline/pipeline-idle-detection) for details.
</ParamField>

<ParamField path="enable_tracing" type="bool" default="False">
  Whether to enable OpenTelemetry tracing. See [The OpenTelemetry guide](/server/utilities/opentelemetry) for details.
</ParamField>

<ParamField path="enable_turn_tracking" type="bool" default="False">
  Whether to enable turn tracking. See [The OpenTelemetry guide](/server/utilities/opentelemetry) for details.
</ParamField>

<ParamField path="conversation_id" type="Optional[str]" default="None">
  Custom ID for the conversation. If not provided, a UUID will be generated. See [The OpenTelemetry guide](/server/utilities/opentelemetry) for details.
</ParamField>

<ParamField path="additional_span_attributes" type="Optional[dict]" default="None">
  Any additional attributes to add to top-level OpenTelemetry conversation span. See [The OpenTelemetry guide](/server/utilities/opentelemetry) for details.
</ParamField>

## Methods

### Task Lifecycle Management

<ResponseField name="run()" type="async">
Starts and manages the pipeline execution until completion or cancellation.

```python
await task.run()
```

</ResponseField>

<ResponseField name="stop_when_done()" type="async">
Sends an EndFrame to the pipeline to gracefully stop the task after all queued
frames have been processed.

```python
await task.stop_when_done()
```

</ResponseField>

<ResponseField name="cancel()" type="async">
Stops the running pipeline immediately by sending a CancelFrame.

```python
  await task.cancel()
```

</ResponseField>

<ResponseField name="has_finished()" type="bool">
Returns whether the task has finished (all processors have stopped).

```python
if task.has_finished(): print("Task is complete")
```

</ResponseField>

### Frame Management

<ResponseField name="queue_frame()" type="async">
Queues a single frame to be pushed down the pipeline.

```python
await task.queue_frame(TTSSpeakFrame("Hello!"))
```

</ResponseField>

<ResponseField name="queue_frames()" type="async">

Queues multiple frames to be pushed down the pipeline.

```python
frames = [TTSSpeakFrame("Hello!"), TTSSpeakFrame("How are you?")]

await task.queue_frames(frames)

```

</ResponseField>

## Event Handlers

PipelineTask provides an event handler that can be registered using the `event_handler` decorator:

### on_idle_timeout

Triggered when no activity frames (as specified by `idle_timeout_frames`) have been received within the idle timeout period.

```python
@task.event_handler("on_idle_timeout")
async def on_idle_timeout(task):
    print("Pipeline has been idle too long")
    await task.queue_frame(TTSSpeakFrame("Are you still there?"))
```



================================================
FILE: server/services/supported-services.mdx
================================================
---
title: "Supported Services"
description: "AI services integrated with Pipecat and their setup requirements"
---

## Transports

Transports exchange audio and video streams between the user and bot.

| Service                                                           | Setup                                 |
| ----------------------------------------------------------------- | ------------------------------------- |
| [Daily](/server/services/transport/daily)                         | `pip install "pipecat-ai[daily]"`     |
| [FastAPI WebSocket](/server/services/transport/fastapi-websocket) | `pip install "pipecat-ai[websocket]"` |
| [LiveKit](/server/services/transport/livekit)                     | `pip install "pipecat-ai[livekit]"`   |
| [SmallWebRTCTransport](/server/services/transport/small-webrtc)   | `pip install "pipecat-ai[webrtc]"`    |
| [WebSocket Server](/server/services/transport/websocket-server)   | `pip install "pipecat-ai[websocket]"` |
| [TavusTransport](/server/services/transport/tavus)                | `pip install "pipecat-ai[tavus]"`     |

## Serializers

Serializers convert between frames and media streams, enabling real-time communication over a websocket.

| Service                                       | Setup                    |
| --------------------------------------------- | ------------------------ |
| [Exotel](/server/services/serializers/exotel) | No dependencies required |
| [Plivo](/server/services/serializers/plivo)   | No dependencies required |
| [Telnyx](/server/services/serializers/telnyx) | No dependencies required |
| [Twilio](/server/services/serializers/twilio) | No dependencies required |

## Speech-to-Text

Speech-to-Text services receive and audio input and output transcriptions.

| Service                                               | Setup                                    |
| ----------------------------------------------------- | ---------------------------------------- |
| [AssemblyAI](/server/services/stt/assemblyai)         | `pip install "pipecat-ai[assemblyai]"`   |
| [AWS Transcribe](/server/services/stt/aws)            | `pip install "pipecat-ai[aws]"`          |
| [Azure](/server/services/stt/azure)                   | `pip install "pipecat-ai[azure]"`        |
| [Cartesia](/server/services/stt/cartesia)             | `pip install "pipecat-ai[cartesia]"`     |
| [Deepgram](/server/services/stt/deepgram)             | `pip install "pipecat-ai[deepgram]"`     |
| [Fal Wizper](/server/services/stt/fal)                | `pip install "pipecat-ai[fal]"`          |
| [Gladia](/server/services/stt/gladia)                 | `pip install "pipecat-ai[gladia]"`       |
| [Google](/server/services/stt/google)                 | `pip install "pipecat-ai[google]"`       |
| [Groq (Whisper)](/server/services/stt/groq)           | `pip install "pipecat-ai[groq]"`         |
| [NVIDIA Riva](/server/services/stt/riva)              | `pip install "pipecat-ai[riva]"`         |
| [OpenAI (Whisper)](/server/services/stt/openai)       | `pip install "pipecat-ai[openai]"`       |
| [SambaNova (Whisper)](/server/services/stt/sambanova) | `pip install "pipecat-ai[sambanova]"`    |
| [Soniox](/server/services/stt/soniox)                 | `pip install "pipecat-ai[soniox]"`       |
| [Speechmatics](/server/services/stt/speechmatics)     | `pip install "pipecat-ai[speechmatics]"` |
| [Ultravox](/server/services/stt/ultravox)             | `pip install "pipecat-ai[ultravox]"`     |
| [Whisper](/server/services/stt/whisper)               | `pip install "pipecat-ai[whisper]"`      |

## Large Language Models

LLMs receive text or audio based input and output a streaming text response.

| Service                                                | Setup                                  |
| ------------------------------------------------------ | -------------------------------------- |
| [Anthropic](/server/services/llm/anthropic)            | `pip install "pipecat-ai[anthropic]"`  |
| [AWS Bedrock](/server/services/llm/aws)                | `pip install "pipecat-ai[aws]"`        |
| [Azure](/server/services/llm/azure)                    | `pip install "pipecat-ai[azure]"`      |
| [Cerebras](/server/services/llm/cerebras)              | `pip install "pipecat-ai[cerebras]"`   |
| [DeepSeek](/server/services/llm/deepseek)              | `pip install "pipecat-ai[deepseek]"`   |
| [Fireworks AI](/server/services/llm/fireworks)         | `pip install "pipecat-ai[fireworks]"`  |
| [Google Gemini](/server/services/llm/gemini)           | `pip install "pipecat-ai[google]"`     |
| [Google Vertex AI](/server/services/llm/google-vertex) | `pip install "pipecat-ai[google]"`     |
| [Grok](/server/services/llm/grok)                      | `pip install "pipecat-ai[grok]"`       |
| [Groq](/server/services/llm/groq)                      | `pip install "pipecat-ai[groq]"`       |
| [NVIDIA NIM](/server/services/llm/nim)                 | `pip install "pipecat-ai[nim]"`        |
| [Ollama](/server/services/llm/ollama)                  | `pip install "pipecat-ai[ollama]"`     |
| [OpenAI](/server/services/llm/openai)                  | `pip install "pipecat-ai[openai]"`     |
| [OpenPipe](/server/services/llm/openpipe)              | `pip install "pipecat-ai[openpipe]"`   |
| [OpenRouter](/server/services/llm/openrouter)          | `pip install "pipecat-ai[openrouter]"` |
| [Perplexity](/server/services/llm/perplexity)          | `pip install "pipecat-ai[perplexity]"` |
| [Qwen](/server/services/llm/qwen)                      | `pip install "pipecat-ai[qwen]"`       |
| [SambaNova](/server/services/llm/sambanova)            | `pip install "pipecat-ai[sambanova]"`  |
| [Together AI](/server/services/llm/together)           | `pip install "pipecat-ai[together]"`   |

## Text-to-Speech

Text-to-Speech services receive text input and output audio streams or chunks.

| Service                                       | Setup                                  |
| --------------------------------------------- | -------------------------------------- |
| [Async](/server/services/tts/asyncai)         | `pip install "pipecat-ai[asyncai]"`    |
| [AWS Polly](/server/services/tts/aws)         | `pip install "pipecat-ai[aws]"`        |
| [Azure](/server/services/tts/azure)           | `pip install "pipecat-ai[azure]"`      |
| [Cartesia](/server/services/tts/cartesia)     | `pip install "pipecat-ai[cartesia]"`   |
| [Deepgram](/server/services/tts/deepgram)     | `pip install "pipecat-ai[deepgram]"`   |
| [ElevenLabs](/server/services/tts/elevenlabs) | `pip install "pipecat-ai[elevenlabs]"` |
| [Fish](/server/services/tts/fish)             | `pip install "pipecat-ai[fish]"`       |
| [Google](/server/services/tts/google)         | `pip install "pipecat-ai[google]"`     |
| [Groq](/server/services/tts/groq)             | `pip install "pipecat-ai[groq]"`       |
| [Inworld](/server/services/tts/inworld)       | No dependencies required               |
| [LMNT](/server/services/tts/lmnt)             | `pip install "pipecat-ai[lmnt]"`       |
| [MiniMax](/server/services/tts/minimax)       | No dependencies required               |
| [Neuphonic](/server/services/tts/neuphonic)   | `pip install "pipecat-ai[neuphonic]"`  |
| [NVIDIA Riva](/server/services/tts/riva)      | `pip install "pipecat-ai[riva]"`       |
| [OpenAI](/server/services/tts/openai)         | `pip install "pipecat-ai[openai]"`     |
| [Piper](/server/services/tts/piper)           | No dependencies required               |
| [PlayHT](/server/services/tts/playht)         | `pip install "pipecat-ai[playht]"`     |
| [Rime](/server/services/tts/rime)             | `pip install "pipecat-ai[rime]"`       |
| [Sarvam](/server/services/tts/sarvam)         | No dependencies required               |
| [XTTS](/server/services/tts/xtts)             | `pip install "pipecat-ai[xtts]"`       |

## Speech-to-Speech

Speech-to-Speech services are multi-modal LLM services that take in audio, video, or text and output audio or text.

| Service                                               | Setup                                      |
| ----------------------------------------------------- | ------------------------------------------ |
| [AWS Nova Sonic](/server/services/s2s/aws)            | `pip install "pipecat-ai[aws-nova-sonic]"` |
| [Gemini Multimodal Live](/server/services/s2s/gemini) | `pip install "pipecat-ai[google]"`         |
| [OpenAI Realtime](/server/services/s2s/openai)        | `pip install "pipecat-ai[openai]"`         |

## Image Generation

Image generation services receive text inputs and output images.

| Service                                                          | Setup                              |
| ---------------------------------------------------------------- | ---------------------------------- |
| [fal](/server/services/image-generation/fal)                     | `pip install "pipecat-ai[fal]"`    |
| [Google Imagen](/server/services/image-generation/google-imagen) | `pip install "pipecat-ai[google]"` |
| [OpenAI](/server/services/image-generation/openai)               | `pip install "pipecat-ai[openai]"` |

## Video

Video services enable you to build an avatar where audio and video are synchronized.

| Service                               | Setup                             |
| ------------------------------------- | --------------------------------- |
| [Simli](/server/services/video/simli) | `pip install "pipecat-ai[simli]"` |
| [Tavus](/server/services/video/tavus) | `pip install "pipecat-ai[tavus]"` |

## Memory

Memory services can be used to store and retrieve conversations.

| Service                              | Setup                            |
| ------------------------------------ | -------------------------------- |
| [mem0](/server/services/memory/mem0) | `pip install "pipecat-ai[mem0]"` |

## Vision

Vision services receive a streaming video input and output text describing the video input.

| Service                                        | Setup                                 |
| ---------------------------------------------- | ------------------------------------- |
| [Moondream](/server/services/vision/moondream) | `pip install "pipecat-ai[moondream]"` |

## Analytics & Monitoring

Analytics services help you better understand how your service operates.

| Service                                     | Setup                              |
| ------------------------------------------- | ---------------------------------- |
| [Sentry](/server/services/analytics/sentry) | `pip install "pipecat-ai[sentry]"` |



================================================
FILE: server/services/analytics/sentry.mdx
================================================
---
title: "Sentry Metrics"
description: "Performance monitoring integration with Sentry for Pipecat frame processors"
---

## Overview

`SentryMetrics` extends `FrameProcessorMetrics` to provide performance monitoring integration with Sentry. It tracks Time to First Byte (TTFB) and processing duration metrics for frame processors.

## Installation

To use Sentry metrics, install the Sentry SDK:

```bash
pip install "pipecat-ai[sentry]"
```

## Configuration

Sentry must be initialized in your application before metrics will be collected:

```python
import sentry_sdk

sentry_sdk.init(
    dsn="your-sentry-dsn",
    traces_sample_rate=1.0,
)
```

## Usage Example

```python
import sentry_sdk

from pipecat.services.openai.llm import OpenAILLMService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.processors.metrics.sentry import SentryMetrics
from pipecat.transports.services.daily import DailyParams, DailyTransport

async def create_metrics_pipeline():
    sentry_sdk.init(
        dsn="your-sentry-dsn",
        traces_sample_rate=1.0,
    )

    transport = DailyTransport(
        room_url,
        token,
        "Chatbot",
        DailyParams(
            audio_out_enabled=True,
            audio_in_enabled=True,
            video_out_enabled=False,
            vad_analyzer=SileroVADAnalyzer(),
            transcription_enabled=True,
        ),
    )

    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"),
        metrics = SentryMetrics(),
    )

    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o"),
        metrics = SentryMetrics(),
    )

    messages = [
        {
            "role": "system",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself. Keep all your responses to 12 words or fewer.",
        },
    ]

    context = OpenAILLMContext(messages)
    context_aggregator = llm.create_context_aggregator(context)

    # Use in pipeline
    pipeline = Pipeline([
        transport.input(),
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ])
```

## Transaction Information

Each transaction includes:

- Operation type (`ttfb` or `processing`)
- Description with processor name
- Start timestamp
- End timestamp
- Unique transaction ID

## Fallback Behavior

If Sentry is not available (not installed or not initialized):

- Warning logs are generated
- Metric methods execute without error
- No data is sent to Sentry

## Notes

- Requires Sentry SDK to be installed and initialized
- Thread-safe metric collection
- Automatic transaction management
- Supports selective TTFB reporting
- Integrates with Sentry's performance monitoring
- Provides detailed timing information
- Maintains timing data even when Sentry is unavailable



================================================
FILE: server/services/image-generation/fal.mdx
================================================
---
title: "fal"
description: "Image generation service implementation using fal’s fast SDXL models"
---

## Overview

`FalImageGenService` provides high-speed image generation capabilities using fal's optimized Stable Diffusion XL models. It supports various image sizes, formats, and generation parameters with a focus on fast inference.

## Installation

To use `FalImageGenService`, install the required dependencies:

```bash
pip install "pipecat-ai[fal]"
```

You'll also need to set up your Fal API key as an environment variable: `FAL_KEY`

<Tip>You can obtain a fal API key by signing up at [fal](https://fal.ai/).</Tip>

## Configuration

### Constructor Parameters

<ParamField path="params" type="InputParams" required>
  Generation parameters configuration
</ParamField>

<ParamField path="aiohttp_session" type="aiohttp.ClientSession" required>
  HTTP session for image downloading
</ParamField>

<ParamField path="model" type="str" default="fal-ai/fast-sdxl">
  Model identifier
</ParamField>

<ParamField path="key" type="str" optional>
  Fal API key (alternative to environment variable)
</ParamField>

### Input Parameters

```python
class InputParams(BaseModel):
    seed: Optional[int] = None              # Random seed for reproducibility
    num_inference_steps: int = 8            # Number of denoising steps
    num_images: int = 1                     # Number of images to generate
    image_size: Union[str, Dict[str, int]] = "square_hd"  # Image dimensions
    expand_prompt: bool = False             # Enhance prompt automatically
    enable_safety_checker: bool = True      # Filter unsafe content
    format: str = "png"                     # Output image format
```

#### Supported Image Sizes

Possible enum values: `square_hd`, `square`, `portrait_4_3`, `portrait_16_9`, `landscape_4_3`, `landscape_16_9`

Note: For custom image sizes, you can pass the width and height as an object:

```json
{
  "image_size": {
    "width": 1280,
    "height": 720
  }
}
```

See the [fal docs](https://fal.ai/models/fal-ai/fast-lightning-sdxl/api) for more information.

## Output Frames

### URLImageRawFrame

<ParamField path="url" type="string">
  Generated image URL
</ParamField>

<ParamField path="image" type="bytes">
  Raw image data
</ParamField>

<ParamField path="size" type="tuple">
  Image dimensions (width, height)
</ParamField>

<ParamField path="format" type="string">
  Image format (e.g., 'PNG')
</ParamField>

### ErrorFrame

<ParamField path="error" type="string">
  Error information if generation fails
</ParamField>

## Methods

See the [Image Generation base class methods](/server/base-classes/media#methods) for additional functionality.

## Usage Example

```python
import aiohttp
from pipecat.services.fal.image import FalImageGenService

# Configure service
async with aiohttp.ClientSession() as session:
    service = FalImageGenService(
        model="fal-ai/fast-sdxl",
        aiohttp_session=session,
        params=FalImageGenService.InputParams(
            num_inference_steps=8,
            image_size="portrait_hd",
            expand_prompt=True
        )
    )

    # Use in pipeline
    pipeline = Pipeline([
        prompt_input,     # Produces text prompts
        service,          # Generates images
        image_handler     # Handles generated images
    ])
```

## Frame Flow

```mermaid
graph TD
    A[TextFrame] --> B[FalImageGenService]
    B --> C[URLImageRawFrame]
    B --> D[ErrorFrame]
```

## Metrics Support

The service collects processing metrics:

- Generation time
- Download time
- API response time
- Total processing duration

## Notes

- Fast inference times with optimized models
- Supports various image sizes and formats
- Automatic prompt enhancement option
- Built-in safety filtering
- Asynchronous operation
- Efficient HTTP session management
- Comprehensive error handling



================================================
FILE: server/services/image-generation/google-imagen.mdx
================================================
---
title: "Google Imagen"
description: "Image generation service implementation using Google’s Imagen models"
---

## Overview

`GoogleImageGenService` provides high-quality image generation capabilities using Google's Imagen models. It supports generating multiple images from text prompts with various customization options.

## Installation

To use `GoogleImageGenService`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You'll also need to set up your Google API key as an environment variable: `GOOGLE_API_KEY`

## Configuration

### Constructor Parameters

<ParamField path="params" type="InputParams" default="InputParams()">
  Generation parameters configuration
</ParamField>

<ParamField path="api_key" type="str" required>
  Google API key for authentication
</ParamField>

### Input Parameters

<ParamField path="number_of_images" type="int" default="1">
  Number of images to generate (1-8)
</ParamField>

<ParamField path="model" type="str" default="imagen-3.0-generate-002">
  Model identifier
</ParamField>

<ParamField path="negative_prompt" type="str" default="None">
  Elements to exclude from generation
</ParamField>

## Input

The service accepts text prompts through its image generation pipeline.

## Output Frames

### URLImageRawFrame

<ParamField path="url" type="string">
  Generated image URL (null for Google implementation as it returns raw bytes)
</ParamField>

<ParamField path="image" type="bytes">
  Raw image data
</ParamField>

<ParamField path="size" type="tuple">
  Image dimensions (width, height)
</ParamField>

<ParamField path="format" type="string">
  Image format (e.g., 'JPEG')
</ParamField>

### ErrorFrame

<ParamField path="error" type="string">
  Error information if generation fails
</ParamField>

## Usage Example

```python
from pipecat.services.google.image import GoogleImageGenService

# Configure service
image_gen = GoogleImageGenService(
    api_key="your-google-api-key",
    params=GoogleImageGenService.InputParams(
        number_of_images=2,
        model="imagen-3.0-generate-002",
        negative_prompt="blurry, distorted, low quality"
    )
)

# Use in pipeline
main_pipeline = Pipeline(
    [
        transport.input(),
        context_aggregator.user(),
        llm_service,
        image_gen,
        tts_service,
        transport.output(),
        context_aggregator.assistant(),
    ]
)
```

## Frame Flow

```mermaid
graph TD
    A[TextFrame] --> B[GoogleImageGenService]
    B --> C[URLImageRawFrame]
    B --> D[ErrorFrame]
```

## Metrics Support

The service supports metrics collection:

- Time to First Byte (TTFB)
- Processing duration
- API response metrics

## Model Support

Google's Imagen service offers different model variants:

| Model ID                | Description                                   |
| ----------------------- | --------------------------------------------- |
| imagen-3.0-generate-002 | Latest Imagen model with high-quality outputs |

See other available models in [Google's Imagen documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api).

## Error Handling

```python
try:
    async for frame in service.run_image_gen(prompt):
        if isinstance(frame, ErrorFrame):
            handle_error(frame.error)
except Exception as e:
    logger.error(f"Image generation error: {e}")
```



================================================
FILE: server/services/image-generation/openai.mdx
================================================
---
title: "OpenAI Image Generation"
sidebarTitle: "OpenAI"
description: "Image generation service implementation using OpenAI’s DALL-E models"
---

## Overview

`OpenAIImageGenService` provides high-quality image generation capabilities using OpenAI's DALL-E models. It transforms text prompts into images with various size options and model configurations.

## Installation

No additional installation is required for the `OpenAIImageGenService` as it is part of the Pipecat AI package.

You'll also need an OpenAI API key for authentication.

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  OpenAI API key for authentication
</ParamField>

<ParamField path="base_url" type="str" default="None">
  Optional base URL for OpenAI API requests
</ParamField>

<ParamField path="aiohttp_session" type="aiohttp.ClientSession" required>
  HTTP session for making requests
</ParamField>

<ParamField path="image_size" type="str" required>
  Image dimensions - one of "256x256", "512x512", "1024x1024", "1792x1024",
  "1024x1792"
</ParamField>

<ParamField path="model" type="str" default="dall-e-3">
  OpenAI model identifier for image generation
</ParamField>

## Input

The service accepts text prompts through its image generation pipeline.

## Output Frames

### URLImageRawFrame

<ParamField path="url" type="string">
  Generated image URL from OpenAI
</ParamField>

<ParamField path="image" type="bytes">
  Raw image data
</ParamField>

<ParamField path="size" type="tuple">
  Image dimensions (width, height)
</ParamField>

<ParamField path="format" type="string">
  Image format (e.g., 'JPEG')
</ParamField>

### ErrorFrame

<ParamField path="error" type="string">
  Error information if generation fails
</ParamField>

## Usage Example

```python
import aiohttp
from pipecat.pipeline.pipeline import Pipeline
from pipecat.services.openai.image import OpenAIImageGenService

# Create an aiohttp session
aiohttp_session = aiohttp.ClientSession()

# Configure service
image_gen = OpenAIImageGenService(
    api_key="your-openai-api-key",
    aiohttp_session=aiohttp_session,
    image_size="1024x1024",
    model="dall-e-3"
)

# Use in pipeline
main_pipeline = Pipeline(
    [
        transport.input(),
        context_aggregator.user(),
        llm_service,
        image_gen,
        tts_service,
        transport.output(),
        context_aggregator.assistant(),
    ]
)
```

## Frame Flow

```mermaid
graph TD
    A[TextFrame] --> B[OpenAIImageGenService]
    B --> C[URLImageRawFrame]
    B --> D[ErrorFrame]
```

## Metrics Support

The service supports metrics collection:

- Time to First Byte (TTFB)
- Processing duration
- API response metrics

## Model Support

OpenAI's image generation service offers different model variants:

| Model ID | Description                                                         |
| -------- | ------------------------------------------------------------------- |
| dall-e-3 | Latest DALL-E model with higher quality and better prompt following |
| dall-e-2 | Previous generation model with good quality and lower cost          |

## Image Size Options

| Size Option | Aspect Ratio | Description                      |
| ----------- | ------------ | -------------------------------- |
| 256x256     | 1:1          | Small square image               |
| 512x512     | 1:1          | Medium square image              |
| 1024x1024   | 1:1          | Large square image               |
| 1792x1024   | 16:9         | Horizontal/landscape orientation |
| 1024x1792   | 9:16         | Vertical/portrait orientation    |

## Error Handling

```python
try:
    async for frame in image_gen.run_image_gen(prompt):
        if isinstance(frame, ErrorFrame):
            logger.error(f"Image generation error: {frame.error}")
        else:
            # Process successful image generation
            pass
except Exception as e:
    logger.error(f"Unexpected error during image generation: {e}")
```



================================================
FILE: server/services/llm/anthropic.mdx
================================================
---
title: "Anthropic"
description: "Large Language Model service implementation using Anthropic’s Claude API"
---

## Overview

`AnthropicLLMService` provides integration with Anthropic's Claude models, supporting streaming responses, function calling, and prompt caching with specialized context handling for Anthropic's message format.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://pipecat-docs--2067.org.readthedocs.build/en/2067/api/pipecat.services.anthropic.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Anthropic Docs"
    icon="book"
    href="https://docs.anthropic.com/en/api/messages"
  >
    Official Anthropic API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14a-function-calling-anthropic.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Anthropic services, install the required dependency:

```bash
pip install "pipecat-ai[anthropic]"
```

You'll also need to set up your Anthropic API key as an environment variable: `ANTHROPIC_API_KEY`.

<Tip>
  Get your API key from [Anthropic Console](https://console.anthropic.com/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates
- `LLMEnablePromptCachingFrame` - Toggle prompt caching

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.anthropic.llm import AnthropicLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure the service
llm = AnthropicLLMService(
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    model="claude-sonnet-4-20250514",
    params=AnthropicLLMService.InputParams(
        temperature=0.7,
        enable_prompt_caching_beta=True
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        }
    },
    required=["location"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context with system message
context = OpenAILLMContext(
    messages=[{"role": "user", "content": "What's the weather like?"}],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler
async def get_weather(params):
    location = params.arguments["location"]
    await params.result_callback(f"Weather in {location}: 72°F and sunny")

llm.register_function("get_weather", get_weather)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),    # Handles user messages
    llm,                          # Processes with Anthropic
    tts,
    transport.output(),
    context_aggregator.assistant() # Captures responses
])
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from request to first response token
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and total usage
- **Cache Metrics** - Cache creation and read token usage

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Streaming Responses**: All responses are streamed for low latency
- **Context Persistence**: Use context aggregators to maintain conversation history
- **Error Handling**: Automatic retry logic for rate limits and transient errors
- **Message Format**: Automatically converts between OpenAI and Anthropic message formats
- **Prompt Caching**: Reduces costs and latency for repeated context patterns



================================================
FILE: server/services/llm/aws.mdx
================================================
---
title: "AWS Bedrock"
description: "Large Language Model service implementation using Amazon Bedrock API"
---

## Overview

AWS Bedrock LLM service provides access to Amazon's foundation models including Anthropic Claude and Amazon Nova, with streaming responses, function calling, and multimodal capabilities through Amazon's managed AI service.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.aws.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="AWS Bedrock Docs"
    icon="book"
    href="https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html"
  >
    Official AWS Bedrock documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14r-function-calling-aws.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use AWS Bedrock services, install the required dependencies:

```bash
pip install "pipecat-ai[aws]"
```

You'll also need to set up your AWS credentials as environment variables:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (if using temporary credentials)
- `AWS_REGION` (defaults to "us-east-1")

<Tip>
  Set up an IAM user with Amazon Bedrock access in your AWS account to obtain
  credentials.
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.aws.llm import AWSBedrockLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure the service
llm = AWSBedrockLLMService(
    aws_region="us-west-2",
    model="us.anthropic.claude-3-5-haiku-20241022-v1:0",
    params=AWSBedrockLLMService.InputParams(
        temperature=0.7,
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Register function handler
async def get_current_weather(params):
    location = params.arguments["location"]
    format_type = params.arguments["format"]
    result = {"conditions": "sunny", "temperature": "75", "unit": format_type}
    await params.result_callback(result)

llm.register_function("get_current_weather", get_current_weather)

# Create context with system message
messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant with access to weather information."
    }
]

context = OpenAILLMContext(messages, tools)
context_aggregator = llm.create_context_aggregator(context)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),    # Handles user messages
    llm,                          # Processes with AWS Bedrock
    tts,
    transport.output(),
    context_aggregator.assistant() # Captures responses
])
```

## Metrics

The service provides comprehensive AWS Bedrock metrics:

- **Time to First Byte (TTFB)** - Latency from request to first response token
- **Processing Duration** - Total request processing time
- **Token Usage** - Input tokens, output tokens, and total usage

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Streaming Responses**: All responses are streamed for low latency
- **Context Persistence**: Use context aggregators to maintain conversation history
- **Error Handling**: Automatic retry logic for rate limits and transient errors
- **Message Format**: Automatically converts between OpenAI and AWS Bedrock message formats
- **Performance Modes**: Choose "standard" or "optimized" latency based on your needs
- **Regional Availability**: Different models available in different AWS regions
- **Vision Support**: Image processing available with compatible models like Claude 3



================================================
FILE: server/services/llm/azure.mdx
================================================
---
title: "Azure"
description: "Large Language Model service implementation using Azure OpenAI API"
---

## Overview

`AzureLLMService` provides access to Azure OpenAI's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Azure OpenAI Docs"
    icon="book"
    href="https://learn.microsoft.com/en-us/azure/ai-services/openai/"
  >
    Official Azure OpenAI documentation and setup
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14h-function-calling-azure.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Azure OpenAI services, install the required dependency:

```bash
pip install "pipecat-ai[azure]"
```

You'll need to set up your Azure OpenAI credentials:

- `AZURE_CHATGPT_API_KEY` - Your Azure OpenAI API key
- `AZURE_CHATGPT_ENDPOINT` - Your Azure OpenAI endpoint URL
- `AZURE_CHATGPT_MODEL` - Your model deployment name

<Tip>
  Get your credentials from the [Azure Portal](https://portal.azure.com/) under
  your Azure OpenAI resource.
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Azure vs OpenAI Differences

| Feature            | Azure OpenAI              | Standard OpenAI      |
| ------------------ | ------------------------- | -------------------- |
| **Authentication** | API key + endpoint        | API key only         |
| **Deployment**     | Custom deployment names   | Model names directly |
| **Compliance**     | Enterprise SOC, HIPAA     | Standard compliance  |
| **Regional**       | Multiple Azure regions    | OpenAI regions only  |
| **Pricing**        | Azure billing integration | OpenAI billing       |

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.azure.llm import AzureLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Azure OpenAI service
llm = AzureLLMService(
    api_key=os.getenv("AZURE_CHATGPT_API_KEY"),
    endpoint=os.getenv("AZURE_CHATGPT_ENDPOINT"),
    model=os.getenv("AZURE_CHATGPT_MODEL"),  # Your deployment name
    params=AzureLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant. Keep responses concise for voice output."
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with event callback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Regional Deployment**: Deploy in your preferred Azure region for compliance and latency
- **Deployment Names**: Use your Azure deployment name as the model parameter, not OpenAI model names
- **Automatic Retries**: Built-in retry logic handles transient Azure service issues



================================================
FILE: server/services/llm/cerebras.mdx
================================================
---
title: "Cerebras"
description: "LLM service implementation using Cerebras’s API with OpenAI-compatible interface"
---

## Overview

`CerebrasLLMService` provides access to Cerebras's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.cerebras.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Cerebras Docs"
    icon="book"
    href="https://inference-docs.cerebras.ai/api-reference/chat-completions"
  >
    Official Cerebras inference API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14k-function-calling-cerebras.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Cerebras services, install the required dependency:

```bash
pip install "pipecat-ai[cerebras]"
```

You'll also need to set up your Cerebras API key as an environment variable: `CEREBRAS_API_KEY`.

<Tip>Get your API key from [Cerebras Cloud](https://cloud.cerebras.ai/).</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.cerebras.llm import CerebrasLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure the service
llm = CerebrasLLMService(
    api_key=os.getenv("CEREBRAS_API_KEY"),
    model="llama-3.3-70b",
    params=CerebrasLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant for weather information. Keep responses concise for voice output."
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI-compatible metrics:

- **Time to First Byte (TTFB)** - Ultra-low latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API parameters and responses
- **Streaming Responses**: All responses are streamed for minimal latency
- **Function Calling**: Full support for OpenAI-style tool calling
- **Open Source Models**: Access to latest Llama models with commercial licensing



================================================
FILE: server/services/llm/deepseek.mdx
================================================
---
title: "DeepSeek"
description: "LLM service implementation using DeepSeek’s API with OpenAI-compatible interface"
---

## Overview

`DeepSeekLLMService` provides access to DeepSeek's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.deepseek.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="DeepSeek Docs"
    icon="book"
    href="https://api-docs.deepseek.com/api/create-chat-completion"
  >
    Official DeepSeek API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14l-function-calling-deepseek.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use DeepSeek services, install the required dependency:

```bash
pip install "pipecat-ai[deepseek]"
```

You'll also need to set up your DeepSeek API key as an environment variable: `DEEPSEEK_API_KEY`.

<Tip>
  Get your API key from [DeepSeek Platform](https://platform.deepseek.com/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.deepseek.llm import DeepSeekLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure DeepSeek service
llm = DeepSeekLLMService(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    model="deepseek-chat",
    params=DeepSeekLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context with reasoning-focused system message
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant with strong reasoning capabilities.
            Infer temperature units based on location unless specified.
            Provide logical, step-by-step responses."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Cost Efficiency**: Competitive pricing compared to other high-capability models
- **Streaming Support**: Real-time response streaming for low-latency applications



================================================
FILE: server/services/llm/fireworks.mdx
================================================
---
title: "Fireworks AI"
description: "LLM service implementation using Fireworks AI’s API with OpenAI-compatible interface"
---

## Overview

`FireworksLLMService` provides access to Fireworks AI's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.fireworks.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Fireworks Docs"
    icon="book"
    href="https://docs.fireworks.ai/api-reference/post-chatcompletions"
  >
    Official Fireworks AI API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14i-function-calling-fireworks.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Fireworks AI services, install the required dependency:

```bash
pip install "pipecat-ai[fireworks]"
```

You'll also need to set up your Fireworks API key as an environment variable: `FIREWORKS_API_KEY`.

<Tip>
  Get your API key from [Fireworks AI
  Console](https://fireworks.ai/account/api-keys).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.fireworks.llm import FireworksLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Fireworks service
llm = FireworksLLMService(
    api_key=os.getenv("FIREWORKS_API_KEY"),
    model="accounts/fireworks/models/firefunction-v2",  # Optimized for function calling
    params=FireworksLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant optimized for voice interactions.
            Keep responses concise and avoid special characters for audio output."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Function Calling**: Specialized firefunction models optimized for tool use
- **Cost Effective**: Competitive pricing for open-source model inference



================================================
FILE: server/services/llm/gemini.mdx
================================================
---
title: "Google Gemini"
description: "Large Language Model service implementation using Google’s Gemini API"
---

## Overview

`GoogleLLMService` provides integration with Google's Gemini models, supporting streaming responses, function calling, and multimodal inputs. It includes specialized context handling for Google's message format while maintaining compatibility with OpenAI-style contexts.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.google.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Gemini Docs"
    icon="book"
    href="https://ai.google.dev/gemini-api/docs"
  >
    Official Google Gemini API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14e-function-calling-google.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use `GoogleLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You'll also need to set up your Google API key as an environment variable: `GOOGLE_API_KEY`.

<Tip>
  Get your API key from [Google AI
  Studio](https://aistudio.google.com/app/apikey).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `LLMSearchResponseFrame` - Search grounding results with citations
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Search Grounding

Google Gemini's search grounding feature enables real-time web search integration, allowing the model to access current information and provide citations. This is particularly valuable for applications requiring up-to-date information.

### Enabling Search Grounding

```python
# Configure search grounding tool
search_tool = {
    "google_search_retrieval": {
        "dynamic_retrieval_config": {
            "mode": "MODE_DYNAMIC",
            "dynamic_threshold": 0.3,  # Lower = more frequent grounding
        }
    }
}

# Initialize with search grounding
llm = GoogleLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    model="gemini-1.5-flash-002",
    system_instruction="You are a helpful assistant with access to current information.",
    tools=[search_tool]
)
```

### Handling Search Results

Search grounding produces `LLMSearchResponseFrame` with detailed citation information:

```python
@pipeline.event_handler("llm_search_response")
async def handle_search_response(frame):
    print(f"Search result: {frame.search_result}")
    print(f"Sources: {len(frame.origins)} citations")
    for origin in frame.origins:
        print(f"- {origin['site_title']}: {origin['site_uri']}")
```

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.google.llm import GoogleLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Gemini service with search grounding
search_tool = {
    "google_search_retrieval": {
        "dynamic_retrieval_config": {
            "mode": "MODE_DYNAMIC",
            "dynamic_threshold": 0.3
        }
    }
}

llm = GoogleLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    model="gemini-2.0-flash",
    system_instruction="""You are a helpful assistant with access to current information.
    When users ask about recent events, use search to provide accurate, up-to-date information.""",
    tools=[search_tool],
    params=GoogleLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        }
    },
    required=["location"]
)

# Define image capture function for multimodal capabilities
image_function = FunctionSchema(
    name="get_image",
    description="Capture and analyze an image from the video stream",
    properties={
        "question": {
            "type": "string",
            "description": "Question about what to analyze in the image"
        }
    },
    required=["question"]
)

tools = ToolsSchema(standard_tools=[weather_function, image_function])

# Create context with multimodal system prompt
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant with access to current information and vision capabilities.
            You can answer questions about weather, analyze images from video streams, and search for current information.
            Keep responses concise for voice output."""
        },
        {"role": "user", "content": "Hello! What can you help me with?"}
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handlers
async def get_weather(params):
    location = params.arguments["location"]
    await params.result_callback(f"Weather in {location}: 72°F and sunny")

async def get_image(params):
    question = params.arguments["question"]
    # Request image from video stream
    await params.llm.request_image_frame(
        user_id=client_id,
        function_name=params.function_name,
        tool_call_id=params.tool_call_id,
        text_content=question
    )
    await params.result_callback(f"Analyzing image for: {question}")

llm.register_function("get_weather", get_weather)
llm.register_function("get_image", get_image)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Google Gemini provides comprehensive usage tracking:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Multimodal Capabilities**: Native support for text, images, audio, and video processing
- **Search Grounding**: Real-time web search with automatic citation and source attribution
- **System Instructions**: Handle system messages differently than OpenAI - set during initialization
- **Vision Functions**: Built-in support for image capture and analysis from video streams



================================================
FILE: server/services/llm/google-vertex.mdx
================================================
---
title: "Google Vertex AI"
description: "LLM service implementation using Google’s Vertex AI with OpenAI-compatible interface"
---

## Overview

`GoogleVertexLLMService` provides access to Google's language models through Vertex AI while maintaining an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports all the features of the OpenAI interface while connecting to Google's AI services.

## Installation

To use `GoogleVertexLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You'll also need to set up Google Cloud credentials. You can either:

- Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable pointing to your service account JSON file
- Provide credentials directly to the service constructor

## Configuration

### Constructor Parameters

<ParamField path="credentials" type="Optional[str]">
  JSON string of Google service account credentials
</ParamField>

<ParamField path="credentials_path" type="Optional[str]">
  Path to the Google service account JSON file
</ParamField>

<ParamField path="model" type="str" default="google/gemini-2.0-flash-001">
  Model identifier
</ParamField>

<ParamField path="params" type="InputParams">
  Vertex AI specific parameters
</ParamField>

### Input Parameters

Extends the OpenAI input parameters with Vertex AI specific options:

<ParamField path="location" type="str" default="us-east4">
  Google Cloud region where the model is deployed
</ParamField>

<ParamField path="project_id" type="str" required>
  Google Cloud project ID
</ParamField>

Also inherits all OpenAI-compatible parameters:

<ParamField path="frequency_penalty" type="Optional[float]">
  Reduces likelihood of repeating tokens based on their frequency. Range: [-2.0,
  2.0]
</ParamField>

<ParamField path="max_tokens" type="Optional[int]">
  Maximum number of tokens to generate. Must be greater than or equal to 1
</ParamField>

<ParamField path="presence_penalty" type="Optional[float]">
  Reduces likelihood of repeating any tokens that have appeared. Range: [-2.0,
  2.0]
</ParamField>

<ParamField path="temperature" type="Optional[float]">
  Controls randomness in the output. Range: [0.0, 2.0]
</ParamField>

<ParamField path="top_p" type="Optional[float]">
  Controls diversity via nucleus sampling. Range: [0.0, 1.0]
</ParamField>

## Usage Example

```python
from pipecat.services.google.llm_vertex import GoogleVertexLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineParams, PipelineTask

# Configure service
llm = GoogleVertexLLMService(
    credentials_path="/path/to/service-account.json",
    model="google/gemini-2.0-flash-001",
    params=GoogleVertexLLMService.InputParams(
        project_id="your-google-cloud-project-id",
        location="us-east4"
    )
)

# Create context with system message
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant in a voice conversation. Keep responses concise."
        }
    ]
)

# Create context aggregator for message handling
context_aggregator = llm.create_context_aggregator(context)

# Set up pipeline
pipeline = Pipeline([
    transport.input(),
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])

# Create and configure task
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
)
```

## Authentication

The service supports multiple authentication methods:

1. **Direct credentials string** - Pass the JSON credentials as a string to the constructor
2. **Credentials file path** - Provide a path to the service account JSON file
3. **Environment variable** - Set `GOOGLE_APPLICATION_CREDENTIALS` to the path of your service account file

The service automatically handles token refresh, with tokens having a 1-hour lifetime.

## Methods

See the [LLM base class methods](https://reference-server.pipecat.ai/en/latest/api/pipecat.services.llm_service.html#llm-service) for additional functionality.

## Function Calling

This service supports function calling (also known as tool calling) through the OpenAI-compatible interface, which allows the LLM to request information from external services and APIs.

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Available Models

| Model Name                    | Description                           |
| ----------------------------- | ------------------------------------- |
| `google/gemini-2.0-flash-001` | Fast, efficient text generation model |
| `google/gemini-2.0-pro-001`   | Comprehensive, high-quality model     |
| `google/gemini-1.5-pro-001`   | Versatile multimodal model            |
| `google/gemini-1.5-flash-001` | Fast, efficient multimodal model      |

<Note>
  See [Google Vertex AI
  documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/overview)
  for a complete list of supported models and their capabilities.
</Note>

## Frame Flow

Inherits the OpenAI LLM Service frame flow:

```mermaid
graph TD
    A[Input Context] --> B[GoogleVertexLLMService]
    B --> C[LLMFullResponseStartFrame]
    B --> D[TextFrame Chunks]
    B --> E[Function Calls]
    B --> F[LLMFullResponseEndFrame]
    E --> G[Function Results]
    G --> B
```

## Metrics Support

The service collects standard LLM metrics:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Notes

- Uses Google Cloud's Vertex AI API
- Maintains OpenAI-compatible interface
- Supports streaming responses
- Handles function calling
- Manages conversation context
- Includes token usage tracking
- Thread-safe processing
- Automatic token refresh
- Requires Google Cloud project setup



================================================
FILE: server/services/llm/grok.mdx
================================================
---
title: "Grok"
description: "LLM service implementation using Grok’s API with OpenAI-compatible interface"
---

## Overview

`GrokLLMService` provides access to Grok's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.grok.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Grok Docs"
    icon="book"
    href="https://docs.x.ai/docs/api-reference#chat-completions"
  >
    Official Grok API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14g-function-calling-grok.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use `GrokLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[grok]"
```

You'll also need to set up your Grok API key as an environment variable: `GROK_API_KEY`.

<Tip>Get your API key from [X.AI Console](https://console.x.ai/).</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.grok.llm import GrokLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Grok service
llm = GrokLLMService(
    api_key=os.getenv("GROK_API_KEY"),
    model="grok-3-beta",
    params=GrokLLMService.InputParams(
        temperature=0.8,  # Higher for creative responses
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context optimized for voice interaction
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful and creative assistant in a voice conversation.
            Your output will be converted to audio, so avoid special characters.
            Respond in an engaging and helpful way while being succinct."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities with specialized token tracking:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Accumulated prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Real-time Information**: Access to current events and up-to-date information
- **Vision Capabilities**: Image understanding and analysis with grok-2-vision model



================================================
FILE: server/services/llm/groq.mdx
================================================
---
title: "Groq"
description: "LLM service implementation using Groq’s API with OpenAI-compatible interface"
---

## Overview

`GroqLLMService` provides access to Groq's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.groq.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Groq Docs"
    icon="book"
    href="https://console.groq.com/docs/api-reference#chat-create"
  >
    Official Groq API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14f-function-calling-groq.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Groq services, install the required dependency:

```bash
pip install "pipecat-ai[groq]"
```

You'll also need to set up your Groq API key as an environment variable: `GROQ_API_KEY`.

<Tip>
  Get your API key for free from [Groq Console](https://console.groq.com/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing (select models)
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.groq.llm import GroqLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Groq service for speed
llm = GroqLLMService(
    api_key=os.getenv("GROQ_API_KEY"),
    model="llama-3.3-70b-versatile",  # Fast, capable model
    params=GroqLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context optimized for voice interaction
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant optimized for voice conversations.
            Keep responses concise and avoid special characters that don't work well in speech."""
        }
    ],
    tools=tools
)

# Create context aggregators with fast timeout for speed
from pipecat.processors.aggregators.llm_response import LLMUserAggregatorParams

context_aggregator = llm.create_context_aggregator(
    context,
    user_params=LLMUserAggregatorParams(aggregation_timeout=0.05)  # Fast aggregation
)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback for better UX
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline with Groq STT for full Groq stack
pipeline = Pipeline([
    transport.input(),
    groq_stt,  # GroqSTTService for consistent ecosystem
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Ultra-low latency measurements
- **Processing Duration** - Hardware-accelerated processing times
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Real-time Optimized**: Ideal for conversational AI and streaming applications
- **Open Source Models**: Access to Llama, Mixtral, and other open-source models
- **Vision Support**: Select models support image understanding capabilities
- **Free Tier**: Generous free tier available for development and testing



================================================
FILE: server/services/llm/nim.mdx
================================================
---
title: "NVIDIA NIM"
description: "LLM service implementation using NVIDIA’s NIM (NVIDIA Inference Microservice) API with OpenAI-compatible interface"
---

## Overview

`NimLLMService` provides access to NVIDIA's NIM language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management, with special handling for NVIDIA's incremental token reporting.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.nim.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card title="NVIDIA NIM Docs" icon="book" href="https://docs.nvidia.com/nim/">
    Official NVIDIA NIM documentation and setup
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14j-function-calling-nim.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use NVIDIA NIM services, install the required dependencies:

```bash
pip install "pipecat-ai[nim]"
```

You'll also need to set up your NVIDIA API key as an environment variable: `NVIDIA_API_KEY`.

<Tip>
  Get your API key from [NVIDIA
  Build](https://build.nvidia.com/explore/discover).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.nim.llm import NimLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure NVIDIA NIM service
llm = NimLLMService(
    api_key=os.getenv("NVIDIA_API_KEY"),
    model="nvidia/llama-3.1-nemotron-70b-instruct",
    params=NimLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context optimized for voice
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant optimized for voice interactions.
            Keep responses concise and avoid special characters for better speech synthesis."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Includes specialized token usage tracking for NIM's incremental reporting:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Tracks tokens used per request, compatible with NIM's incremental reporting

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **NVIDIA Optimization**: Hardware-accelerated inference on NVIDIA infrastructure
- **Token Reporting**: Custom handling for NIM's incremental vs. OpenAI's final token reporting
- **Model Variety**: Access to Nemotron and other NVIDIA-optimized model variants



================================================
FILE: server/services/llm/ollama.mdx
================================================
---
title: "Ollama"
description: "LLM service implementation using Ollama with OpenAI-compatible interface"
---

## Overview

`OLLamaLLMService` provides access to locally-run Ollama models through an OpenAI-compatible interface. It inherits from `BaseOpenAILLMService` and allows you to run various open-source models locally while maintaining compatibility with OpenAI's API format.

<CardGroup cols={3}>
  <Card title="API Reference" icon="code" href="/api/services/ollama-llm">
    Complete API documentation and method details
  </Card>
  <Card title="Ollama Docs" icon="book" href="https://ollama.com/library">
    Official Ollama documentation and model library
  </Card>
  <Card
    title="Download Ollama"
    icon="download"
    href="https://ollama.com/download"
  >
    Download and setup instructions for Ollama
  </Card>
</CardGroup>

## Installation

To use Ollama services, you need to install both Ollama and the Pipecat dependency:

1. **Install Ollama** on your system from [ollama.com/download](https://ollama.com/download)
2. **Install Pipecat dependency**:

```bash
pip install "pipecat-ai[ollama]"
```

3. **Pull a model** (first time only):

```bash
ollama pull llama2
```

<Tip>Ollama runs as a local service on port 11434. No API key required!</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision models
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - Connection or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
from pipecat.services.ollama.llm import OLLamaLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure local Ollama service
llm = OLLamaLLMService(
    model="llama3.1",  # Must be pulled first: ollama pull llama3.1
    base_url="http://localhost:11434/v1",  # Default Ollama endpoint
    params=OLLamaLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for local processing
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context optimized for local model
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant running locally.
            Be concise and efficient in your responses while maintaining helpfulness."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler - all processing stays local
async def fetch_weather(params):
    location = params.arguments["location"]
    # Local weather lookup or cached data
    await params.result_callback({"conditions": "sunny", "temperature": "22°C"})

llm.register_function("get_current_weather", fetch_weather)

# Use in pipeline - completely offline capable
pipeline = Pipeline([
    transport.input(),
    stt,  # Can use local STT too
    context_aggregator.user(),
    llm,  # All inference happens locally
    tts,  # Can use local TTS too
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities for local monitoring:

- **Time to First Byte (TTFB)** - Local inference latency
- **Processing Duration** - Model execution time
- **Token Usage** - Local token counting (if supported by model)

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Run models locally**: Ollama allows you to run various open-source models on your own hardware, providing flexibility and control.
- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Privacy centric**: All processing happens locally, ensuring data privacy and security.



================================================
FILE: server/services/llm/openai.mdx
================================================
---
title: "OpenAI"
description: "Large Language Model services using OpenAI’s chat completion API"
---

## Overview

`OpenAILLMService` provides chat completion capabilities using OpenAI's API, supporting streaming responses, function calling, vision input, and advanced context management for conversational AI applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.base_llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenAI Docs"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/chat"
  >
    Official OpenAI API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14-function-calling.py"
  >
    Function calling example with weather API
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependencies:

```bash
pip install "pipecat-ai[openai]"
```

You'll also need to set up your OpenAI API key as an environment variable: `OPENAI_API_KEY`.

<Tip>
  Get your API key from the [OpenAI
  Platform](https://platform.openai.com/api-keys).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - OpenAI-specific conversation context
- `LLMMessagesFrame` - Standard conversation messages
- `VisionImageRawFrame` - Images for vision model processing
- `LLMUpdateSettingsFrame` - Runtime model configuration updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

### Basic Conversation with Function Calling

```python
import os
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.llm_service import FunctionCallParams

# Configure the service
llm = OpenAILLMService(
    model="gpt-4o",
    api_key=os.getenv("OPENAI_API_KEY"),
    params=OpenAILLMService.InputParams(
        temperature=0.7,
    )
)

# Define function schema
weather_function = FunctionSchema(
    name="get_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City name"
        }
    },
    required=["location"]
)

# Create tools and context
tools = ToolsSchema(standard_tools=[weather_function])
context = OpenAILLMContext(
    messages=[{
        "role": "system",
        "content": "You are a helpful assistant. Keep responses concise."
    }],
    tools=tools
)

# Register function handler
async def get_weather_handler(params: FunctionCallParams):
    location = params.arguments.get("location")
    # Call weather API here...
    weather_data = {"temperature": "75°F", "conditions": "sunny"}
    await params.result_callback(weather_data)

llm.register_function("get_weather", get_weather_handler)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),    # Handles user messages
    llm,                          # Processes with OpenAI
    tts,
    transport.output(),
    context_aggregator.assistant() # Captures responses
])
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from request to first response token
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and total usage

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Streaming Responses**: All responses are streamed for low latency
- **Context Persistence**: Use context aggregators to maintain conversation history
- **Error Handling**: Automatic retry logic for rate limits and transient errors
- **Compatible Services**: Works with OpenAI-compatible APIs by setting `base_url`



================================================
FILE: server/services/llm/openpipe.mdx
================================================
---
title: "OpenPipe"
description: "LLM service implementation using OpenPipe for LLM request logging and fine-tuning"
---

## Overview

`OpenPipeLLMService` extends the BaseOpenAILLMService to provide integration with OpenPipe, enabling request logging, model fine-tuning, and performance monitoring. It maintains compatibility with OpenAI's API while adding OpenPipe's logging and optimization capabilities.

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openpipe.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenPipe Docs"
    icon="book"
    href="https://docs.openpipe.ai/api-reference/post-chatcompletions"
  >
    Official OpenPipe API documentation and features
  </Card>
</CardGroup>

## Installation

To use `OpenPipeLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[openpipe]"
```

You'll need to set up both API keys as environment variables:

- `OPENPIPE_API_KEY` - Your OpenPipe API key
- `OPENAI_API_KEY` - Your OpenAI API key

<Tip>
  Get your OpenPipe API key from [OpenPipe Dashboard](https://app.openpipe.ai/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.openpipe.llm import OpenPipeLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure OpenPipe service with comprehensive logging
llm = OpenPipeLLMService(
    model="gpt-4o",
    api_key=os.getenv("OPENAI_API_KEY"),
    openpipe_api_key=os.getenv("OPENPIPE_API_KEY"),
    tags={
        "environment": "production",
        "feature": "conversational-ai",
        "deployment": "voice-assistant",
        "version": "v1.2"
    },
    params=OpenPipeLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000
    )
)

# Define function for monitoring tool usage
weather_function = FunctionSchema(
    name="get_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        }
    },
    required=["location"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context with system optimization
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful voice assistant. Keep responses
            concise and natural for speech synthesis. All conversations are
            logged for quality improvement."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function with logging awareness
async def get_weather(params):
    location = params.arguments["location"]
    # Function calls are automatically logged by OpenPipe
    await params.result_callback(f"Weather in {location}: 72°F and sunny")

llm.register_function("get_weather", get_weather)

# Use in pipeline - all requests automatically logged
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,  # Automatic logging happens here
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics plus OpenPipe-specific logging:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Detailed consumption tracking

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Privacy Aware**: Configurable data retention and filtering policies
- **Cost Optimization**: Detailed analytics help optimize model usage and costs
- **Fine-tuning Pipeline**: Seamless transition from logging to custom model training



================================================
FILE: server/services/llm/openrouter.mdx
================================================
---
title: "OpenRouter"
description: "LLM service implementation using OpenRouter’s API with OpenAI-compatible interface"
---

## Overview

`OpenRouterLLMService` provides access to OpenRouter's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openrouter.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenRouter Docs"
    icon="book"
    href="https://openrouter.ai/docs/api-reference/chat-completion"
  >
    Official OpenRouter API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14m-function-calling-openrouter.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use `OpenRouterLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[openrouter]"
```

You'll also need to set up your OpenRouter API key as an environment variable: `OPENROUTER_API_KEY`.

<Tip>
  Get your API key from [OpenRouter](https://openrouter.ai/). Free tier includes
  $1 of credits.
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing (select models)
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.openrouter.llm import OpenRouterLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure OpenRouter service
llm = OpenRouterLLMService(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    model="openai/gpt-4o-2024-11-20",  # Easy model switching
    params=OpenRouterLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant optimized for voice conversations.
            Keep responses concise and avoid special characters for better speech synthesis."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])

# Easy model switching for different use cases
# llm.set_model_name("anthropic/claude-3.5-sonnet")  # Switch to Claude
# llm.set_model_name("meta-llama/llama-3.1-70b-instruct")  # Switch to Llama
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Model Variety**: Access 70+ models from OpenAI, Anthropic, Meta, Google, and more
- **OpenAI Compatibility**: Full compatibility with existing OpenAI code
- **Easy Switching**: Change models with a single parameter update
- **Fallback Support**: Built-in model fallbacks for high availability



================================================
FILE: server/services/llm/perplexity.mdx
================================================
---
title: "Perplexity"
description: "LLM service implementation using Perplexity’s API with OpenAI-compatible interface"
---

## Overview

`PerplexityLLMService` provides access to Perplexity's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses and context management, with special handling for Perplexity's incremental token reporting.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.perplexity.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Perplexity Docs"
    icon="book"
    href="https://docs.perplexity.ai/api-reference/chat-completions-post"
  >
    Official Perplexity API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14n-function-calling-perplexity.py"
  >
    Working example with search capabilities
  </Card>
</CardGroup>

<Note>
  Unlike other LLM services, Perplexity does not support function calling.
  Instead, they offer native internet search built in without requiring special
  function calls.
</Note>

## Installation

To use `PerplexityLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[perplexity]"
```

You'll also need to set up your Perplexity API key as an environment variable: `PERPLEXITY_API_KEY`.

<Tip>
  Get your API key from [Perplexity
  API](https://www.perplexity.ai/settings/api).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks with citations
- `ErrorFrame` - API or processing errors

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.perplexity.llm import PerplexityLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext

# Configure Perplexity service
llm = PerplexityLLMService(
    api_key=os.getenv("PERPLEXITY_API_KEY"),
    model="sonar-pro",  # Pro model for enhanced capabilities
    params=PerplexityLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Create context optimized for search and current information
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a knowledgeable assistant with access to real-time information.
            When answering questions, use your search capabilities to provide current, accurate information.
            Always cite your sources when possible. Keep responses concise for voice output."""
        }
    ]
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Use in pipeline for information-rich conversations
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,  # Will automatically search and cite sources
    tts,
    transport.output(),
    context_aggregator.assistant()
])

# Enable metrics with special TTFB reporting for Perplexity
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        enable_metrics=True,
        enable_usage_metrics=True,
        report_only_initial_ttfb=True,  # Optimized for Perplexity's response pattern
    )
)
```

## Metrics

The service provides specialized token tracking for Perplexity's incremental reporting:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Accumulated prompt and completion tokens

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **No Function Calling**: Perplexity doesn't support traditional function calling but provides superior built-in search
- **Real-time Data**: Access to current information without complex function orchestration
- **Source Citations**: Automatic citation of web sources in responses
- **OpenAI Compatible**: Uses familiar OpenAI-style interface and parameters



================================================
FILE: server/services/llm/qwen.mdx
================================================
---
title: "Qwen"
description: "LLM service implementation using Alibaba Cloud’s Qwen models through an OpenAI-compatible interface"
---

## Overview

`QwenLLMService` provides access to Alibaba Cloud's Qwen language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management, with particularly strong capabilities for Chinese language processing.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.qwen.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Qwen Docs"
    icon="book"
    href="https://www.alibabacloud.com/help/en/model-studio/use-qwen-by-calling-api"
  >
    Official Qwen API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14q-function-calling-qwen.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use `QwenLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[qwen]"
```

You'll also need to set up your DashScope API key as an environment variable: `QWEN_API_KEY`.

<Tip>
  Get your API key from [Alibaba Cloud Model
  Studio](https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.qwen.llm import QwenLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Qwen service
llm = QwenLLMService(
    api_key=os.getenv("QWEN_API_KEY"),
    model="qwen2.5-72b-instruct",  # High-quality open source model
    params=QwenLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and country, e.g. Beijing, China or San Francisco, USA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create bilingual context for Chinese/English support
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant in voice conversations.
            Keep responses concise for speech output. You can respond in Chinese when
            the user speaks Chinese, or English when they speak English.

            你是一个语音对话助手。请保持简洁的回答以适合语音输出。
            当用户用中文交流时用中文回答，用英文交流时用英文回答。"""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler
async def fetch_weather(params):
    location = params.arguments["location"]
    # Return response that works well in both languages
    await params.result_callback({
        "conditions": "sunny",
        "temperature": "22°C",
        "location": location
    })

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,  # Consider QwenTTSService for Chinese speech
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Long Context Support**: Models support up to 1M token contexts for extensive conversations
- **Multilingual Excellence**: Superior performance in Chinese with strong English capabilities
- **Code-Switching**: Seamlessly handles mixed Chinese-English conversations
- **Alibaba Cloud Integration**: Native integration with Alibaba Cloud ecosystem



================================================
FILE: server/services/llm/sambanova.mdx
================================================
---
title: "SambaNova"
description: "LLM service implementation using SambaNova's API with OpenAI-compatible interface"
---

## Overview

`SambaNovaLLMService` provides access to SambaNova's language models through an OpenAI-compatible interface.
It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

## Installation

To use `SambaNovaLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[sambanova]"
```

You also need to set up your SambaNova API key as an environment variable: `SAMBANOVA_API_KEY`.

<Tip>
  Get your SambaNova API key
  [here](https://cloud.sambanova.ai/?utm_source=pipecat&utm_medium=external&utm_campaign=cloud_signup).
</Tip>

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your SambaNova API key
</ParamField>

<ParamField
  path="model"
  type="str"
  default="Llama-4-Maverick-17B-128E-Instruct"
>
  Model identifier
</ParamField>

<ParamField path="base_url" type="str" default="https://api.sambanova.ai/v1">
  SambaNova API endpoint
</ParamField>

### Input Parameters

Inherits OpenAI-compatible parameters:

<ParamField path="max_tokens" type="Optional[int]">
  Maximum number of tokens to generate. Must be greater than or equal to 1.
</ParamField>

<ParamField path="temperature" type="Optional[float]">
  Controls randomness in the output. Range: [0.0, 1.0].
</ParamField>

<ParamField path="top_p" type="Optional[float]">
  Controls diversity via nucleus sampling. Range: [0.0, 1.0]
</ParamField>

## Usage Example

```python
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.sambanova.llm import SambaNovaLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from openai.types.chat import ChatCompletionToolParam
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.services.llm_service import FunctionCallParams

# Configure service
llm = SambaNovaLLMService(
    api_key'your-sambanova-api-key',
    model='Llama-4-Maverick-17B-128E-Instruct',
    params=SambaNovaLLMService.InputParams(temperature=0.7, max_tokens=1024),
)

# Define function to call
async def fetch_weather(params: FunctionCallParams) -> Any:
    """Mock function that fetches the weather forcast from an API."""

    await params.result_callback({'conditions': 'nice', 'temperature': '20 Degrees Celsius'})

# Register function handlers
llm.register_function('get_current_weather', fetch_weather)

# Define weather function using standardized schema
weather_function = FunctionSchema(
    name='get_current_weather',
    description='Get the current weather',
    properties={
        'location': {
            'type': 'string',
            'description': 'The city and state.',
        },
        'format': {
            'type': 'string',
            'enum': ['celsius', 'fahrenheit'],
            'description': "The temperature unit to use. Infer this from the user's location.",
        },
    },
    required=['location', 'format'],
)

# Create tools schema
tools = ToolsSchema(standard_tools=[weather_function])

# Define system message
messages = [
    {
        'role': 'system',
        'content': 'You are a helpful LLM in a WebRTC call. '
        'Your goal is to demonstrate your capabilities of weather forecasting in a succinct way. '
        'Introduce yourself to the user and then wait for their question. '
        'Elaborate your response into a conversational answer in a creative and helpful way. '
        'Your output will be converted to audio so do not include special characters in your answer. '
        'Once the final answer has been provided, please stop, unless the user asks another question. ',
    },
]

# Create context with system message and tools
context = OpenAILLMContext(messages, tools)

# Context aggregator
context_aggregator = llm.create_context_aggregator(context)

# Create context aggregator for message handling
context_aggregator = llm.create_context_aggregator(context)

# Set up pipeline
pipeline = Pipeline(
    [
        transport.input(),
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ]
)

# Create and configure task
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
)
```

## Methods

See the [LLM base class methods](https://reference-server.pipecat.ai/en/latest/api/pipecat.services.llm_service.html#llm-service) for additional functionality.

## Function Calling

This service supports function calling (also known as tool calling) which allows the LLM to request information from external services and APIs. For example, you can enable your bot to:

- Check current weather conditions.
- Query databases.
- Access external APIs.
- Perform custom actions.

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Available Models

| Model Name                            | Description                                       |
| ------------------------------------- | ------------------------------------------------- |
| `DeepSeek-R1`                         | deepseek-ai/DeepSeek-R1                           |
| `DeepSeek-R1-Distill-Llama-70B`       | deepseek-ai/DeepSeek-R1-Distill-Llama-70B         |
| `DeepSeek-V3-0324`                    | deepseek-ai/DeepSeek-V3-0324                      |
| `Llama-4-Maverick-17B-128E-Instruct`  | meta-llama/Llama-4-Maverick-17B-128E-Instruct     |
| `Llama-4-Scout-17B-16E-Instruct`      | meta-llama/Llama-4-Scout-17B-16E-Instruct         |
| `Meta-Llama-3.3-70B-Instruct`         | meta-llama/Llama-3.3-70B-Instruct                 |
| `Meta-Llama-3.2-3B-Instruct`          | meta-llama/Llama-3.2-3B-Instruct                  |
| `Meta-Llama-3.2-1B-Instruct`          | meta-llama/Llama-3.2-1B-Instruct                  |
| `Meta-Llama-3.1-405B-Instruct`        | meta-llama/Llama-3.1-405B-Instruct                |
| `Meta-Llama-3.1-8B-Instruct`          | meta-llama/Llama-3.1-8B-Instruct                  |
| `Meta-Llama-Guard-3-8B`               | meta-llama/Llama-Guard-3-8B                       |
| `QwQ-32B`                             | Qwen/QwQ-32B                                      |
| `Qwen3-32B`                           | Qwen/Qwen3-32B                                    |
| `Llama-3.3-Swallow-70B-Instruct-v0.4` | Tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4 |

<Note>
  See [SambaNova's
  docs](https://docs.sambanova.ai/cloud/docs/get-started/supported-models) for a
  complete list of supported models.
</Note>

## Frame Flow

Inherits the OpenAI LLM Service frame flow:

```mermaid
graph TD
    A[Input Context] --> B[SambaNovaLLMService]
    B --> C[LLMFullResponseStartFrame]
    B --> D[TextFrame Chunks]
    B --> E[Function Calls]
    B --> F[LLMFullResponseEndFrame]
    E --> G[Function Results]
    G --> B
```

## Metrics Support

The service collects standard LLM metrics:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Notes

- OpenAI-compatible interface.
- Supports streaming responses.
- Handles function calling.
- Manages conversation context.
- Includes token usage tracking.
- Thread-safe processing.
- Automatic error handling.



================================================
FILE: server/services/llm/together.mdx
================================================
---
title: "Together AI"
description: "LLM service implementation using Together AI’s API with OpenAI-compatible interface"
---

## Overview

`TogetherLLMService` provides access to Together AI's language models, including Meta's Llama 3.1 and 3.2 models, through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.together.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Together AI Docs"
    icon="book"
    href="https://docs.together.ai/reference/chat-completions-1"
  >
    Official Together AI API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14c-function-calling-together.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use `TogetherLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[together]"
```

You'll also need to set up your Together AI API key as an environment variable: `TOGETHER_API_KEY`.

<Tip>
  Get your API key from [Together AI Console](https://api.together.xyz/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing (select models)
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.together.llm import TogetherLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Together AI service
llm = TogetherLLMService(
    api_key=os.getenv("TOGETHER_API_KEY"),
    model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",  # Balanced performance
    params=TogetherLLMService.InputParams(
        temperature=0.7,
        max_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context optimized for voice
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": """You are a helpful assistant in a voice conversation.
            Keep responses concise and avoid special characters for better speech synthesis."""
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with feedback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Open Source Models**: Access to cutting-edge open-source models like Llama
- **Vision Support**: Select models support multimodal image and text understanding
- **Competitive Pricing**: Cost-effective alternative to proprietary model APIs
- **Flexible Scaling**: Choose model size based on performance vs cost requirements



================================================
FILE: server/services/memory/mem0.mdx
================================================
---
title: "Mem0"
description: "Long-term conversation memory service powered by Mem0"
---

## Overview

`Mem0MemoryService` provides long-term memory capabilities for conversational agents by integrating with Mem0's API. It automatically stores conversation history and retrieves relevant past context based on the current conversation, enhancing LLM responses with persistent memory across sessions.

## Installation

To use the Mem0 memory service, install the required dependencies:

```bash
pip install "pipecat-ai[mem0]"
```

You'll also need to set up your Mem0 API key as an environment variable: `MEM0_API_KEY`.

<Tip>
  You can obtain a Mem0 API key by signing up at [mem0.ai](https://mem0.ai).
</Tip>

## Mem0MemoryService

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Mem0 API key for accessing the service
</ParamField>

<ParamField path="host" type="str" required>
  Mem0 host for accessing the service
</ParamField>

<ParamField path="user_id" type="str" optional>
  Unique identifier for the end user to associate with memories
</ParamField>

<ParamField path="agent_id" type="str" optional>
  Identifier for the agent using the memory service
</ParamField>

<ParamField path="run_id" type="str" optional>
  Identifier for the specific conversation session
</ParamField>

<ParamField path="params" type="InputParams" optional>
  Configuration parameters for memory retrieval (see below)
</ParamField>

<ParamField path="local_config" type="dict" optional>
  Configuration for using local LLMs and embedders instead of Mem0's cloud API (see Local Configuration section)
</ParamField>

<Tip>
  At least one of `user_id`, `agent_id`, or `run_id` must be provided to
  organize memories.
</Tip>

### Input Parameters

The `params` object accepts the following configuration settings:

<ParamField path="search_limit" type="int" default="10">
  Maximum number of relevant memories to retrieve per query
</ParamField>

<ParamField path="search_threshold" type="float" default="0.1">
  Relevance threshold for memory retrieval (0.0 to 1.0)
</ParamField>

<ParamField path="api_version" type="str" default="v2">
  Mem0 API version to use
</ParamField>

<ParamField
  path="system_prompt"
  type="str"
  default="Based on previous conversations, I recall: \n\n"
>
  Prefix text to add before retrieved memories
</ParamField>

<ParamField path="add_as_system_message" type="bool" default="True">
  Whether to add memories as a system message (True) or user message (False)
</ParamField>

<ParamField path="position" type="int" default="1">
  Position in the context where memories should be inserted
</ParamField>

## Input Frames

The service processes the following input frames:

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Contains OpenAI-specific conversation context
</ParamField>

<ParamField path="LLMMessagesFrame" type="Frame">
  Contains conversation messages in standard format
</ParamField>

## Output Frames

The service may produce the following output frames:

<ParamField path="LLMMessagesFrame" type="Frame">
  Enhanced messages with relevant memories included
</ParamField>

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Enhanced OpenAI context with memories included
</ParamField>

<ParamField path="ErrorFrame" type="Frame">
  Contains error information if memory operations fail
</ParamField>

## Memory Operations

The service performs two main operations automatically:

### Message Storage

All conversation messages are stored in Mem0 for future reference. The service:

- Captures full message history from context frames
- Associates messages with the specified user/agent/run IDs
- Stores metadata to enable efficient retrieval

### Memory Retrieval

When a new user message is detected, the service:

1. Uses the message as a search query
2. Retrieves relevant past memories from Mem0
3. Formats memories with the configured system prompt
4. Adds the formatted memories to the conversation context
5. Passes the enhanced context downstream in the pipeline

## Pipeline Positioning

The memory service should be positioned **after** the user context aggregator but **before** the LLM service:

```
context_aggregator.user() → memory_service → llm
```

This ensures that:

1. The user's latest message is included in the context
2. The memory service can enhance the context before the LLM processes it
3. The LLM receives the enhanced context with relevant memories

## Usage Examples

### Basic Integration

```python
from pipecat.services.mem0.memory import Mem0MemoryService
from pipecat.pipeline.pipeline import Pipeline

# Create the memory service
memory = Mem0MemoryService(
    api_key=os.getenv("MEM0_API_KEY"),
    user_id="user123",  # Unique user identifier
)

# Position the memory service between context aggregator and LLM
pipeline = Pipeline([
    transport.input(),
    context_aggregator.user(),
    memory,           # <-- Memory service enhances context here
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Using Local Configuration

The `local_config` parameter allows you to use your own LLM and embedding providers instead of Mem0's cloud API. This is useful for self-hosted deployments or when you want more control over the memory processing.

```python
local_config = {
    "llm": {
        "provider": str,  # LLM provider name (e.g., "anthropic", "openai")
        "config": {
            # Provider-specific configuration
            "model": str,  # Model name
            "api_key": str,  # API key for the provider
            # Other provider-specific parameters
        }
    },
    "embedder": {
        "provider": str,  # Embedding provider name (e.g., "openai")
        "config": {
            # Provider-specific configuration
            "model": str,  # Model name
            # Other provider-specific parameters
        }
    }
}

# Initialize Mem0 memory service with local configuration
memory = Mem0MemoryService(
    local_config=local_config,  # Use local LLM for memory processing
    user_id="user123",          # Unique identifier for the user
)
```

<Warning>
  When using `local_config` do not provide the `api_key` parameter.
</Warning>

## Frame Flow

```mermaid
graph TD
    A[Context Aggregator] --> B[LLMMessagesFrame] --> C[Mem0MemoryService]
    C -->|Query Mem0| D[Mem0 API]
    D -->|Relevant Memories| C
    C -->|Enhanced Context| E[LLM Service]
    E --> F[TTS]
    F --> G[Output to User]
    E -->|Store Response| H[Mem0 API]
```

## Error Handling

The service includes basic error handling to ensure conversation flow continues even when memory operations fail:

- Exceptions during memory storage and retrieval are caught and logged
- If an error occurs during frame processing, an `ErrorFrame` is emitted with error details
- The original frame is still passed downstream to prevent the pipeline from stalling
- Connection and authentication errors from the Mem0 API will be logged but won't interrupt the conversation

<Warning>
  While the service attempts to handle errors gracefully, memory operations that
  fail may result in missing context in conversations. Monitor your application
  logs for memory-related errors.
</Warning>



================================================
FILE: server/services/s2s/aws.mdx
================================================
---
title: "AWS Nova Sonic"
description: "Real-time speech-to-speech service implementation using AWS Nova Sonic"
---

The `AWSNovaSonicLLMService` enables natural, real-time conversations with AWS Nova Sonic. It provides built-in audio transcription, voice activity detection, and context management for creating interactive AI experiences. It provides:

<CardGroup cols={2}>
  <Card title="Real-time Interaction" icon="bolt">
    Stream audio in real-time with low latency response times
  </Card>

{" "}

<Card title="Speech Processing" icon="waveform-lines">
  Built-in speech-to-text and text-to-speech capabilities with multiple voice
  options
</Card>

{" "}

<Card title="Voice Activity Detection" icon="microphone">
  Automatic detection of speech start/stop for natural conversations
</Card>

  <Card title="Context Management" icon="brain">
    Intelligent handling of conversation history and system instructions
  </Card>
</CardGroup>

## Installation

To use `AWSNovaSonicLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[aws-nova-sonic]"
```

We recommend setting up your AWS credentials as environment variables, as you'll need them to initialize `AWSNovaSonicLLMService`:

- `AWS_SECRET_ACCESS_KEY`
- `AWS_ACCESS_KEY_ID`
- `AWS_REGION`

## Basic Usage

Here’s a simple example of setting up a conversational AI bot with AWS Nova Sonic:

```python
from pipecat.services.aws_nova_sonic.aws import AWSNovaSonicLLMService

llm = AWSNovaSonicLLMService(
    secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    region=os.getenv("AWS_REGION")
    voice_id="tiffany",                    # Voices: matthew, tiffany, amy
)
```

## Configuration

### Constructor Parameters

<ParamField path="secret_access_key" type="str" required>
  Your AWS secret access key
</ParamField>

<ParamField path="access_key_id" type="str" required>
  Your AWS access key ID
</ParamField>

<ParamField path="region" type="str" required>
  Specify the AWS region for the service (e.g., `"us-east-1"`). Note that the
  service may not be available in all AWS regions: check the [AWS Bedrock User
  Guide's support
  table](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).
</ParamField>

<ParamField path="model" type="str" default="amazon.nova-sonic-v1:0">
  AWS Nova Sonic model to use. Note that `"amazon.nova-sonic-v1:0"` is the only
  supported model as of 2025-05-08.
</ParamField>

<ParamField path="voice_id" type="str" default="matthew">
  Voice for text-to-speech (options: `"matthew"`, `"tiffany"`, `"amy"`)
</ParamField>

<ParamField path="params" type="Params" optional>
  Configuration for model parameters
</ParamField>

<ParamField path="system_instruction" type="str" optional>
  High-level instructions that guide the model's behavior. Note that more
  commonly these instructions will be included as part of the context provided
  to kick off the conversation.
</ParamField>

<ParamField path="tools" type="ToolsSchema" optional>
  List of function definitions for tool/function calling. Note that more
  commonly tools will be included as part of the context provided to kick off
  the conversation.
</ParamField>

<ParamField path="send_transcription_frames" type="bool" default="True">
  Whether to emit transcription frames
</ParamField>

### Model Parameters

The `Params` object configures the behavior of the AWS Nova Sonic model.

<Warning>
  It is strongly recommended to stick with default values (most easily by
  omitting `params` when constructing `AWSNovaSonicLLMService`) unless you have
  a good understanding of the parameters and their impact. Deviating from the
  defaults may lead to unexpected behavior.
</Warning>

<ParamField path="temperature" type="float" optional default="0.7">
  Controls randomness in responses. Range: 0.0 to 2.0
</ParamField>

<ParamField path="max_tokens" type="int" optional default="1024">
  Maximum number of tokens to generate
</ParamField>

<ParamField path="top_p" type="float" optional default="0.9">
  Cumulative probability cutoff for token selection. Range: 0.0 to 1.0
</ParamField>

<ParamField path="input_sample_rate" type="int" optional default="16000">
  Sample rate for input audio
</ParamField>

<ParamField path="output_sample_rate" type="int" optional default="24000">
  Sample rate for output audio
</ParamField>

<ParamField path="input_sample_size" type="int" optional default="16">
  Bit depth for input audio
</ParamField>

<ParamField path="input_channel_count" type="int" optional default="1">
  Number of channels for input audio
</ParamField>

<ParamField path="output_sample_size" type="int" optional default="16">
  Bit depth for output audio
</ParamField>

<ParamField path="output_channel_count" type="int" optional default="1">
  Number of channels for output audio
</ParamField>

## Frame Types

### Input Frames

<ParamField path="InputAudioRawFrame" type="Frame">
  Raw audio data for speech input
</ParamField>

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Contains conversation context
</ParamField>

<ParamField path="BotStoppedSpeakingFrame" type="Frame">
  Signals the bot has stopped speaking
</ParamField>

### Output Frames

<ParamField path="TTSAudioRawFrame" type="Frame">
  Generated speech audio
</ParamField>

<ParamField path="LLMFullResponseStartFrame" type="Frame">
  Signals the start of a response from the LLM
</ParamField>

<ParamField path="LLMFullResponseEndFrame" type="Frame">
  Signals the end of a response from the LLM
</ParamField>

<ParamField path="TTSStartedFrame" type="Frame">
  Signals start of speech synthesis (coincides with the start of the LLM
  response, as this is a speech-to-speech model)
</ParamField>

<ParamField path="TTSStoppedFrame" type="Frame">
  Signals end of speech synthesis (coincides with the end of the LLM response,
  as this is a speech-to-speech model)
</ParamField>

<ParamField path="LLMTextFrame" type="Frame">
  Generated text responses from the LLM
</ParamField>

<ParamField path="TTSTextFrame" type="Frame">
  Generated text responses
</ParamField>

<ParamField path="TranscriptionFrame" type="Frame">
  Speech transcriptions. Only output if `send_transcription_frames` is `True`.
</ParamField>

## Function Calling

This service supports function calling (also known as tool calling) which allows the LLM to request information from external services and APIs. For example, you can enable your bot to:

- Check current weather conditions
- Query databases
- Access external APIs
- Perform custom actions

See the [Function Calling guide](/guides/fundamentals/function-calling) for:

- Detailed implementation instructions
- Provider-specific function definitions
- Handler registration examples
- Control over function call behavior
- Complete usage examples

## Next Steps

### Examples

- [Foundational Example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/40-aws-nova-sonic.py)
  Basic implementation showing core features

- [Persistent Content Example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/20e-persistent-context-aws-nova-sonic.py)
  Implementation showing saving and loading conversation history



================================================
FILE: server/services/s2s/gemini.mdx
================================================
---
title: "Gemini Multimodal Live"
description: "A real-time, multimodal conversational AI service powered by Google’s Gemini"
---

The `GeminiMultimodalLiveLLMService` enables natural, real-time conversations with Google’s Gemini model. It provides built-in audio transcription, voice activity detection, and context management for creating interactive AI experiences. It provides:

<CardGroup cols={2}>
  <Card title="Real-time Interaction" icon="video">
    Stream audio and video in real-time with low latency response times
  </Card>

<Card title="Speech Processing" icon="waveform-lines">
  Built-in speech-to-text and text-to-speech capabilities with multiple voice
  options
</Card>

<Card title="Voice Activity Detection" icon="microphone">
  Automatic detection of speech start/stop for natural conversations
</Card>

  <Card title="Context Management" icon="brain">
    Intelligent handling of conversation history and system instructions
  </Card>
</CardGroup>

<Tip>
  Want to start building? Check out our [Gemini Multimodal Live
  Guide](/guides/features/gemini-multimodal-live).
</Tip>

## Installation

To use `GeminiMultimodalLiveLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You’ll need to set up your Google API key as an environment variable: `GOOGLE_API_KEY`.

## Basic Usage

Here’s a simple example of setting up a conversational AI bot with Gemini Multimodal Live:

```python
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiMultimodalLiveLLMService,
    InputParams,
    GeminiMultimodalModalities
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    voice_id="Aoede",                                # Voices: Aoede, Charon, Fenrir, Kore, Puck
    params=InputParams(
        temperature=0.7,                             # Set model input params
        language=Language.EN_US,                     # Set language (30+ languages supported)
        modalities=GeminiMultimodalModalities.AUDIO  # Response modality
    )
)
```

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your Google API key
</ParamField>

<ParamField
  path="base_url"
  type="str"
  default="generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent"
>
  API endpoint URL
</ParamField>

<ParamField
  path="model"
  type="str"
  default="models/gemini-2.5-flash-preview-native-audio-dialog"
>
  Gemini model to use (upgraded to new v1beta model)
</ParamField>

<ParamField path="voice_id" type="str" default="Charon">
  Voice for text-to-speech (options: Aoede, Charon, Fenrir, Kore, Puck)
</ParamField>

```python
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    voice_id="Puck",  # Choose your preferred voice
)
```

<ParamField path="system_instruction" type="str" optional>
  High-level instructions that guide the model's behavior
</ParamField>

```python
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    system_instruction="Talk like a pirate.",
)
```

<ParamField path="start_audio_paused" type="bool" default="False">
  Whether to start with audio input paused
</ParamField>

<ParamField path="start_video_paused" type="bool" default="False">
  Whether to start with video input paused
</ParamField>

<ParamField path="tools" type="Union[List[dict], ToolsSchema]" optional>
  Tools/functions available to the model
</ParamField>

<ParamField
  path="inference_on_context_initialization"
  type="bool"
  default="True"
>
  Whether to generate a response when context is first set
</ParamField>

### Input Parameters

<ParamField path="frequency_penalty" type="float" optional default="None">
  Penalizes repeated tokens. Range: 0.0 to 2.0
</ParamField>

<ParamField path="max_tokens" type="int" optional default="4096">
  Maximum number of tokens to generate
</ParamField>

<ParamField
  path="modalities"
  type="GeminiMultimodalModalities"
  optional
  default="AUDIO"
>
  Response modalities to include (options: `AUDIO`, `TEXT`).
</ParamField>

<ParamField path="presence_penalty" type="float" optional default="None">
  Penalizes tokens based on their presence in the text. Range: 0.0 to 2.0
</ParamField>

<ParamField path="temperature" type="float" optional default="None">
  Controls randomness in responses. Range: 0.0 to 2.0
</ParamField>

<ParamField path="language" type="Language" optional default="Language.EN_US">
  Language for generation. Over 30 languages are supported.
</ParamField>

<ParamField
  path="media_resolution"
  type="GeminiMediaResolution"
  optional
  default="UNSPECIFIED"
>
  
Controls image processing quality and token usage:

- `LOW`: Uses 64 tokens
- `MEDIUM`: Uses 256 tokens
- `HIGH`: Zoomed reframing with 256 tokens

</ParamField>

<ParamField path="vad" type="GeminiVADParams" optional>

Voice Activity Detection configuration:

- `disabled`: Toggle VAD on/off
- `start_sensitivity`: How quickly speech is detected (HIGH/LOW)
- `end_sensitivity`: How quickly turns end after pauses (HIGH/LOW)
- `prefix_padding_ms`: Milliseconds of audio to keep before speech
- `silence_duration_ms`: Milliseconds of silence to end a turn

</ParamField>

```python
from pipecat.services.gemini_multimodal_live.events import (
    StartSensitivity,
    EndSensitivity
)
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiVADParams,
    GeminiMediaResolution,
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(
        temperature=0.7,
        language=Language.ES,                         # Spanish language
        media_resolution=GeminiMediaResolution.HIGH,  # Higher quality image processing
        vad=GeminiVADParams(
            start_sensitivity=StartSensitivity.HIGH,  # Detect speech quickly
            end_sensitivity=EndSensitivity.LOW,       # Allow longer pauses
            prefix_padding_ms=300,                    # Keep 300ms before speech
            silence_duration_ms=1000,                 # End turn after 1s silence
        )
    )
)
```

<ParamField path="top_k" type="int" optional default="None">
  Limits vocabulary to k most likely tokens. Minimum: 0
</ParamField>

<ParamField path="top_p" type="float" optional default="None">
  Cumulative probability cutoff for token selection. Range: 0.0 to 1.0
</ParamField>

<ParamField
  path="context_window_compression"
  type="ContextWindowCompressionParams"
  optional
>
  Parameters for managing the context window: - `enabled`: Enable/disable
  compression (default: False) - `trigger_tokens`: Number of tokens that trigger
  compression (default: None, uses 80% of context window)
</ParamField>

```python
from pipecat.services.gemini_multimodal_live.gemini import (
    ContextWindowCompressionParams
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(
        top_p=0.9,               # More focused token selection
        top_k=40,                # Limit vocabulary options
        context_window_compression=ContextWindowCompressionParams(
            enabled=True,
            trigger_tokens=8000  # Compress when reaching 8000 tokens
        )
    )
)
```

## Methods

<ParamField path="set_audio_input_paused(paused: bool)" type="method">
  Pause or unpause audio input processing
</ParamField>

<ParamField path="set_video_input_paused(paused: bool)" type="method">
  Pause or unpause video input processing
</ParamField>

<ParamField
  path="set_model_modalities(modalities: GeminiMultimodalModalities)"
  type="method"
>
  Change the response modality (TEXT or AUDIO)
</ParamField>

<ParamField path="set_language(language: Language)" type="method">
  Change the language for generation
</ParamField>

<ParamField path="set_context(context: OpenAILLMContext)" type="method">
  Set the conversation context explicitly
</ParamField>

<ParamField
  path="create_context_aggregator(context: OpenAILLMContext, user_params: LLMUserAggregatorParams, assistant_params: LLMAssistantAggregatorParams)"
  type="method"
>
  Create context aggregators for managing conversation state
</ParamField>

## Frame Types

### Input Frames

<ParamField path="InputAudioRawFrame" type="Frame">
  Raw audio data for speech input
</ParamField>

<ParamField path="InputImageRawFrame" type="Frame">
  Raw image data for visual input
</ParamField>

<ParamField path="StartInterruptionFrame" type="Frame">
  Signals start of user interruption
</ParamField>

<ParamField path="UserStartedSpeakingFrame" type="Frame">
  Signals user started speaking
</ParamField>

<ParamField path="UserStoppedSpeakingFrame" type="Frame">
  Signals user stopped speaking
</ParamField>

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Contains conversation context
</ParamField>

<ParamField path="LLMMessagesAppendFrame" type="Frame">
  Adds messages to the conversation
</ParamField>

<ParamField path="LLMUpdateSettingsFrame" type="Frame">
  Updates LLM settings
</ParamField>

<ParamField path="LLMSetToolsFrame" type="Frame">
  Sets available tools for the LLM
</ParamField>

### Output Frames

<ParamField path="TTSAudioRawFrame" type="Frame">
  Generated speech audio
</ParamField>

<ParamField path="TTSStartedFrame" type="Frame">
  Signals start of speech synthesis
</ParamField>

<ParamField path="TTSStoppedFrame" type="Frame">
  Signals end of speech synthesis
</ParamField>

<ParamField path="LLMTextFrame" type="Frame">
  Generated text responses from the LLM
</ParamField>

<ParamField path="TTSTextFrame" type="Frame">
  Text used for speech synthesis
</ParamField>

<ParamField path="TranscriptionFrame" type="Frame">
  Speech transcriptions from user audio
</ParamField>

<ParamField path="LLMFullResponseStartFrame" type="Frame">
  Signals the start of a complete LLM response
</ParamField>

<ParamField path="LLMFullResponseEndFrame" type="Frame">
  Signals the end of a complete LLM response
</ParamField>

## Function Calling

This service supports function calling (also known as tool calling) which allows the LLM to request information from external services and APIs. For example, you can enable your bot to:

- Check current weather conditions
- Query databases
- Access external APIs
- Perform custom actions

See the [Function Calling guide](/guides/fundamentals/function-calling) for:

- Detailed implementation instructions
- Provider-specific function definitions
- Handler registration examples
- Control over function call behavior
- Complete usage examples

## Token Usage Tracking

Gemini Multimodal Live automatically tracks token usage metrics, providing:

- Prompt token counts
- Completion token counts
- Total token counts
- Detailed token breakdowns by modality (text, audio)

These metrics can be used for monitoring usage, optimizing costs, and understanding model performance.

## Language Support

Gemini Multimodal Live supports the following languages:

| Language Code     | Description          | Gemini Code |
| ----------------- | -------------------- | ----------- |
| `Language.AR`     | Arabic               | `ar-XA`     |
| `Language.BN_IN`  | Bengali (India)      | `bn-IN`     |
| `Language.CMN_CN` | Chinese (Mandarin)   | `cmn-CN`    |
| `Language.DE_DE`  | German (Germany)     | `de-DE`     |
| `Language.EN_US`  | English (US)         | `en-US`     |
| `Language.EN_AU`  | English (Australia)  | `en-AU`     |
| `Language.EN_GB`  | English (UK)         | `en-GB`     |
| `Language.EN_IN`  | English (India)      | `en-IN`     |
| `Language.ES_ES`  | Spanish (Spain)      | `es-ES`     |
| `Language.ES_US`  | Spanish (US)         | `es-US`     |
| `Language.FR_FR`  | French (France)      | `fr-FR`     |
| `Language.FR_CA`  | French (Canada)      | `fr-CA`     |
| `Language.GU_IN`  | Gujarati (India)     | `gu-IN`     |
| `Language.HI_IN`  | Hindi (India)        | `hi-IN`     |
| `Language.ID_ID`  | Indonesian           | `id-ID`     |
| `Language.IT_IT`  | Italian (Italy)      | `it-IT`     |
| `Language.JA_JP`  | Japanese (Japan)     | `ja-JP`     |
| `Language.KN_IN`  | Kannada (India)      | `kn-IN`     |
| `Language.KO_KR`  | Korean (Korea)       | `ko-KR`     |
| `Language.ML_IN`  | Malayalam (India)    | `ml-IN`     |
| `Language.MR_IN`  | Marathi (India)      | `mr-IN`     |
| `Language.NL_NL`  | Dutch (Netherlands)  | `nl-NL`     |
| `Language.PL_PL`  | Polish (Poland)      | `pl-PL`     |
| `Language.PT_BR`  | Portuguese (Brazil)  | `pt-BR`     |
| `Language.RU_RU`  | Russian (Russia)     | `ru-RU`     |
| `Language.TA_IN`  | Tamil (India)        | `ta-IN`     |
| `Language.TE_IN`  | Telugu (India)       | `te-IN`     |
| `Language.TH_TH`  | Thai (Thailand)      | `th-TH`     |
| `Language.TR_TR`  | Turkish (Turkey)     | `tr-TR`     |
| `Language.VI_VN`  | Vietnamese (Vietnam) | `vi-VN`     |

You can set the language using the `language` parameter:

```python
from pipecat.transcriptions.language import Language
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiMultimodalLiveLLMService,
    InputParams
)

# Set language during initialization
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(language=Language.ES_ES)  # Spanish (Spain)
)
```

## Next Steps

### Examples

- [Foundational Example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/26a-gemini-multimodal-live-transcription.py)
  Basic implementation showing core features and transcription

- [Simple Chatbot](https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot)
  A client/server example showing how to build a Pipecat JS or React client that connects to a Gemini Live Pipecat bot.

### Learn More

Check out our [Gemini Multimodal Live Guide](/guides/features/gemini-multimodal-live) for detailed explanations and best practices.



================================================
FILE: server/services/s2s/openai.mdx
================================================
---
title: "OpenAI Realtime Beta"
description: "Real-time speech-to-speech service implementation using OpenAI’s Realtime Beta API"
---

`OpenAIRealtimeBetaLLMService` provides real-time, multimodal conversation capabilities using OpenAI's Realtime Beta API. It supports speech-to-speech interactions with integrated LLM processing, function calling, and advanced conversation management.

<CardGroup cols={2}>
  <Card title="Real-time Interaction" icon="bolt">
    Stream audio in real-time with minimal latency response times
  </Card>
  <Card title="Speech Processing" icon="waveform-lines">
    Built-in speech-to-text and text-to-speech capabilities with voice options
  </Card>
  <Card title="Advanced Turn Detection" icon="microphone">
    Multiple voice activity detection options including semantic turn detection
  </Card>
  <Card title="Powerful Function Calling" icon="code">
    Seamless support for calling external functions and APIs
  </Card>
</CardGroup>

## Installation

To use `OpenAIRealtimeBetaLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[openai]"
```

You'll also need to set up your OpenAI API key as an environment variable: `OPENAI_API_KEY`.

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your OpenAI API key
</ParamField>

<ParamField
  path="model"
  type="str"
  default="gpt-4o-realtime-preview-2025-06-03"
>
  The speech-to-speech model used for processing
</ParamField>

<ParamField
  path="base_url"
  type="str"
  default="wss://api.openai.com/v1/realtime"
>
  WebSocket endpoint URL
</ParamField>

<ParamField path="session_properties" type="SessionProperties">
  Configuration for the realtime session
</ParamField>

<ParamField path="start_audio_paused" type="bool" default="False">
  Whether to start with audio input paused
</ParamField>

<ParamField path="send_transcription_frames" type="bool" default="True">
  Whether to emit transcription frames
</ParamField>

### Session Properties

The `SessionProperties` object configures the behavior of the realtime session:

<ParamField path="modalities" type="List[Literal['text', 'audio']]" optional>
  The modalities to enable (default includes both text and audio)
</ParamField>

<ParamField path="instructions" type="str" optional>
  System instructions that guide the model's behavior
</ParamField>

```python
service = OpenAIRealtimeBetaLLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    session_properties=SessionProperties(
        instructions="You are a helpful assistant. Be concise and friendly."
    )
)
```

<ParamField path="voice" type="str" optional>
  Voice ID for text-to-speech (options: alloy, echo, fable, onyx, nova, shimmer)
</ParamField>

<ParamField
  path="input_audio_format"
  type="Literal['pcm16', 'g711_ulaw', 'g711_alaw']"
  optional
>
  Format of the input audio
</ParamField>

<ParamField
  path="output_audio_format"
  type="Literal['pcm16', 'g711_ulaw', 'g711_alaw']"
  optional
>
  Format of the output audio
</ParamField>

<ParamField
  path="input_audio_transcription"
  type="InputAudioTranscription"
  optional
>
  Configuration for audio transcription
</ParamField>

```python
from pipecat.services.openai_realtime_beta.events import InputAudioTranscription

service = OpenAIRealtimeBetaLLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    session_properties=SessionProperties(
        input_audio_transcription=InputAudioTranscription(
            model="gpt-4o-transcribe",
            language="en",
            prompt="This is a technical conversation about programming"
        )
    )
)
```

<ParamField
  path="input_audio_noise_reduction"
  type="InputAudioNoiseReduction"
  optional
>
  Configuration for audio noise reduction
</ParamField>

<ParamField
  path="turn_detection"
  type="Union[TurnDetection, SemanticTurnDetection, bool]"
  optional
>
  Configuration for turn detection (set to False to disable)
</ParamField>

<ParamField path="tools" type="List[Dict]" optional>
  List of function definitions for tool/function calling
</ParamField>

<ParamField
  path="tool_choice"
  type="Literal['auto', 'none', 'required']"
  optional
>
  Controls when the model calls functions
</ParamField>

<ParamField path="temperature" type="float" optional>
  Controls randomness in responses (0.0 to 2.0)
</ParamField>

<ParamField
  path="max_response_output_tokens"
  type="Union[int, Literal['inf']]"
  optional
>
  Maximum number of tokens to generate
</ParamField>

## Input Frames

### Audio Input

<ParamField path="InputAudioRawFrame" type="Frame">
  Raw audio data for speech input
</ParamField>

### Control Input

<ParamField path="StartInterruptionFrame" type="Frame">
  Signals start of user interruption
</ParamField>

<ParamField path="UserStartedSpeakingFrame" type="Frame">
  Signals user started speaking
</ParamField>

<ParamField path="UserStoppedSpeakingFrame" type="Frame">
  Signals user stopped speaking
</ParamField>

### Context Input

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Contains conversation context
</ParamField>

<ParamField path="LLMMessagesAppendFrame" type="Frame">
  Appends messages to conversation
</ParamField>

## Output Frames

### Audio Output

<ParamField path="TTSAudioRawFrame" type="Frame">
  Generated speech audio
</ParamField>

### Control Output

<ParamField path="TTSStartedFrame" type="Frame">
  Signals start of speech synthesis
</ParamField>

<ParamField path="TTSStoppedFrame" type="Frame">
  Signals end of speech synthesis
</ParamField>

### Text Output

<ParamField path="TextFrame" type="Frame">
  Generated text responses
</ParamField>

<ParamField path="TranscriptionFrame" type="Frame">
  Speech transcriptions
</ParamField>

## Events

<ParamField path="on_conversation_item_created" type="event">

Emitted when a conversation item on the server is created. Handler receives:

- `item_id: str`
- `item: ConversationItem`

</ParamField>

<ParamField path="on_conversation_item_updated" type="event">

Emitted when a conversation item on the server is updated. Handler receives:

- `item_id: str`
- `item: Optional[ConversationItem]` (may not exist for some updates)

</ParamField>

## Methods

<ResponseField name="retrieve_conversation_item" type="method">
  Retrieves a conversation item's details from the server.

```python
async def retrieve_conversation_item(self, item_id: str) -> ConversationItem
```

</ResponseField>

## Usage Example

```python
from pipecat.services.openai_realtime_beta import OpenAIRealtimeBetaLLMService
from pipecat.services.openai_realtime_beta.events import SessionProperties, TurnDetection

# Configure service
service = OpenAIRealtimeBetaLLMService(
    api_key="your-api-key",
    session_properties=SessionProperties(
        modalities=["audio", "text"],
        voice="alloy",
        turn_detection=TurnDetection(
            threshold=0.5,
            silence_duration_ms=800
        ),
        temperature=0.7
    )
)

# Use in pipeline
pipeline = Pipeline([
    audio_input,       # Produces InputAudioRawFrame
    service,           # Processes speech/generates responses
    audio_output       # Handles TTSAudioRawFrame
])
```

## Function Calling

The service supports function calling with automatic response handling:

```python
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.openai_realtime_beta import SessionProperties

# Define weather function using standardized schema
weather_function = FunctionSchema(
    name="get_weather",
    description="Get weather information",
    properties={
        "location": {"type": "string"}
    },
    required=["location"]
)

# Create tools schema
tools = ToolsSchema(standard_tools=[weather_function])

# Configure service with tools
llm = OpenAIRealtimeBetaLLMService(
    api_key="your-api-key",
    session_properties=SessionProperties(
        tools=tools,
        tool_choice="auto"
    )
)

llm.register_function("get_weather", fetch_weather_from_api)
```

See the [Function Calling guide](/guides/fundamentals/function-calling) for:

- Detailed implementation instructions
- Provider-specific function definitions
- Handler registration examples
- Control over function call behavior
- Complete usage examples

## Frame Flow

```mermaid
graph TD
    A[InputAudioRawFrame] --> B[OpenAIRealtimeBetaLLMService]
    B --> C[TranscriptionFrame]
    B --> H[TTSStartedFrame]
    B --> I[TTSAudioRawFrame]
    B --> J[TTSStoppedFrame]
    B --> K[ErrorFrame]
    B --> D[LLMFullResponseStartFrame]
    B --> E[Function Calls]
    B --> F[LLMFullResponseEndFrame]
    E --> G[Function Results]
    G --> B

```

## Metrics Support

The service collects comprehensive metrics:

- Token usage (prompt and completion)
- Processing duration
- Time to First Byte (TTFB)
- Audio processing metrics
- Function call metrics

## Advanced Features

### Turn Detection

```python
# Server-side basic VAD
turn_detection = TurnDetection(
    type="server_vad",
    threshold=0.5,
    prefix_padding_ms=300,
    silence_duration_ms=800
)

# Server-side semantic VAD
turn_detection = SemanticTurnDetection(
  type="semantic_vad",
  eagerness="auto", # default. could also be "low" | "medium" | "high"
  create_response=True # default
  interrupt_response=True # default
)

# Disable turn detection
turn_detection = False
```

### Context Management

```python
# Create context
context = OpenAIRealtimeLLMContext(
    messages=[],
    tools=[],
    system="You are a helpful assistant"
)

# Create aggregators
aggregators = service.create_context_aggregator(context)
```

## Foundational Examples

<Card
  title="OpenAI Realtime Beta Example"
  icon="code"
  href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/19-openai-realtime-beta.py"
>
  Basic implementation showing core realtime features including audio streaming,
  turn detection, and function calling.
</Card>

## Notes

- Supports real-time speech-to-speech conversation
- Handles interruptions and turn-taking
- Manages WebSocket connection lifecycle
- Provides function calling capabilities
- Supports conversation context management
- Includes comprehensive error handling
- Manages audio streaming and processing
- Handles both text and audio modalities



================================================
FILE: server/services/serializers/exotel.mdx
================================================
---
title: "ExotelFrameSerializer"
description: "Serializer for Exotel WebSocket media streaming protocol"
---

## Overview

`ExotelFrameSerializer` enables integration with Exotel's WebSocket media streaming protocol, allowing your Pipecat application to handle phone calls via Exotel's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Exotel
- DTMF (touch-tone) event handling

## Installation

The `ExotelFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_id" type="str" required>
  The Stream ID for Exotel
</ParamField>

<ParamField path="call_sid" type="Optional[str]" default="None">
  The associated Exotel Call SID.
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="exotel_sample_rate" type="int" default="8000">
  Sample rate used by Exotel (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

## Basic Usage

```python
from pipecat.serializers.exotel import ExotelFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Exotel WebSocket connection
stream_id = call_data["stream_id"]
call_sid = call_data["start"]["call_sid"]

# Create serializer
serializer = ExotelFrameSerializer(
    stream_id=stream_id,
    call_sid,
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Server Code Example

Here's a complete example of handling a Exotel WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.exotel import ExotelFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial messages from Exotel
    start_data = websocket.iter_text()
    await start_data.__anext__()  # Skip first message

    # Parse the second message to get call details
    call_data = json.loads(await start_data.__anext__())

    # Extract Exotel-specific IDs and encoding
    stream_id = call_data["stream_id"]
    call_sid = call_data["start"]["call_sid"]

    # Create serializer with API key for auto hang-up
    serializer = ExotelFrameSerializer(
        stream_id=stream_id,
        call_sid=call_sid,
    )

    # Continue with transport and pipeline setup...
```



================================================
FILE: server/services/serializers/introduction.mdx
================================================
---
title: "Frame Serializer Overview"
description: "Overview of frame serializers for converting between Pipecat frames and external protocols"
---

## Overview

Frame serializers are components that convert between Pipecat's internal frame format and external protocols or formats. They're essential when integrating with third-party services or APIs that have their own message formats.

## Core Responsibilities

Serializers handle:

1. **Serialization**: Converting Pipecat frames to external formats or protocols
2. **Deserialization**: Converting external messages to Pipecat frames
3. **Protocol-specific behaviors**: Managing unique aspects of each integration

## Available Serializers

Pipecat includes serializers for popular voice and communications platforms:

<CardGroup cols={2}>
  <Card
    title="Exotel Serializer"
    icon="phone"
    href="/server/services/serializers/exotel"
  >
    For integrating with Exotel WebSocket media streaming
  </Card>
  <Card
    title="Plivo Serializer"
    icon="phone"
    href="/server/services/serializers/plivo"
  >
    For integrating with Telnyx WebSocket media streaming
  </Card>
  <Card
    title="Telnyx Serializer"
    icon="phone"
    href="/server/services/serializers/telnyx"
  >
    For integrating with Telnyx WebSocket media streaming
  </Card>
  <Card
    title="Twilio Serializer"
    icon="phone"
    href="/server/services/serializers/twilio"
  >
    For integrating with Twilio Media Streams WebSocket protocol
  </Card>
</CardGroup>

## Custom Serializers

You can create custom serializers by implementing the `FrameSerializer` base class:

```python
from pipecat.serializers.base_serializer import FrameSerializer, FrameSerializerType
from pipecat.frames.frames import Frame, StartFrame

class MyCustomSerializer(FrameSerializer):
    @property
    def type(self) -> FrameSerializerType:
        return FrameSerializerType.TEXT  # or BINARY

    async def setup(self, frame: StartFrame):
        # Initialize with pipeline configuration
        pass

    async def serialize(self, frame: Frame) -> str | bytes | None:
        # Convert Pipecat frame to external format
        pass

    async def deserialize(self, data: str | bytes) -> Frame | None:
        # Convert external data to Pipecat frame
        pass
```



================================================
FILE: server/services/serializers/plivo.mdx
================================================
---
title: "PlivoFrameSerializer"
description: "Serializer for Plivo Audio Streaming WebSocket protocol"
---

## Overview

`PlivoFrameSerializer` enables integration with Plivo's Audio Streaming WebSocket protocol, allowing your Pipecat application to handle phone calls via Plivo's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Plivo
- DTMF (touch-tone) event handling
- Automatic call termination via Plivo's REST API
- μ-law audio encoding/decoding

## Installation

The `PlivoFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_id" type="str" required>
  The Plivo Stream ID
</ParamField>

<ParamField path="call_id" type="Optional[str]" default="None">
  The associated Plivo Call ID (required for auto hang-up)
</ParamField>

<ParamField path="auth_id" type="Optional[str]" default="None">
  Plivo auth ID (required for auto hang-up)
</ParamField>

<ParamField path="auth_token" type="Optional[str]" default="None">
  Plivo auth token (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="plivo_sample_rate" type="int" default="8000">
  Sample rate used by Plivo (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.plivo import PlivoFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Plivo WebSocket connection
stream_id = start_message["start"]["streamId"]
call_id = start_message["start"]["callId"]

# Create serializer
serializer = PlivoFrameSerializer(
    stream_id=stream_id,
    call_id=call_id,
    auth_id="your_plivo_auth_id",
    auth_token="your_plivo_auth_token"
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Plivo call when an `EndFrame` or `CancelFrame` is processed, using Plivo's REST API:

```python
# Properly configured with hang-up support
serializer = PlivoFrameSerializer(
    stream_id=stream_id,
    call_id=call_id,                             # Required for auto hang-up
    auth_id=os.getenv("PLIVO_AUTH_ID"),          # Required for auto hang-up
    auth_token=os.getenv("PLIVO_AUTH_TOKEN"),    # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Plivo WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.plivo import PlivoFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read the start message from Plivo
    start_data = websocket.iter_text()
    start_message = json.loads(await start_data.__anext__())

    # Extract Plivo-specific IDs from the start event
    start_info = start_message.get("start", {})
    stream_id = start_info.get("streamId")
    call_id = start_info.get("callId")

    # Create serializer with authentication for auto hang-up
    serializer = PlivoFrameSerializer(
        stream_id=stream_id,
        call_id=call_id,
        auth_id=os.getenv("PLIVO_AUTH_ID"),
        auth_token=os.getenv("PLIVO_AUTH_TOKEN"),
    )

    # Continue with transport and pipeline setup...
```

## Plivo XML Configuration

To enable audio streaming with Plivo, you'll need to configure your Plivo application to return appropriate XML:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Stream
    keepCallAlive="true"
    bidirectional="true"
    contentType="audio/x-mulaw;rate=8000"
  >
    wss://your-websocket-url/ws
  </Stream>
</Response>
```

<Note>
  The `bidirectional="true"` attribute is required for two-way audio
  communication, and `keepCallAlive="true"` prevents the call from being
  disconnected after XML execution.
</Note>

## Key Differences from Twilio

- **Stream Identifier**: Plivo uses `streamId` instead of `streamSid`
- **Call Identifier**: Plivo uses `callId` instead of `callSid`
- **XML Structure**: Plivo uses `<Stream>` element directly instead of `<Connect><Stream>`
- **Authentication**: Plivo uses Auth ID and Auth Token instead of Account SID and Auth Token

<Note>
  See the [Plivo Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/plivo-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/services/serializers/telnyx.mdx
================================================
---
title: "TelnyxFrameSerializer"
description: "Serializer for Telnyx WebSocket media streaming protocol"
---

## Overview

`TelnyxFrameSerializer` enables integration with Telnyx's WebSocket media streaming protocol, allowing your Pipecat application to handle phone calls via Telnyx's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Telnyx
- DTMF (touch-tone) event handling
- Automatic call termination via Telnyx's REST API
- Support for multiple audio encodings (PCMU, PCMA)

## Installation

The `TelnyxFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_id" type="str" required>
  The Stream ID for Telnyx
</ParamField>

<ParamField path="outbound_encoding" type="str" required>
  The encoding type for outbound audio (e.g., "PCMU", "PCMA")
</ParamField>

<ParamField path="inbound_encoding" type="str" required>
  The encoding type for inbound audio (e.g., "PCMU", "PCMA")
</ParamField>

<ParamField path="call_control_id" type="Optional[str]" default="None">
  The Call Control ID for the Telnyx call (required for auto hang-up)
</ParamField>

<ParamField path="api_key" type="Optional[str]" default="None">
  Your Telnyx API key (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="telnyx_sample_rate" type="int" default="8000">
  Sample rate used by Telnyx (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="inbound_encoding" type="str" default="PCMU">
  Audio encoding for data sent to Telnyx
</ParamField>

<ParamField path="outbound_encoding" type="str" default="PCMU">
  Audio encoding for data received from Telnyx
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.telnyx import TelnyxFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Telnyx WebSocket connection
stream_id = call_data["stream_id"]
call_control_id = call_data["start"]["call_control_id"]
outbound_encoding = call_data["start"]["media_format"]["encoding"]

# Create serializer
serializer = TelnyxFrameSerializer(
    stream_id=stream_id,
    outbound_encoding=outbound_encoding,
    inbound_encoding="PCMU",
    call_control_id=call_control_id,
    api_key=os.getenv("TELNYX_API_KEY")
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Telnyx call when an `EndFrame` or `CancelFrame` is processed, using Telnyx's REST API:

```python
# Properly configured with hang-up support
serializer = TelnyxFrameSerializer(
    stream_id=stream_id,
    outbound_encoding=outbound_encoding,
    inbound_encoding="PCMU",
    call_control_id=call_control_id,    # Required for auto hang-up
    api_key=os.getenv("TELNYX_API_KEY") # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Telnyx WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.telnyx import TelnyxFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial messages from Telnyx
    start_data = websocket.iter_text()
    await start_data.__anext__()  # Skip first message

    # Parse the second message to get call details
    call_data = json.loads(await start_data.__anext__())

    # Extract Telnyx-specific IDs and encoding
    stream_id = call_data["stream_id"]
    call_control_id = call_data["start"]["call_control_id"]
    outbound_encoding = call_data["start"]["media_format"]["encoding"]

    # Create serializer with API key for auto hang-up
    serializer = TelnyxFrameSerializer(
        stream_id=stream_id,
        outbound_encoding=outbound_encoding,
        inbound_encoding="PCMU",
        call_control_id=call_control_id,
        api_key=os.getenv("TELNYX_API_KEY"),
    )

    # Continue with transport and pipeline setup...
```

<Note>
  See the [Telnyx Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/telnyx-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/services/serializers/twilio.mdx
================================================
---
title: "TwilioFrameSerializer"
description: "Serializer for Twilio Media Streams WebSocket protocol"
---

## Overview

`TwilioFrameSerializer` enables integration with Twilio's Media Streams WebSocket protocol, allowing your Pipecat application to handle phone calls via Twilio's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Twilio
- DTMF (touch-tone) event handling
- Automatic call termination via Twilio's REST API
- μ-law audio encoding/decoding

## Installation

The `TwilioFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_sid" type="str" required>
  The Twilio Media Stream SID
</ParamField>

<ParamField path="call_sid" type="Optional[str]" default="None">
  The associated Twilio Call SID (required for auto hang-up)
</ParamField>

<ParamField path="account_sid" type="Optional[str]" default="None">
  Twilio account SID (required for auto hang-up)
</ParamField>

<ParamField path="auth_token" type="Optional[str]" default="None">
  Twilio auth token (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="twilio_sample_rate" type="int" default="8000">
  Sample rate used by Twilio (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Twilio WebSocket connection
stream_sid = call_data["start"]["streamSid"]
call_sid = call_data["start"]["callSid"]

# Create serializer
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,
    account_sid="your_twilio_account_sid",
    auth_token="your_twilio_auth_token"
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Twilio call when an `EndFrame` or `CancelFrame` is processed, using Twilio's REST API:

```python
# Properly configured with hang-up support
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,              # Required for auto hang-up
    account_sid=os.getenv("TWILIO_ACCOUNT_SID"),  # Required for auto hang-up
    auth_token=os.getenv("TWILIO_AUTH_TOKEN"),    # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Twilio WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.twilio import TwilioFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial messages from Twilio
    start_data = websocket.iter_text()
    await start_data.__anext__()  # Skip first message

    # Parse the second message to get call details
    call_data = json.loads(await start_data.__anext__())

    # Extract Twilio-specific IDs
    stream_sid = call_data["start"]["streamSid"]
    call_sid = call_data["start"]["callSid"]

    # Create serializer with authentication for auto hang-up
    serializer = TwilioFrameSerializer(
        stream_sid=stream_sid,
        call_sid=call_sid,
        account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN"),
    )

    # Continue with transport and pipeline setup...
```

<Note>
  See the [Twilio Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/services/stt/assemblyai.mdx
================================================
---
title: "AssemblyAI"
description: "Speech-to-text service implementation using AssemblyAI’s real-time transcription API"
---

## Overview

`AssemblyAISTTService` provides real-time speech recognition using AssemblyAI's WebSocket API with support for interim results, end-of-turn detection, and configurable audio processing parameters.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.assemblyai.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="AssemblyAI Docs"
    icon="book"
    href="https://www.assemblyai.com/docs/api-reference/overview"
  >
    Official AssemblyAI documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07o-interruptible-assemblyai.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>

## Installation

To use AssemblyAI services, install the required dependency:

```bash
pip install "pipecat-ai[assemblyai]"
```

You'll also need to set up your AssemblyAI API key as an environment variable: `ASSEMBLYAI_API_KEY`.

<Tip>
  Get your API key from [AssemblyAI
  Console](https://www.assemblyai.com/dashboard/signup).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)
- `UserStartedSpeakingFrame` - VAD start signal (triggers TTFB metrics)
- `UserStoppedSpeakingFrame` - VAD stop signal (triggers force endpoint if enabled)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results
- `TranslationFrame` - Translated text (if translation is enabled)
- `ErrorFrame` - Connection or processing errors

## Language Support

AssemblyAI Streaming STT currently supports English only.

## Usage Example

### Basic Configuration

Initialize the `AssemblyAISTTService` and use it in a pipeline:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService
from pipecat.services.assemblyai.models import AssemblyAIConnectionParams

# Basic configuration
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
)

# Configuration with custom parameters
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    connection_params=AssemblyAIConnectionParams(
        sample_rate=16000,
        formatted_finals=True,
        end_of_turn_confidence_threshold=0.8,
        max_turn_silence=1000
    ),
    vad_force_turn_endpoint=True
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `AssemblyAISTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from speech start to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Connection Management**: Automatically handles WebSocket connections, reconnections, and proper termination handshakes
- **VAD Integration**: Supports force endpoint triggering when VAD detects speech stop, requiring a VAD processor in the pipeline
- **Error Handling**: Error handling for connection issues and message processing failures



================================================
FILE: server/services/stt/aws.mdx
================================================
---
title: "AWS Transcribe"
description: "Speech-to-text service implementation using Amazon Transcribe’s real-time transcription API"
---

## Overview

`AWSTranscribeSTTService` provides real-time speech recognition using Amazon Transcribe's WebSocket streaming API with support for interim results, multiple languages, and configurable audio processing parameters.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.aws.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="AWS Transcribe Docs"
    icon="book"
    href="https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html"
  >
    Official AWS Transcribe documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07m-interruptible-aws.py"
  >
    Working example with AWS services integration
  </Card>
</CardGroup>

## Installation

To use AWS Transcribe services, install the required dependency:

```bash
pip install "pipecat-ai[aws]"
```

You'll also need to set up your AWS credentials as environment variables:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (if using temporary credentials)
- `AWS_REGION` (defaults to "us-east-1")

<Tip>
  Get your AWS credentials by setting up an IAM user with Amazon Transcribe
  access in your [AWS Console](https://console.aws.amazon.com/).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 8kHz or 16kHz, mono)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - Connection or processing errors

## Language Support

AWS Transcribe supports multiple languages with regional variants:

| Language Code | Description         | Service Codes |
| ------------- | ------------------- | ------------- |
| `Language.EN` | English (US)        | `en-US`       |
| `Language.ES` | Spanish             | `es-US`       |
| `Language.FR` | French              | `fr-FR`       |
| `Language.DE` | German              | `de-DE`       |
| `Language.IT` | Italian             | `it-IT`       |
| `Language.PT` | Portuguese (Brazil) | `pt-BR`       |
| `Language.JA` | Japanese            | `ja-JP`       |
| `Language.KO` | Korean              | `ko-KR`       |
| `Language.ZH` | Chinese (Mandarin)  | `zh-CN`       |
| `Language.PL` | Polish              | `pl-PL`       |

<Note>
  AWS Transcribe supports additional languages and regional variants. See the
  [AWS
  documentation](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html)
  for a complete list.
</Note>

## Usage Example

### Basic Configuration

Initialize the `AWSTranscribeSTTService` and use it in a pipeline:

```python
from pipecat.services.aws.stt import AWSTranscribeSTTService
from pipecat.transcriptions.language import Language

# Configuration with explicit credentials
stt = AWSTranscribeSTTService(
    aws_access_key_id="YOUR_ACCESS_KEY_ID",
    api_key="YOUR_SECRET_ACCESS_KEY",
    aws_session_token="YOUR_SESSION_TOKEN",  # If using temporary credentials
    region="us-west-2",
    language=Language.FR
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `AWSTranscribeSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Sample Rate**: Supports 8kHz and 16kHz sample rates. Other rates are automatically resampled to 16kHz.
- **Connection Management**: Handles WebSocket connections with automatic reconnection and proper connection state management.
- **Credentials**: Supports both environment variables and explicit credential parameters for flexible deployment
- **Error Handling**: Comprehensive error handling with graceful degradation and connection recovery



================================================
FILE: server/services/stt/azure.mdx
================================================
---
title: "Azure"
description: "Speech-to-text service using Azure Cognitive Services Speech SDK"
---

## Overview

`AzureSTTService` provides real-time speech recognition using Azure's Cognitive Services Speech SDK with support for continuous recognition, extensive language support, and configurable audio processing.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Azure Speech Docs"
    icon="book"
    href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-to-text"
  >
    Official Azure Speech Service documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07f-interruptible-azure.py"
  >
    Working example with Azure services integration
  </Card>
</CardGroup>

## Installation

To use Azure Speech services, install the required dependency:

```bash
pip install "pipecat-ai[azure]"
```

You'll also need to set up your Azure credentials as environment variables:

- `AZURE_API_KEY` (or `AZURE_SPEECH_API_KEY`)
- `AZURE_REGION` (or `AZURE_SPEECH_REGION`)

<Tip>
  Get your API key and region from the [Azure Portal](https://portal.azure.com/)
  by creating a Speech Services resource.
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (configurable sample rate, mono or stereo)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - Connection or processing errors

## Language Support

Azure Speech STT supports extensive language coverage with regional variants:

<Accordion title="View All Supported Languages">

| Language Code    | Description           | Service Codes |
| ---------------- | --------------------- | ------------- |
| `Language.AF`    | Afrikaans             | `af-ZA`       |
| `Language.AM`    | Amharic               | `am-ET`       |
| `Language.AR`    | Arabic (UAE)          | `ar-AE`       |
| `Language.AR_SA` | Arabic (Saudi Arabia) | `ar-SA`       |
| `Language.AR_EG` | Arabic (Egypt)        | `ar-EG`       |
| `Language.AS`    | Assamese              | `as-IN`       |
| `Language.AZ`    | Azerbaijani           | `az-AZ`       |
| `Language.BG`    | Bulgarian             | `bg-BG`       |
| `Language.BN`    | Bengali               | `bn-IN`       |
| `Language.BS`    | Bosnian               | `bs-BA`       |
| `Language.CA`    | Catalan               | `ca-ES`       |
| `Language.CS`    | Czech                 | `cs-CZ`       |
| `Language.CY`    | Welsh                 | `cy-GB`       |
| `Language.DA`    | Danish                | `da-DK`       |
| `Language.DE`    | German                | `de-DE`       |
| `Language.DE_AT` | German (Austria)      | `de-AT`       |
| `Language.DE_CH` | German (Switzerland)  | `de-CH`       |
| `Language.EL`    | Greek                 | `el-GR`       |
| `Language.EN`    | English (US)          | `en-US`       |
| `Language.EN_AU` | English (Australia)   | `en-AU`       |
| `Language.EN_CA` | English (Canada)      | `en-CA`       |
| `Language.EN_GB` | English (UK)          | `en-GB`       |
| `Language.EN_IN` | English (India)       | `en-IN`       |
| `Language.ES`    | Spanish (Spain)       | `es-ES`       |
| `Language.ES_MX` | Spanish (Mexico)      | `es-MX`       |
| `Language.ES_US` | Spanish (US)          | `es-US`       |
| `Language.ET`    | Estonian              | `et-EE`       |
| `Language.EU`    | Basque                | `eu-ES`       |
| `Language.FA`    | Persian               | `fa-IR`       |
| `Language.FI`    | Finnish               | `fi-FI`       |
| `Language.FIL`   | Filipino              | `fil-PH`      |
| `Language.FR`    | French                | `fr-FR`       |
| `Language.FR_CA` | French (Canada)       | `fr-CA`       |
| `Language.GA`    | Irish                 | `ga-IE`       |
| `Language.GL`    | Galician              | `gl-ES`       |
| `Language.GU`    | Gujarati              | `gu-IN`       |
| `Language.HE`    | Hebrew                | `he-IL`       |
| `Language.HI`    | Hindi                 | `hi-IN`       |
| `Language.HR`    | Croatian              | `hr-HR`       |
| `Language.HU`    | Hungarian             | `hu-HU`       |
| `Language.HY`    | Armenian              | `hy-AM`       |
| `Language.ID`    | Indonesian            | `id-ID`       |
| `Language.IS`    | Icelandic             | `is-IS`       |
| `Language.IT`    | Italian               | `it-IT`       |
| `Language.JA`    | Japanese              | `ja-JP`       |
| `Language.JV`    | Javanese              | `jv-ID`       |
| `Language.KA`    | Georgian              | `ka-GE`       |
| `Language.KK`    | Kazakh                | `kk-KZ`       |
| `Language.KM`    | Khmer                 | `km-KH`       |
| `Language.KN`    | Kannada               | `kn-IN`       |
| `Language.KO`    | Korean                | `ko-KR`       |
| `Language.LO`    | Lao                   | `lo-LA`       |
| `Language.LT`    | Lithuanian            | `lt-LT`       |
| `Language.LV`    | Latvian               | `lv-LV`       |
| `Language.MK`    | Macedonian            | `mk-MK`       |
| `Language.ML`    | Malayalam             | `ml-IN`       |
| `Language.MN`    | Mongolian             | `mn-MN`       |
| `Language.MR`    | Marathi               | `mr-IN`       |
| `Language.MS`    | Malay                 | `ms-MY`       |
| `Language.MT`    | Maltese               | `mt-MT`       |
| `Language.MY`    | Burmese               | `my-MM`       |
| `Language.NB`    | Norwegian             | `nb-NO`       |
| `Language.NE`    | Nepali                | `ne-NP`       |
| `Language.NL`    | Dutch                 | `nl-NL`       |
| `Language.OR`    | Odia                  | `or-IN`       |
| `Language.PA`    | Punjabi               | `pa-IN`       |
| `Language.PL`    | Polish                | `pl-PL`       |
| `Language.PS`    | Pashto                | `ps-AF`       |
| `Language.PT`    | Portuguese            | `pt-PT`       |
| `Language.PT_BR` | Portuguese (Brazil)   | `pt-BR`       |
| `Language.RO`    | Romanian              | `ro-RO`       |
| `Language.RU`    | Russian               | `ru-RU`       |
| `Language.SI`    | Sinhala               | `si-LK`       |
| `Language.SK`    | Slovak                | `sk-SK`       |
| `Language.SL`    | Slovenian             | `sl-SI`       |
| `Language.SO`    | Somali                | `so-SO`       |
| `Language.SQ`    | Albanian              | `sq-AL`       |
| `Language.SR`    | Serbian               | `sr-RS`       |
| `Language.SU`    | Sundanese             | `su-ID`       |
| `Language.SV`    | Swedish               | `sv-SE`       |
| `Language.SW`    | Swahili               | `sw-KE`       |
| `Language.TA`    | Tamil                 | `ta-IN`       |
| `Language.TE`    | Telugu                | `te-IN`       |
| `Language.TH`    | Thai                  | `th-TH`       |
| `Language.TR`    | Turkish               | `tr-TR`       |
| `Language.UK`    | Ukrainian             | `uk-UA`       |
| `Language.UR`    | Urdu                  | `ur-IN`       |
| `Language.UZ`    | Uzbek                 | `uz-UZ`       |
| `Language.VI`    | Vietnamese            | `vi-VN`       |
| `Language.ZH`    | Chinese (Mandarin)    | `zh-CN`       |
| `Language.ZH_HK` | Chinese (Hong Kong)   | `zh-HK`       |
| `Language.ZH_TW` | Chinese (Taiwan)      | `zh-TW`       |
| `Language.ZU`    | Zulu                  | `zu-ZA`       |

</Accordion>

Common languages:

- `Language.EN_US` - English (US) - `en-US`
- `Language.ES` - Spanish - `es-ES`
- `Language.FR` - French - `fr-FR`
- `Language.DE` - German - `de-DE`
- `Language.IT` - Italian - `it-IT`
- `Language.JA` - Japanese - `ja-JP`

## Usage Example

### Basic Configuration

Initialize the `AzureSTTService` and use it in a pipeline:

```python
from pipecat.services.azure.stt import AzureSTTService
from pipecat.transcriptions.language import Language

# Basic configuration
stt = AzureSTTService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region=os.getenv("AZURE_SPEECH_REGION"),
    language=Language.EN_US,
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `AzureSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Continuous Recognition**: Uses Azure's continuous recognition mode for real-time processing
- **Audio Flexibility**: Supports configurable sample rates and both mono/stereo input
- **Resource Management**: Automatic cleanup of Azure speech recognizer and audio streams
- **Threading**: Thread-safe operation with proper async event loop handling using `asyncio.run_coroutine_threadsafe`
- **Regional Support**: Requires Azure region specification for optimal performance and compliance
- **Connection Management**: Handles Azure SDK connection lifecycle with proper start/stop/cancel operations



================================================
FILE: server/services/stt/cartesia.mdx
================================================
---
title: "Cartesia"
description: "Speech-to-text service implementation using Cartesia’s real-time transcription API"
---

## Overview

`CartesiaSTTService` provides real-time speech recognition using Cartesia's WebSocket API with the `ink-whisper` model, supporting streaming transcription with both interim and final results.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.cartesia.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Cartesia Docs"
    icon="book"
    href="https://docs.cartesia.ai/api-reference/stt/stt"
  >
    Official Cartesia STT documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/13f-cartesia-transcription.py"
  >
    Working example with transcription logging
  </Card>
</CardGroup>

## Installation

To use Cartesia services, install the required dependency:

```bash
pip install "pipecat-ai[cartesia]"
```

You'll also need to set up your Cartesia API key as an environment variable: `CARTESIA_API_KEY`.

<Tip>Get your API key from [Cartesia](https://cartesia.ai/).</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)
- `UserStartedSpeakingFrame` - Triggers metrics collection
- `UserStoppedSpeakingFrame` - Sends finalize command to flush session
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - Connection or processing errors

## Models

Cartesia currently offers one primary STT model:

| Model         | Description                                 | Best For                                |
| ------------- | ------------------------------------------- | --------------------------------------- |
| `ink-whisper` | Cartesia's optimized Whisper implementation | General-purpose real-time transcription |

## Language Support

Cartesia STT supports multiple languages through standard language codes:

| Language Code | Description  | Service Codes |
| ------------- | ------------ | ------------- |
| `Language.EN` | English (US) | `en`          |
| `Language.ES` | Spanish      | `es`          |
| `Language.FR` | French       | `fr`          |
| `Language.DE` | German       | `de`          |
| `Language.IT` | Italian      | `it`          |
| `Language.PT` | Portuguese   | `pt`          |
| `Language.NL` | Dutch        | `nl`          |
| `Language.PL` | Polish       | `pl`          |
| `Language.RU` | Russian      | `ru`          |
| `Language.JA` | Japanese     | `ja`          |
| `Language.KO` | Korean       | `ko`          |
| `Language.ZH` | Chinese      | `zh`          |

<Note>
  Language support may vary. Check [Cartesia's
  documentation](https://docs.cartesia.ai/api-reference/stt/stt) for the most
  current language list.
</Note>

## Usage Example

### Basic Configuration

Initialize the `CartesiaSTTService` and use it in a pipeline:

```python
from pipecat.services.cartesia.stt import CartesiaSTTService

# Simple setup with defaults
stt = CartesiaSTTService(
    api_key=os.getenv("CARTESIA_API_KEY")
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `CartesiaSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

### Live Options Configuration

```python
from pipecat.services.cartesia.stt import CartesiaSTTService, CartesiaLiveOptions
from pipecat.transcriptions.language import Language

# Custom configuration with live options
live_options = CartesiaLiveOptions(
    model="ink-whisper",
    language=Language.ES,  # Spanish transcription
)

stt = CartesiaSTTService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    base_url="api.cartesia.ai",  # Custom endpoint if needed
    live_options=live_options
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Audio Format**: Expects PCM S16LE format at 16kHz sample rate by default
- **Session Management**: Each connection represents a transcription session that can be finalized
- **Interim Results**: Provides real-time interim transcriptions before final results
- **Language Detection**: Automatic language detection available in transcription responses



================================================
FILE: server/services/stt/deepgram.mdx
================================================
---
title: "Deepgram"
description: "Speech-to-text service implementation using Deepgram’s real-time transcription API"
---

## Overview

`DeepgramSTTService` provides real-time speech recognition using Deepgram's WebSocket API with support for interim results, language detection, and voice activity detection (VAD).

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.deepgram.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Deepgram Docs"
    icon="book"
    href="https://developers.deepgram.com/docs/pre-recorded-audio"
  >
    Official Deepgram documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07c-interruptible-deepgram.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>

## Installation

To use `DeepgramSTTService`, install the required dependencies:

```bash
pip install "pipecat-ai[deepgram]"
```

You'll also need to set up your Deepgram API key as an environment variable: `DEEPGRAM_API_KEY`.

<Tip>
  Get your API key from the [Deepgram
  Console](https://console.deepgram.com/signup).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)
- `UserStartedSpeakingFrame` - Triggers metrics collection
- `UserStoppedSpeakingFrame` - Sends finalize command to flush session
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - Connection or processing errors

## Models

Deepgram offers several models optimized for different use cases. Popular models include:

| Model              | Best For                       | Features                          |
| ------------------ | ------------------------------ | --------------------------------- |
| `nova-3-general`   | General purpose, meetings      | Latest accuracy, punctuation      |
| `nova-2-general`   | General purpose, meetings      | Latest accuracy, punctuation      |
| `nova-2-phonecall` | Phone calls, low quality audio | Noise robust, telephony optimized |

See [Deepgram's model docs](https://developers.deepgram.com/docs/models-languages-overview) for detailed performance metrics.

## Language Support

Deepgram STT supports the following languages and regional variants:

| Language Code    | Description                      | Service Codes            |
| ---------------- | -------------------------------- | ------------------------ |
| `Language.BG`    | Bulgarian                        | `bg`                     |
| `Language.CA`    | Catalan                          | `ca`                     |
| `Language.ZH`    | Chinese (Mandarin, Simplified)   | `zh`, `zh-CN`, `zh-Hans` |
| `Language.ZH_TW` | Chinese (Mandarin, Traditional)  | `zh-TW`, `zh-Hant`       |
| `Language.ZH_HK` | Chinese (Cantonese, Traditional) | `zh-HK`                  |
| `Language.CS`    | Czech                            | `cs`                     |
| `Language.DA`    | Danish                           | `da`, `da-DK`            |
| `Language.NL`    | Dutch                            | `nl`                     |
| `Language.NL_BE` | Dutch (Flemish)                  | `nl-BE`                  |
| `Language.EN`    | English                          | `en`                     |
| `Language.EN_US` | English (US)                     | `en-US`                  |
| `Language.EN_AU` | English (Australia)              | `en-AU`                  |
| `Language.EN_GB` | English (UK)                     | `en-GB`                  |
| `Language.EN_NZ` | English (New Zealand)            | `en-NZ`                  |
| `Language.EN_IN` | English (India)                  | `en-IN`                  |
| `Language.ET`    | Estonian                         | `et`                     |
| `Language.FI`    | Finnish                          | `fi`                     |
| `Language.FR`    | French                           | `fr`                     |
| `Language.FR_CA` | French (Canada)                  | `fr-CA`                  |
| `Language.DE`    | German                           | `de`                     |
| `Language.DE_CH` | German (Switzerland)             | `de-CH`                  |
| `Language.EL`    | Greek                            | `el`                     |
| `Language.HI`    | Hindi                            | `hi`                     |
| `Language.HU`    | Hungarian                        | `hu`                     |
| `Language.ID`    | Indonesian                       | `id`                     |
| `Language.IT`    | Italian                          | `it`                     |
| `Language.JA`    | Japanese                         | `ja`                     |
| `Language.KO`    | Korean                           | `ko`, `ko-KR`            |
| `Language.LV`    | Latvian                          | `lv`                     |
| `Language.LT`    | Lithuanian                       | `lt`                     |
| `Language.MS`    | Malay                            | `ms`                     |
| `Language.NO`    | Norwegian                        | `no`                     |
| `Language.PL`    | Polish                           | `pl`                     |
| `Language.PT`    | Portuguese                       | `pt`                     |
| `Language.PT_BR` | Portuguese (Brazil)              | `pt-BR`                  |
| `Language.PT_PT` | Portuguese (Portugal)            | `pt-PT`                  |
| `Language.RO`    | Romanian                         | `ro`                     |
| `Language.RU`    | Russian                          | `ru`                     |
| `Language.SK`    | Slovak                           | `sk`                     |
| `Language.ES`    | Spanish                          | `es`, `es-419`           |
| `Language.SV`    | Swedish                          | `sv`, `sv-SE`            |
| `Language.TH`    | Thai                             | `th`, `th-TH`            |
| `Language.TR`    | Turkish                          | `tr`                     |
| `Language.UK`    | Ukrainian                        | `uk`                     |
| `Language.VI`    | Vietnamese                       | `vi`                     |

## Usage Example

### Basic Configuration

Initialize the `DeepgramSTTService` and use it in a pipeline:

```python
from pipecat.services.deepgram.stt import DeepgramSTTService, LiveOptions
from pipecat.transcriptions.language import Language

# Configure service
stt = DeepgramSTTService(
    api_key="your-api-key",
    live_options=LiveOptions(
        model="nova-3-general",
        language=Language.EN,
        smart_format=True
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Update settings dynamically by pushing an `STTUpdateSettingsFrame`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Connection Management**: Automatically handles WebSocket connections and reconnections
- **VAD Integration**: Supports Deepgram's built-in VAD, though we recommend using local VAD services like [Silero](/server/utilities/audio/silero-vad-analyzer) for better performance
- **Sample Rate**: Can be configured per service, but we recommend setting it globally in `PipelineParams` for consistency across services



================================================
FILE: server/services/stt/fal.mdx
================================================
---
title: "Fal (Wizper)"
description: "Speech-to-text service implementation using Fal’s Wizper API"
---

## Overview

`FalSTTService` provides speech-to-text capabilities using Fal's Wizper API with Voice Activity Detection (VAD) to process only speech segments, optimizing API usage and improving response time.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.fal.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card title="Fal Docs" icon="book" href="https://fal.ai/models/fal-ai/wizper">
    Official Fal Wizper documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07w-interruptible-fal.py"
  >
    Working example with VAD integration
  </Card>
</CardGroup>

## Installation

To use Fal services, install the required dependency:

```bash
pip install "pipecat-ai[fal]"
```

You'll also need to set up your Fal API key as an environment variable: `FAL_KEY`.

<Tip>Get your API key from the [Fal platform](https://fal.ai/).</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `UserStartedSpeakingFrame` - VAD detection of speech start
- `UserStoppedSpeakingFrame` - VAD detection of speech end (triggers processing)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `TranscriptionFrame` - Final transcription results after speech segment ends
- `ErrorFrame` - API or processing errors

## Models

Fal offers the Wizper model with version options:

| Model    | Version | Description                        |
| -------- | ------- | ---------------------------------- |
| `wizper` | `3`     | Latest Wizper model (default)      |
| `wizper` | `2`     | Previous version for compatibility |

## VAD-Based Processing

`FalSTTService` extends `SegmentedSTTService`, which uses Voice Activity Detection to process complete speech segments:

- **Segment Processing**: Only processes complete utterances, not continuous audio
- **Audio Buffering**: Maintains a 1-second buffer to capture speech before VAD detection
- **VAD Requirement**: Requires a VAD component like `SileroVADAnalyzer` in your transport

## Language Support

<Accordion title="View All Supported Languages (90+)">

| Language Code | Description       | Wizper Code |
| ------------- | ----------------- | ----------- |
| `Language.AF` | Afrikaans         | `af`        |
| `Language.AM` | Amharic           | `am`        |
| `Language.AR` | Arabic            | `ar`        |
| `Language.AS` | Assamese          | `as`        |
| `Language.AZ` | Azerbaijani       | `az`        |
| `Language.BA` | Bashkir           | `ba`        |
| `Language.BE` | Belarusian        | `be`        |
| `Language.BG` | Bulgarian         | `bg`        |
| `Language.BN` | Bengali           | `bn`        |
| `Language.BO` | Tibetan           | `bo`        |
| `Language.BR` | Breton            | `br`        |
| `Language.BS` | Bosnian           | `bs`        |
| `Language.CA` | Catalan           | `ca`        |
| `Language.CS` | Czech             | `cs`        |
| `Language.CY` | Welsh             | `cy`        |
| `Language.DA` | Danish            | `da`        |
| `Language.DE` | German            | `de`        |
| `Language.EL` | Greek             | `el`        |
| `Language.EN` | English           | `en`        |
| `Language.ES` | Spanish           | `es`        |
| `Language.ET` | Estonian          | `et`        |
| `Language.EU` | Basque            | `eu`        |
| `Language.FA` | Persian           | `fa`        |
| `Language.FI` | Finnish           | `fi`        |
| `Language.FO` | Faroese           | `fo`        |
| `Language.FR` | French            | `fr`        |
| `Language.GL` | Galician          | `gl`        |
| `Language.GU` | Gujarati          | `gu`        |
| `Language.HA` | Hausa             | `ha`        |
| `Language.HE` | Hebrew            | `he`        |
| `Language.HI` | Hindi             | `hi`        |
| `Language.HR` | Croatian          | `hr`        |
| `Language.HT` | Haitian Creole    | `ht`        |
| `Language.HU` | Hungarian         | `hu`        |
| `Language.HY` | Armenian          | `hy`        |
| `Language.ID` | Indonesian        | `id`        |
| `Language.IS` | Icelandic         | `is`        |
| `Language.IT` | Italian           | `it`        |
| `Language.JA` | Japanese          | `ja`        |
| `Language.JW` | Javanese          | `jw`        |
| `Language.KA` | Georgian          | `ka`        |
| `Language.KK` | Kazakh            | `kk`        |
| `Language.KM` | Khmer             | `km`        |
| `Language.KN` | Kannada           | `kn`        |
| `Language.KO` | Korean            | `ko`        |
| `Language.LA` | Latin             | `la`        |
| `Language.LB` | Luxembourgish     | `lb`        |
| `Language.LN` | Lingala           | `ln`        |
| `Language.LO` | Lao               | `lo`        |
| `Language.LT` | Lithuanian        | `lt`        |
| `Language.LV` | Latvian           | `lv`        |
| `Language.MG` | Malagasy          | `mg`        |
| `Language.MI` | Maori             | `mi`        |
| `Language.MK` | Macedonian        | `mk`        |
| `Language.ML` | Malayalam         | `ml`        |
| `Language.MN` | Mongolian         | `mn`        |
| `Language.MR` | Marathi           | `mr`        |
| `Language.MS` | Malay             | `ms`        |
| `Language.MT` | Maltese           | `mt`        |
| `Language.MY` | Burmese           | `my`        |
| `Language.NE` | Nepali            | `ne`        |
| `Language.NL` | Dutch             | `nl`        |
| `Language.NN` | Norwegian Nynorsk | `nn`        |
| `Language.NO` | Norwegian         | `no`        |
| `Language.OC` | Occitan           | `oc`        |
| `Language.PA` | Punjabi           | `pa`        |
| `Language.PL` | Polish            | `pl`        |
| `Language.PS` | Pashto            | `ps`        |
| `Language.PT` | Portuguese        | `pt`        |
| `Language.RO` | Romanian          | `ro`        |
| `Language.RU` | Russian           | `ru`        |
| `Language.SA` | Sanskrit          | `sa`        |
| `Language.SD` | Sindhi            | `sd`        |
| `Language.SI` | Sinhala           | `si`        |
| `Language.SK` | Slovak            | `sk`        |
| `Language.SL` | Slovenian         | `sl`        |
| `Language.SN` | Shona             | `sn`        |
| `Language.SO` | Somali            | `so`        |
| `Language.SQ` | Albanian          | `sq`        |
| `Language.SR` | Serbian           | `sr`        |
| `Language.SU` | Sundanese         | `su`        |
| `Language.SV` | Swedish           | `sv`        |
| `Language.SW` | Swahili           | `sw`        |
| `Language.TA` | Tamil             | `ta`        |
| `Language.TE` | Telugu            | `te`        |
| `Language.TG` | Tajik             | `tg`        |
| `Language.TH` | Thai              | `th`        |
| `Language.TK` | Turkmen           | `tk`        |
| `Language.TL` | Tagalog           | `tl`        |
| `Language.TR` | Turkish           | `tr`        |
| `Language.TT` | Tatar             | `tt`        |
| `Language.UK` | Ukrainian         | `uk`        |
| `Language.UR` | Urdu              | `ur`        |
| `Language.UZ` | Uzbek             | `uz`        |
| `Language.VI` | Vietnamese        | `vi`        |
| `Language.YI` | Yiddish           | `yi`        |
| `Language.YO` | Yoruba            | `yo`        |
| `Language.ZH` | Chinese           | `zh`        |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`

## Usage Example

### Basic Configuration

Initialize the `FalSTTService` and use it in a pipeline:

```python
from pipecat.services.fal.stt import FalSTTService
from pipecat.transcriptions.language import Language

# Simple setup
stt = FalSTTService(
    api_key=os.getenv("FAL_KEY")
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `FalSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides performance metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **VAD Dependency**: Requires a VAD component in your transport for speech segment detection
- **Segment Processing**: Processes complete utterances rather than streaming audio
- **Translation Support**: Can translate foreign speech directly to English when using `translate` task
- **Error Handling**: Comprehensive error handling for API failures and network issues



================================================
FILE: server/services/stt/gladia.mdx
================================================
---
title: "Gladia"
description: "Speech-to-text service implementation using Gladia’s API"
---

## Overview

`GladiaSTTService` provides real-time speech recognition using Gladia's WebSocket API with support for 99+ languages, custom vocabulary, translation, sentiment analysis, and advanced audio processing features.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.gladia.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Gladia Docs"
    icon="book"
    href="https://docs.gladia.io/api-reference/live-flow"
  >
    Official Gladia documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07j-interruptible-gladia.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>

## Installation

To use Gladia services, install the required dependency:

```bash
pip install "pipecat-ai[gladia]"
```

You'll also need to set up your Gladia API key as an environment variable: `GLADIA_API_KEY`.

<Tip>Get your API key from [Gladia](https://www.gladia.io/).</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results
- `TranslationFrame` - Real-time translation results (when enabled)
- `ErrorFrame` - Connection or processing errors

## Models

Gladia offers several models optimized for different use cases:

| Model       | Description                  | Best For                            |
| ----------- | ---------------------------- | ----------------------------------- |
| `solaria-1` | Latest general-purpose model | High accuracy, balanced performance |

See [Gladia's model documentation](https://docs.gladia.io/api-reference/v2/live/init#body-model) for detailed comparisons.

## Language Support

Gladia STT supports 99+ languages with automatic detection and code-switching:

<Accordion title="View All Supported Languages">

| Language Code    | Description         | Service Code |
| ---------------- | ------------------- | ------------ |
| `Language.AF`    | Afrikaans           | `af`         |
| `Language.AM`    | Amharic             | `am`         |
| `Language.AR`    | Arabic              | `ar`         |
| `Language.AS`    | Assamese            | `as`         |
| `Language.AZ`    | Azerbaijani         | `az`         |
| `Language.BA`    | Bashkir             | `ba`         |
| `Language.BE`    | Belarusian          | `be`         |
| `Language.BG`    | Bulgarian           | `bg`         |
| `Language.BN`    | Bengali             | `bn`         |
| `Language.BO`    | Tibetan             | `bo`         |
| `Language.BR`    | Breton              | `br`         |
| `Language.BS`    | Bosnian             | `bs`         |
| `Language.CA`    | Catalan             | `ca`         |
| `Language.CS`    | Czech               | `cs`         |
| `Language.CY`    | Welsh               | `cy`         |
| `Language.DA`    | Danish              | `da`         |
| `Language.DE`    | German              | `de`         |
| `Language.EL`    | Greek               | `el`         |
| `Language.EN`    | English             | `en`         |
| `Language.ES`    | Spanish             | `es`         |
| `Language.ET`    | Estonian            | `et`         |
| `Language.EU`    | Basque              | `eu`         |
| `Language.FA`    | Persian             | `fa`         |
| `Language.FI`    | Finnish             | `fi`         |
| `Language.FO`    | Faroese             | `fo`         |
| `Language.FR`    | French              | `fr`         |
| `Language.GL`    | Galician            | `gl`         |
| `Language.GU`    | Gujarati            | `gu`         |
| `Language.HA`    | Hausa               | `ha`         |
| `Language.HAW`   | Hawaiian            | `haw`        |
| `Language.HE`    | Hebrew              | `he`         |
| `Language.HI`    | Hindi               | `hi`         |
| `Language.HR`    | Croatian            | `hr`         |
| `Language.HT`    | Haitian Creole      | `ht`         |
| `Language.HU`    | Hungarian           | `hu`         |
| `Language.HY`    | Armenian            | `hy`         |
| `Language.ID`    | Indonesian          | `id`         |
| `Language.IS`    | Icelandic           | `is`         |
| `Language.IT`    | Italian             | `it`         |
| `Language.JA`    | Japanese            | `ja`         |
| `Language.JV`    | Javanese            | `jv`         |
| `Language.KA`    | Georgian            | `ka`         |
| `Language.KK`    | Kazakh              | `kk`         |
| `Language.KM`    | Khmer               | `km`         |
| `Language.KN`    | Kannada             | `kn`         |
| `Language.KO`    | Korean              | `ko`         |
| `Language.LA`    | Latin               | `la`         |
| `Language.LB`    | Luxembourgish       | `lb`         |
| `Language.LN`    | Lingala             | `ln`         |
| `Language.LO`    | Lao                 | `lo`         |
| `Language.LT`    | Lithuanian          | `lt`         |
| `Language.LV`    | Latvian             | `lv`         |
| `Language.MG`    | Malagasy            | `mg`         |
| `Language.MI`    | Maori               | `mi`         |
| `Language.MK`    | Macedonian          | `mk`         |
| `Language.ML`    | Malayalam           | `ml`         |
| `Language.MN`    | Mongolian           | `mn`         |
| `Language.MR`    | Marathi             | `mr`         |
| `Language.MS`    | Malay               | `ms`         |
| `Language.MT`    | Maltese             | `mt`         |
| `Language.MY_MR` | Burmese             | `mymr`       |
| `Language.NE`    | Nepali              | `ne`         |
| `Language.NL`    | Dutch               | `nl`         |
| `Language.NN`    | Norwegian (Nynorsk) | `nn`         |
| `Language.NO`    | Norwegian           | `no`         |
| `Language.OC`    | Occitan             | `oc`         |
| `Language.PA`    | Punjabi             | `pa`         |
| `Language.PL`    | Polish              | `pl`         |
| `Language.PS`    | Pashto              | `ps`         |
| `Language.PT`    | Portuguese          | `pt`         |
| `Language.RO`    | Romanian            | `ro`         |
| `Language.RU`    | Russian             | `ru`         |
| `Language.SA`    | Sanskrit            | `sa`         |
| `Language.SD`    | Sindhi              | `sd`         |
| `Language.SI`    | Sinhala             | `si`         |
| `Language.SK`    | Slovak              | `sk`         |
| `Language.SL`    | Slovenian           | `sl`         |
| `Language.SN`    | Shona               | `sn`         |
| `Language.SO`    | Somali              | `so`         |
| `Language.SQ`    | Albanian            | `sq`         |
| `Language.SR`    | Serbian             | `sr`         |
| `Language.SU`    | Sundanese           | `su`         |
| `Language.SV`    | Swedish             | `sv`         |
| `Language.SW`    | Swahili             | `sw`         |
| `Language.TA`    | Tamil               | `ta`         |
| `Language.TE`    | Telugu              | `te`         |
| `Language.TG`    | Tajik               | `tg`         |
| `Language.TH`    | Thai                | `th`         |
| `Language.TK`    | Turkmen             | `tk`         |
| `Language.TL`    | Tagalog             | `tl`         |
| `Language.TR`    | Turkish             | `tr`         |
| `Language.TT`    | Tatar               | `tt`         |
| `Language.UK`    | Ukrainian           | `uk`         |
| `Language.UR`    | Urdu                | `ur`         |
| `Language.UZ`    | Uzbek               | `uz`         |
| `Language.VI`    | Vietnamese          | `vi`         |
| `Language.YI`    | Yiddish             | `yi`         |
| `Language.YO`    | Yoruba              | `yo`         |
| `Language.ZH`    | Chinese             | `zh`         |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`

## Advanced Features

### Automatic Language Detection

- Single language: Fixed language for entire session
- Multiple languages: Auto-detect per utterance with `code_switching=True`
- No languages specified: Auto-detect from all supported languages

### Custom Vocabulary

- Add domain-specific terms with bias intensity (0.0-1.0)
- Mix strings and `CustomVocabularyItem` objects
- Configure default intensity for simple strings

### Real-time Translation

- Translate to multiple target languages simultaneously
- Enhanced model for higher accuracy
- Align translations with original utterances

## Usage Example

### Basic Configuration

Initialize the `GladiaSTTService` and use it in a pipeline:

```python
from pipecat.services.gladia.stt import GladiaSTTService
from pipecat.services.gladia.config import GladiaInputParams, LanguageConfig

# Simple setup
stt = GladiaSTTService(
    api_key=os.getenv("GLADIA_API_KEY"),
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `GladiaSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

### Multi-language Configuration

```python
from pipecat.services.gladia.config import LanguageConfig

# Multi-language with auto-detection
params = GladiaInputParams(
    language_config=LanguageConfig(
        languages=["en", "es", "fr"],  # English, Spanish, French
        code_switching=True  # Auto-detect language changes
    )
)

stt = GladiaSTTService(
    api_key=os.getenv("GLADIA_API_KEY"),
    params=params
)
```

### Real-time Translation

```python
from pipecat.services.gladia.config import (
    RealtimeProcessingConfig,
    TranslationConfig
)

# Enable real-time translation
params = GladiaInputParams(
    language_config=LanguageConfig(languages=["en"]),
    realtime_processing=RealtimeProcessingConfig(
        translation=True,
        translation_config=TranslationConfig(
            target_languages=["es"],
            match_original_utterances=True
        )
    )
)

stt = GladiaSTTService(
    api_key=os.getenv("GLADIA_API_KEY"),
    params=params
)
```

### Custom Vocabulary

```python
from pipecat.services.gladia.config import (
    CustomVocabularyConfig,
    CustomVocabularyItem,
    RealtimeProcessingConfig
)

# Add domain-specific vocabulary
custom_vocab = CustomVocabularyConfig(
    vocabulary=[
        CustomVocabularyItem(value="Pipecat", intensity=0.9),
        CustomVocabularyItem(value="WebRTC", intensity=0.8),
        "JavaScript",  # Simple string with default intensity
    ],
    default_intensity=0.6
)

params = GladiaInputParams(
    realtime_processing=RealtimeProcessingConfig(
        custom_vocabulary=True,
        custom_vocabulary_config=custom_vocab
    )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>



================================================
FILE: server/services/stt/google.mdx
================================================
---
title: "Google"
description: "Speech-to-text service implementation using Google Cloud’s Speech-to-Text V2 API"
---

## Overview

`GoogleSTTService` provides real-time speech recognition using Google Cloud's Speech-to-Text V2 API with support for 125+ languages, multiple models, voice activity detection, and advanced features like automatic punctuation and word-level confidence scores.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.google.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Google Cloud Docs"
    icon="book"
    href="https://cloud.google.com/speech-to-text/v2/docs"
  >
    Official Google Cloud Speech-to-Text documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07n-interruptible-google.py"
  >
    Working example with Google Cloud services
  </Card>
</CardGroup>

## Installation

To use Google Cloud Speech services, install the required dependency:

```bash
pip install "pipecat-ai[google]"
```

You'll need Google Cloud credentials either as environment variables, a JSON string, or a service account file.

<Tip>
  Get your credentials by creating a service account in the [Google Cloud
  Console](https://console.cloud.google.com/iam-admin/serviceaccounts) with
  Speech-to-Text API access.
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, configurable sample rate, mono)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results with confidence scores
- `ErrorFrame` - Connection or processing errors

## Models

Google Cloud offers specialized models for different use cases:

| Model                  | Description                           | Best For                          |
| ---------------------- | ------------------------------------- | --------------------------------- |
| `latest_long`          | Optimized for long-form speech        | Conversations, meetings, podcasts |
| `chirp_2`              | LLM powered ASR model                 | Streaming and multilingual        |
| `telephony`            | Optimized for phone call audio        | Call centers, phone interviews    |
| `medical_dictation`    | Medical terminology optimized         | Healthcare dictation              |
| `medical_conversation` | Doctor-patient conversation optimized | Medical consultations             |

See [Google's model documentation](https://cloud.google.com/speech-to-text/docs/transcription-model) for detailed performance comparisons.

## Regional Support

Google Cloud Speech-to-Text V2 supports different regional endpoints:

| Region            | Description             | Best For                  |
| ----------------- | ----------------------- | ------------------------- |
| `global`          | Default global endpoint | General use, auto-routing |
| `us-central1`     | US Central region       | North American users      |
| `europe-west1`    | Europe West region      | European users            |
| `asia-northeast1` | Asia Northeast region   | Asian users               |

Configure region for improved latency and data residency:

```python
stt = GoogleSTTService(
    location="us-central1",  # Regional endpoint
    credentials_path="credentials.json"
)
```

## Language Support

Google Cloud STT supports 125+ languages with regional variants:

<Accordion title="View All Supported Languages">

| Language Code    | Description           | Service Codes |
| ---------------- | --------------------- | ------------- |
| `Language.AF`    | Afrikaans             | `af-ZA`       |
| `Language.SQ`    | Albanian              | `sq-AL`       |
| `Language.AM`    | Amharic               | `am-ET`       |
| `Language.AR`    | Arabic (Egypt)        | `ar-EG`       |
| `Language.AR_AE` | Arabic (UAE)          | `ar-AE`       |
| `Language.AR_SA` | Arabic (Saudi Arabia) | `ar-SA`       |
| `Language.HY`    | Armenian              | `hy-AM`       |
| `Language.AZ`    | Azerbaijani           | `az-AZ`       |
| `Language.EU`    | Basque                | `eu-ES`       |
| `Language.BN`    | Bengali (India)       | `bn-IN`       |
| `Language.BN_BD` | Bengali (Bangladesh)  | `bn-BD`       |
| `Language.BS`    | Bosnian               | `bs-BA`       |
| `Language.BG`    | Bulgarian             | `bg-BG`       |
| `Language.MY`    | Burmese               | `my-MM`       |
| `Language.CA`    | Catalan               | `ca-ES`       |
| `Language.ZH`    | Chinese (Simplified)  | `cmn-Hans-CN` |
| `Language.ZH_TW` | Chinese (Traditional) | `cmn-Hant-TW` |
| `Language.YUE`   | Chinese (Cantonese)   | `yue-Hant-HK` |
| `Language.HR`    | Croatian              | `hr-HR`       |
| `Language.CS`    | Czech                 | `cs-CZ`       |
| `Language.DA`    | Danish                | `da-DK`       |
| `Language.NL`    | Dutch                 | `nl-NL`       |
| `Language.NL_BE` | Dutch (Belgium)       | `nl-BE`       |
| `Language.EN`    | English (US)          | `en-US`       |
| `Language.EN_AU` | English (Australia)   | `en-AU`       |
| `Language.EN_CA` | English (Canada)      | `en-CA`       |
| `Language.EN_GB` | English (UK)          | `en-GB`       |
| `Language.EN_IN` | English (India)       | `en-IN`       |
| `Language.ET`    | Estonian              | `et-EE`       |
| `Language.FIL`   | Filipino              | `fil-PH`      |
| `Language.FI`    | Finnish               | `fi-FI`       |
| `Language.FR`    | French                | `fr-FR`       |
| `Language.FR_CA` | French (Canada)       | `fr-CA`       |
| `Language.GL`    | Galician              | `gl-ES`       |
| `Language.KA`    | Georgian              | `ka-GE`       |
| `Language.DE`    | German                | `de-DE`       |
| `Language.DE_AT` | German (Austria)      | `de-AT`       |
| `Language.EL`    | Greek                 | `el-GR`       |
| `Language.GU`    | Gujarati              | `gu-IN`       |
| `Language.HE`    | Hebrew                | `iw-IL`       |
| `Language.HI`    | Hindi                 | `hi-IN`       |
| `Language.HU`    | Hungarian             | `hu-HU`       |
| `Language.IS`    | Icelandic             | `is-IS`       |
| `Language.ID`    | Indonesian            | `id-ID`       |
| `Language.IT`    | Italian               | `it-IT`       |
| `Language.JA`    | Japanese              | `ja-JP`       |
| `Language.JV`    | Javanese              | `jv-ID`       |
| `Language.KN`    | Kannada               | `kn-IN`       |
| `Language.KK`    | Kazakh                | `kk-KZ`       |
| `Language.KM`    | Khmer                 | `km-KH`       |
| `Language.KO`    | Korean                | `ko-KR`       |
| `Language.LO`    | Lao                   | `lo-LA`       |
| `Language.LV`    | Latvian               | `lv-LV`       |
| `Language.LT`    | Lithuanian            | `lt-LT`       |
| `Language.MK`    | Macedonian            | `mk-MK`       |
| `Language.MS`    | Malay                 | `ms-MY`       |
| `Language.ML`    | Malayalam             | `ml-IN`       |
| `Language.MR`    | Marathi               | `mr-IN`       |
| `Language.MN`    | Mongolian             | `mn-MN`       |
| `Language.NE`    | Nepali                | `ne-NP`       |
| `Language.NO`    | Norwegian             | `no-NO`       |
| `Language.FA`    | Persian               | `fa-IR`       |
| `Language.PL`    | Polish                | `pl-PL`       |
| `Language.PT`    | Portuguese            | `pt-PT`       |
| `Language.PT_BR` | Portuguese (Brazil)   | `pt-BR`       |
| `Language.PA`    | Punjabi               | `pa-Guru-IN`  |
| `Language.RO`    | Romanian              | `ro-RO`       |
| `Language.RU`    | Russian               | `ru-RU`       |
| `Language.SR`    | Serbian               | `sr-RS`       |
| `Language.SI`    | Sinhala               | `si-LK`       |
| `Language.SK`    | Slovak                | `sk-SK`       |
| `Language.SL`    | Slovenian             | `sl-SI`       |
| `Language.ES`    | Spanish (Spain)       | `es-ES`       |
| `Language.ES_MX` | Spanish (Mexico)      | `es-MX`       |
| `Language.ES_US` | Spanish (US)          | `es-US`       |
| `Language.SU`    | Sundanese             | `su-ID`       |
| `Language.SW`    | Swahili               | `sw-TZ`       |
| `Language.SV`    | Swedish               | `sv-SE`       |
| `Language.TA`    | Tamil                 | `ta-IN`       |
| `Language.TE`    | Telugu                | `te-IN`       |
| `Language.TH`    | Thai                  | `th-TH`       |
| `Language.TR`    | Turkish               | `tr-TR`       |
| `Language.UK`    | Ukrainian             | `uk-UA`       |
| `Language.UR`    | Urdu                  | `ur-IN`       |
| `Language.UZ`    | Uzbek                 | `uz-UZ`       |
| `Language.VI`    | Vietnamese            | `vi-VN`       |
| `Language.XH`    | Xhosa                 | `xh-ZA`       |
| `Language.ZU`    | Zulu                  | `zu-ZA`       |

</Accordion>

Common languages:

- `Language.EN_US` - English (US) - `en-US`
- `Language.ES` - Spanish - `es-ES`
- `Language.FR` - French - `fr-FR`
- `Language.DE` - German - `de-DE`
- `Language.ZH` - Chinese (Simplified) - `cmn-Hans-CN`
- `Language.JA` - Japanese - `ja-JP`

## Usage Example

### Basic Configuration

Initialize the `GoogleSTTService` and use it in a pipeline:

```python
from pipecat.services.google.stt import GoogleSTTService
from pipecat.transcriptions.language import Language

# Using environment credentials
stt = GoogleSTTService(
    params=GoogleSTTService.InputParams(
        languages=Language.EN_US,
        model="latest_long",
        enable_automatic_punctuation=True,
        enable_interim_results=True
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Credentials Configuration

```python
# Using service account file
stt = GoogleSTTService(
    credentials_path="path/to/service-account.json",
    location="us-central1",
    params=GoogleSTTService.InputParams(languages=Language.EN_US)
)

# Using credentials JSON string
stt = GoogleSTTService(
    credentials=os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON"),
    params=GoogleSTTService.InputParams(languages=Language.EN_US)
)
```

### Multi-language Configuration

```python
# Multiple languages (first is primary)
params = GoogleSTTService.InputParams(
    languages=[Language.EN_US, Language.ES_MX, Language.FR],
    model="latest_long",
    enable_automatic_punctuation=True
)

stt = GoogleSTTService(
    credentials_path="credentials.json",
    params=params
)
```

### Dynamic Configuration Updates

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `GoogleSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await stt.update_options(
    languages=[Language.FR, Language.EN_US],
)
```

## Advanced Features

### Multi-language Support

- Support for multiple languages simultaneously
- First language in list is considered primary
- Automatic language detection within configured set

### Voice Activity Detection

- Built-in VAD events from Google's service
- Integrates with Pipecat's VAD framework
- Configurable sensitivity and detection

### Content Processing

- **Automatic Punctuation**: Smart punctuation insertion
- **Profanity Filtering**: Optional content filtering
- **Format Control**: Handle spoken vs written formats

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>



================================================
FILE: server/services/stt/groq.mdx
================================================
---
title: "Groq (Whisper)"
description: "Speech-to-text service implementation using Groq’s Whisper API"
---

## Overview

`GroqSTTService` provides high-accuracy speech recognition using Groq's hosted Whisper API with ultra-fast inference speeds. It uses Voice Activity Detection (VAD) to process speech segments efficiently to create speech segments to send to the API.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.groq.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Groq Docs"
    icon="book"
    href="https://console.groq.com/docs/api-reference#audio-transcription"
  >
    Official Groq STT documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07l-interruptible-groq.py"
  >
    Working example with Groq ecosystem integration
  </Card>
</CardGroup>

## Installation

To use Groq services, install the required dependency:

```bash
pip install "pipecat-ai[groq]"
```

You'll need to set up your Groq API key as an environment variable: `GROQ_API_KEY`.

<Tip>
  Get your API key from the [Groq Console](https://console.groq.com/keys).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `UserStartedSpeakingFrame` - VAD signal to start buffering audio
- `UserStoppedSpeakingFrame` - VAD signal to process buffered audio

### Output

- `TranscriptionFrame` - Final transcription results (no interim results)
- `ErrorFrame` - API or processing errors

## Models

Groq currently offers one optimized Whisper model:

| Model                    | Description                       | Performance                             |
| ------------------------ | --------------------------------- | --------------------------------------- |
| `whisper-large-v3-turbo` | Groq's optimized Whisper large v3 | Ultra-fast inference with high accuracy |

<Note>
  Groq's hardware acceleration makes this model significantly faster than
  standard Whisper implementations while maintaining accuracy.
</Note>

## Language Support

Groq's Whisper API supports 60+ languages with automatic language detection:

<Accordion title="View All Supported Languages">

| Language Code | Description | Whisper Code |
| ------------- | ----------- | ------------ |
| `Language.AF` | Afrikaans   | `af`         |
| `Language.AR` | Arabic      | `ar`         |
| `Language.HY` | Armenian    | `hy`         |
| `Language.AZ` | Azerbaijani | `az`         |
| `Language.BE` | Belarusian  | `be`         |
| `Language.BS` | Bosnian     | `bs`         |
| `Language.BG` | Bulgarian   | `bg`         |
| `Language.CA` | Catalan     | `ca`         |
| `Language.ZH` | Chinese     | `zh`         |
| `Language.HR` | Croatian    | `hr`         |
| `Language.CS` | Czech       | `cs`         |
| `Language.DA` | Danish      | `da`         |
| `Language.NL` | Dutch       | `nl`         |
| `Language.EN` | English     | `en`         |
| `Language.ET` | Estonian    | `et`         |
| `Language.FI` | Finnish     | `fi`         |
| `Language.FR` | French      | `fr`         |
| `Language.GL` | Galician    | `gl`         |
| `Language.DE` | German      | `de`         |
| `Language.EL` | Greek       | `el`         |
| `Language.HE` | Hebrew      | `he`         |
| `Language.HI` | Hindi       | `hi`         |
| `Language.HU` | Hungarian   | `hu`         |
| `Language.IS` | Icelandic   | `is`         |
| `Language.ID` | Indonesian  | `id`         |
| `Language.IT` | Italian     | `it`         |
| `Language.JA` | Japanese    | `ja`         |
| `Language.KN` | Kannada     | `kn`         |
| `Language.KK` | Kazakh      | `kk`         |
| `Language.KO` | Korean      | `ko`         |
| `Language.LV` | Latvian     | `lv`         |
| `Language.LT` | Lithuanian  | `lt`         |
| `Language.MK` | Macedonian  | `mk`         |
| `Language.MS` | Malay       | `ms`         |
| `Language.MR` | Marathi     | `mr`         |
| `Language.MI` | Maori       | `mi`         |
| `Language.NE` | Nepali      | `ne`         |
| `Language.NO` | Norwegian   | `no`         |
| `Language.FA` | Persian     | `fa`         |
| `Language.PL` | Polish      | `pl`         |
| `Language.PT` | Portuguese  | `pt`         |
| `Language.RO` | Romanian    | `ro`         |
| `Language.RU` | Russian     | `ru`         |
| `Language.SR` | Serbian     | `sr`         |
| `Language.SK` | Slovak      | `sk`         |
| `Language.SL` | Slovenian   | `sl`         |
| `Language.ES` | Spanish     | `es`         |
| `Language.SW` | Swahili     | `sw`         |
| `Language.SV` | Swedish     | `sv`         |
| `Language.TL` | Tagalog     | `tl`         |
| `Language.TA` | Tamil       | `ta`         |
| `Language.TH` | Thai        | `th`         |
| `Language.TR` | Turkish     | `tr`         |
| `Language.UK` | Ukrainian   | `uk`         |
| `Language.UR` | Urdu        | `ur`         |
| `Language.VI` | Vietnamese  | `vi`         |
| `Language.CY` | Welsh       | `cy`         |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`

<Note>
  Regional variants (like `EN_US`, `FR_CA`) are automatically mapped to their
  base language codes.
</Note>

## Usage Example

### Basic Configuration

Initialize the `GroqSTTService` and use it in a pipeline:

```python
from pipecat.services.groq.stt import GroqSTTService
from pipecat.transcriptions.language import Language

# Simple setup with defaults
stt = GroqSTTService(
    api_key=os.getenv("GROQ_API_KEY"),
    language=Language.EN
)

# Use in pipeline with VAD
pipeline = Pipeline([
    transport.input(),  # Must include VAD analyzer
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Advanced Configuration

```python
# Optimized for specific use case
stt = GroqSTTService(
    api_key=os.getenv("GROQ_API_KEY"),
    model="whisper-large-v3-turbo",
    language=Language.EN,
    prompt="Transcribe this technical discussion about AI and machine learning.",
    temperature=0.0  # Deterministic output
)
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `GroqSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - API response latency
- **Processing Duration** - Total transcription time

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Segmented Processing**: Processes complete utterances, not continuous streams
- **No Interim Results**: Only final transcriptions are provided (typical for batch APIs)
- **Audio Buffer**: Maintains 1-second buffer to capture speech before VAD detection
- **Language Variants**: Regional language codes automatically map to base languages
- **Context Prompts**: Use prompts to improve accuracy for specific domains or conversation styles
- **Rate Limits**: Check your Groq plan for concurrent request and usage limits
- **Hardware Optimization**: Leverages Groq's custom inference chips for maximum performance



================================================
FILE: server/services/stt/openai.mdx
================================================
---
title: "OpenAI"
description: "Speech-to-text service implementation using OpenAI’s Speech-to-Text APIs"
---

## Overview

`OpenAISTTService` provides high-accuracy speech recognition using OpenAI's advanced transcription models, including the latest GPT-4o transcription model and the proven Whisper API. It uses Voice Activity Detection (VAD) to efficiently process speech segments with superior accuracy and context understanding.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenAI Docs"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/audio/createTranscription"
  >
    Official OpenAI transcription documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py"
  >
    Working example with OpenAI ecosystem integration
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependency:

```bash
pip install "pipecat-ai[openai]"
```

You'll need to set up your OpenAI API key as an environment variable: `OPENAI_API_KEY`.

<Tip>
  Get your API key from the [OpenAI
  Platform](https://platform.openai.com/api-keys).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `UserStartedSpeakingFrame` - VAD signal to start buffering audio
- `UserStoppedSpeakingFrame` - VAD signal to process buffered audio

### Output

- `TranscriptionFrame` - Final transcription results (no interim results)
- `ErrorFrame` - API or processing errors

## Models

OpenAI offers two transcription models with different strengths:

| Model               | Description                                      | Best For                                                    | Accuracy | Speed |
| ------------------- | ------------------------------------------------ | ----------------------------------------------------------- | -------- | ----- |
| `gpt-4o-transcribe` | Latest GPT-4o model fine-tuned for transcription | High accuracy, robustness to accents, context understanding | Highest  | Fast  |
| `whisper-1`         | OpenAI's proven Whisper model                    | Broad language support, clean audio                         | High     | Fast  |

<Note>
  **Recommended**: Use `gpt-4o-transcribe` for the best accuracy and context
  understanding, especially with challenging audio or technical content.
</Note>

## Language Support

OpenAI's speech-to-text models support 60+ languages with automatic language detection:

<Accordion title="View All Supported Languages">

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.AF` | Afrikaans   | `af`         |
| `Language.AR` | Arabic      | `ar`         |
| `Language.HY` | Armenian    | `hy`         |
| `Language.AZ` | Azerbaijani | `az`         |
| `Language.BE` | Belarusian  | `be`         |
| `Language.BS` | Bosnian     | `bs`         |
| `Language.BG` | Bulgarian   | `bg`         |
| `Language.CA` | Catalan     | `ca`         |
| `Language.ZH` | Chinese     | `zh`         |
| `Language.HR` | Croatian    | `hr`         |
| `Language.CS` | Czech       | `cs`         |
| `Language.DA` | Danish      | `da`         |
| `Language.NL` | Dutch       | `nl`         |
| `Language.EN` | English     | `en`         |
| `Language.ET` | Estonian    | `et`         |
| `Language.FI` | Finnish     | `fi`         |
| `Language.FR` | French      | `fr`         |
| `Language.GL` | Galician    | `gl`         |
| `Language.DE` | German      | `de`         |
| `Language.EL` | Greek       | `el`         |
| `Language.HE` | Hebrew      | `he`         |
| `Language.HI` | Hindi       | `hi`         |
| `Language.HU` | Hungarian   | `hu`         |
| `Language.IS` | Icelandic   | `is`         |
| `Language.ID` | Indonesian  | `id`         |
| `Language.IT` | Italian     | `it`         |
| `Language.JA` | Japanese    | `ja`         |
| `Language.KN` | Kannada     | `kn`         |
| `Language.KK` | Kazakh      | `kk`         |
| `Language.KO` | Korean      | `ko`         |
| `Language.LV` | Latvian     | `lv`         |
| `Language.LT` | Lithuanian  | `lt`         |
| `Language.MK` | Macedonian  | `mk`         |
| `Language.MS` | Malay       | `ms`         |
| `Language.MR` | Marathi     | `mr`         |
| `Language.MI` | Maori       | `mi`         |
| `Language.NE` | Nepali      | `ne`         |
| `Language.NO` | Norwegian   | `no`         |
| `Language.FA` | Persian     | `fa`         |
| `Language.PL` | Polish      | `pl`         |
| `Language.PT` | Portuguese  | `pt`         |
| `Language.RO` | Romanian    | `ro`         |
| `Language.RU` | Russian     | `ru`         |
| `Language.SR` | Serbian     | `sr`         |
| `Language.SK` | Slovak      | `sk`         |
| `Language.SL` | Slovenian   | `sl`         |
| `Language.ES` | Spanish     | `es`         |
| `Language.SW` | Swahili     | `sw`         |
| `Language.SV` | Swedish     | `sv`         |
| `Language.TL` | Tagalog     | `tl`         |
| `Language.TA` | Tamil       | `ta`         |
| `Language.TH` | Thai        | `th`         |
| `Language.TR` | Turkish     | `tr`         |
| `Language.UK` | Ukrainian   | `uk`         |
| `Language.UR` | Urdu        | `ur`         |
| `Language.VI` | Vietnamese  | `vi`         |
| `Language.CY` | Welsh       | `cy`         |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`

<Note>
  Regional variants (like `EN_US`, `FR_CA`) are automatically mapped to their
  base language codes.
</Note>

## Usage Example

### Basic Configuration

Initialize the `OpenAISTTService` and use it in a pipeline:

```python
from pipecat.services.openai.stt import OpenAISTTService
from pipecat.transcriptions.language import Language

# Simple setup with GPT-4o (recommended)
stt = OpenAISTTService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-transcribe",
    language=Language.EN
)

# Use in pipeline with VAD
pipeline = Pipeline([
    transport.input(),  # Must include VAD analyzer
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Advanced Configuration

```python
# Optimized for technical content
stt = OpenAISTTService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-transcribe",
    language=Language.EN,
    prompt="Transcribe technical terms accurately. Format numbers as digits rather than words.",
    temperature=0.0  # Deterministic output
)
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `OpenAISTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
))
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - API response latency
- **Processing Duration** - Total transcription time

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Segmented Processing**: Processes complete utterances, not continuous streams
- **No Interim Results**: Only final transcriptions are provided (typical for batch APIs)
- **Audio Buffer**: Maintains 1-second buffer to capture speech before VAD detection
- **Language Variants**: Regional language codes automatically map to base languages
- **Context Prompts**: GPT-4o especially benefits from domain-specific prompts
- **Rate Limits**: Check your OpenAI plan for concurrent request and usage limits
- **Quality Focus**: OpenAI prioritizes accuracy and context understanding over speed



================================================
FILE: server/services/stt/riva.mdx
================================================
---
title: "NVIDIA Riva"
description: "Speech-to-text service implementation using NVIDIA Riva"
---

## Overview

NVIDIA Riva provides two STT services:

- `RivaSTTService` for real-time streaming transcription using Parakeet models
- `RivaSegmentedSTTService` for segmented transcription using Canary models with advanced language support

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.riva.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="NVIDIA Riva Docs"
    icon="book"
    href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html"
  >
    Official NVIDIA Riva ASR documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07r-interruptible-riva-nim.py"
  >
    Working example with NVIDIA services integration
  </Card>
</CardGroup>

## Installation

To use NVIDIA Riva services, install the required dependency:

```bash
pip install "pipecat-ai[riva]"
```

You'll also need to set up your NVIDIA API key as an environment variable: `NVIDIA_API_KEY`.

<Tip>
  Get your API key from [NVIDIA's developer
  portal](https://developer.nvidia.com).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates (streaming only)
- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - Connection or processing errors

## Service Comparison

| Feature             | RivaSTTService         | RivaSegmentedSTTService   |
| ------------------- | ---------------------- | ------------------------- |
| **Processing**      | Real-time streaming    | Segmented (VAD-based)     |
| **Model**           | Parakeet CTC 1.1B      | Canary 1B                 |
| **Latency**         | Ultra-low              | Higher (batch processing) |
| **Languages**       | English-focused        | Multi-language            |
| **Interim Results** | ✅ Yes                 | ❌ No                     |
| **Best For**        | Real-time conversation | Multi-language accuracy   |

## Models

| Model                   | Service Class           | Description                             | Languages                 |
| ----------------------- | ----------------------- | --------------------------------------- | ------------------------- |
| `parakeet-ctc-1.1b-asr` | RivaSTTService          | Streaming ASR optimized for low latency | English (various accents) |
| `canary-1b-asr`         | RivaSegmentedSTTService | Multilingual ASR with high accuracy     | 15+ languages             |

See [NVIDIA's model cards](https://build.nvidia.com/nvidia) for detailed performance metrics.

## Language Support

### RivaSTTService (Parakeet)

Primarily supports English with various regional accents:

- `Language.EN_US` - English (US) - `en-US`

### RivaSegmentedSTTService (Canary)

Supports multiple languages:

| Language Code    | Description         | Service Codes |
| ---------------- | ------------------- | ------------- |
| `Language.EN_US` | English (US)        | `en-US`       |
| `Language.EN_GB` | English (UK)        | `en-GB`       |
| `Language.ES`    | Spanish             | `es-ES`       |
| `Language.ES_US` | Spanish (US)        | `es-US`       |
| `Language.FR`    | French              | `fr-FR`       |
| `Language.DE`    | German              | `de-DE`       |
| `Language.IT`    | Italian             | `it-IT`       |
| `Language.PT_BR` | Portuguese (Brazil) | `pt-BR`       |
| `Language.JA`    | Japanese            | `ja-JP`       |
| `Language.KO`    | Korean              | `ko-KR`       |
| `Language.RU`    | Russian             | `ru-RU`       |
| `Language.HI`    | Hindi               | `hi-IN`       |
| `Language.AR`    | Arabic              | `ar-AR`       |

## Usage Example

### Real-time Streaming (RivaSTTService)

Initialize the `RivaSTTService` and use it in a pipeline:

```python
from pipecat.services.riva.stt import RivaSTTService
from pipecat.transcriptions.language import Language

# Basic streaming configuration
stt = RivaSTTService(
    api_key=os.getenv("NVIDIA_API_KEY"),
    params=RivaSTTService.InputParams(
        language=Language.EN_US
    )
)

# Use in pipeline for real-time conversation
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Segmented Multi-language (RivaSegmentedSTTService)

Initialize the `RivaSegmentedSTTService` for segmented transcription:

```python
from pipecat.services.riva.stt import RivaSegmentedSTTService
from pipecat.audio.vad.silero import SileroVADAnalyzer

# Multi-language segmented transcription
stt = RivaSegmentedSTTService(
    api_key=os.getenv("NVIDIA_API_KEY"),
    params=RivaSegmentedSTTService.InputParams(
        language=Language.ES,  # Spanish
        profanity_filter=False,
        automatic_punctuation=True,
        boosted_lm_words=["inteligencia", "artificial"],
        boosted_lm_score=5.0
    )
)



# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,  # Processes audio segments
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for either service:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,  # Change to French
  )
)
```

## Advanced Configuration

Both services support advanced ASR parameters:

### Word Boosting

- `boosted_lm_words`: List of domain-specific terms to emphasize
- `boosted_lm_score`: Boost intensity (default: 4.0, recommended: 4.0-8.0)

### Audio Processing

- `profanity_filter`: Filter inappropriate content
- `automatic_punctuation`: Add punctuation automatically
- `verbatim_transcripts`: Control transcript formatting

### Voice Activity Detection (Streaming only)

- `start_history`: History frames for speech start detection
- `start_threshold`: Confidence threshold for speech start
- `stop_threshold`: Confidence threshold for speech end

## Metrics

- **Time to First Byte (TTFB)** - Latency from audio segment to transcription
- **Processing Duration** - Time spent processing each segment

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Authentication**: Uses NVIDIA Cloud Functions with Bearer token authentication
- **Real-time vs Batch**: Choose streaming for conversation, segmented for accuracy and multi-language
- **VAD Requirement**: Segmented service requires Voice Activity Detection in the pipeline
- **Custom Endpoints**: Supports custom Riva server endpoints for on-premise deployments



================================================
FILE: server/services/stt/sambanova.mdx
================================================
---
title: "SambaNova (Whisper)"
description: "Speech-to-text service implementation using SambaNova’s Whisper API"
---

## Overview

`SambaNovaSTTService` provides speech-to-text capabilities using SambaNova's hosted Whisper API with Voice Activity Detection (VAD) for optimized processing. It uses Voice Activity Detection (VAD) to process speech segments efficiently to create speech segments to send to the API.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.sambanova.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="SambaNova Docs"
    icon="book"
    href="https://docs.sambanova.ai/sambastudio/latest/open-ai-api.html"
  >
    Official SambaNova API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14s-function-calling-sambanova.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use SambaNova services, install the required dependency:

```bash
pip install "pipecat-ai[sambanova]"
```

You'll also need to set up your SambaNova API key as an environment variable: `SAMBANOVA_API_KEY`.

<Tip>
  Get your SambaNova API key from [SambaNova
  Cloud](https://cloud.sambanova.ai/?utm_source=pipecat&utm_medium=external&utm_campaign=cloud_signup).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `UserStartedSpeakingFrame` - VAD start signal (begins audio buffering)
- `UserStoppedSpeakingFrame` - VAD stop signal (triggers transcription)

### Output

- `TranscriptionFrame` - Final transcription results
- `ErrorFrame` - API or processing errors

## Models

SambaNova currently offers one Whisper model:

| Model              | Description               | Features                                      |
| ------------------ | ------------------------- | --------------------------------------------- |
| `Whisper-Large-v3` | OpenAI's Whisper Large v3 | High accuracy, 99+ languages, robust to noise |

## Language Support

SambaNova's Whisper implementation supports 99+ languages with automatic language detection:

<Accordion title="View All Supported Languages">

| Language Code | Description | Whisper Code |
| ------------- | ----------- | ------------ |
| `Language.AF` | Afrikaans   | `af`         |
| `Language.AR` | Arabic      | `ar`         |
| `Language.HY` | Armenian    | `hy`         |
| `Language.AZ` | Azerbaijani | `az`         |
| `Language.BE` | Belarusian  | `be`         |
| `Language.BS` | Bosnian     | `bs`         |
| `Language.BG` | Bulgarian   | `bg`         |
| `Language.CA` | Catalan     | `ca`         |
| `Language.ZH` | Chinese     | `zh`         |
| `Language.HR` | Croatian    | `hr`         |
| `Language.CS` | Czech       | `cs`         |
| `Language.DA` | Danish      | `da`         |
| `Language.NL` | Dutch       | `nl`         |
| `Language.EN` | English     | `en`         |
| `Language.ET` | Estonian    | `et`         |
| `Language.FI` | Finnish     | `fi`         |
| `Language.FR` | French      | `fr`         |
| `Language.GL` | Galician    | `gl`         |
| `Language.DE` | German      | `de`         |
| `Language.EL` | Greek       | `el`         |
| `Language.HE` | Hebrew      | `he`         |
| `Language.HI` | Hindi       | `hi`         |
| `Language.HU` | Hungarian   | `hu`         |
| `Language.IS` | Icelandic   | `is`         |
| `Language.ID` | Indonesian  | `id`         |
| `Language.IT` | Italian     | `it`         |
| `Language.JA` | Japanese    | `ja`         |
| `Language.KN` | Kannada     | `kn`         |
| `Language.KK` | Kazakh      | `kk`         |
| `Language.KO` | Korean      | `ko`         |
| `Language.LV` | Latvian     | `lv`         |
| `Language.LT` | Lithuanian  | `lt`         |
| `Language.MK` | Macedonian  | `mk`         |
| `Language.MS` | Malay       | `ms`         |
| `Language.MR` | Marathi     | `mr`         |
| `Language.MI` | Maori       | `mi`         |
| `Language.NE` | Nepali      | `ne`         |
| `Language.NO` | Norwegian   | `no`         |
| `Language.FA` | Persian     | `fa`         |
| `Language.PL` | Polish      | `pl`         |
| `Language.PT` | Portuguese  | `pt`         |
| `Language.RO` | Romanian    | `ro`         |
| `Language.RU` | Russian     | `ru`         |
| `Language.SR` | Serbian     | `sr`         |
| `Language.SK` | Slovak      | `sk`         |
| `Language.SL` | Slovenian   | `sl`         |
| `Language.ES` | Spanish     | `es`         |
| `Language.SW` | Swahili     | `sw`         |
| `Language.SV` | Swedish     | `sv`         |
| `Language.TL` | Tagalog     | `tl`         |
| `Language.TA` | Tamil       | `ta`         |
| `Language.TH` | Thai        | `th`         |
| `Language.TR` | Turkish     | `tr`         |
| `Language.UK` | Ukrainian   | `uk`         |
| `Language.UR` | Urdu        | `ur`         |
| `Language.VI` | Vietnamese  | `vi`         |
| `Language.CY` | Welsh       | `cy`         |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`

<Note>
  Language variants (like `en-US`, `fr-CA`) are automatically mapped to their
  base language codes.
</Note>

## Usage Example

### Basic Configuration

Initialize the `SambaNovaSTTService` and use it in a pipeline:

```python
from pipecat.services.sambanova.stt import SambaNovaSTTService
from pipecat.transcriptions.language import Language

# Simple setup
stt = SambaNovaSTTService(
    api_key=os.getenv("SAMBANOVA_API_KEY"),
    model="Whisper-Large-v3",
    language=Language.EN
)

# Use in pipeline with VAD
pipeline = Pipeline([
    transport.input(),  # Must include VAD analyzer
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Advanced Configuration

```python
# Production-ready configuration
stt = SambaNovaSTTService(
    api_key=os.getenv("SAMBANOVA_API_KEY"),
    model="Whisper-Large-v3",
    language=Language.EN,
    prompt="Transcribe the following professional conversation:",
    temperature=0.1,  # More deterministic output
    base_url="https://api.sambanova.ai/v1"  # Custom endpoint if needed
)
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `SambaNovaSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **VAD Requirement**: Must use a transport with VAD analyzer for proper operation
- **Segmented Processing**: Transcribes complete utterances, not continuous streams
- **OpenAI Compatibility**: Uses OpenAI-compatible API interface
- **Language Detection**: Automatic language detection when no language is specified



================================================
FILE: server/services/stt/soniox.mdx
================================================
---
title: "Soniox"
description: "Speech-to-text service implementation using Soniox’s WebSocket API"
---

## Overview

`SonioxSTTService` is a speech-to-text (STT) service that integrates with Soniox’s WebSocket API to provide real-time transcription capabilities. It processes audio input and produces transcription frames and interim transcription frames in real time, supporting over 60 languages. Supports custom context, multiple languages in same conversation and more.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://pipecat-docs.readthedocs.io/en/latest/api/pipecat.services.soniox.stt.html"
  >
    Complete API documentation
  </Card>
  <Card
    title="Soniox Docs"
    icon="book"
    href="https://soniox.com/docs/speech-to-text/get-started"
  >
    Official Soniox documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07aa-interruptible-soniox.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>


## Installation

To use `SonioxSTTService`, you need to install the Soniox dependencies:

```bash
pip install "pipecat-ai[soniox]"
```

You'll also need to set up your Soniox API key as an environment variable: `SONIOX_API_KEY`.

<Tip>
  You can obtain a Soniox API key by signing up at [Soniox console](https://console.soniox.com/).
</Tip>

## Frames

### Input

By default the service processes raw audio data with the following requirements:

- PCM audio format
- 16-bit depth
- 16kHz sample rate
- Single channel

Other audio formats, custom sample rates and number of channels are supported. Refer to [Audio formats](https://soniox.com/docs/speech-to-text/core-concepts/audio-formats) for more information.

### Output

The service produces two types of frames during transcription:

- `TranscriptionFrame` - Final transcription results
- `InterimTranscriptionFrame` - Real-time transcription updates
- `ErrorFrame` - Connection or processing errors


## Advanced Features

### Language Hints

There is no need to pre-select a language — the model automatically detects and transcribes any supported language. It also handles multilingual audio seamlessly, even when multiple languages are mixed within a single sentence or conversation.

However, when you have prior knowledge of the languages likely to be spoken in your audio, you can use language hints to guide the model toward those languages for even greater recognition accuracy.

```python
from pipecat.services.soniox import SonioxInputParams
from pipecat.transcriptions.language import Language

SonioxInputParams(
  language_hints=[Language.EN, Language.ES, Language.JA, Language.ZH],
)
```

Language variants are ignored, for example `Language.EN_GB` will be treated same as `Language.EN`. See [Supported Languages](https://soniox.com/docs/speech-to-text/core-concepts/supported-languages) for a list of supported languages.

You can learn more about language hints in the [Soniox documentation](https://soniox.com/docs/speech-to-text/core-concepts/language-hints).

### Customization with Context

By providing context, you help the AI model better understand and anticipate the language in your audio - even if some terms do not appear clearly or completely. 

```python
from pipecat.services.soniox import SonioxInputParams

SonioxInputParams(
  context="Celebrex, Zyrtec, Xanax, Prilosec, Amoxicillin Clavulanate Potassium",
),
```

### Endpoint Detection and VAD

The `SonioxSTTService` processes your speech and has two ways of knowing when to finalize the text.

#### Automatic Pause Detection

By default, the service listens for natural pauses in your speech. When it detects that you've likely finished a sentence, it finalizes the transcription.
You can learn more about Endpoint Detection in [Soniox documentation](https://soniox.com/docs/speech-to-text/core-concepts/endpoint-detection).

#### Using Voice Activity Detection (VAD)

For more explicit control, you can use a dedicated Voice Activity Detection (VAD) component within your Pipecat pipeline. The VAD's job is to detect when a user has completely stopped talking.

To enable this behavior, set `vad_force_turn_endpoint` to `True`. This will disable the automatic endpoint detection and force the service to return transcription results as soon as the user stops talking.


## Usage Example

```python
from pipecat.pipeline.pipeline import Pipeline
from pipecat.services.soniox.stt import SonioxSTTService, SonioxInputParams
from pipecat.transcriptions.language import Language

# Configure the service
stt = SonioxSTTService(
    api_key=os.getenv("SONIOX_API_KEY"),
    params=SonioxInputParams(
      language_hints=[Language.EN, Language.ES, Language.JA, Language.ZH],
    ),
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    ...
])
```



================================================
FILE: server/services/stt/speechmatics.mdx
================================================
---
title: "Speechmatics"
description: "Speech-to-text service implementation using Speechmatics’ real-time transcription STT API"
---

## Overview

`SpeechmaticsSTTService` enables real-time speech transcription using Speechmatics' WebSocket API with partial + final results, speaker diarization, and end of utterance detection (VAD).

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.speechmatics.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Speechmatics Docs"
    icon="book"
    href="https://docs.speechmatics.com/rt-api-ref"
  >
    Official Speechmatics documentation and features
  </Card>
  <Card
    title="Speaker Diarization"
    icon="book"
    href="https://docs.speechmatics.com/speech-to-text/features/diarization#speaker-diarization"
  >
    Separating out different speakers in the audio
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07a-interruptible-speechmatics.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>

## Installation

To use `SpeechmaticsSTTService`, install the required dependencies:

```bash
pip install "pipecat-ai[speechmatics]"
```

You'll also need to set up your Speechmatics API key as an environment variable: `SPEECHMATICS_API_KEY`.

<Tip>
  Get your API key from the [Speechmatics
  Portal](https://portal.speechmatics.com/dashboard).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)

### Output

- `InterimTranscriptionFrame` - Real-time transcription updates
- `TranscriptionFrame` - Final transcription results

## Endpoints

Speechmatics STT supports the following endpoints (defaults to `EU2`):

| Region | Environment   | STT Endpoint                     | Access                    |
| ------ | ------------- | -------------------------------- | ------------------------- |
| EU     | EU1           | `wss://neu.rt.speechmatics.com/` | Self-Service / Enterprise |
| EU     | EU2 (Default) | `wss://eu2.rt.speechmatics.com/` | Self-Service / Enterprise |
| US     | US1           | `wss://wus.rt.speechmatics.com/` | Enterprise                |

## Feature Discovery

To check the languages and features supported by Speechmatics STT, you can use the following code:

```shell
curl "https://eu2.rt.speechmatics.com/v1/discovery/features"
```

### Language Support

<Note>
  Refer to the [Speechmatics
  docs](https://docs.speechmatics.com/introduction/supported-languages) for more
  information on supported languages.
</Note>

Speechmatics STT supports the following languages and regional variants.

Setting a language can be done using the `language` parameter when creating the STT object. The exception to this is English / Mandarin which has the code `cmn_en` and must be set using the `language_code` parameter.

| Language Code  | Description | Locales                   |
| -------------- | ----------- | ------------------------- |
| `Language.AR`  | Arabic      | -                         |
| `Language.BA`  | Bashkir     | -                         |
| `Language.EU`  | Basque      | -                         |
| `Language.BE`  | Belarusian  | -                         |
| `Language.BG`  | Bulgarian   | -                         |
| `Language.BN`  | Bengali     | -                         |
| `Language.YUE` | Cantonese   | -                         |
| `Language.CA`  | Catalan     | -                         |
| `Language.HR`  | Croatian    | -                         |
| `Language.CS`  | Czech       | -                         |
| `Language.DA`  | Danish      | -                         |
| `Language.NL`  | Dutch       | -                         |
| `Language.EN`  | English     | `en-US`, `en-GB`, `en-AU` |
| `Language.EO`  | Esperanto   | -                         |
| `Language.ET`  | Estonian    | -                         |
| `Language.FA`  | Persian     | -                         |
| `Language.FI`  | Finnish     | -                         |
| `Language.FR`  | French      | -                         |
| `Language.GL`  | Galician    | -                         |
| `Language.DE`  | German      | -                         |
| `Language.EL`  | Greek       | -                         |
| `Language.HE`  | Hebrew      | -                         |
| `Language.HI`  | Hindi       | -                         |
| `Language.HU`  | Hungarian   | -                         |
| `Language.IA`  | Interlingua | -                         |
| `Language.IT`  | Italian     | -                         |
| `Language.ID`  | Indonesian  | -                         |
| `Language.GA`  | Irish       | -                         |
| `Language.JA`  | Japanese    | -                         |
| `Language.KO`  | Korean      | -                         |
| `Language.LV`  | Latvian     | -                         |
| `Language.LT`  | Lithuanian  | -                         |
| `Language.MS`  | Malay       | -                         |
| `Language.MT`  | Maltese     | -                         |
| `Language.CMN` | Mandarin    | `cmn-Hans`, `cmn-Hant`    |
| `Language.MR`  | Marathi     | -                         |
| `Language.MN`  | Mongolian   | -                         |
| `Language.NO`  | Norwegian   | -                         |
| `Language.PL`  | Polish      | -                         |
| `Language.PT`  | Portuguese  | -                         |
| `Language.RO`  | Romanian    | -                         |
| `Language.RU`  | Russian     | -                         |
| `Language.SK`  | Slovakian   | -                         |
| `Language.SL`  | Slovenian   | -                         |
| `Language.ES`  | Spanish     | -                         |
| `Language.SV`  | Swedish     | -                         |
| `Language.SW`  | Swahili     | -                         |
| `Language.TA`  | Tamil       | -                         |
| `Language.TH`  | Thai        | -                         |
| `Language.TR`  | Turkish     | -                         |
| `Language.UG`  | Uyghur      | -                         |
| `Language.UK`  | Ukrainian   | -                         |
| `Language.UR`  | Urdu        | -                         |
| `Language.VI`  | Vietnamese  | -                         |
| `Language.CY`  | Welsh       | -                         |

For bilingual transcription, use the `language_code` and `domain` parameters as follows:

| Language Code | Description        | Domain Options |
| ------------- | ------------------ | -------------- |
| `cmn_en`      | English / Mandarin | -              |
| `en_ms`       | English / Malay    | -              |
| `Language.ES` | English / Spanish  | `bilingual-en` |
| `en_ta`       | English / Tamil    | -              |

## Speaker Diarization

Speechmatics STT supports speaker diarization, which separates out different speakers in the audio. The identity of each speaker is returned in the TranscriptionFrame objects in the `user_id` attribute.

To enable this feature, set `enable_diarization` to `True`. Additionally, if `speaker_active_format` or `speaker_passive_format` are provided, then the text output for the TranscriptionFrame will be formatted to this specification. Your system context can then be updated to include information about this format to understand which speaker spoke which words. The passive format is optional and is used when the engine has been told to focus on specific speakers and other speakers will then be formatted using the `speaker_passive_format` format.

- `speaker_active_format` -> the formatter for active speakers
- `speaker_passive_format` -> the formatter for passive / background speakers

Examples:

- `<{speaker_id}>{text}</{speaker_id}>` -> `<S1>Good morning.</S1>`.
- `@{speaker_id}: {text}` -> `@S1: Good morning.`.

### Available attributes

| Attribute    | Description           | Example         |
| ------------ | --------------------- | --------------- |
| `speaker_id` | The ID of the speaker | `S1`            |
| `text`       | The transcribed text  | `Good morning.` |

## Usage Examples

Examples are included in the Pipecat project:

- Using Speechmatics STT service -> [07a-interruptible-speechmatics.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07a-interruptible-speechmatics.py)
- Using Speechmatics STT service with VAD -> [07a-interruptible-speechmatics-vad.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07a-interruptible-speechmatics-vad.py)
- Transcribing with Speechmatics STT -> [13h-speechmatics-transcription.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/13h-speechmatics-transcription.py)

Sample projects:

- Guess Who -> [Guess Who](https://github.com/sam-s10s/pipecat-guess-who)
- Guess Who Board Game -> [Guess Who](https://github.com/sam-s10s/pipecat-guess-who-irl)

### Basic Configuration

Initialize the `SpeechmaticsSTTService` and use it in a pipeline:

```python
from pipecat.services.speechmatics.stt import SpeechmaticsSTTService
from pipecat.transcriptions.language import Language

# Configure service
stt = SpeechmaticsSTTService(
    api_key="your-api-key",
    params=SpeechmaticsSTTService.InputParams(
        language=Language.FR,
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### With Diarization

This will enable diarization and also only go to the LLM if words are spoken from the first speaker (`S1`). Words from other speakers are transcribed but only sent when the first speaker speaks. When using the `enable_vad` option, this will use the speaker diarization to determine when a speaker is speaking. You will need to disable VAD options within the selected transport object to ensure this works correctly (see [07b-interruptible-speechmatics-vad.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07b-interruptible-speechmatics-vad.py) as an example).

Initialize the `SpeechmaticsSTTService` and use it in a pipeline:

```python
from pipecat.services.speechmatics.stt import SpeechmaticsSTTService
from pipecat.transcriptions.language import Language

# Configure service
stt = SpeechmaticsSTTService(
    api_key="your-api-key",
    params=SpeechmaticsSTTService.InputParams(
        language=Language.EN,
        enable_diarization=True,
        enable_vad=True,
        speaker_active_format="<{speaker_id}>{text}</{speaker_id}>",
        speaker_passive_format="<PASSIVE><{speaker_id}>{text}</{speaker_id}></PASSIVE>",
        focus_speakers=["S1"],
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Additional Notes

- **Connection Management**: Automatically handles WebSocket connections and reconnections
- **Sample Rate**: The default sample rate of `16000` in `pcm_s16le` format
- **VAD Integration**: Optionally supports Speechmatics' built-in VAD and end of utterance detection



================================================
FILE: server/services/stt/ultravox.mdx
================================================
---
title: "Ultravox"
description: "Speech-to-text service implementation using a locally-loaded Ultravox multimodal model"
---

## Overview

`UltravoxSTTService` provides real-time speech-to-text using the Ultravox multimodal model running locally. Ultravox directly encodes audio into the LLM's embedding space, eliminating traditional ASR components and providing faster, more efficient transcription with built-in conversational understanding.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.ultravox.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card title="Ultravox Docs" icon="book" href="https://docs.ultravox.ai/">
    Official Ultravox documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07u-interruptible-ultravox.py"
  >
    Working example with GPU optimization
  </Card>
</CardGroup>

## Installation

To use Ultravox services, install the required dependency:

```bash
pip install "pipecat-ai[ultravox]"
```

You'll also need a Hugging Face token to access the models: `HF_TOKEN`.

<Tip>
  Get your Hugging Face token from [Hugging Face
  Settings](https://huggingface.co/settings/tokens).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, 16kHz, mono)
- `UserStartedSpeakingFrame` - Triggers audio buffering
- `UserStoppedSpeakingFrame` - Processes collected audio
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `LLMFullResponseStartFrame` - Indicates transcription generation start
- `LLMTextFrame` - Streaming text tokens as they're generated
- `LLMFullResponseEndFrame` - Indicates transcription completion
- `ErrorFrame` - Processing errors or resource issues

## Models

Ultravox offers several models with different resource requirements:

- `fixie-ai/ultravox-v0_6-llama-3_3-70b` - Latest model with improved accuracy and efficiency
- `fixie-ai/ultravox-v0_5-llama-3_3-70b` - Recommended for new deployments
- `fixie-ai/ultravox-v0_5-llama-3_1-8b` - Smaller model for resource-constrained environments
- `fixie-ai/ultravox-v0_4_1-llama-3_1-8b` - Previous version for compatibility
- `fixie-ai/ultravox-v0_4_1-llama-3_1-70b` - Larger model for high accuracy

See the [Ultravox models](https://huggingface.co/fixie-ai/collections) on Hugging Face for more details.

## Usage Example

### Basic Configuration

```python
from pipecat.services.ultravox.stt import UltravoxSTTService

# Simple setup with default model
ultravox_processor = UltravoxSTTService(
    hf_token=os.getenv("HF_TOKEN"),
    temperature=0.7,
    max_tokens=100
)

# Use in pipeline (requires VAD for speech detection)
pipeline = Pipeline([
    transport.input(),
    ultravox_processor,  # Note: Ultravox outputs LLM frames, not transcription frames
    tts,
    transport.output()
])
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from speech end to first token
- **Processing Duration** - Total time for audio processing and generation

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **VAD Dependency**: Requires Voice Activity Detection (VAD) to trigger audio processing
- **GPU Acceleration**: Designed for GPU deployment; consider using Cerebrium, Modal, or other GPU-optimized environments
- **Model Loading**: First model load can take several minutes; consider pre-initialization
- **Memory Usage**: Audio buffer grows with speech duration; automatically cleared after processing
- **Output Format**: Generates `LLMTextFrame` objects, not traditional `TranscriptionFrame`
- **Local Processing**: All processing happens locally; no external API calls after model download
- **Hugging Face Authentication**: Required for downloading models from Hugging Face Hub



================================================
FILE: server/services/stt/whisper.mdx
================================================
---
title: "Whisper"
description: "Speech-to-text service implementation using locally-downloaded Whisper models"
---

## Overview

`WhisperSTTService` provides offline speech recognition using OpenAI's Whisper models running locally. Supports multiple model sizes and hardware acceleration options including CPU, CUDA, and Apple Silicon (MLX).

<CardGroup cols={2}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.whisper.stt.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Whisper Github"
    icon="github"
    href="https://github.com/openai/whisper"
  >
    OpenAI's Whisper research paper and model details
  </Card>
  <Card
    title="Faster Whisper Example"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/13-whisper-transcription.py"
  >
    Working example with standard Whisper
  </Card>
  <Card
    title="MLX Whisper Example"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/13e-whisper-mlx.py"
  >
    Working example with Apple Silicon optimization
  </Card>
</CardGroup>

## Installation

Choose your installation based on your hardware:

### Standard Whisper (CPU/CUDA)

```bash
pip install "pipecat-ai[whisper]"
```

### MLX Whisper (Apple Silicon)

```bash
pip install "pipecat-ai[mlx-whisper]"
```

<Note>
  First run will download the selected model from Hugging Face. Model sizes
  range from 39MB to 1.5GB.
</Note>

## Frames

### Input

- `InputAudioRawFrame` - Raw PCM audio data (16-bit, mono)
- `UserStartedSpeakingFrame` - VAD signal to start buffering audio
- `UserStoppedSpeakingFrame` - VAD signal indicating speech segment completion
- `STTUpdateSettingsFrame` - Runtime transcription configuration updates
- `STTMuteFrame` - Mute audio input for transcription

### Output

- `TranscriptionFrame` - Final transcription results (segmented processing)
- `ErrorFrame` - Model loading or processing errors

## Service Comparison

| Service                | Hardware      | Performance | Memory   | Best For                         |
| ---------------------- | ------------- | ----------- | -------- | -------------------------------- |
| `WhisperSTTService`    | CPU/CUDA      | Good        | Moderate | General use, GPU acceleration    |
| `WhisperSTTServiceMLX` | Apple Silicon | Better      | Lower    | Mac users, optimized performance |

## Model Selection

### Standard Whisper Models

- `TINY`: Smallest multilingual model, fastest inference
- `BASE`: Basic multilingual model, good speed/quality balance
- `SMALL`: Small multilingual model, better speed/quality balance than BASE
- `MEDIUM`: Medium-sized multilingual model, better quality
- `LARGE`: Best quality multilingual model, slower inference
- `LARGE_V3_TURBO`: Fast multilingual model, slightly lower quality than LARGE
- `DISTIL_LARGE_V2`: Fast multilingual distilled model.
- `DISTIL_MEDIUM_EN`: Fast English-only distilled model.

### MLX Whisper Models (Apple Silicon)

- `TINY`: Smallest multilingual model for MLX
- `MEDIUM`: Medium-sized multilingual model for MLX
- `LARGE_V3`: Best quality multilingual model for MLX
- `LARGE_V3_TURBO`: Finetuned, pruned Whisper large-v3, much faster with slightly lower quality
- `DISTIL_LARGE_V3`: Fast multilingual distilled model for MLX
- `LARGE_V3_TURBO_Q4`: LARGE_V3_TURBO quantized to Q4 for reduced memory usage

## Language Support

Whisper supports a number of languages with automatic detection and regional variants:

<Accordion title="View All Supported Languages">

| Language Code    | Description                  | Service Codes |
| ---------------- | ---------------------------- | ------------- |
| `Language.AR`    | Arabic                       | `ar`          |
| `Language.AR_AE` | Arabic (UAE)                 | `ar`          |
| `Language.AR_BH` | Arabic (Bahrain)             | `ar`          |
| `Language.AR_DZ` | Arabic (Algeria)             | `ar`          |
| `Language.AR_EG` | Arabic (Egypt)               | `ar`          |
| `Language.AR_IQ` | Arabic (Iraq)                | `ar`          |
| `Language.AR_JO` | Arabic (Jordan)              | `ar`          |
| `Language.AR_KW` | Arabic (Kuwait)              | `ar`          |
| `Language.AR_LB` | Arabic (Lebanon)             | `ar`          |
| `Language.AR_LY` | Arabic (Libya)               | `ar`          |
| `Language.AR_MA` | Arabic (Morocco)             | `ar`          |
| `Language.AR_OM` | Arabic (Oman)                | `ar`          |
| `Language.AR_QA` | Arabic (Qatar)               | `ar`          |
| `Language.AR_SA` | Arabic (Saudi Arabia)        | `ar`          |
| `Language.AR_SY` | Arabic (Syria)               | `ar`          |
| `Language.AR_TN` | Arabic (Tunisia)             | `ar`          |
| `Language.AR_YE` | Arabic (Yemen)               | `ar`          |
| `Language.BN`    | Bengali                      | `bn`          |
| `Language.BN_BD` | Bengali (Bangladesh)         | `bn`          |
| `Language.BN_IN` | Bengali (India)              | `bn`          |
| `Language.CS`    | Czech                        | `cs`          |
| `Language.CS_CZ` | Czech (Czech Republic)       | `cs`          |
| `Language.DA`    | Danish                       | `da`          |
| `Language.DA_DK` | Danish (Denmark)             | `da`          |
| `Language.DE`    | German                       | `de`          |
| `Language.DE_AT` | German (Austria)             | `de`          |
| `Language.DE_CH` | German (Switzerland)         | `de`          |
| `Language.DE_DE` | German (Germany)             | `de`          |
| `Language.EL`    | Greek                        | `el`          |
| `Language.EL_GR` | Greek (Greece)               | `el`          |
| `Language.EN`    | English                      | `en`          |
| `Language.EN_AU` | English (Australia)          | `en`          |
| `Language.EN_CA` | English (Canada)             | `en`          |
| `Language.EN_GB` | English (UK)                 | `en`          |
| `Language.EN_HK` | English (Hong Kong)          | `en`          |
| `Language.EN_IE` | English (Ireland)            | `en`          |
| `Language.EN_IN` | English (India)              | `en`          |
| `Language.EN_KE` | English (Kenya)              | `en`          |
| `Language.EN_NG` | English (Nigeria)            | `en`          |
| `Language.EN_NZ` | English (New Zealand)        | `en`          |
| `Language.EN_PH` | English (Philippines)        | `en`          |
| `Language.EN_SG` | English (Singapore)          | `en`          |
| `Language.EN_TZ` | English (Tanzania)           | `en`          |
| `Language.EN_US` | English (US)                 | `en`          |
| `Language.EN_ZA` | English (South Africa)       | `en`          |
| `Language.ES`    | Spanish                      | `es`          |
| `Language.ES_AR` | Spanish (Argentina)          | `es`          |
| `Language.ES_BO` | Spanish (Bolivia)            | `es`          |
| `Language.ES_CL` | Spanish (Chile)              | `es`          |
| `Language.ES_CO` | Spanish (Colombia)           | `es`          |
| `Language.ES_CR` | Spanish (Costa Rica)         | `es`          |
| `Language.ES_CU` | Spanish (Cuba)               | `es`          |
| `Language.ES_DO` | Spanish (Dominican Republic) | `es`          |
| `Language.ES_EC` | Spanish (Ecuador)            | `es`          |
| `Language.ES_ES` | Spanish (Spain)              | `es`          |
| `Language.ES_GQ` | Spanish (Equatorial Guinea)  | `es`          |
| `Language.ES_GT` | Spanish (Guatemala)          | `es`          |
| `Language.ES_HN` | Spanish (Honduras)           | `es`          |
| `Language.ES_MX` | Spanish (Mexico)             | `es`          |
| `Language.ES_NI` | Spanish (Nicaragua)          | `es`          |
| `Language.ES_PA` | Spanish (Panama)             | `es`          |
| `Language.ES_PE` | Spanish (Peru)               | `es`          |
| `Language.ES_PR` | Spanish (Puerto Rico)        | `es`          |
| `Language.ES_PY` | Spanish (Paraguay)           | `es`          |
| `Language.ES_SV` | Spanish (El Salvador)        | `es`          |
| `Language.ES_US` | Spanish (US)                 | `es`          |
| `Language.ES_UY` | Spanish (Uruguay)            | `es`          |
| `Language.ES_VE` | Spanish (Venezuela)          | `es`          |
| `Language.FA`    | Persian                      | `fa`          |
| `Language.FA_IR` | Persian (Iran)               | `fa`          |
| `Language.FI`    | Finnish                      | `fi`          |
| `Language.FI_FI` | Finnish (Finland)            | `fi`          |
| `Language.FR`    | French                       | `fr`          |
| `Language.FR_BE` | French (Belgium)             | `fr`          |
| `Language.FR_CA` | French (Canada)              | `fr`          |
| `Language.FR_CH` | French (Switzerland)         | `fr`          |
| `Language.FR_FR` | French (France)              | `fr`          |
| `Language.HI`    | Hindi                        | `hi`          |
| `Language.HI_IN` | Hindi (India)                | `hi`          |
| `Language.HU`    | Hungarian                    | `hu`          |
| `Language.HU_HU` | Hungarian (Hungary)          | `hu`          |
| `Language.ID`    | Indonesian                   | `id`          |
| `Language.ID_ID` | Indonesian (Indonesia)       | `id`          |
| `Language.IT`    | Italian                      | `it`          |
| `Language.IT_IT` | Italian (Italy)              | `it`          |
| `Language.JA`    | Japanese                     | `ja`          |
| `Language.JA_JP` | Japanese (Japan)             | `ja`          |
| `Language.KO`    | Korean                       | `ko`          |
| `Language.KO_KR` | Korean (Korea)               | `ko`          |
| `Language.NL`    | Dutch                        | `nl`          |
| `Language.NL_BE` | Dutch (Belgium)              | `nl`          |
| `Language.NL_NL` | Dutch (Netherlands)          | `nl`          |
| `Language.PL`    | Polish                       | `pl`          |
| `Language.PL_PL` | Polish (Poland)              | `pl`          |
| `Language.PT`    | Portuguese                   | `pt`          |
| `Language.PT_BR` | Portuguese (Brazil)          | `pt`          |
| `Language.PT_PT` | Portuguese (Portugal)        | `pt`          |
| `Language.RO`    | Romanian                     | `ro`          |
| `Language.RO_RO` | Romanian (Romania)           | `ro`          |
| `Language.RU`    | Russian                      | `ru`          |
| `Language.RU_RU` | Russian (Russia)             | `ru`          |
| `Language.SK`    | Slovak                       | `sk`          |
| `Language.SK_SK` | Slovak (Slovakia)            | `sk`          |
| `Language.SV`    | Swedish                      | `sv`          |
| `Language.SV_SE` | Swedish (Sweden)             | `sv`          |
| `Language.TH`    | Thai                         | `th`          |
| `Language.TH_TH` | Thai (Thailand)              | `th`          |
| `Language.TR`    | Turkish                      | `tr`          |
| `Language.TR_TR` | Turkish (Turkey)             | `tr`          |
| `Language.UK`    | Ukrainian                    | `uk`          |
| `Language.UK_UA` | Ukrainian (Ukraine)          | `uk`          |
| `Language.UR`    | Urdu                         | `ur`          |
| `Language.UR_IN` | Urdu (India)                 | `ur`          |
| `Language.UR_PK` | Urdu (Pakistan)              | `ur`          |
| `Language.VI`    | Vietnamese                   | `vi`          |
| `Language.VI_VN` | Vietnamese (Vietnam)         | `vi`          |
| `Language.ZH`    | Chinese                      | `zh`          |
| `Language.ZH_CN` | Chinese (China)              | `zh`          |
| `Language.ZH_HK` | Chinese (Hong Kong)          | `zh`          |
| `Language.ZH_TW` | Chinese (Taiwan)             | `zh`          |

</Accordion>

Common languages:

- `Language.EN` - English - `en`
- `Language.ES` - Spanish - `es`
- `Language.FR` - French - `fr`
- `Language.DE` - German - `de`
- `Language.IT` - Italian - `it`
- `Language.JA` - Japanese - `ja`
- `Language.KO` - Korean - `ko`
- `Language.ZH` - Chinese - `zh`
- `Language.PT` - Portuguese - `pt`
- `Language.RU` - Russian - `ru`
- `Language.AR` - Arabic - `ar`
- `Language.HI` - Hindi - `hi`

<Note>
  Whisper can automatically detect language or you can specify it for better
  performance. All regional variants map to the same base language code.
</Note>

## Usage Example

### Basic Configuration

Initialize the `WhisperSTTService` and use it in a pipeline:

```python
from pipecat.services.whisper.stt import WhisperSTTService, Model
from pipecat.transcriptions.language import Language

# Standard Whisper with default settings
stt = WhisperSTTService()

# Use in pipeline with VAD
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### GPU Acceleration

```python
# CUDA acceleration for faster processing
stt = WhisperSTTService(
    model=Model.LARGE_V3_TURBO,
    device="cuda",
    compute_type="float16",  # Reduce memory usage
    no_speech_prob=0.3,      # Lower threshold for speech detection
    language=Language.EN     # Specify language for better performance
)
```

### Apple Silicon Optimization

```python
from pipecat.services.whisper.stt import WhisperSTTServiceMLX, MLXModel

# MLX Whisper optimized for Apple Silicon
stt = WhisperSTTServiceMLX(
    model=MLXModel.LARGE_V3_TURBO_Q4,  # Quantized for efficiency
    no_speech_prob=0.6,
    language=Language.EN,
)
```

### Dynamic Configuration

Make settings updates by pushing an `STTUpdateSettingsFrame` for the `WhisperSTTService`:

```python
from pipecat.frames.frames import STTUpdateSettingsFrame

await task.queue_frame(STTUpdateSettingsFrame(
    language=Language.FR,
  )
)
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from audio input to first transcription
- **Processing Duration** - Total time spent processing audio segments

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Segmented Processing**: Both services use VAD to process speech in segments rather than continuously
- **Offline Operation**: Runs completely offline after initial model download
- **Speech Filtering**: `no_speech_prob` threshold filters out non-speech audio segments
- **Automatic Normalization**: Audio is automatically normalized to float32 [-1.0, 1.0] range



================================================
FILE: server/services/transport/daily.mdx
================================================
---
title: "DailyTransport"
description: "WebRTC transport implementation using Daily for real-time audio/video communication"
---

## Overview

`DailyTransport` provides real-time audio and video communication capabilities using Daily's hosted WebRTC platform. It supports bidirectional audio/video streams, transcription, voice activity detection, participant management, and advanced features like dial-in/out and recording without requiring your own WebRTC infrastructure.

<CardGroup cols={2}>
  <Card
    title="DailyTransport API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.daily.html"
  >
    Transport methods and configuration options
  </Card>
  <Card
    title="Daily API Docs"
    icon="book"
    href="https://docs.daily.co/reference/rest-api"
  >
    Official Daily REST API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/04a-transports-daily.py"
  >
    Working example with voice agent implementation
  </Card>
  <Card
    title="Daily Dashboard"
    icon="external-link"
    href="https://dashboard.daily.co/u/signup?pipecat=true"
  >
    Sign up for Daily API access
  </Card>
</CardGroup>

## Installation

To use `DailyTransport`, install the required dependencies:

```bash
pip install "pipecat-ai[daily]"
```

You'll also need to set up your Daily API key as an environment variable: `DAILY_API_KEY`.

<Tip>
  Get your API key by signing up at [Daily
  Dashboard](https://dashboard.daily.co/u/signup?pipecat=true).
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Audio data from room participants
- `UserImageRawFrame` - Video frames from participant cameras
- `TranscriptionFrame` - Final speech transcriptions
- `InterimTranscriptionFrame` - Real-time transcription updates
- `TransportMessageUrgentFrame` - Application messages from participants

### Output

- `OutputAudioRawFrame` - Audio data to room participants
- `OutputImageRawFrame` - Video frames to participants
- `OutputDTMFFrame` - DTMF tones for phone calls
- `TransportMessageFrame` - Application messages to participants
- `TransportMessageUrgentFrame` - Urgent messages to participants

## Key Features

- **Hosted WebRTC**: No infrastructure setup required - Daily handles all WebRTC complexity
- **Multi-participant Support**: Handle multiple participants with individual audio/video tracks
- **Built-in Transcription**: Real-time speech-to-text with Deepgram integration
- **Dial-in/Dial-out**: Connect to phone numbers via SIP/PSTN
- **Recording & Streaming**: Built-in call recording and live streaming capabilities
- **Advanced Controls**: Participant management, permissions, and media routing

## Usage Example

```python
import os
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.transports.services.daily import DailyTransport, DailyParams

async def main():
    # Create or get a Daily room (see Room Creation section below)
    room_url = "https://your-domain.daily.co/room-name"
    token = "your-meeting-token"  # Created via Daily API

    # Configure the transport
    transport = DailyTransport(
        room_url=room_url,
        token=token,
        bot_name="AI Assistant",
        params=DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            transcription_enabled=True,  # Enable real-time transcription
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive audio/video from participants
        # ... your processing chain
        transport.output(),             # Send audio/video to participants
    ])

    # Event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
         # Enable transcription for the participant
        await transport.capture_participant_transcription(participant["id"])
        # Start the conversation
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()  # End the session

    # Run the pipeline
    # ...
```

## Room Creation

Daily rooms must be created before connecting.

Pipecat's [development runner](/server/utilities/runner/guide) provides a helper function to create room URL, token, and room name:

```python
import aiohttp
from pipecat.runner.daily import configure

async def create_room():
    async with aiohttp.ClientSession() as session:
        # This helper creates a room and token automatically
        room_url, token = await configure(session)
        return room_url, token
```

Or manually via [Daily REST API](https://docs.daily.co/reference/rest-api/rooms):

```python
import aiohttp
from pipecat.transports.services.helpers.daily_rest import DailyRESTHelper

async def create_room_manual():
    daily_helper = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY"),
        aiohttp_session=session
    )

    # Create room
    room = await daily_helper.create_room()
    room_url = room["url"]

    # Create token with permissions
    token = await daily_helper.get_token(room_url, expiry_time=3600)

    return room_url, token
```

## Methods

`DailyTransport` provides comprehensive methods for controlling calls and participants:

### Room Management

- `participants()` - Get current participants in the room
- `participant_counts()` - Get participant count statistics
- `send_message()` - Send messages to participants

### Media Control

- `send_image()` - Send image frames to the room
- `send_audio()` - Send audio frames to the room
- `capture_participant_video()` - Capture video from specific participants
- `capture_participant_audio()` - Capture audio from specific participants

### Transcription Control

- `capture_participant_transcription()` - Enable transcription for participants
- `start_transcription()` - Start room-wide transcription
- `stop_transcription()` - Stop transcription
- `update_transcription()` - Update transcription settings

### Recording Control

- `start_recording()` - Start recording the call
- `stop_recording()` - Stop active recordings

### Dial Control

- `start_dialout()` - Initiate dial-out calls
- `stop_dialout()` - Stop dial-out calls
- `send_dtmf()` - Send DTMF tones
- `sip_call_transfer()` - Transfer SIP calls
- `sip_refer()` - Send SIP REFER requests

### Participant Management

- `update_remote_participants()` - Control participant permissions and settings
- `update_subscriptions()` - Manage media subscriptions
- `update_publishing()` - Control media publishing settings

### Chat & Messaging

- `send_prebuilt_chat_message()` - Send messages to Daily Prebuilt chat

<Tip>
  For complete method signatures, parameters, and examples, see the
  [`DailyTransport` API
  reference](https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.daily.html).
</Tip>

### Example Usage

```python
# Capture participant media
await transport.capture_participant_transcription("participant-123")
await transport.capture_participant_video("participant-123", framerate=30)

# Control participant permissions
await transport.update_remote_participants({
    "participant-123": {
        "inputsEnabled": {"microphone": False}
    }
})

# Start recording
await transport.start_recording({
    "rtmpUrl": "rtmp://your-server.com/live/stream"
})
```

## Event Handling

`DailyTransport` provides comprehensive event callbacks organized by category. Register callbacks using the `@transport.event_handler()` decorator:

### Connection Events

- `on_joined` - Bot successfully joined the room
- `on_left` - Bot left the room
- `on_call_state_updated` - Call state changes
- `on_error` - Transport errors occur

### Participant Events

- `on_first_participant_joined` - First participant joins (useful for starting conversations)
- `on_participant_joined` - Any participant joins
- `on_participant_left` - Participant leaves
- `on_participant_updated` - Participant information changes
- `on_client_connected` - Client (participant) connects
- `on_client_disconnected` - Client (participant) disconnects
- `on_active_speaker_changed` - Active speaker changes

### Communication Events

- `on_app_message` - Application messages received
- `on_transcription_message` - Speech transcription received

### Dial Events

- `on_dialin_*` - Dial-in connection events (ready, connected, stopped, error, warning)
- `on_dialout_*` - Dial-out call events (answered, connected, stopped, error, warning)

### Recording Events

- `on_recording_started` - Recording begins
- `on_recording_stopped` - Recording ends
- `on_recording_error` - Recording errors

<Tip>
  For complete event details, parameters, and examples, see the
  [`DailyTransport` API
  reference](https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.daily.html#pipecat.transports.services.daily.DailyCallbacks).
</Tip>

### Example Usage

```python
@transport.event_handler("on_transcription_message")
async def on_transcription(transport, message):
    text = message["text"]
    participant_id = message["participantId"]
    is_final = message["rawResponse"]["is_final"]
    print(f"Transcription from {participant_id}: {text}")

@transport.event_handler("on_recording_started")
async def on_recording_started(transport, status):
    print(f"Recording started: {status}")

@transport.event_handler("on_dialout_answered")
async def on_dialout_answered(transport, data):
    print(f"Dial-out call answered: {data}")
```

## Additional Notes

- **Scalability**: Daily handles the WebRTC infrastructure, supporting many concurrent rooms
- **Global Infrastructure**: Daily's global edge network ensures low latency worldwide
- **Enterprise Features**: Advanced routing, analytics, and compliance features available
- **Browser Compatibility**: Works with all modern browsers via Daily's client SDKs
- **Mobile Support**: Native iOS and Android SDKs available for mobile applications
- **SIP Integration**: Enterprise-grade phone system integration with dial-in/out capabilities

The hosted nature of Daily makes it ideal for production applications where you want professional WebRTC infrastructure without the operational complexity.



================================================
FILE: server/services/transport/fastapi-websocket.mdx
================================================
---
title: "FastAPIWebsocketTransport"
description: "WebSocket transport implementation for FastAPI web applications with telephony integration"
---

## Overview

`FastAPIWebsocketTransport` provides WebSocket support for FastAPI web applications, enabling real-time audio communication over WebSocket connections. It's primarily designed for telephony integrations with providers like Twilio, Telnyx, and Plivo, supporting bidirectional audio streams with configurable serializers and voice activity detection.

<Warning>
FastAPIWebsocketTransport is best suited for telephony applications and server-side WebSocket integrations.

For general client/server applications, we recommend using WebRTC-based transports for more robust network and media handling.

</Warning>

<CardGroup cols={2}>
  <Card
    title="FastAPIWebsocketTransport API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.fastapi_websocket.html"
  >
    Transport methods and configuration options
  </Card>
  <Card
    title="FastAPI Docs"
    icon="book"
    href="https://fastapi.tiangolo.com/advanced/websockets/"
  >
    Official FastAPI WebSocket documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot"
  >
    Working example with Twilio integration
  </Card>
  <Card
    title="Telephony Providers"
    icon="settings"
    href="/server/services/serializers/introduction"
  >
    Learn about supported FrameSerializers for telephony providers
  </Card>
</CardGroup>

## Installation

To use `FastAPIWebsocketTransport`, install the required dependencies:

```bash
pip install "pipecat-ai[websocket]"
```

No additional API keys are required for the transport itself, but you'll need credentials for your chosen telephony provider.

<Tip>
  This transport is commonly used with telephony providers. See the [Frame
  Serializers](/server/services/serializers/introduction) documentation for
  provider-specific setup.
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw audio data from WebSocket client
- `Frame` - Other frame types based on configured serializer

### Output

- `OutputAudioRawFrame` - Audio data to WebSocket client (with optional WAV headers)
- `TransportMessageFrame` - Application messages to client
- `TransportMessageUrgentFrame` - Urgent messages to client

## Key Features

- **Telephony Integration**: Optimized for phone call audio streaming with major providers
- **Frame Serialization**: Configurable serializers for different telephony protocols
- **Session Management**: Built-in connection monitoring and timeout handling
- **Audio Timing**: Simulates audio device timing for proper call flow
- **WAV Header Support**: Optional WAV header generation for compatibility

## Usage Example

### Using the Development Runner (Recommended)

The easiest way to use `FastAPIWebsocketTransport` for telephony is with Pipecat's development runner:

```python
import os
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.runner.types import RunnerArguments, WebSocketRunnerArguments
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

async def run_bot(transport):
    """Your core bot logic - works with any transport."""
    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive audio from caller
        stt,                            # Convert speech to text
        context_aggregator.user(),      # Add user messages to context
        llm,                            # Process text with LLM
        tts,                            # Convert text to speech
        transport.output(),             # Send audio to caller
        context_aggregator.assistant(), # Add assistant responses to context
    ])

    # Event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Start conversation when caller connects
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    # Run the pipeline
    # ...

async def bot(runner_args: RunnerArguments):
    """Entry point called by the development runner."""
    if isinstance(runner_args, WebSocketRunnerArguments):
        # Auto-detect telephony provider and create appropriate serializer
        from pipecat.runner.utils import parse_telephony_websocket

        transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)

        # Create serializer based on detected provider
        if transport_type == "twilio":
            from pipecat.serializers.twilio import TwilioFrameSerializer
            serializer = TwilioFrameSerializer(
                stream_sid=call_data["stream_id"],
                call_sid=call_data["call_id"],
                account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
                auth_token=os.getenv("TWILIO_AUTH_TOKEN"),
            )

        transport = FastAPIWebsocketTransport(
            websocket=runner_args.websocket,
            params=FastAPIWebsocketParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
                serializer=serializer,
            ),
        )
        await run_bot(transport)

if __name__ == "__main__":
    from pipecat.runner.run import main
    main()
```

Run your telephony bot with:

```bash
python bot.py -t twilio -x your-ngrok-domain.ngrok.io
python bot.py -t telnyx -x your-ngrok-domain.ngrok.io
python bot.py -t plivo -x your-ngrok-domain.ngrok.io
```

The development runner automatically:

- Creates FastAPI server with telephony webhook endpoints
- Sets up WebSocket endpoints for audio streaming
- Handles provider-specific message parsing and serialization
- Manages WebSocket connection lifecycle

<Tip>
  The `-x` flag specifies your public domain (like ngrok) that telephony
  providers can reach for webhooks.
</Tip>

### Manual FastAPI Implementation

For custom deployments, you can implement the FastAPI server manually:

#### 1. FastAPI Server Setup

```python
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import HTMLResponse

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/")
async def start_call():
    """Handle telephony provider webhook."""
    # Return TwiML/XML response that establishes WebSocket connection
    xml_response = f"""<?xml version="1.0" encoding="UTF-8"?>
    <Response>
      <Connect>
        <Stream url="wss://your-domain.com/ws"></Stream>
      </Connect>
      <Pause length="40"/>
    </Response>"""
    return HTMLResponse(content=xml_response, media_type="application/xml")

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connections for audio streaming."""
    await websocket.accept()

    # Parse initial connection data from provider
    start_data = websocket.iter_text()
    await start_data.__anext__()
    call_data = json.loads(await start_data.__anext__())

    # Extract call information
    stream_sid = call_data["start"]["streamSid"]
    call_sid = call_data["start"]["callSid"]

    await run_bot(websocket, stream_sid, call_sid)
```

<Warning>
  Websocket message parsing varies based on the telephony provider. Use the
  `parse_telephony_websocket()` utility to auto-detect and extract call data.
</Warning>

#### 2. Bot Implementation

```python
import json
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

async def run_bot(websocket: WebSocket, stream_sid: str, call_sid: str):
    """Run the Pipecat bot for a specific call."""
    # Create serializer for telephony provider
    serializer = TwilioFrameSerializer(
        stream_sid=stream_sid,
        call_sid=call_sid,
        account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN"),
    )

    # Create transport
    transport = FastAPIWebsocketTransport(
        websocket=websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
            session_timeout=30,  # Optional timeout
        ),
    )

    # Your pipeline setup
    # ...

    # Run the pipeline
    runner = PipelineRunner(handle_sigint=False, force_gc=True)
    await runner.run(task)
```

## Event Handling

FastAPIWebsocketTransport provides event callbacks for connection management. Register callbacks using the `@transport.event_handler()` decorator:

### Connection Events

- `on_client_connected` - Client connects to WebSocket endpoint
- `on_client_disconnected` - Client disconnects from WebSocket endpoint
- `on_session_timeout` - Session timeout occurs (if configured)

<Tip>
  For complete event details and parameters, see the [FastAPIWebsocketTransport
  API
  reference](https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.fastapi_websocket.html#pipecat.transports.network.fastapi_websocket.FastAPIWebsocketCallbacks).
</Tip>

### Example Usage

```python
@transport.event_handler("on_client_connected")
async def on_client_connected(transport, websocket):
    logger.info("Caller connected")
    # Start conversation
    await task.queue_frames([TextFrame("Hello! How can I help you today?")])

@transport.event_handler("on_client_disconnected")
async def on_client_disconnected(transport, websocket):
    logger.info("Call ended")
    await task.cancel()

@transport.event_handler("on_session_timeout")
async def on_session_timeout(transport, websocket):
    logger.info("Call timed out")
    # Handle timeout (e.g., play message, end call)
```

## Telephony Integration

FastAPIWebsocketTransport works with major telephony providers through frame serializers:

### Supported Providers

- **Twilio**: Use [`TwilioFrameSerializer`](/server/services/serializers/twilio) for Media Streams integration
- **Telnyx**: Use [`TelnyxFrameSerializer`](/server/services/serializers/telnyx) for WebSocket streaming
- **Plivo**: Use [`PlivoFrameSerializer`](/server/services/serializers/plivo) for WebSocket streaming
- **Exotel**: Use [`ExotelFrameSerializer`](/server/services/serializers/exotel) for WebSocket integration

### Provider-Specific Setup

Each provider requires specific configuration and webhook URLs. The development runner handles this automatically, or you can configure manually:

```python
# Twilio setup
serializer = TwilioFrameSerializer(
    stream_sid="stream_id_from_webhook",
    call_sid="call_id_from_webhook",
    account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
    auth_token=os.getenv("TWILIO_AUTH_TOKEN"),
)

# Telnyx setup
serializer = TelnyxFrameSerializer(
    stream_id="stream_id_from_webhook",
    call_control_id="call_control_id_from_webhook",
    api_key=os.getenv("TELNYX_API_KEY"),
)
```

See the [Frame Serializers](/server/services/serializers/introduction) documentation for complete provider setup guides.

## Advanced Configuration

### Audio Processing

```python
params = FastAPIWebsocketParams(
    audio_in_enabled=True,
    audio_out_enabled=True,
    audio_in_sample_rate=8000,   # Common for telephony
    audio_out_sample_rate=8000,  # Common for telephony
    add_wav_header=False,        # Depending on provider requirements
    vad_analyzer=SileroVADAnalyzer(),
)
```

### Session Management

```python
params = FastAPIWebsocketParams(
    session_timeout=300,  # 5 minute timeout
    # Other parameters...
)

@transport.event_handler("on_session_timeout")
async def handle_timeout(transport, websocket):
    # Play timeout message before ending call
    await task.queue_frames([
        TTSTextFrame("I haven't heard from you in a while. Goodbye!"),
        EndFrame()
    ])
```

## Additional Notes

- **Telephony Focus**: Optimized for phone call audio with 8kHz sample rates
- **Provider Integration**: Works seamlessly with major telephony providers
- **Audio Timing**: Simulates real audio device timing for proper call flow
- **Session Management**: Built-in timeout handling for abandoned calls
- **Webhook Requirements**: Requires publicly accessible endpoints for telephony providers
- **Development**: Use ngrok or similar tools for local development with real phone numbers

FastAPIWebsocketTransport is the preferred choice for building phone-based voice AI applications with reliable audio streaming and provider compatibility.



================================================
FILE: server/services/transport/livekit.mdx
================================================
---
title: "LiveKitTransport"
description: "WebRTC transport implementation using LiveKit for real-time audio communication"
---

# Overview

`LiveKitTransport` provides real-time audio communication capabilities using LiveKit's open-source WebRTC platform. It supports bidirectional audio streaming, data messaging, participant management, and room event handling for conversational AI applications with the flexibility of self-hosted or cloud infrastructure.

<CardGroup cols={2}>
  <Card
    title="LiveKitTransport API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.livekit.html"
  >
    Transport methods and configuration options
  </Card>
  <Card
    title="LiveKit Docs"
    icon="book"
    href="https://docs.livekit.io/reference/"
  >
    Official LiveKit API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/04b-transports-livekit.py"
  >
    Working example with voice agent implementation
  </Card>
  <Card
    title="LiveKit Cloud"
    icon="external-link"
    href="https://cloud.livekit.io/login"
  >
    Sign up for hosted LiveKit service
  </Card>
</CardGroup>

## Installation

To use LiveKitTransport, install the required dependencies:

```bash
pip install "pipecat-ai[livekit]"
```

You'll need a LiveKit server URL and access token for room authentication.

<Tip>
  Get started quickly with [LiveKit Cloud](https://cloud.livekit.io/login) or
  [self-host LiveKit](https://docs.livekit.io/realtime/self-hosting/deployment/)
  for full control.
</Tip>

## Frames

### Input

- `UserAudioRawFrame` - Audio data from room participants
- `LiveKitTransportMessageUrgentFrame` - Data messages from participants

### Output

- `OutputAudioRawFrame` - Audio data to room participants
- `LiveKitTransportMessageFrame` - Data messages to participants
- `LiveKitTransportMessageUrgentFrame` - Urgent messages to participants

## Key Features

- **Open Source Platform**: Self-hosted or cloud options with full control over infrastructure
- **Data Messaging**: Bidirectional data channels for application messaging
- **Participant Management**: Built-in participant controls and metadata management
- **Scalable Architecture**: Handles multiple concurrent rooms and participants

## Usage Example

```python
import os
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.transports.services.livekit import LiveKitTransport, LiveKitParams

async def main():
    # LiveKit connection details (see Room Creation section below)
    url = "wss://your-livekit-server.com"
    token = "your-access-token"  # Generated for specific room/participant
    room_name = "conversation-room"

    # Configure the transport
    transport = LiveKitTransport(
        url=url,
        token=token,
        room_name=room_name,
        params=LiveKitParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive audio from participants
        # ... your processing chain
        transport.output(),             # Send audio to participants
    ])

    # Event handlers
    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant_id):
        # Start conversation when first participant joins
        await task.queue_frame(TextFrame("Hello! How can I help you today?"))

    @transport.event_handler("on_data_received")
    async def on_data_received(transport, data, participant_id):
        # Handle text messages from participants
        message_data = json.loads(data)
        await task.queue_frames([
            TranscriptionFrame(
                user_id=participant_id,
                text=message_data["message"],
                timestamp=message_data["timestamp"]
            )
        ])

    @transport.event_handler("on_participant_disconnected")
    async def on_participant_disconnected(transport, participant_id):
        await task.cancel()  # End session when participant leaves

    # Run the pipeline
    # ...
```

## Room Creation

LiveKit requires a url, room, and corresponding token for authentication.

Pipecat's [development runner](/server/utilities/runner/guide) provides a helper function to create room URL, token, and room name:

```python
from pipecat.runner.livekit import configure

async def create_room():
    # This helper creates room URL, token, and room name
    url, token, room_name = await configure()
    return url, token, room_name
```

## Methods

`LiveKitTransport` provides methods for room and participant management:

### Room Management

- `get_participants()` - Get list of participant IDs in the room
- `send_message()` - Send messages to participants
- `send_message_urgent()` - Send urgent messages to participants

### Audio Control

- `send_audio()` - Send audio frames to the room

### Participant Management

- `get_participant_metadata()` - Get metadata for specific participants
- `set_metadata()` - Set metadata for the local participant
- `mute_participant()` - Mute a participant's audio tracks
- `unmute_participant()` - Unmute a participant's audio tracks

<Tip>
  For complete method signatures, parameters, and examples, see the
  [`LiveKitTransport` API
  reference](https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.livekit.html).
</Tip>

### Example Usage

```python
# Get participants in the room
participants = transport.get_participants()
print(f"Active participants: {participants}")

# Send a data message to all participants
await transport.send_message("Welcome to the conversation!")

# Send urgent message to specific participant
await transport.send_message_urgent("Connection issue detected", participant_id="user-123")

# Control participant audio
await transport.mute_participant("user-123")
await transport.unmute_participant("user-123")

# Set bot metadata
await transport.set_metadata('{"role": "assistant", "capabilities": ["voice", "text"]}')
```

## Event Handling

`LiveKitTransport` provides event callbacks for room and participant management. Register callbacks using the `@transport.event_handler()` decorator:

### Connection Events

- `on_connected` - Connected to the LiveKit room
- `on_disconnected` - Disconnected from the LiveKit room

### Participant Events

- `on_first_participant_joined` - First participant joins (useful for starting conversations)
- `on_participant_connected` - Any participant joins the room
- `on_participant_disconnected` - Participant leaves the room
- `on_participant_left` - Participant left (alias for compatibility)

### Audio Events

- `on_audio_track_subscribed` - Audio track from participant is available
- `on_audio_track_unsubscribed` - Audio track is no longer available

### Communication Events

- `on_data_received` - Data messages received from participants

<Tip>
  For complete event details, parameters, and examples, see the
  [`LiveKitTransport` API
  reference](https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.services.livekit.html#pipecat.transports.services.livekit.LiveKitCallbacks).
</Tip>

### Example Usage

```python
@transport.event_handler("on_participant_connected")
async def on_participant_connected(transport, participant_id):
    print(f"Participant {participant_id} joined the room")

@transport.event_handler("on_audio_track_subscribed")
async def on_audio_track_subscribed(transport, participant_id):
    print(f"Now receiving audio from {participant_id}")

@transport.event_handler("on_data_received")
async def on_data_received(transport, data, participant_id):
    message = json.loads(data.decode())
    print(f"Received message from {participant_id}: {message}")
```

## Additional Notes

- **Deployment Flexibility**: Choose between LiveKit Cloud, self-hosted, or hybrid deployments
- **Open Source**: Full access to LiveKit server source code for customization
- **Scalability**: Horizontal scaling with multiple LiveKit servers
- **Security**: End-to-end encryption and configurable authentication
- **Platform Support**: Native SDKs for web, iOS, Android, and server applications



================================================
FILE: server/services/transport/small-webrtc.mdx
================================================
---
title: "SmallWebRTCTransport"
description: "A lightweight WebRTC transport for peer-to-peer audio and video communication in Pipecat"
---

## Overview

SmallWebRTCTransport enables peer-to-peer ("serverless") WebRTC connections between clients and your Pipecat application. It implements bidirectional audio, video and data channels using WebRTC for real-time communication. This transport is open source and self-contained, with no dependencies on any other infrastructure.

SmallWebRTCTransport is the default transport for the Pipecat examples and starter kits. It is heavily tested and can be used in production.

<Tip>
  For detailed notes on how to decide between using the SmallWebRTCTransport or
  other WebRTC transports like the DailyTransport, see [this
  post](https://www.daily.co/blog/you-dont-need-a-webrtc-server-for-your-voice-agents/).
</Tip>

<CardGroup cols={2}>
  <Card
    title="SmallWebRTCTransport API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.small_webrtc.html"
  >
    Transport methods and configuration options
  </Card>
  <Card
    title="SmallWebRTCConnection API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.webrtc_connection.html"
  >
    Connection management and signaling methods
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/p2p-webrtc/voice-agent"
  >
    Working example with voice agent implementation
  </Card>
  <Card
    title="Official WebRTC Docs"
    icon="book"
    href="https://webrtc.org/getting-started/overview"
  >
    Learn more about WebRTC!
  </Card>
</CardGroup>

## Installation

To use `SmallWebRTCTransport`, install the required dependencies:

```bash
pip install "pipecat-ai[webrtc]"
```

No API keys are required since this is a peer-to-peer transport implementation.

<Tip>
  For production deployments across different networks, you may need to
  configure STUN/TURN servers for NAT traversal.
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Audio data from WebRTC peer
- `UserImageRawFrame` - Video frames from peer's camera
- `TransportMessageUrgentFrame` - Application messages from peer

### Output

- `OutputAudioRawFrame` - Audio data to WebRTC peer
- `OutputImageRawFrame` - Video frames to peer
- `TransportMessageFrame` - Application messages to peer
- `TransportMessageUrgentFrame` - Urgent messages to peer

## Key Features

- **Serverless Architecture**: Direct peer-to-peer connections with no intermediate servers
- **Bidirectional Media**: Full-duplex audio and video streaming
- **Data Channels**: Application messaging and signaling support
- **Production Ready**: Heavily tested and used in Pipecat examples
- **ICE Support**: Configurable STUN/TURN servers for NAT traversal

## Usage Example

SmallWebRTCTransport requires two components working together: a **signaling server** to handle the WebRTC handshake, and your **Pipecat bot** that processes the media streams. Unlike other transports, you cannot use SmallWebRTCTransport without implementing both parts.

### Using the Development Runner (Recommended)

The easiest way to get started is using Pipecat's development runner, which handles all the server infrastructure automatically:

```python
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport

async def run_bot(transport):
    """Your core bot logic - works with any transport."""
    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive audio from client
        stt,                            # Convert speech to text
        context_aggregator.user(),      # Add user messages to context
        llm,                            # Process text with LLM
        tts,                            # Convert text to speech
        transport.output(),             # Send audio responses to client
        context_aggregator.assistant(), # Add assistant responses to context
    ])

    # Event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Start conversation when client connects
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    # Run the pipeline
    # ...

# The `bot()` function is the entry point for the development runner.
async def bot(runner_args: RunnerArguments):
    """Entry point called by the development runner."""

    # RunnerArguments are types that can be used to configure the transport.
    # The SmallWebRTCRunnerArguments contains the WebRTC connection
    # parameters needed to establish the connection.
    if isinstance(runner_args, SmallWebRTCRunnerArguments):
        transport = SmallWebRTCTransport(
            webrtc_connection=runner_args.webrtc_connection,
            params=TransportParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )
        await run_bot(transport)

if __name__ == "__main__":
    # Run the bot using the development runner
    from pipecat.runner.run import main
    main()
```

Run your bot with:

```bash
python bot.py --transport webrtc
# Opens http://localhost:7860/client
```

The development runner automatically:

- Creates a FastAPI server with WebRTC signaling endpoints
- Serves a built-in web interface for testing
- Handles WebRTC connection management and cleanup
- Spawns your bot for each new connection

<Tip>
  The development runner is the recommended approach for development and can
  also be used in production with Pipecat Cloud.
</Tip>

### Manual Server Implementation

For advanced use cases or custom deployment requirements, you can implement the server infrastructure manually:

#### 1. Signaling Server (Required)

First, create a web server to handle WebRTC connection establishment. This server receives offers from clients and creates the WebRTC connections that your bot will use:

```python
import asyncio
from contextlib import asynccontextmanager
from typing import Dict

from fastapi import BackgroundTasks, FastAPI
from fastapi.responses import FileResponse
from pipecat.transports.network.webrtc_connection import IceServer, SmallWebRTCConnection

app = FastAPI()

# Store active WebRTC connections by their unique ID
connections: Dict[str, SmallWebRTCConnection] = {}

# Configure ICE servers for NAT traversal
ice_servers = [
    IceServer(urls="stun:stun.l.google.com:19302"),
]

@app.post("/api/offer")
async def handle_offer(request: dict, background_tasks: BackgroundTasks):
    """Handle WebRTC offer from client and return SDP answer."""
    pc_id = request.get("pc_id")

    if pc_id and pc_id in connections:
        # Handle reconnections
        webrtc_connection = connections[pc_id]
        await webrtc_connection.renegotiate(
            sdp=request["sdp"],
            type=request["type"]
        )
    else:
        # Create new WebRTC connection
        webrtc_connection = SmallWebRTCConnection(ice_servers=ice_servers)
        await webrtc_connection.initialize(
            sdp=request["sdp"],
            type=request["type"]
        )

        # Clean up when client disconnects
        @webrtc_connection.event_handler("closed")
        async def on_closed(connection):
            connections.pop(connection.pc_id, None)

        # Start bot for this connection (defined below)
        background_tasks.add_task(run_bot, webrtc_connection)

    answer = webrtc_connection.get_answer()
    connections[answer["pc_id"]] = webrtc_connection
    return answer

# Run with: uvicorn server:app --host 0.0.0.0 --port 7860
```

#### 2. Pipecat Bot Implementation

Next, implement your bot function that receives the WebRTC connection from the server and creates the transport:

```python
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport

async def run_bot(webrtc_connection):
    """Run the Pipecat bot for a specific WebRTC connection."""
    # Create transport using the connection from the server
    transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,   # Accept audio from client
            audio_out_enabled=True,  # Send audio to client
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive audio from client
        stt,                            # Convert speech to text
        context_aggregator.user(),      # Add user messages to context
        llm,                            # Process text with LLM
        tts,                            # Convert text to speech
        transport.output(),             # Send audio responses to client
        context_aggregator.assistant(), # Add assistant responses to context
    ])

    # Handle connection events
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Start conversation when client connects
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    # Run the pipeline
    # ...
```

### How It Works Together

1. **Client connects** to your server at `/api/offer` with WebRTC offer
2. **Server creates** `SmallWebRTCConnection` and initializes it with the offer
3. **Server starts** `run_bot()` in background with the connection
4. **Bot creates** `SmallWebRTCTransport` using the connection
5. **Media flows** directly between client and bot via WebRTC peer connection

This architecture gives you the benefits of peer-to-peer WebRTC (low latency, no media servers) while maintaining a simple server for connection management.

## How to connect with SmallWebRTCTransport

For client connections, you have two options:

1. **Pipecat Client SDK** (Recommended): Use the [JavaScript SDK](/client/js/transports/small-webrtc) for easy integration
2. **Custom WebRTC Client**: Implement WebRTC signaling manually for advanced use cases

The server setup is shown in the [Usage Example](#usage-example) above.

## Examples

To see a complete implementation, check out the following examples:

<CardGroup>
  <Card
    title="Video Transform"
    icon="camera"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/p2p-webrtc/video-transform"
  >
    Demonstrates real-time video processing using WebRTC transport
  </Card>
  <Card
    title="Voice Agent"
    icon="microphone"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/p2p-webrtc/voice-agent"
  >
    Implements a voice assistant using WebRTC for audio communication
  </Card>
</CardGroup>

## Media Handling

### Audio

Audio is processed in 20ms chunks by default. The transport handles audio format conversion and resampling as needed:

- Input audio is processed at 16kHz (mono) to be compatible with speech recognition services
- Output audio can be configured to match your application's requirements, but it must be mono, 16-bit PCM audio

### Video

Video is streamed using RGB format by default. The transport provides:

- Frame conversion between different color formats (RGB, YUV, etc.)
- Configurable resolution and framerate

## WebRTC ICE Servers Configuration

When implementing WebRTC in your project, **STUN** (Session Traversal Utilities for NAT) and **TURN** (Traversal Using Relays around NAT)
servers are usually needed in cases where users are behind routers or firewalls.

In local networks (e.g., testing within the same home or office network), you usually don't need to configure STUN or TURN servers.
In such cases, WebRTC can often directly establish peer-to-peer connections without needing to traverse NAT or firewalls.

### What are STUN and TURN Servers?

- **STUN Server**: Helps clients discover their public IP address and port when they're behind a NAT (Network Address Translation) device (like a router).
  This allows WebRTC to attempt direct peer-to-peer communication by providing the public-facing IP and port.

- **TURN Server**: Used as a fallback when direct peer-to-peer communication isn't possible due to strict NATs or firewalls blocking connections.
  The TURN server relays media traffic between peers.

### Why are ICE Servers Important?

**ICE (Interactive Connectivity Establishment)** is a framework used by WebRTC to handle network traversal and NAT issues.
The `iceServers` configuration provides a list of **STUN** and **TURN** servers that WebRTC uses to find the best way to connect two peers.

## Advanced Configuration

### ICE Servers

For better connectivity, especially when testing across different networks, you can provide STUN servers:

```python
webrtc_connection = SmallWebRTCConnection(
    ice_servers=["stun:stun.l.google.com:19302", "stun:stun1.l.google.com:19302"]
)
```

You can also use IceServer objects for more advanced configuration:

```python
from pipecat.transports.network.webrtc_connection import IceServer

webrtc_connection = SmallWebRTCConnection(
    ice_servers=[
        IceServer(urls="stun:stun.l.google.com:19302"),
        IceServer(
            urls="turn:turn.example.com:3478",
            username="username",
            credential="password"
        )
    ]
)
```

### ESP32 Compatibility

For ESP32 WebRTC connections, use the development runner with ESP32 mode:

```bash
python bot.py -t webrtc --esp32 --host 192.168.1.100
```

This enables SDP munging required for ESP32 WebRTC compatibility.

## Troubleshooting

If clients have trouble connecting or streaming:

1. Check browser console for WebRTC errors
2. Ensure you're using HTTPS in production (required for WebRTC)
3. For testing across networks, consider using Daily which provides TURN servers
4. Verify browser permissions for camera and microphone
5. Try the development runner first to isolate connection issues

## Additional Notes

- **HTTPS Requirement**: WebRTC requires HTTPS in production environments
- **Browser Compatibility**: Modern browsers support WebRTC natively
- **Firewall Considerations**: May need TURN servers for restrictive network environments
- **Latency**: Direct peer-to-peer connections typically provide lower latency than server-mediated solutions
- **Scalability**: Each connection is independent; consider connection pooling for high-traffic applications
- **Development**: Use the development runner for rapid prototyping and testing

For comparing with other transport options, see the [transport comparison guide](https://www.daily.co/blog/you-dont-need-a-webrtc-server-for-your-voice-agents/).



================================================
FILE: server/services/transport/tavus.mdx
================================================
---
title: TavusTransport
description: "Connects your Pipecat app to a Tavus conversation, allowing the bot to join the same virtual room as the Tavus avatar and participants."
---

## Overview

`TavusTransport` integrate a Tavus Replica into your Pipecat application as a participant.

The Tavus agent joins as a third participant alongside the Pipecat bot and human user. It receives audio from the Pipecat pipeline’s TTS layer and renders synchronized video and audio.

## Installation

To use `TavusTransport`, install the required dependencies:

```bash
pip install pipecat-ai[tavus]
```

## Basic Usage

This basic usage example shows the transport specific parts of a bot.py file required to configure your bot:

```python
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.transports.services.tavus import TavusParams, TavusTransport

async def run_bot():
    async with aiohttp.ClientSession() as session:
        # Create the Tavus transport
        transport = TavusTransport(
            bot_name="Pipecat bot",
            api_key=os.getenv("TAVUS_API_KEY"),
            replica_id=os.getenv("TAVUS_REPLICA_ID"),
            session=session,
            params=TavusParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                microphone_out_enabled=False,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

        # Set up your services and context

        # Create the pipeline
        pipeline = Pipeline([
            transport.input(),              # Receive audio from client
            stt,                            # Convert speech to text
            context_aggregator.user(),      # Add user messages to context
            llm,                            # Process text with LLM
            tts,                            # Convert text to speech
            transport.output(),             # Send audio responses to client
            context_aggregator.assistant(), # Add assistant responses to context
        ])

        # Register event handlers
        @transport.event_handler("on_client_connected")
        async def on_client_connected(transport, client):
            logger.info("Client connected")
            # Start the conversation when client connects
            await task.queue_frames([context_aggregator.user().get_context_frame()])

        @transport.event_handler("on_client_disconnected")
        async def on_client_disconnected(transport, client):
            logger.info("Client disconnected")
```

## How to connect with `TavusTransport`

Use a Pipecat client SDK with the `DailyTransport`. See the [Client SDK docs](/client/js/transports/daily) to get started.

## Examples

To see a complete implementation, check out the following example:

<Card
  title="Tavus Transport"
  icon="camera"
  href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/21-tavus-transport.py"
>
  Demonstrates real-time video processing using WebRTC transport
</Card>

---

## See Also

- [Tavus integration guide on docs.tavus.io](https://docs.tavus.io/sections/integrations/pipecat#pipecat)



================================================
FILE: server/services/transport/websocket-server.mdx
================================================
---
title: "WebSocket Transports"
description: "WebSocket transport implementations for real-time client-server communication"
---

## Overview

WebSocket transports provide both client and server WebSocket implementations for real-time bidirectional communication. These transports support audio streaming, frame serialization, and connection management, making them ideal for prototyping and lightweight client-server applications where WebRTC might be overkill.

<Warning>
WebSocket transports are best suited for prototyping and controlled network environments.

For production client-server applications, we recommend WebRTC-based transports for more robust network handling, NAT traversal, and media optimization.

</Warning>

<CardGroup cols={3}>
  <Card
    title="WebSocket Client API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.websocket_client.html"
  >
    Client transport methods and configuration
  </Card>
  <Card
    title="WebSocket Server API"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.transports.network.websocket_server.html"
  >
    Server transport methods and configuration
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/websocket"
  >
    Working examples with client and server setup
  </Card>
</CardGroup>

## Installation

To use WebSocket transports, install the required dependencies:

```bash
pip install "pipecat-ai[websocket]"
```

No additional API keys are required for WebSocket communication.

<Tip>
  WebSocket transports use Protobuf serialization by default for efficient
  binary messaging, but support custom serializers for different protocols.
</Tip>

## Frames

### Input

- `InputAudioRawFrame` - Raw audio data from WebSocket peer
- `Frame` - Other frame types based on configured serializer

### Output

- `OutputAudioRawFrame` - Audio data to WebSocket peer (with optional WAV headers)
- `TransportMessageFrame` - Application messages to peer
- `TransportMessageUrgentFrame` - Urgent messages to peer

## Key Features

- **Client & Server Support**: Both WebSocket client and server implementations
- **Frame Serialization**: Configurable serializers including Protobuf by default
- **Audio Timing**: Simulates audio device timing for proper streaming flow
- **Session Management**: Built-in connection monitoring and timeout handling
- **WAV Header Support**: Optional WAV header generation for audio compatibility
- **Single Connection**: Server supports one active client at a time (new connections replace existing)

## Usage Example

### WebSocket Server Transport

The server transport creates a WebSocket server that clients can connect to:

```python
import os
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.transports.network.websocket_server import (
    WebsocketServerParams,
    WebsocketServerTransport,
)

async def run_websocket_server():
    # Create WebSocket server transport
    transport = WebsocketServerTransport(
        host="localhost",
        port=8765,
        params=WebsocketServerParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=ProtobufFrameSerializer(),
            session_timeout=180,  # 3 minutes
        ),
    )

    # Your services (STT, LLM, TTS, etc.)
    # ...

    # Create pipeline
    pipeline = Pipeline([
        transport.input(),              # Receive data from WebSocket clients
        # ... your processing chain
        transport.output(),             # Send data to WebSocket clients
    ])

    # Event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, websocket):
        logger.info(f"Client connected from {websocket.remote_address}")
        # Start conversation
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, websocket):
        logger.info(f"Client disconnected: {websocket.remote_address}")
        await task.cancel()

    @transport.event_handler("on_session_timeout")
    async def on_session_timeout(transport, websocket):
        logger.info(f"Session timeout for {websocket.remote_address}")
        await task.cancel()

    # Run the pipeline
    runner = PipelineRunner()
    await runner.run(task)
```

### WebSocket Client Transport

The client transport connects to an existing WebSocket server:

```python
from pipecat.transports.network.websocket_client import (
    WebsocketClientParams,
    WebsocketClientTransport,
)

async def run_websocket_client():
    # Create WebSocket client transport
    transport = WebsocketClientTransport(
        uri="ws://localhost:8765",
        params=WebsocketClientParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=ProtobufFrameSerializer(),
            add_wav_header=True,  # Optional WAV headers
        ),
    )

    # Your pipeline setup
    pipeline = Pipeline([
        transport.input(),
        # ... your processing chain
        transport.output(),
    ])

    # Event handlers
    @transport.event_handler("on_connected")
    async def on_connected(transport, websocket):
        logger.info("Connected to WebSocket server")

    @transport.event_handler("on_disconnected")
    async def on_disconnected(transport, websocket):
        logger.info("Disconnected from WebSocket server")
        await task.cancel()

    # Run the pipeline
    runner = PipelineRunner()
    await runner.run(task)
```

### Combined Server Setup

For a complete application, you might run both a FastAPI web server and WebSocket server:

```python
import asyncio
import uvicorn
from fastapi import FastAPI

app = FastAPI()

@app.post("/connect")
async def get_websocket_url():
    return {"ws_url": "ws://localhost:8765"}

async def main():
    # Run both WebSocket bot and web server concurrently
    tasks = [
        run_websocket_server(),  # Your WebSocket bot
        uvicorn.Server(uvicorn.Config(app, host="0.0.0.0", port=7860)).serve()
    ]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
```

## Event Handling

WebSocket transports provide event callbacks for connection management. Register callbacks using the `@transport.event_handler()` decorator:

### Server Events

- `on_client_connected` - Client connects to the server
- `on_client_disconnected` - Client disconnects from the server
- `on_session_timeout` - Client session times out (if configured)
- `on_websocket_ready` - WebSocket server is ready to accept connections

### Client Events

- `on_connected` - Successfully connected to WebSocket server
- `on_disconnected` - Disconnected from WebSocket server

<Tip>
  For complete event details and parameters, see the API reference documentation
  for specific callback signatures and usage examples.
</Tip>

### Example Usage

```python
# Server event handling
@server_transport.event_handler("on_websocket_ready")
async def on_server_ready(transport):
    logger.info("WebSocket server ready for connections")

@server_transport.event_handler("on_client_connected")
async def on_client_connected(transport, websocket):
    logger.info(f"New client: {websocket.remote_address}")
    # Initialize conversation or send welcome message

# Client event handling
@client_transport.event_handler("on_connected")
async def on_connected(transport, websocket):
    logger.info("Connected to server")
    # Start sending data or audio

@client_transport.event_handler("on_disconnected")
async def on_disconnected(transport, websocket):
    logger.info("Server connection lost")
    # Handle reconnection or cleanup
```

## Advanced Configuration

### Audio Processing

```python
# Server configuration
server_params = WebsocketServerParams(
    audio_in_enabled=True,
    audio_out_enabled=True,
    audio_out_sample_rate=24000,
    audio_out_channels=1,
    add_wav_header=False,  # Binary audio data
    vad_analyzer=SileroVADAnalyzer(),
)

# Client configuration
client_params = WebsocketClientParams(
    audio_in_enabled=True,
    audio_out_enabled=True,
    add_wav_header=True,   # WAV headers for compatibility
    serializer=ProtobufFrameSerializer(),
)
```

### Session Management

```python
params = WebsocketServerParams(
    session_timeout=300,  # 5 minute timeout
    # ... other parameters
)

@transport.event_handler("on_session_timeout")
async def handle_timeout(transport, websocket):
    logger.info("Session timed out, cleaning up")
    await task.cancel()
```

## Use Cases

### Server Transport

- **Voice AI Applications**: Host voice assistants that clients connect to
- **Real-time Audio Processing**: Server-side audio analysis and response
- **Prototyping**: Quick setup for testing conversational AI flows
- **Controlled Environments**: Internal networks where WebRTC complexity isn't needed

### Client Transport

- **Bot Clients**: Connect bots to existing WebSocket servers
- **Testing Tools**: Automated testing of WebSocket-based services
- **Integration**: Connect Pipecat to third-party WebSocket APIs
- **Monitoring**: Client bots that connect to monitor or interact with services



================================================
FILE: server/services/tts/asyncai.mdx
================================================
---
title: "Async"
description: "Text-to-speech services using Async’s WebSocket and HTTP APIs"
---

## Overview

Async provides two TTS service implementations:

- `AsyncAITTSService`: WebSocket-based streaming TTS with interruption support
- `AsyncAIHttpTTSService`: HTTP-based streaming TTS service for simpler synthesis

<Tip>`AsyncAITTSService` is recommended for real-time applications.</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.asyncai.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Async Docs"
    icon="book"
    href="https://docs.async.ai/"
  >
    Official Async documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07ac-interruptible-asyncai.py"
  >
    Working example with WebSocket streaming and interruption handling
  </Card>
</CardGroup>

## Installation

To use Async services, install the required dependencies:

```bash
pip install "pipecat-ai[asyncai]"
```

You'll also need to set up your Async API key as an environment variable: `ASYNCAI_API_KEY`.

<Tip>
  Get your API key by signing up at
  [async](https://async.ai).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that the TTS service should speak
- `TTSUpdateSettingsFrame` - Runtime configuration updates (e.g., voice)
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - Connection or processing errors

## Service Comparison

| Feature             | AsyncAITTSService (WebSocket) | AsyncAIHttpTTSService (HTTP) |
| ------------------- | ------------------------------ | -----------------------------|
| **Streaming**       | ✅ Low-latency chunks          | ✅ Response streaming         |
| **Interruption**    | ✅ Advanced handling           | ⚠️ Basic support              |
| **Latency**         | 🚀 Low                         | 📈 Higher                     |
| **Connection**      | WebSocket persistent           | 	HTTP per-request            |

## Language Support

Async currently supports:

| Language Code | Description        | Service Code |
| ------------- | ------------------ | ------------ |
| `Language.EN` | English            | `en`         |

<Note>
  Async is expanding language support. Check the [official
  documentation](https://docs.async.ai) for the latest available languages.
</Note>

## Usage Example

### WebSocket Service (Recommended)

Initialize the WebSocket service with your API key and desired voice:

```python
from pipecat.services.asyncai.tts import AsyncAITTSService
import os

# Configure WebSocket service
tts = AsyncAITTSService(
    api_key=os.getenv("ASYNCAI_API_KEY"),
    voice_id=os.getenv("ASYNCAI_VOICE_ID"),
    model="asyncflow_v2.0"
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the `AsyncAIHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.aysncai.tts import AsyncAIHttpTTSService
import aiohttp

# For simpler, non-persistent connections
async with aiohttp.ClientSession() as session:
    http_tts = AsyncAIHttpTTSService(
        api_key=os.getenv("ASYNCAI_API_KEY"),
        voice_id=os.getenv("ASYNCAI_VOICE_ID"),
        aiohttp_session=session,
        model="asyncflow_v2.0"
    )
```

### Dynamic Configuration

Make settings updates by pushing an `TTSUpdateSettingsFrame` for either service:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="new-voice-id",
  )
)
```

## Metrics

Both services provide:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Usage Metrics** - Character count and synthesis statistics

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Recommended**: Use `AsyncAITTSService` for low-latency streaming cases
- **Connection Management**: WebSocket maintains persistent connection with automatic keepalive (10-second intervals)
- **Sample Rate**: Set globally in `PipelineParams` rather than per-service for consistency


================================================
FILE: server/services/tts/aws.mdx
================================================
---
title: "AWS Polly"
description: "Text-to-speech service using Amazon Polly"
---

## Overview

AWS Polly provides text-to-speech synthesis through Amazon's cloud service with support for standard, neural, and generative engines. The service offers extensive language support, SSML features, and voice customization options including prosody controls for pitch, rate, and volume.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.aws.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="AWS Polly Docs"
    icon="book"
    href="https://docs.aws.amazon.com/polly/"
  >
    Official AWS Polly documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07m-interruptible-aws.py"
  >
    Working example with generative engine
  </Card>
</CardGroup>

## Installation

To use AWS Polly services, install the required dependencies:

```bash
pip install "pipecat-ai[aws]"
```

You'll also need to set up your AWS credentials as environment variables:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (if using temporary credentials)
- `AWS_REGION` (defaults to "us-east-1")

<Tip>
  Set up AWS credentials through the [AWS
  Console](https://console.aws.amazon.com/iam/) or use AWS CLI configuration.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (PCM, resampled from 16kHz)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - AWS API or processing errors

## Language Support

<Accordion title="View All Supported Languages">
  
| Language Code | Description | Service Code |
|---------------|-------------|--------------|
| `Language.AR` | Arabic | `arb` |
| `Language.AR_AE` | Arabic (UAE) | `ar-AE` |
| `Language.CA` | Catalan | `ca-ES` |
| `Language.ZH` | Chinese (Mandarin) | `cmn-CN` |
| `Language.YUE` | Chinese (Cantonese) | `yue-CN` |
| `Language.CS` | Czech | `cs-CZ` |
| `Language.DA` | Danish | `da-DK` |
| `Language.NL` | Dutch | `nl-NL` |
| `Language.NL_BE` | Dutch (Belgium) | `nl-BE` |
| `Language.EN` | English (US) | `en-US` |
| `Language.EN_AU` | English (Australia) | `en-AU` |
| `Language.EN_GB` | English (UK) | `en-GB` |
| `Language.EN_IN` | English (India) | `en-IN` |
| `Language.EN_NZ` | English (New Zealand) | `en-NZ` |
| `Language.EN_ZA` | English (South Africa) | `en-ZA` |
| `Language.FI` | Finnish | `fi-FI` |
| `Language.FR` | French | `fr-FR` |
| `Language.FR_BE` | French (Belgium) | `fr-BE` |
| `Language.FR_CA` | French (Canada) | `fr-CA` |
| `Language.DE` | German | `de-DE` |
| `Language.DE_AT` | German (Austria) | `de-AT` |
| `Language.DE_CH` | German (Switzerland) | `de-CH` |
| `Language.HI` | Hindi | `hi-IN` |
| `Language.IS` | Icelandic | `is-IS` |
| `Language.IT` | Italian | `it-IT` |
| `Language.JA` | Japanese | `ja-JP` |
| `Language.KO` | Korean | `ko-KR` |
| `Language.NO` | Norwegian | `nb-NO` |
| `Language.PL` | Polish | `pl-PL` |
| `Language.PT` | Portuguese | `pt-PT` |
| `Language.PT_BR` | Portuguese (Brazil) | `pt-BR` |
| `Language.RO` | Romanian | `ro-RO` |
| `Language.RU` | Russian | `ru-RU` |
| `Language.ES` | Spanish | `es-ES` |
| `Language.ES_MX` | Spanish (Mexico) | `es-MX` |
| `Language.ES_US` | Spanish (US) | `es-US` |
| `Language.SV` | Swedish | `sv-SE` |
| `Language.TR` | Turkish | `tr-TR` |
| `Language.CY` | Welsh | `cy-GB` |

</Accordion>

Common languages supported include:

- `Language.EN` - English (US)
- `Language.ES` - Spanish
- `Language.FR` - French
- `Language.DE` - German
- `Language.IT` - Italian
- `Language.JA` - Japanese

## Usage Example

### Basic Configuration

Initialize the `AWSPollyTTSService` and use it in a pipeline:

```python
from pipecat.services.aws.tts import AWSPollyTTSService
from pipecat.transcriptions.language import Language
import os

tts = AWSPollyTTSService(
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    api_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region="us-west-2",
    voice_id="Joanna",
    params=AWSPollyTTSService.InputParams(
        engine="neural",
        language=Language.EN,
        rate="+10%",
        volume="loud"
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `AWSPollyTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="Matthew",
  )
)
```

## SSML Features

AWS Polly automatically constructs SSML for advanced speech control:

```python
# Prosody controls (engine-dependent)
service = AWSPollyTTSService(
    voice_id="Joanna",
    params=AWSPollyTTSService.InputParams(
        engine="standard",   # Full prosody support
        rate="slow",         # SSML rate values
        pitch="low",         # Pitch adjustment
        volume="loud"        # Volume control
    )
)

# Lexicon support for custom pronunciations
service = AWSPollyTTSService(
    voice_id="Joanna",
    params=AWSPollyTTSService.InputParams(
        lexicon_names=["custom-pronunciations"]
    )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Engine Selection**: Use generative for highest quality, neural for balance, standard for lowest latency
- **Region Requirements**: Generative engine only available in select regions (us-west-2, us-east-1, etc.)
- **Audio Format**: Service outputs PCM audio resampled from 16kHz to your specified rate
- **Credential Management**: Supports both environment variables and direct credential passing
- **SSML Automatic**: Service automatically wraps text in appropriate SSML tags based on parameters
- **Prosody Limitations**: Generative engine only supports rate adjustment, not pitch or volume



================================================
FILE: server/services/tts/azure.mdx
================================================
---
title: "Azure"
description: "Text-to-speech service using Azure Cognitive Services Speech SDK"
---

## Overview

Azure Cognitive Services provides high-quality text-to-speech synthesis with two implementations:

- `AzureTTSService` (WebSocket-based streaming)
- `AzureHttpTTSService` (HTTP-based batch synthesis).

<Tip>
  `AzureTTSService` is recommended for real-time applications requiring low
  latency and streaming capabilities.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Azure Speech Docs"
    icon="book"
    href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/"
  >
    Official Azure Speech Services documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07f-interruptible-azure.py"
  >
    Working example with streaming synthesis
  </Card>
</CardGroup>

## Installation

To use Azure services, install the required dependencies:

```bash
pip install "pipecat-ai[azure]"
```

You'll also need to set up your Azure credentials as environment variables:

- `AZURE_API_KEY` (or `AZURE_SPEECH_API_KEY`)
- `AZURE_REGION` (or `AZURE_SPEECH_REGION`)

<Tip>
  Get your API key and region from the [Azure Portal](https://portal.azure.com/)
  under Cognitive Services > Speech.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (PCM format)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - Azure API or processing errors

## Service Comparison

| Feature        | AzureTTSService (Streaming) | AzureHttpTTSService (HTTP) |
| -------------- | --------------------------- | -------------------------- |
| **Streaming**  | ✅ Real-time chunks         | ❌ Single audio block      |
| **Latency**    | 🚀 Low                      | 📈 Higher                  |
| **Complexity** | ⚠️ WebSocket management     | ✅ Simple HTTP             |
| **Connection** | WebSocket-based             | HTTP-based                 |

## Language Support

<Accordion title="View All Supported Languages">

| Language Code    | Description           | Service Code |
| ---------------- | --------------------- | ------------ |
| `Language.BG`    | Bulgarian             | `bg-BG`      |
| `Language.CA`    | Catalan               | `ca-ES`      |
| `Language.ZH`    | Chinese (Simplified)  | `zh-CN`      |
| `Language.ZH_TW` | Chinese (Traditional) | `zh-TW`      |
| `Language.CS`    | Czech                 | `cs-CZ`      |
| `Language.DA`    | Danish                | `da-DK`      |
| `Language.NL`    | Dutch (Netherlands)   | `nl-NL`      |
| `Language.NL_BE` | Dutch (Belgium)       | `nl-BE`      |
| `Language.EN`    | English (US)          | `en-US`      |
| `Language.EN_US` | English (US)          | `en-US`      |
| `Language.EN_AU` | English (Australia)   | `en-AU`      |
| `Language.EN_GB` | English (UK)          | `en-GB`      |
| `Language.EN_NZ` | English (New Zealand) | `en-NZ`      |
| `Language.EN_IN` | English (India)       | `en-IN`      |
| `Language.ET`    | Estonian              | `et-EE`      |
| `Language.FI`    | Finnish               | `fi-FI`      |
| `Language.FR`    | French (France)       | `fr-FR`      |
| `Language.FR_CA` | French (Canada)       | `fr-CA`      |
| `Language.DE`    | German (Germany)      | `de-DE`      |
| `Language.DE_CH` | German (Switzerland)  | `de-CH`      |
| `Language.EL`    | Greek                 | `el-GR`      |
| `Language.HI`    | Hindi                 | `hi-IN`      |
| `Language.HU`    | Hungarian             | `hu-HU`      |
| `Language.ID`    | Indonesian            | `id-ID`      |
| `Language.IT`    | Italian               | `it-IT`      |
| `Language.JA`    | Japanese              | `ja-JP`      |
| `Language.KO`    | Korean                | `ko-KR`      |
| `Language.LV`    | Latvian               | `lv-LV`      |
| `Language.LT`    | Lithuanian            | `lt-LT`      |
| `Language.MS`    | Malay                 | `ms-MY`      |
| `Language.NO`    | Norwegian             | `nb-NO`      |
| `Language.PL`    | Polish                | `pl-PL`      |
| `Language.PT`    | Portuguese (Portugal) | `pt-PT`      |
| `Language.PT_BR` | Portuguese (Brazil)   | `pt-BR`      |
| `Language.RO`    | Romanian              | `ro-RO`      |
| `Language.RU`    | Russian               | `ru-RU`      |
| `Language.SK`    | Slovak                | `sk-SK`      |
| `Language.ES`    | Spanish               | `es-ES`      |
| `Language.SV`    | Swedish               | `sv-SE`      |
| `Language.TH`    | Thai                  | `th-TH`      |
| `Language.TR`    | Turkish               | `tr-TR`      |
| `Language.UK`    | Ukrainian             | `uk-UA`      |
| `Language.VI`    | Vietnamese            | `vi-VN`      |

</Accordion>

Common languages supported include:

- `Language.EN_US` - English (US)
- `Language.EN_GB` - English (UK)
- `Language.FR` - French
- `Language.DE` - German
- `Language.ES` - Spanish
- `Language.IT` - Italian

## Supported Sample Rates

Azure supports multiple sample rates with automatic format selection:

- **8000 Hz**: `Raw8Khz16BitMonoPcm`
- **16000 Hz**: `Raw16Khz16BitMonoPcm`
- **22050 Hz**: `Raw22050Hz16BitMonoPcm`
- **24000 Hz**: `Raw24Khz16BitMonoPcm` (default)
- **44100 Hz**: `Raw44100Hz16BitMonoPcm`
- **48000 Hz**: `Raw48Khz16BitMonoPcm`

## Usage Example

### Streaming Service (Recommended)

Initialize the `AzureTTSService` and use it in a pipeline:

```python
from pipecat.services.azure.tts import AzureTTSService
from pipecat.transcriptions.language import Language
import os

# Configure streaming service
tts = AzureTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region=os.getenv("AZURE_SPEECH_REGION"),
    voice="en-US-JennyNeural",
    params=AzureTTSService.InputParams(
        language=Language.EN_US,
        rate="1.1",
        style="cheerful",
        style_degree="1.5"
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the `AzureHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.azure.tts import AzureHttpTTSService

# For simpler, non-streaming use cases
http_tts = AzureHttpTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region=os.getenv("AZURE_SPEECH_REGION"),
    voice="en-US-AriaNeural",
    params=AzureHttpTTSService.InputParams(
        language=Language.EN_US,
        rate="1.05"
    )
)
```

## SSML Features

Azure TTS supports rich SSML customization through parameters:

```python
# Advanced SSML configuration
params = AzureTTSService.InputParams(
    language=Language.EN_US,
    style="cheerful",           # Speaking style
    style_degree="2.0",         # Style intensity (0.01-2.0)
    role="YoungAdultFemale",    # Voice role
    rate="1.2",                 # Speech rate
    pitch="+2st",               # Pitch adjustment
    volume="loud",              # Volume level
    emphasis="strong"           # Text emphasis
)

tts = AzureTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region="eastus",
    voice="en-US-JennyNeural",
    params=params
)
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `AzureTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice="en-US-AriaNeural",
  )
)
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Neural Voices**: Use neural voices (ending in "Neural") for highest quality
- **Regional Availability**: Some voices and features may be region-specific
- **SSML Automatic**: Service automatically constructs SSML based on parameters
- **Audio Format**: Automatic format selection based on sample rate
- **Voice Matching**: Ensure voice selection matches the specified language
- **Streaming Recommended**: Use `AzureTTSService` for real-time applications requiring low latency
- **Connection Management**: WebSocket lifecycle handled automatically in streaming service



================================================
FILE: server/services/tts/cartesia.mdx
================================================
---
title: "Cartesia"
description: "Text-to-speech services using Cartesia’s WebSocket and HTTP APIs"
---

## Overview

Cartesia provides two TTS service implementations:

- `CartesiaTTSService`: WebSocket-based with streaming and word timestamps
- `CartesiaHttpTTSService`: HTTP-based for simpler synthesis

<Tip>`CartesiaTTSService` is recommended for real-time applications.</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.cartesia.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Cartesia Docs"
    icon="book"
    href="https://docs.cartesia.ai/2024-11-13/get-started/overview"
  >
    Official Cartesia documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07-interruptible.py"
  >
    Working example with interruption handling
  </Card>
</CardGroup>

## Installation

To use Cartesia services, install the required dependencies:

```bash
pip install "pipecat-ai[cartesia]"
```

You'll also need to set up your Cartesia API key as an environment variable: `CARTESIA_API_KEY`.

<Tip>
  Get your API key by signing up at
  [Cartesia](https://play.cartesia.ai/sign-up).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that the TTS service should speak
- `TTSUpdateSettingsFrame` - Runtime configuration updates (e.g., voice)
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - Connection or processing errors

## Service Comparison

| Feature             | CartesiaTTSService (WebSocket) | CartesiaHttpTTSService (HTTP) |
| ------------------- | ------------------------------ | ----------------------------- |
| **Streaming**       | ✅ Real-time chunks            | ❌ Single audio block         |
| **Word Timestamps** | ✅ Precise timing              | ❌ Not available              |
| **Interruption**    | ✅ Advanced handling           | ⚠️ Basic support              |
| **Latency**         | 🚀 Low                         | 📈 Higher                     |
| **Best For**        | Interactive apps               | Batch processing              |

## Language Support

Supports multiple languages through the `Language` enum:

| Language Code | Description        | Service Code |
| ------------- | ------------------ | ------------ |
| `Language.DE` | German             | `de`         |
| `Language.EN` | English            | `en`         |
| `Language.ES` | Spanish            | `es`         |
| `Language.FR` | French             | `fr`         |
| `Language.HI` | Hindi              | `hi`         |
| `Language.IT` | Italian            | `it`         |
| `Language.JA` | Japanese           | `ja`         |
| `Language.KO` | Korean             | `ko`         |
| `Language.NL` | Dutch              | `nl`         |
| `Language.PL` | Polish             | `pl`         |
| `Language.PT` | Portuguese         | `pt`         |
| `Language.RU` | Russian            | `ru`         |
| `Language.SV` | Swedish            | `sv`         |
| `Language.TR` | Turkish            | `tr`         |
| `Language.ZH` | Chinese (Mandarin) | `zh`         |

## Usage Example

### WebSocket Service (Recommended)

Initialize the WebSocket service with your API key and desired voice:

```python
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.transcriptions.language import Language
import os

# Configure WebSocket service
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="your-voice-id",
    model="sonic-2",
    params=CartesiaTTSService.InputParams(
        language=Language.EN,
        speed="normal"
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    tts,  # Word timestamps enable precise context updates
    transport.output()
])
```

### HTTP Service

Initialize the HTTP service and use it in a pipeline:

```python
# For simpler, non-streaming use cases
http_tts = CartesiaHttpTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="your-voice-id",
    model="sonic-2",
    params=CartesiaHttpTTSService.InputParams(
        language=Language.EN
    )
)
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `CartesiaTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice="your-new-voice-id",
    speed="fast"
))
```

## Metrics

Both services provide:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Usage Metrics** - Character count and synthesis statistics

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Recommended**: Use `CartesiaTTSService` for low-latency streaming and accurate context updates with word timestamps
- **Connection Management**: WebSocket lifecycle is handled automatically with reconnection support
- **Sample Rate**: Set globally in `PipelineParams` rather than per-service for consistency



================================================
FILE: server/services/tts/deepgram.mdx
================================================
---
title: "Deepgram"
description: "Text-to-speech service implementation using Deepgram’s Aura API"
---

## Overview

Deepgram's Aura API provides high-quality text-to-speech synthesis with streaming capabilities and ultra-low latency. The service offers various voice models optimized for conversational AI applications with efficient audio streaming.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.deepgram.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Deepgram TTS Docs"
    icon="book"
    href="https://developers.deepgram.com/reference/text-to-speech-api/speak"
  >
    Official Deepgram text-to-speech API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07c-interruptible-deepgram.py"
  >
    Working example with Silero VAD
  </Card>
</CardGroup>

## Installation

To use Deepgram services, install the required dependencies:

```bash
pip install "pipecat-ai[deepgram]"
```

You'll also need to set up your Deepgram API key as an environment variable: `DEEPGRAM_API_KEY`.

<Tip>
  Get your API key from the [Deepgram Console](https://console.deepgram.com/).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Voice Models

Deepgram offers various Aura voice models optimized for different use cases. Here are some highlights:

| Voice Model           | Description                 | Language |
| --------------------- | --------------------------- | -------- |
| `aura-2-helena-en`    | Natural female voice        | English  |
| `aura-2-andromeda-en` | Expressive female voice     | English  |
| `aura-helios-en`      | Warm male voice             | English  |
| `aura-luna-en`        | Conversational female voice | English  |
| `aura-stella-en`      | Professional female voice   | English  |
| `aura-zeus-en`        | Authoritative male voice    | English  |

<Note>
  Deepgram regularly adds new voice models. Check the [official
  documentation](https://developers.deepgram.com/reference/text-to-speech-api/speak)
  for the latest available voices.
</Note>

## Supported Sample Rates

- **8000 Hz** - Phone quality
- **16000 Hz** - Standard quality
- **24000 Hz** - High quality (default)
- **44100 Hz** - CD quality
- **48000 Hz** - Professional quality

## Integration with VAD

Deepgram TTS works seamlessly with Voice Activity Detection:

### Using Silero VAD (Recommended)

```python
from pipecat.audio.vad.silero import SileroVADAnalyzer

transport_params = DailyParams(
    audio_in_enabled=True,
    audio_out_enabled=True,
    vad_analyzer=SileroVADAnalyzer()
)
```

### Using Deepgram's Built-in VAD Events

```python
from deepgram import LiveOptions

stt = DeepgramSTTService(
    api_key=os.getenv("DEEPGRAM_API_KEY"),
    live_options=LiveOptions(
        vad_events=True,
        utterance_end_ms="1000"
    )
)

@stt.event_handler("on_speech_started")
async def on_speech_started(stt, *args, **kwargs):
    await task.queue_frames([BotInterruptionFrame()])

@stt.event_handler("on_utterance_end")
async def on_utterance_end(stt, *args, **kwargs):
    await task.queue_frames([StopInterruptionFrame()])
```

## Usage Example

### Basic Configuration

Initialize the `DeepgramTTSService` with your API key and use it in your pipeline:

```python
from pipecat.services.deepgram.tts import DeepgramTTSService
import os

# Configure service
tts = DeepgramTTSService(
    api_key=os.getenv("DEEPGRAM_API_KEY"),
    voice="aura-2-andromeda-en",
    sample_rate=24000,
    encoding="linear16"
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Custom Configuration

```python
# Advanced configuration with custom settings
tts = DeepgramTTSService(
    api_key=os.getenv("DEEPGRAM_API_KEY"),
    voice="aura-helios-en",        # Male voice
    base_url="https://api.deepgram.com",  # Custom endpoint
    sample_rate=48000,             # High-quality audio
    encoding="linear16"
)
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `DeepgramTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice="aura-luna-en",  # Change to a different voice
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Streaming Audio**: Service streams audio in chunks for low-latency playback
- **Voice Selection**: Choose voices based on your application's tone and audience
- **Sample Rate Matching**: Ensure sample rate matches your pipeline's audio output sample rate



================================================
FILE: server/services/tts/elevenlabs.mdx
================================================
---
title: "ElevenLabs"
description: "Text-to-speech service using ElevenLab’s streaming API with word-level timing"
---

## Overview

ElevenLabs provides high-quality text-to-speech synthesis with two implementations:

- `ElevenLabsTTSService`: WebSocket-based with word timestamps and audio context management
- `ElevenLabsHttpTTSService`: HTTP-based for simpler integration

<Tip>
  `ElevenLabsTTSService` is recommended for real-time applications requiring
  precise timing.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.elevenlabs.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="ElevenLabs TTS Docs"
    icon="book"
    href="https://elevenlabs.io/docs/api-reference/text-to-speech/v-1-text-to-speech-voice-id-multi-stream-input"
  >
    Official ElevenLabs text-to-speech API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07d-interruptible-elevenlabs.py"
  >
    Working example with WebSocket streaming
  </Card>
</CardGroup>

## Installation

To use ElevenLabs services, install the required dependencies:

```bash
pip install "pipecat-ai[elevenlabs]"
```

You'll also need to set up your ElevenLabs API key as an environment variable: `ELEVENLABS_API_KEY`.

<Tip>
  Get your API key by signing up at
  [ElevenLabs](https://elevenlabs.io/app/sign-up).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks with word timing
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Service Comparison

| Feature             | ElevenLabsTTSService (WebSocket) | ElevenLabsHttpTTSService (HTTP) |
| ------------------- | -------------------------------- | ------------------------------- |
| **Word Timestamps** | ✅ Real-time precision           | ✅ Batch processing             |
| **Streaming**       | ✅ Low-latency chunks            | ✅ Response streaming           |
| **Audio Context**   | ✅ Advanced management           | ❌ Basic                        |
| **Interruption**    | ✅ Context-aware                 | ⚠️ Limited                      |
| **Connection**      | WebSocket persistent             | HTTP per-request                |

## Language Support

<Accordion title="View All Supported Languages">

| Language Code  | Description | Service Code |
| -------------- | ----------- | ------------ |
| `Language.AR`  | Arabic      | `ar`         |
| `Language.BG`  | Bulgarian   | `bg`         |
| `Language.CS`  | Czech       | `cs`         |
| `Language.DA`  | Danish      | `da`         |
| `Language.DE`  | German      | `de`         |
| `Language.EL`  | Greek       | `el`         |
| `Language.EN`  | English     | `en`         |
| `Language.ES`  | Spanish     | `es`         |
| `Language.FI`  | Finnish     | `fi`         |
| `Language.FIL` | Filipino    | `fil`        |
| `Language.FR`  | French      | `fr`         |
| `Language.HI`  | Hindi       | `hi`         |
| `Language.HR`  | Croatian    | `hr`         |
| `Language.HU`  | Hungarian   | `hu`         |
| `Language.ID`  | Indonesian  | `id`         |
| `Language.IT`  | Italian     | `it`         |
| `Language.JA`  | Japanese    | `ja`         |
| `Language.KO`  | Korean      | `ko`         |
| `Language.MS`  | Malay       | `ms`         |
| `Language.NL`  | Dutch       | `nl`         |
| `Language.NO`  | Norwegian   | `no`         |
| `Language.PL`  | Polish      | `pl`         |
| `Language.PT`  | Portuguese  | `pt`         |
| `Language.RO`  | Romanian    | `ro`         |
| `Language.RU`  | Russian     | `ru`         |
| `Language.SK`  | Slovak      | `sk`         |
| `Language.SV`  | Swedish     | `sv`         |
| `Language.TA`  | Tamil       | `ta`         |
| `Language.TR`  | Turkish     | `tr`         |
| `Language.UK`  | Ukrainian   | `uk`         |
| `Language.VI`  | Vietnamese  | `vi`         |
| `Language.ZH`  | Chinese     | `zh`         |

</Accordion>

Common languages supported include:

- `Language.EN` - English
- `Language.ES` - Spanish
- `Language.FR` - French
- `Language.DE` - German
- `Language.IT` - Italian
- `Language.JA` - Japanese

<Note>
  Language support varies by model. Use multilingual models
  (`eleven_flash_v2_5`, `eleven_turbo_v2_5`) for language specification.
</Note>

## Supported Sample Rates

ElevenLabs supports specific sample rates with automatic format selection:

- **8000 Hz** - `pcm_8000`
- **16000 Hz** - `pcm_16000`
- **22050 Hz** - `pcm_22050`
- **24000 Hz** - `pcm_24000` (default)
- **44100 Hz** - `pcm_44100`

## Model Selection

Choose the right model for your use case:

| Model                    | Quality | Latency   | Multilingual | Best For                |
| ------------------------ | ------- | --------- | ------------ | ----------------------- |
| `eleven_flash_v2_5`      | High    | Ultra-low | ✅           | Real-time conversations |
| `eleven_turbo_v2_5`      | High    | Ultra-low | ✅           | Real-time conversations |
| `eleven_multilingual_v2` | High    | Medium    | ✅           | Quality + languages     |
| `eleven_flash_v2`        | High    | Low       | ❌           | English-only apps       |

## Usage Example

### WebSocket Service (Recommended)

Initialize the `ElevenLabsTTSService` with your API key and use it in your pipeline:

```python
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.transcriptions.language import Language
import os

# Configure WebSocket service with voice customization
tts = ElevenLabsTTSService(
    api_key=os.getenv("ELEVENLABS_API_KEY"),
    voice_id=os.getenv("ELEVENLABS_VOICE_ID"),
    model="eleven_flash_v2_5",
    params=ElevenLabsTTSService.InputParams(
        language=Language.EN,
        stability=0.7,
        similarity_boost=0.8,
        style=0.5,
        use_speaker_boost=True,
        speed=1.1
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,  # Word timestamps enable precise context updates
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the `ElevenLabsHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.elevenlabs.tts import ElevenLabsHttpTTSService
import aiohttp

# For simpler, non-persistent connections
async with aiohttp.ClientSession() as session:
    http_tts = ElevenLabsHttpTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"),
        voice_id=os.getenv("ELEVENLABS_VOICE_ID"),
        aiohttp_session=session,
        params=ElevenLabsHttpTTSService.InputParams(
            language=Language.EN,
            optimize_streaming_latency=3
        )
    )
```

### Dynamic Configuration

Make settings updates by pushing an `TTSUpdateSettingsFrame` for either service:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="new-voice-id",
  )
)
```

## Voice Customization

ElevenLabs offers extensive voice control parameters:

```python
# Advanced voice settings
params = ElevenLabsTTSService.InputParams(
    stability=0.7,              # Voice consistency (0.0-1.0)
    similarity_boost=0.8,       # Voice similarity (0.0-1.0)
    style=0.5,                  # Expression style (0.0-1.0, V2+ models)
    use_speaker_boost=True,     # Enhanced clarity (V2+ models)
    speed=1.2,                  # Speech rate (0.25-4.0)
    auto_mode=True,             # Latency optimization
    enable_ssml_parsing=True    # SSML support
)

tts = ElevenLabsTTSService(
    api_key=os.getenv("ELEVENLABS_API_KEY"),
    voice_id="your-voice-id",
    model="eleven_flash_v2_5",
    params=params
)
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Recommended**: Use `ElevenLabsTTSService` for real-time applications with word timestamps and audio context management
- **Connection Management**: WebSocket maintains persistent connection with automatic keepalive (10-second intervals)
- **Audio Context**: WebSocket service manages multiple audio contexts for handling interruptions and overlapping requests
- **Voice Settings**: Both `stability` and `similarity_boost` must be set together for voice customization
- **Language Specification**: Only works with multilingual models (`eleven_flash_v2_5`, `eleven_turbo_v2_5`, `eleven_multilingual_v2`)
- **Sample Rate Constraints**: Must use supported sample rates (8000, 16000, 22050, 24000, or 44100 Hz)
- **SSML Support**: Enable SSML parsing for advanced speech markup control



================================================
FILE: server/services/tts/fish.mdx
================================================
---
title: "Fish Audio"
description: "Real-time text-to-speech service using Fish Audio's WebSocket API"
---

## Overview

Fish Audio provides real-time text-to-speech synthesis through a WebSocket-based streaming API. The service offers custom voice models, prosody controls, and multiple audio formats optimized for conversational AI applications with low latency.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.fish.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Fish Audio Docs"
    icon="book"
    href="https://docs.fish.audio/text-to-speech/text-to-speech-ws"
  >
    Official Fish Audio WebSocket API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07t-interruptible-fish.py"
  >
    Working example with custom voice model
  </Card>
</CardGroup>

## Installation

To use Fish Audio services, install the required dependencies:

```bash
pip install "pipecat-ai[fish]"
```

You'll also need to set up your Fish Audio API key as an environment variable: `FISH_API_KEY`.

<Tip>
  Get your API key from the [Fish Audio Console](https://console.fish.audio/).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Sample Rate Options

Supported sample rates for different quality levels:

- **8000 Hz** - Phone quality
- **16000 Hz** - Standard quality
- **24000 Hz** - High quality (recommended)
- **44100 Hz** - CD quality
- **48000 Hz** - Professional quality

## Language Support

Fish Audio currently supports:

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.EN` | English     | `en`         |
| `Language.JA` | Japanese    | `ja`         |
| `Language.ZH` | Chinese     | `zh`         |

<Note>
  Fish Audio is expanding language support. Check the [official
  documentation](https://docs.fish.audio) for the latest available languages.
</Note>

## Latency Modes

Choose the appropriate latency mode for your application:

| Mode       | Description                | Best For                |
| ---------- | -------------------------- | ----------------------- |
| `normal`   | Standard latency (Default) | General applications    |
| `balanced` | Balanced quality/speed     | Real-time conversations |

## Usage Example

### Basic Configuration

```python
from pipecat.services.fish.tts import FishAudioTTSService
from pipecat.transcriptions.language import Language
import os

# Configure service with custom voice
tts = FishAudioTTSService(
    api_key=os.getenv("FISH_API_KEY"),
    reference_id="4ce7e917cedd4bc2bb2e6ff3a46acaa1",  # Voice model ID
    model_id="speech-1.5",
    output_format="pcm",
    sample_rate=24000,
    params=FishAudioTTSService.InputParams(
        language=Language.EN,
        latency="normal",
        prosody_speed=1.0,
        prosody_volume=0
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Advanced Prosody Control

```python
# Custom prosody settings
tts = FishAudioTTSService(
    api_key=os.getenv("FISH_API_KEY"),
    reference_id="your-voice-model-id",
    params=FishAudioTTSService.InputParams(
        language=Language.EN,
        latency="balanced",      # Balance quality vs speed
        prosody_speed=1.2,       # 20% faster speech
        prosody_volume=3,        # +3dB volume boost
        normalize=True           # Normalize audio output
    )
)
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    reference_id="new-voice-model-id",  # Change voice model
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Streaming**: Real-time audio generation with automatic chunking
- **Interruption Handling**: Built-in support for conversation interruptions
- **Custom Voice Models**: Use your own trained voice models via reference IDs
- **Audio Buffering**: Efficient streaming with configurable buffer sizes
- **Connection Management**: Automatic reconnection on connection failures
- **Format Flexibility**: Multiple audio formats for different deployment scenarios
- **Prosody Control**: Fine-tune speech characteristics including speed and volume



================================================
FILE: server/services/tts/google.mdx
================================================
---
title: "Google"
description: "Text-to-speech service using Google’s Cloud Text-to-Speech API"
---

## Overview

Google Cloud Text-to-Speech provides high-quality speech synthesis with two implementations:

- `GoogleTTSService`: Websocket-based streaming service
- `GoogleHttpTTSService`: HTTP-based streaming service

<Tip>
  `GoogleTTSService` offers the lowest latency and is the recommended option.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.google.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Google Cloud TTS Docs"
    icon="book"
    href="https://cloud.google.com/text-to-speech/docs"
  >
    Official Google Cloud Text-to-Speech documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07n-interruptible-google.py"
  >
    Working example with Chirp 3 HD voice
  </Card>
</CardGroup>

## Installation

To use Google services, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You'll need to set up Google Cloud credentials through one of these methods:

- Environment variable: `GOOGLE_APPLICATION_CREDENTIALS` (path to service account JSON)
- Service account JSON string
- Service account file path

<Tip>
  Create a service account in the [Google Cloud
  Console](https://console.cloud.google.com/iam-admin/serviceaccounts) with
  Text-to-Speech API permissions.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (PCM format)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - Google Cloud API or processing errors

## Service Comparison

| Feature           | GoogleTTSService (Streaming) | GoogleHttpTTSService (HTTP) |
| ----------------- | ---------------------------- | --------------------------- |
| **Streaming**     | ✅ Real-time chunks          | ❌ Single audio block       |
| **Latency**       | 🚀 Ultra-low                 | 📈 Higher                   |
| **Voice Support** | Chirp 3 HD, Journey only     | All Google voices           |
| **SSML Support**  | ❌ Plain text only           | ✅ Full SSML                |
| **Customization** | ⚠️ Basic                     | ✅ Extensive                |

## Language Support

<Accordion title="View All Supported Languages">

| Language Code    | Description         | Service Code |
| ---------------- | ------------------- | ------------ |
| `Language.AF`    | Afrikaans           | `af-ZA`      |
| `Language.AR`    | Arabic              | `ar-XA`      |
| `Language.BN`    | Bengali             | `bn-IN`      |
| `Language.BG`    | Bulgarian           | `bg-BG`      |
| `Language.CA`    | Catalan             | `ca-ES`      |
| `Language.ZH`    | Chinese (Mandarin)  | `cmn-CN`     |
| `Language.ZH_TW` | Chinese (Taiwan)    | `cmn-TW`     |
| `Language.ZH_HK` | Chinese (Hong Kong) | `yue-HK`     |
| `Language.CS`    | Czech               | `cs-CZ`      |
| `Language.DA`    | Danish              | `da-DK`      |
| `Language.NL`    | Dutch               | `nl-NL`      |
| `Language.NL_BE` | Dutch (Belgium)     | `nl-BE`      |
| `Language.EN`    | English (US)        | `en-US`      |
| `Language.EN_AU` | English (Australia) | `en-AU`      |
| `Language.EN_GB` | English (UK)        | `en-GB`      |
| `Language.EN_IN` | English (India)     | `en-IN`      |
| `Language.ET`    | Estonian            | `et-EE`      |
| `Language.FIL`   | Filipino            | `fil-PH`     |
| `Language.FI`    | Finnish             | `fi-FI`      |
| `Language.FR`    | French              | `fr-FR`      |
| `Language.FR_CA` | French (Canada)     | `fr-CA`      |
| `Language.GL`    | Galician            | `gl-ES`      |
| `Language.DE`    | German              | `de-DE`      |
| `Language.EL`    | Greek               | `el-GR`      |
| `Language.GU`    | Gujarati            | `gu-IN`      |
| `Language.HE`    | Hebrew              | `he-IL`      |
| `Language.HI`    | Hindi               | `hi-IN`      |
| `Language.HU`    | Hungarian           | `hu-HU`      |
| `Language.IS`    | Icelandic           | `is-IS`      |
| `Language.ID`    | Indonesian          | `id-ID`      |
| `Language.IT`    | Italian             | `it-IT`      |
| `Language.JA`    | Japanese            | `ja-JP`      |
| `Language.KN`    | Kannada             | `kn-IN`      |
| `Language.KO`    | Korean              | `ko-KR`      |
| `Language.LV`    | Latvian             | `lv-LV`      |
| `Language.LT`    | Lithuanian          | `lt-LT`      |
| `Language.MS`    | Malay               | `ms-MY`      |
| `Language.ML`    | Malayalam           | `ml-IN`      |
| `Language.MR`    | Marathi             | `mr-IN`      |
| `Language.NO`    | Norwegian           | `nb-NO`      |
| `Language.PA`    | Punjabi             | `pa-IN`      |
| `Language.PL`    | Polish              | `pl-PL`      |
| `Language.PT`    | Portuguese          | `pt-PT`      |
| `Language.PT_BR` | Portuguese (Brazil) | `pt-BR`      |
| `Language.RO`    | Romanian            | `ro-RO`      |
| `Language.RU`    | Russian             | `ru-RU`      |
| `Language.SR`    | Serbian             | `sr-RS`      |
| `Language.SK`    | Slovak              | `sk-SK`      |
| `Language.ES`    | Spanish             | `es-ES`      |
| `Language.ES_US` | Spanish (US)        | `es-US`      |
| `Language.SV`    | Swedish             | `sv-SE`      |
| `Language.TA`    | Tamil               | `ta-IN`      |
| `Language.TE`    | Telugu              | `te-IN`      |
| `Language.TH`    | Thai                | `th-TH`      |
| `Language.TR`    | Turkish             | `tr-TR`      |
| `Language.UK`    | Ukrainian           | `uk-UA`      |
| `Language.VI`    | Vietnamese          | `vi-VN`      |

</Accordion>

Common languages supported include:

- `Language.EN_US` - English (US)
- `Language.EN_GB` - English (UK)
- `Language.FR` - French
- `Language.DE` - German
- `Language.ES` - Spanish
- `Language.IT` - Italian

## Credential Setup

### Environment Variable Method

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
```

### Direct Credentials

```python
# Using credentials string
tts = GoogleTTSService(
    credentials='{"type": "service_account", "project_id": "...", ...}'
)

# Using credentials file path
tts = GoogleTTSService(
    credentials_path="/path/to/service-account.json"
)
```

## Usage Example

### Streaming Service (Recommended for Real-time)

Initialize `GoogleTTSService` and use it in a pipeline:

```python
from pipecat.services.google.tts import GoogleTTSService
from pipecat.transcriptions.language import Language
import os

# Configure streaming service with Chirp 3 HD
tts = GoogleTTSService(
    credentials=os.getenv("GOOGLE_TEST_CREDENTIALS"),
    voice_id="en-US-Chirp3-HD-Charon",
    params=GoogleTTSService.InputParams(
        language=Language.EN_US
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service (Full SSML Support)

Initialize `GoogleHttpTTSService` for more customization options:

```python
from pipecat.services.google.tts import GoogleHttpTTSService

# Configure HTTP service with SSML customization
http_tts = GoogleHttpTTSService(
    credentials_path="/path/to/service-account.json",
    voice_id="en-US-Neural2-A",
    params=GoogleHttpTTSService.InputParams(
        language=Language.EN_US,
        pitch="+2st",
        rate="1.2",
        volume="loud",
        emphasis="moderate",
        google_style="empathetic"
    )
)
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="new-voice-id",
  )
)
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Voice Compatibility**: Streaming service only supports Chirp 3 HD and Journey voices
- **SSML Limitations**: Chirp and Journey voices don't support SSML - use plain text input
- **Credential Management**: Supports multiple authentication methods for flexibility
- **Regional Voices**: Match voice selection with language code for optimal results
- **Streaming Advantage**: Use streaming service for conversational AI requiring ultra-low latency
- **HTTP Advantage**: Use HTTP service when you need extensive voice customization via SSML



================================================
FILE: server/services/tts/groq.mdx
================================================
---
title: "Groq"
description: "Text-to-speech service implementation using Groq’s TTS API"
---

## Overview

Groq's TTS API provides fast text-to-speech synthesis with multiple voice options. The service operates at a fixed 48kHz sample rate and offers efficient audio streaming for real-time applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.groq.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Groq TTS Docs"
    icon="book"
    href="https://console.groq.com/docs/api-reference#models"
  >
    Official Groq API documentation and models
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07l-interruptible-groq.py"
  >
    Working example with Groq STT and LLM
  </Card>
</CardGroup>

## Installation

To use Groq services, install the required dependencies:

```bash
pip install "pipecat-ai[groq]"
```

You'll also need to set up your Groq API key as an environment variable: `GROQ_API_KEY`.

<Tip>
  Get your API key by signing up at [Groq
  Console](https://console.groq.com/login).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (WAV format)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Voice Models

Groq TTS supports various voice options through the PlayAI model:

| Voice ID         | Description                    | Gender |
| ---------------- | ------------------------------ | ------ |
| `Celeste-PlayAI` | Natural, conversational voice  | Female |
| `Iris-PlayAI`    | Professional, clear voice      | Female |
| `Oliver-PlayAI`  | Warm, friendly voice           | Male   |
| `William-PlayAI` | Authoritative, confident voice | Male   |

<Note>
  Voice availability may vary. Check the [Groq
  documentation](https://console.groq.com/docs/text-to-speech) for the latest
  available voices.
</Note>

## Audio Configuration

### Sample Rate

- **Fixed at 48kHz** - Groq TTS only supports 48,000 Hz sample rate
- Automatic resampling if pipeline uses different rates

## Usage Example

### Basic Configuration

Initialize `GroqTTSService` and use it in your pipeline:

```python
from pipecat.services.groq.tts import GroqTTSService
from pipecat.transcriptions.language import Language
import os

# Configure service
tts = GroqTTSService(
    api_key=os.getenv("GROQ_API_KEY"),
    model_name="playai-tts",
    voice_id="Celeste-PlayAI",
    params=GroqTTSService.InputParams(
        language=Language.EN,
        speed=1.0
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `GroqTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="Iris-PlayAI",
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Fixed Sample Rate**: Service operates at 48kHz only - resampling handled automatically
- **Speed Control**: Adjust speech rate from 0.5x to 2.0x normal speed



================================================
FILE: server/services/tts/inworld.mdx
================================================
---
title: "Inworld"
description: "Text-to-speech service using Inworld AI's TTS APIs"
---

## Overview

Inworld AI provides high-quality text-to-speech synthesis with natural-sounding voices and real-time streaming capabilities. The service supports both streaming and non-streaming modes, making it suitable for various use cases from low-latency conversational AI to batch audio generation.

<Tip>
  Streaming mode is recommended for real-time applications requiring low
  latency.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.inworld.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Inworld AI Docs"
    icon="book"
    href="https://docs.inworld.ai/docs/tts/tts"
  >
    Official Inworld TTS API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07ab-interruptible-inworld-http.py"
  >
    Working example with Inworld TTS
  </Card>
</CardGroup>

## Installation

To use Inworld services, no additional dependencies are required beyond the base installation:

```bash
pip install "pipecat-ai"
```

You'll also need to set up your Inworld API key as an environment variable: `INWORLD_API_KEY`.

<Tip>
  Get your API key from [Inworld Studio](https://studio.inworld.ai/). Make sure
  to base64-encode your API key.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (LINEAR16 PCM, WAV header stripped)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Features

- **High-Quality Voices**: Natural-sounding voices including Ashley, Hades, and more
- **Streaming & Non-Streaming**: Unified interface supporting both real-time and batch processing
- **Automatic Language Detection**: No need to specify language manually - Inworld detects it from your text
- **Voice Temperature Control**: Accepts 0-2 (best results 0.6 to 1.0); lower values yield steadier, deterministic speech, while higher values add expressive variation.
- **Model Selection**: Choose `inworld‑tts‑1` for real‑time, cost‑sensitive use (lowest latency); use `inworld‑tts‑1‑max` (experimental) when you can trade a bit more latency for richer expressiveness and broader multilingual support.
- **Professional-quality Audio Output**: LINEAR16 PCM audio at up to 48kHz

## Audio Markups

Inworld supports experimental audio markups for enhanced expressiveness in English:

**Emotion and Delivery Style** (use at beginning of text):

- **Emotions**: `[happy]`, `[sad]`, `[angry]`, `[surprised]`, `[fearful]`, `[disgusted]`
- **Delivery Styles**: `[laughing]`, `[whispering]`

**Non-verbal Vocalizations** (place anywhere in text):

- **Sound Effects**: `[breathe]`, `[clear_throat]`, `[cough]`, `[laugh]`, `[sigh]`, `[yawn]`

<Note>
  Audio markup features are experimental and currently support English only. For
  best results, use only one emotion/delivery style at the beginning of text.
  For detailed usage guidelines and best practices, refer to Inworld's
  documentation on [Audio Markups Best
  Practices](https://docs.inworld.ai/docs/tts/best-practices/generating-speech#audio-markups).
</Note>

## Usage Examples

### Streaming Mode (Real-time)

Perfect for conversational AI applications requiring low latency:

<CodeGroup>

```python Python
import asyncio
import aiohttp
import os
from pipecat.services.inworld.tts import InworldTTSService

async def main():
    async with aiohttp.ClientSession() as session:
        tts = InworldTTSService(
            api_key=os.getenv("INWORLD_API_KEY"),
            aiohttp_session=session,
            voice_id="Ashley",
            model="inworld-tts-1",
            streaming=True,  # Use streaming mode for real-time audio
            params=InworldTTSService.InputParams(
                temperature=0.8,
            ),
        )

        # Use in your pipeline
        # pipeline = Pipeline([...other_processors..., tts, ...])

asyncio.run(main())
```

</CodeGroup>

### Non-Streaming Mode (Complete Audio)

Ideal for scenarios where you need the complete audio file before playback:

<CodeGroup>

```python Python
tts = InworldTTSService(
    api_key=os.getenv("INWORLD_API_KEY"),
    aiohttp_session=session,
    voice_id="Hades",
    model="inworld-tts-1-max",  # Higher quality model
    streaming=False,  # Complete audio generation first
    params=InworldTTSService.InputParams(
        temperature=1.2,  # More expressive speech
    ),
)
```

</CodeGroup>

### Streaming vs Non-Streaming

| Mode              | Best For               | Use Cases                                                                                |
| ----------------- | ---------------------- | ---------------------------------------------------------------------------------------- |
| **Streaming**     | Real-time applications | Building conversational AI, minimal latency interactions, processing text as available   |
| **Non-Streaming** | Batch processing       | Longer content generation, complete audio files, batch scenarios, slighly better quality |

### Audio Specifications

- **Sample Rate Range**: 8kHz - 48kHz (default comes from StartFrame)
- **Bit Depth**: 16-bit
- **Encoding**: LINEAR16 PCM (uncompressed)
- **Format**: WAV headers automatically stripped

| Sample Rate | Quality | Use Case                         |
| ----------- | ------- | -------------------------------- |
| 16000 Hz    | Basic   | Voice calls, simple applications |
| 24000 Hz    | Good    | General conversational AI        |
| 48000 Hz    | High    | Professional applications, music |

## Monitoring and Metrics

- **Time To First Byte (TTFB)**: Latency measurement from request start to first audio chunk
- **Processing Time**: Total duration for the complete TTS operation
- **Usage Metrics**: Character count of processed text for billing and analytics

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Resources

- [Inworld AI Documentation](https://docs.inworld.ai/)
- [TTS API Reference](https://docs.inworld.ai/api-reference/ttsAPI/texttospeech/)
- [Inworld Studio](https://studio.inworld.ai/) - Voice management and API keys
- [Audio Markups Best Practices](https://docs.inworld.ai/docs/tts/best-practices/generating-speech#audio-markups) - Techniques for optimal markup usage
- [Pipecat Examples](https://github.com/pipecat-ai/pipecat/tree/main/examples) - Sample implementations



================================================
FILE: server/services/tts/lmnt.mdx
================================================
---
title: "LMNT"
description: "Text-to-speech service implementation using LMNT’s streaming API"
---

## Overview

LMNT provides real-time text-to-speech synthesis through a WebSocket-based streaming API optimized for conversational AI. The service offers ultra-low latency with high-quality voice models and supports multiple languages with automatic interruption handling.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.lmnt.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="LMNT Speech Docs"
    icon="book"
    href="https://docs.lmnt.com/api-reference/speech/streaming"
  >
    Official LMNT streaming speech API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07k-interruptible-lmnt.py"
  >
    Working example with voice synthesis
  </Card>
</CardGroup>

## Installation

To use LMNT services, install the required dependencies:

```bash
pip install "pipecat-ai[lmnt]"
```

You'll also need to set up your LMNT API key as an environment variable: `LMNT_API_KEY`.

<Tip>Get your API key from the [LMNT Console](https://app.lmnt.com/).</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming PCM)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - WebSocket or API errors

## Language Support

<Accordion title="View All Supported Languages">

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.DE` | German      | `de`         |
| `Language.EN` | English     | `en`         |
| `Language.ES` | Spanish     | `es`         |
| `Language.FR` | French      | `fr`         |
| `Language.HI` | Hindi       | `hi`         |
| `Language.ID` | Indonesian  | `id`         |
| `Language.IT` | Italian     | `it`         |
| `Language.JA` | Japanese    | `ja`         |
| `Language.KO` | Korean      | `ko`         |
| `Language.NL` | Dutch       | `nl`         |
| `Language.PL` | Polish      | `pl`         |
| `Language.PT` | Portuguese  | `pt`         |
| `Language.RU` | Russian     | `ru`         |
| `Language.SV` | Swedish     | `sv`         |
| `Language.TH` | Thai        | `th`         |
| `Language.TR` | Turkish     | `tr`         |
| `Language.UK` | Ukrainian   | `uk`         |
| `Language.VI` | Vietnamese  | `vi`         |
| `Language.ZH` | Chinese     | `zh`         |

</Accordion>

Most common languages supported:

- `Language.EN` - English
- `Language.ES` - Spanish
- `Language.FR` - French
- `Language.DE` - German
- `Language.ZH` - Chinese
- `Language.JA` - Japanese

## Usage Example

### Basic Configuration

Initialize the `LmntTTSService` and use it in a pipeline:

```python
from pipecat.services.lmnt.tts import LmntTTSService
from pipecat.transcriptions.language import Language
import os

# Configure service
tts = LmntTTSService(
    api_key=os.getenv("LMNT_API_KEY"),
    voice_id="morgan",
    model="aurora",
    language=Language.EN,
    sample_rate=24000
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `LmntTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="new_voice",
  )
)
```

## Metrics

The service provides real-time metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Streaming**: Uses persistent WebSocket connection for ultra-low latency
- **Custom Voices**: Supports custom voice creation through LMNT dashboard
- **Language Detection**: Automatically handles language variants and fallbacks



================================================
FILE: server/services/tts/minimax.mdx
================================================
---
title: "MiniMax"
description: "Text-to-speech service implementation using MiniMax T2A API"
---

## Overview

MiniMax's T2A (Text-to-Audio) API provides high-quality text-to-speech synthesis with streaming capabilities, emotional voice control, and support for multiple languages. The service offers various models optimized for different use cases, from low-latency to high-definition audio quality.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.minimax.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="MiniMax T2A Docs"
    icon="book"
    href="https://www.minimax.io/platform/document/T2A%20V2?key=66719005a427f0c8a5701643"
  >
    Official MiniMax T2A API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07y-interruptible-minimax.py"
  >
    Working example with emotional voice settings
  </Card>
</CardGroup>

## Installation

To use MiniMax services, no additional dependencies are required beyond the base installation:

```bash
pip install "pipecat-ai"
```

You'll need MiniMax API credentials:

- `MINIMAX_API_KEY`
- `MINIMAX_GROUP_ID`

<Tip>
  Get your API credentials from the [MiniMax
  Platform](https://www.minimax.io/platform/).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming PCM)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Model Comparison

| Model               | Quality | Latency | Features                             |
| ------------------- | ------- | ------- | ------------------------------------ |
| **speech-02-hd**    | Highest | Higher  | Superior rhythm and stability        |
| **speech-02-turbo** | High    | Lower   | Enhanced multilingual capabilities   |
| **speech-01-hd**    | High    | Medium  | Rich voices with expressive emotions |
| **speech-01-turbo** | Good    | Lowest  | Regular updates, fast response       |

<Note>
  Refer to the [MiniMax
  documentation](https://www.minimax.io/platform/document/T2A%20V2?key=66719005a427f0c8a5701643#TJeyxusWAUP0l3tX67brbAyE)
  for up-to-date model information.
</Note>

## Voice Selection

MiniMax offers diverse voice personalities:

| Voice ID          | Description         | Tone                         |
| ----------------- | ------------------- | ---------------------------- |
| `Wise_Woman`      | Mature female voice | Authoritative, knowledgeable |
| `Friendly_Person` | Warm, approachable  | Conversational, welcoming    |
| `Patient_Man`     | Calm male voice     | Steady, reassuring           |
| `Lively_Girl`     | Young female voice  | Energetic, enthusiastic      |
| `Deep_Voice_Man`  | Rich male voice     | Professional, commanding     |
| `Calm_Woman`      | Serene female voice | Peaceful, soothing           |
| `Elegant_Man`     | Sophisticated male  | Refined, articulate          |

<Note>
  See the [MiniMax
  documentation](https://www.minimax.io/platform/document/T2A%20V2?key=66719005a427f0c8a5701643)
  for the complete list of available voices.
</Note>

## Supported Sample Rates

MiniMax supports multiple sample rates for different quality levels:

- **8000 Hz**
- **16000 Hz**
- **22050 Hz**
- **24000 Hz**
- **32000 Hz**
- **44100 Hz**

## Language Support

<Accordion title="View All Supported Languages">

| Language Code  | Description         | Service Code  |
| -------------- | ------------------- | ------------- |
| `Language.AR`  | Arabic              | `Arabic`      |
| `Language.CS`  | Czech               | `Czech`       |
| `Language.DE`  | German              | `German`      |
| `Language.EL`  | Greek               | `Greek`       |
| `Language.EN`  | English             | `English`     |
| `Language.ES`  | Spanish             | `Spanish`     |
| `Language.FI`  | Finnish             | `Finnish`     |
| `Language.FR`  | French              | `French`      |
| `Language.HI`  | Hindi               | `Hindi`       |
| `Language.ID`  | Indonesian          | `Indonesian`  |
| `Language.IT`  | Italian             | `Italian`     |
| `Language.JA`  | Japanese            | `Japanese`    |
| `Language.KO`  | Korean              | `Korean`      |
| `Language.NL`  | Dutch               | `Dutch`       |
| `Language.PL`  | Polish              | `Polish`      |
| `Language.PT`  | Portuguese          | `Portuguese`  |
| `Language.RO`  | Romanian            | `Romanian`    |
| `Language.RU`  | Russian             | `Russian`     |
| `Language.TH`  | Thai                | `Thai`        |
| `Language.TR`  | Turkish             | `Turkish`     |
| `Language.UK`  | Ukrainian           | `Ukrainian`   |
| `Language.VI`  | Vietnamese          | `Vietnamese`  |
| `Language.YUE` | Chinese (Cantonese) | `Chinese,Yue` |
| `Language.ZH`  | Chinese (Mandarin)  | `Chinese`     |

</Accordion>

Common languages supported include:

- `Language.EN` - English
- `Language.ZH` - Chinese (Mandarin)
- `Language.ES` - Spanish
- `Language.FR` - French
- `Language.DE` - German
- `Language.JA` - Japanese

## Usage Example

### Basic Configuration

Initialize the `MiniMaxHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.minimax.tts import MiniMaxHttpTTSService
from pipecat.transcriptions.language import Language
import os

# Configure service
tts = MiniMaxHttpTTSService(
    api_key=os.getenv("MINIMAX_API_KEY"),
    group_id=os.getenv("MINIMAX_GROUP_ID"),
    aiohttp_session=session,
    params=MiniMaxHttpTTSService.InputParams(
        language=Language.EN
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `MiniMaxHttpTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="new_voice",
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **HTTP Session Required**: Must provide an `aiohttp.ClientSession` for API communication
- **Emotional AI**: Advanced emotional expression capabilities with voice-specific optimizations
- **Text Normalization**: Optional English normalization for better number and abbreviation handling



================================================
FILE: server/services/tts/neuphonic.mdx
================================================
---
title: "Neuphonic"
description: "Text-to-speech service implementation using Neuphonic’s API"
---

## Overview

Neuphonic provides high-quality text-to-speech synthesis with two implementations:

- `NeuphonicTTSService`: WebSocket-based with real-time streaming and interruption support
- `NeuphonicHttpTTSService`: HTTP-based with server-sent events.

<Tip>
  `NeuphonicTTSService` is the recommended option for interactive applications
  requiring low latency.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.neuphonic.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Neuphonic Docs"
    icon="book"
    href="https://docs.neuphonic.com/api-reference/tts/websocket"
  >
    Official Neuphonic TTS API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07v-interruptible-neuphonic.py"
  >
    Working example with WebSocket streaming
  </Card>
</CardGroup>

## Installation

To use Neuphonic services, install the required dependencies:

```bash
pip install "pipecat-ai[neuphonic]"
```

You'll also need to set up your Neuphonic API key as an environment variable: `NEUPHONIC_API_KEY`.

<Tip>
  Get your API key from the [Neuphonic Console](https://app.neuphonic.com/).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates (voice, speed, etc.)
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Service Comparison

| Feature          | NeuphonicTTSService (WebSocket) | NeuphonicHttpTTSService (HTTP) |
| ---------------- | ------------------------------- | ------------------------------ |
| **Streaming**    | ✅ Real-time chunks             | ✅ Server-sent events          |
| **Interruption** | ✅ Advanced handling            | ❌ Limited support             |
| **Latency**      | 🚀 Ultra-low                    | 📈 Moderate                    |

## Language Support

Neuphonic supports multiple languages with automatic base language detection:

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.EN` | English     | `en`         |
| `Language.ES` | Spanish     | `es`         |
| `Language.DE` | German      | `de`         |
| `Language.NL` | Dutch       | `nl`         |
| `Language.AR` | Arabic      | `ar`         |
| `Language.FR` | French      | `fr`         |
| `Language.PT` | Portuguese  | `pt`         |
| `Language.RU` | Russian     | `ru`         |
| `Language.HI` | Hindi       | `hi`         |
| `Language.ZH` | Chinese     | `zh`         |

<Note>
  Regional variants (e.g., `EN_US`, `ES_ES`) are automatically mapped to their
  base language.
</Note>

## Usage Example

### WebSocket Service (Recommended)

Initialize the `NeuphonicTTSService` and use it in a pipeline:

```python
from pipecat.services.neuphonic.tts import NeuphonicTTSService
from pipecat.transcriptions.language import Language
import os

# Configure WebSocket service
tts = NeuphonicTTSService(
    api_key=os.getenv("NEUPHONIC_API_KEY"),
    voice_id="fc854436-2dac-4d21-aa69-ae17b54e98eb",  # Emily voice
    params=NeuphonicTTSService.InputParams(
        language=Language.EN,
        speed=1.2
    ),
    sample_rate=22050
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the `NeuphonicHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.neuphonic.tts import NeuphonicHttpTTSService

# For simpler, HTTP-based synthesis
http_tts = NeuphonicHttpTTSService(
    api_key=os.getenv("NEUPHONIC_API_KEY"),
    voice_id="your-voice-id",
    params=NeuphonicHttpTTSService.InputParams(
        language=Language.ES,
        speed=1.0
    )
)
```

### Dynamic Voice Switching

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

# Change voice during conversation
await task.queue_frame(TTSUpdateSettingsFrame({
    "voice_id": "new-voice-id"
}))
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Recommended**: Use `NeuphonicTTSService` for real-time applications requiring low latency and interruption support



================================================
FILE: server/services/tts/openai.mdx
================================================
---
title: "OpenAI"
description: "Text-to-speech service using OpenAI’s TTS API"
---

## Overview

OpenAI's TTS API provides high-quality text-to-speech synthesis with multiple voice models including traditional TTS models and advanced GPT-based models. The service outputs 24kHz PCM audio with streaming capabilities for real-time applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenAI TTS Docs"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/audio/createSpeech"
  >
    Official OpenAI text-to-speech API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py"
  >
    Working example with voice customization
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependencies:

```bash
pip install "pipecat-ai[openai]"
```

You'll also need to set up your OpenAI API key as an environment variable: `OPENAI_API_KEY`.

<Tip>
  Get your API key from the [OpenAI
  Platform](https://platform.openai.com/api-keys).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (24kHz PCM, mono)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Models

| Model             | Description                | Best For                                                            |
| ----------------- | -------------------------- | ------------------------------------------------------------------- |
| `gpt-4o-mini-tts` | Latest GPT-based TTS model | Faster generation, improved prosody, recommended for most use cases |
| `tts-1`           | Original TTS model         | Standard quality speech synthesis                                   |
| `tts-1-hd`        | High-definition TTS model  | Premium quality speech with higher fidelity                         |

## Voice Options

OpenAI provides multiple voice personalities:

| Voice     | Description           | Characteristics           |
| --------- | --------------------- | ------------------------- |
| `alloy`   | Balanced, neutral     | Professional, clear       |
| `echo`    | Calm, measured        | Thoughtful, deliberate    |
| `fable`   | Warm, engaging        | Storytelling, expressive  |
| `onyx`    | Deep, authoritative   | Commanding, confident     |
| `nova`    | Bright, energetic     | Enthusiastic, friendly    |
| `shimmer` | Soft, gentle          | Soothing, approachable    |
| `ash`     | Mature, sophisticated | Experienced, wise         |
| `ballad`  | Smooth, melodic       | Musical, flowing          |
| `coral`   | Vibrant, lively       | Dynamic, spirited         |
| `sage`    | Wise, contemplative   | Reflective, knowledgeable |
| `verse`   | Poetic, rhythmic      | Artistic, expressive      |

Refer to the [OpenAI TTS documentation](https://platform.openai.com/docs/api-reference/audio/createSpeech#audio-createspeech-voice) for the latest voice options and their characteristics.

## Usage Example

### Basic Configuration

Initialize `OpenAITTSService` and use it in a pipeline:

```python
from pipecat.services.openai.tts import OpenAITTSService
import os

# Configure service with default settings
tts = OpenAITTSService(
    api_key=os.getenv("OPENAI_API_KEY"),
    voice="nova",
    model="gpt-4o-mini-tts"
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Voice Changes

```python
# Runtime voice switching via settings update
await task.queue_frame(TTSUpdateSettingsFrame({
    "voice": "sage"
}))
```

## Audio Specifications

### Sample Rate

- **Fixed Rate**: 24kHz (24,000 Hz)
- **Format**: 16-bit PCM
- **Channels**: Mono (1 channel)
- **Streaming**: Chunked delivery for low latency

<Warning>
  OpenAI TTS only outputs at 24kHz. Ensure your pipeline sample rate matches to
  avoid audio issues.
</Warning>

## Advanced Features

### Voice Instructions (GPT Models)

```python
# Guide voice behavior with instructions
tts = OpenAITTSService(
    model="gpt-4o-mini-tts",
    voice="nova",
    instructions="Speak enthusiastically about technology topics, but use a calm tone for sensitive subjects"
)
```

### Custom Endpoints

```python
# Use custom OpenAI-compatible endpoints
tts = OpenAITTSService(
    base_url="https://api.custom-provider.com/v1",
    api_key="custom-api-key",
    model="custom-tts-model"
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Sample Rate Constraint**: OpenAI TTS always outputs at 24kHz - ensure pipeline compatibility
- **Streaming Optimized**: Audio chunks delivered as generated for low-latency playback
- **Voice Quality**: GPT-based models offer superior prosody and naturalness
- **Instructions Support**: GPT models accept behavioral instructions for voice customization
- **Error Handling**: Robust error handling with detailed error messages
- **Thread Safety**: Safe for concurrent use in multi-threaded applications
- **Cost Efficiency**: Character-based billing with usage metrics tracking



================================================
FILE: server/services/tts/piper.mdx
================================================
---
title: "Piper"
description: "Text-to-speech service implementation using the Piper TTS server"
---

## Overview

Piper provides high-quality neural text-to-speech synthesis through a self-hosted HTTP server. The service offers complete privacy and control with no external API dependencies, making it ideal for on-premise deployments and applications requiring data sovereignty.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.piper.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Piper TTS Docs"
    icon="book"
    href="https://github.com/OHF-Voice/piper1-gpl"
  >
    Official Piper TTS documentation and setup
  </Card>
</CardGroup>

## Installation

To use Piper services, no additional Pipecat dependencies are required:

```bash
pip install "pipecat-ai"  # Base installation is sufficient
```

You'll need to set up a running Piper TTS server following the [HTTP server documentation](https://github.com/OHF-Voice/piper1-gpl/blob/main/docs/API_HTTP.md).

<Tip>
  Piper runs entirely locally, providing complete privacy and eliminating API
  key requirements.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (WAV headers automatically removed)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - HTTP server or processing errors

## Voice Models

Piper offers various pre-trained voice models with different qualities and languages:

### English Models

- `en_US-lessac-medium` - Natural female voice, balanced quality
- `en_US-ryan-high` - High-quality male voice
- `en_US-amy-medium` - Clear female voice
- `en_GB-alan-medium` - British male voice

### Quality Levels

- **low** - Fastest, smallest file size
- **medium** - Balanced quality and speed
- **high** - Best quality, larger models

<Note>
  Check the [Piper voices
  repository](https://huggingface.co/rhasspy/piper-voices) for the complete list
  of available models and languages.
</Note>

### Supported Sample Rates

Piper supports multiple sample rates depending on the model quality:

- **Low quality**: 16kHz
- **Medium quality**: 22.05kHz
- **High quality**: 24kHz

## Usage Example

### Basic Configuration

Initialize the Piper TTS service and use it in a pipeline:

```python
import aiohttp
from pipecat.services.piper.tts import PiperTTSService

# Create aiohttp session (reuse across requests)
session = aiohttp.ClientSession()

# Configure service
tts = PiperTTSService(
    base_url="http://localhost:5000/api/tts",
    aiohttp_session=session,
    sample_rate=22050  # Match your Piper model's sample rate
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Voice Switching

You can dynamically switch voices by updating the `voice_id` parameter:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="en_US-amy-medium",  # Switch to Amy's voice
))
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for monitoring

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Self-Hosted**: Complete control over TTS infrastructure and data privacy
- **No API Keys**: No external service dependencies or API costs
- **Language Support**: Multiple languages available through different voice models



================================================
FILE: server/services/tts/playht.mdx
================================================
---
title: "PlayHT"
description: "Text-to-speech services using PlayHT’s WebSocket and HTTP APIs"
---

## Overview

PlayHT provides high-quality text-to-speech synthesis with two implementations:

- `PlayHTTTSService`: WebSocket-based with real-time streaming
- `PlayHTHttpTTSService`: HTTP-based for simpler synthesis

<Tip>
  `PlayHTTTSService` is recommended for interactive applications requiring low
  latency.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.playht.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="PlayHT Docs"
    icon="book"
    href="https://docs.play.ht/reference/playht-tts-websocket-api"
  >
    Official PlayHT WebSocket API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07e-interruptible-playht.py"
  >
    Working example with voice cloning
  </Card>
</CardGroup>

## Installation

To use PlayHT services, install the required dependencies:

```bash
pip install "pipecat-ai[playht]"
```

You'll also need to set up your PlayHT credentials as environment variables:

- `PLAY_HT_USER_ID`
- `PLAY_HT_API_KEY`

<Tip>
  Get your credentials from the [PlayHT Dashboard](https://play.ht/app).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (WAV format)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Service Comparison

| Feature          | PlayHTTTSService (WebSocket) | PlayHTHttpTTSService (HTTP) |
| ---------------- | ---------------------------- | --------------------------- |
| **Streaming**    | ✅ Real-time chunks          | ❌ Single audio block       |
| **Latency**      | 🚀 Ultra-low                 | 📈 Higher                   |
| **Interruption** | ✅ Advanced handling         | ⚠️ Basic support            |
| **Connection**   | WebSocket-based              | HTTP-based                  |

## Language Support

<Accordion title="View All Supported Languages">

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.AF` | Afrikaans   | `afrikans`   |
| `Language.AM` | Amharic     | `amharic`    |
| `Language.AR` | Arabic      | `arabic`     |
| `Language.BN` | Bengali     | `bengali`    |
| `Language.BG` | Bulgarian   | `bulgarian`  |
| `Language.CA` | Catalan     | `catalan`    |
| `Language.CS` | Czech       | `czech`      |
| `Language.DA` | Danish      | `danish`     |
| `Language.DE` | German      | `german`     |
| `Language.EL` | Greek       | `greek`      |
| `Language.EN` | English     | `english`    |
| `Language.ES` | Spanish     | `spanish`    |
| `Language.FR` | French      | `french`     |
| `Language.GL` | Galician    | `galician`   |
| `Language.HE` | Hebrew      | `hebrew`     |
| `Language.HI` | Hindi       | `hindi`      |
| `Language.HR` | Croatian    | `croatian`   |
| `Language.HU` | Hungarian   | `hungarian`  |
| `Language.ID` | Indonesian  | `indonesian` |
| `Language.IT` | Italian     | `italian`    |
| `Language.JA` | Japanese    | `japanese`   |
| `Language.KO` | Korean      | `korean`     |
| `Language.MS` | Malay       | `malay`      |
| `Language.NL` | Dutch       | `dutch`      |
| `Language.PL` | Polish      | `polish`     |
| `Language.PT` | Portuguese  | `portuguese` |
| `Language.RU` | Russian     | `russian`    |
| `Language.SQ` | Albanian    | `albanian`   |
| `Language.SR` | Serbian     | `serbian`    |
| `Language.SV` | Swedish     | `swedish`    |
| `Language.TH` | Thai        | `thai`       |
| `Language.TL` | Tagalog     | `tagalog`    |
| `Language.TR` | Turkish     | `turkish`    |
| `Language.UK` | Ukrainian   | `ukrainian`  |
| `Language.UR` | Urdu        | `urdu`       |
| `Language.XH` | Xhosa       | `xhosa`      |
| `Language.ZH` | Mandarin    | `mandarin`   |

</Accordion>

Common languages supported include:

- `Language.EN` - English
- `Language.ES` - Spanish
- `Language.FR` - French
- `Language.DE` - German
- `Language.IT` - Italian
- `Language.JA` - Japanese

## Usage Example

### WebSocket Service (Recommended)

Initialize the `PlayHTTTSService` and use it in a pipeline:

```python
from pipecat.services.playht.tts import PlayHTTTSService
from pipecat.transcriptions.language import Language
import os

# Configure WebSocket service
tts = PlayHTTTSService(
    user_id=os.getenv("PLAYHT_USER_ID"),
    api_key=os.getenv("PLAYHT_API_KEY"),
    voice_url="s3://voice-cloning-zero-shot/your-voice-id/manifest.json",
    voice_engine="PlayHT3.0-mini",
    params=PlayHTTTSService.InputParams(
        language=Language.EN,
        speed=1.2,
        seed=42  # For consistent voice output
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the `PlayHTHttpTTSService` and use it in a pipeline:

```python
from pipecat.services.playht.tts import PlayHTHttpTTSService

# For simpler, non-streaming use cases
http_tts = PlayHTHttpTTSService(
    user_id=os.getenv("PLAYHT_USER_ID"),
    api_key=os.getenv("PLAYHT_API_KEY"),
    voice_url="your-voice-url",
    voice_engine="Play3.0-mini",
    protocol="http",
    params=PlayHTHttpTTSService.InputParams(
        language=Language.EN,
        speed=1.0
    )
)
```

### Dynamic Voice Switching

Make settings updates by pushing a `TTSUpdateSettingsFrame`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="your-voice-id",
))
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Voice URLs**: Use S3 URLs for both standard and cloned voices from PlayHT
- **Engine Selection**: Choose based on latency requirements and quality needs
- **WebSocket Recommended**: Use `PlayHTTTSService` for real-time interactive applications



================================================
FILE: server/services/tts/rime.mdx
================================================
---
title: "Rime"
description: "Text-to-speech service implementations using Rime AI"
---

## Overview

Rime AI provides two TTS service implementations:

- `RimeTTSService`: WebSocket-based with word-level timing and interruption support
- `RimeHttpTTSService`: HTTP-based for simpler use cases

<Tip>
  `RimeTTSService` is recommended for real-time interactive applications.
</Tip>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.rime.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Rime Docs"
    icon="book"
    href="https://docs.rime.ai/api-reference/endpoint/websockets-json"
  >
    Official Rime WebSocket and HTTP API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07q-interruptible-rime.py"
  >
    Working example with word timestamps
  </Card>
</CardGroup>

## Installation

To use Rime services, install the required dependencies:

```bash
pip install "pipecat-ai[rime]"
```

You'll also need to set up your Rime API key as an environment variable: `RIME_API_KEY`.

<Tip>Get your API key by signing up at [Rime](https://rime.ai/signup).</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (PCM format)
- `TTSTextFrame` - Word-level timing information (WebSocket service only)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Service Comparison

| Feature             | RimeTTSService (WebSocket) | RimeHttpTTSService (HTTP) |
| ------------------- | -------------------------- | ------------------------- |
| **Word Timestamps** | ✅ Precise timing          | ❌ Not available          |
| **Interruption**    | ✅ Context tracking        | ⚠️ Basic support          |
| **Streaming**       | ✅ Real-time chunks        | ✅ Chunked response       |
| **Inline Speed**    | ❌ Not supported           | ✅ Word-level control     |
| **Arcana Model**    | ❌ Not supported           | ✅ Latest model           |

## Model Options

| Model      | Description                                         | Availability  |
| ---------- | --------------------------------------------------- | ------------- |
| **mistv2** | Hyper-realistic conversational voices (recommended) | Both services |
| **mist**   | Previous generation model                           | Both services |
| **arcana** | Latest high-quality model                           | HTTP only     |

## Supported Sample Rates

### WebSocket Service

Sample rates must be between 4000 Hz and 44100 Hz. Default: 24000 Hz.

### HTTP Service

Sample rates must be between 8000 Hz and 96000 Hz. Default: 24000 Hz. Anything above 24000 Hz is up-sampling.

## Language Support

| Language Code | Description | Service Code |
| ------------- | ----------- | ------------ |
| `Language.DE` | German      | `ger`        |
| `Language.EN` | English     | `eng`        |
| `Language.ES` | Spanish     | `spa`        |
| `Language.FR` | French      | `fra`        |

## Usage Example

### WebSocket Service (Recommended)

Initialize the WebSocket service with your API key and desired voice:

```python
from pipecat.services.rime.tts import RimeTTSService
from pipecat.transcriptions.language import Language
import os

# Configure WebSocket service
tts = RimeTTSService(
    api_key=os.getenv("RIME_API_KEY"),
    voice_id="rex",
    model="mistv2",
    params=RimeTTSService.InputParams(
        language=Language.EN,
        speed_alpha=1.0,
        reduce_latency=False,
        pause_between_brackets=True,
        phonemize_between_brackets=False
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,  # Word timestamps enable precise context updates
    transport.output(),
    context_aggregator.assistant()
])
```

### HTTP Service

Initialize the HTTP service and use it in a pipeline:

```python
import aiohttp
from pipecat.services.rime.tts import RimeHttpTTSService

# Configure HTTP service for batch processing
async with aiohttp.ClientSession() as session:
    http_tts = RimeHttpTTSService(
        api_key=os.getenv("RIME_API_KEY"),
        voice_id="eva",
        aiohttp_session=session,
        model="arcana",  # Latest model
        params=RimeHttpTTSService.InputParams(
            language=Language.EN,
            speed_alpha=1.2,
            inline_speed_alpha="0.8,1.5",  # Word-level speed control
            pause_between_brackets=True
        )
    )
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="your-voice-id",
  )
)
```

## Metrics

Both services provide comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **WebSocket Recommended**: Use `RimeTTSService` for interactive applications requiring word timestamps and precise context management
- **Context Tracking**: WebSocket service maintains context across multiple messages within a turn
- **Text Aggregation**: WebSocket service uses `SkipTagsAggregator` by default to handle Rime's `spell()` tags
- **Model Selection**: Use `mistv2` for best balance of quality and performance, `arcana` for highest quality (HTTP only)
- **Advanced Controls**: HTTP service supports more text markup features like inline speed control



================================================
FILE: server/services/tts/riva.mdx
================================================
---
title: "NVIDIA Riva"
description: "Text-to-speech service implementation using NVIDIA Riva"
---

## Overview

NVIDIA Riva provides high-quality text-to-speech synthesis through cloud-based AI models accessible via gRPC API. The service offers multilingual support, configurable quality settings, and streaming audio generation optimized for real-time applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.riva.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="NVIDIA Riva Docs"
    icon="book"
    href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html"
  >
    Official NVIDIA Riva TTS documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07r-interruptible-riva-nim.py"
  >
    Working example with Riva NIM
  </Card>
</CardGroup>

## Installation

To use NVIDIA Riva services, install the required dependencies:

```bash
pip install "pipecat-ai[riva]"
```

You'll also need to set up your NVIDIA API key as an environment variable: `NVIDIA_API_KEY`.

<Tip>
  Get your API key from the [NVIDIA Developer
  Portal](https://developer.nvidia.com/) and access to Riva services.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Available Models

| Model                     | Description                            | Best For                              |
| ------------------------- | -------------------------------------- | ------------------------------------- |
| `magpie-tts-multilingual` | Multilingual model with natural voices | Conversational AI, multiple languages |
| `fastpitch-hifigan-tts`   | High-quality English synthesis         | English-only applications             |

<Note>
  The `magpie-tts-multilingual` model is the default and recommended for most
  use cases due to its multilingual capabilities and natural voice quality.
</Note>

## Language Support

The `magpie-tts-multilingual` model supports:

| Language Code    | Description      | Service Code |
| ---------------- | ---------------- | ------------ |
| `Language.EN_US` | English (US)     | `en-US`      |
| `Language.ES_US` | Spanish (US)     | `es-US`      |
| `Language.FR_FR` | French (France)  | `fr-FR`      |
| `Language.DE_DE` | German (Germany) | `de-DE`      |
| `Language.IT_IT` | Italian (Italy)  | `it-IT`      |
| `Language.ZH_CN` | Chinese (China)  | `zh-CN`      |

## Usage Example

### Basic Configuration

Initialize the Riva TTS service with your API key and desired voice:

```python
from pipecat.services.riva.tts import RivaTTSService
from pipecat.transcriptions.language import Language
import os

# Configure with default multilingual model
tts = RivaTTSService(
    api_key=os.getenv("NVIDIA_API_KEY"),
    voice_id="Magpie-Multilingual.EN-US.Ray",
    params=RivaTTSService.InputParams(
        language=Language.EN_US,
        quality=20
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `RivaTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="Magpie-Multilingual.ES-US.Luna",
    params=RivaTTSService.InputParams(
        language=Language.ES_US,
    )
 ))
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Model Set at Initialization**: Models cannot be changed after initialization - configure `model_function_map` during construction
- **Deprecated Classes**: `FastPitchTTSService` is deprecated - use `RivaTTSService` instead
- **Quality vs Speed**: Higher quality settings increase synthesis time but improve audio quality



================================================
FILE: server/services/tts/sarvam.mdx
================================================
---
title: "Sarvam AI"
description: "Text-to-speech service implementation using Sarvam AI’s TTS API"
---

## Overview

Sarvam AI provides text-to-speech synthesis specialized for Indian languages and voices. The service offers extensive voice customization options including pitch, pace, and loudness control, with support for multiple Indian languages and preprocessing for mixed-language content.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.sarvam.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Sarvam AI Docs"
    icon="book"
    href="https://docs.sarvam.ai/api-reference-docs/text-to-speech/convert"
  >
    Official Sarvam AI text-to-speech API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07z-interruptible-sarvam.py"
  >
    Working example with Indian language support
  </Card>
</CardGroup>

## Installation

To use Sarvam AI services, no additional dependencies are required beyond the base installation:

```bash
pip install "pipecat-ai"
```

You'll also need to set up your Sarvam AI API key as an environment variable: `SARVAM_API_KEY`.

<Tip>
  Get your API key from the [Sarvam AI Console](https://www.sarvam.ai/).
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (PCM, WAV header stripped)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Supported Sample Rates

- **8000 Hz** - Phone quality
- **16000 Hz** - Standard quality
- **22050 Hz** - High quality
- **24000 Hz** - Premium quality (default)

## Language Support

Sarvam AI specializes in Indian languages with high-quality voice synthesis:

| Language Code | Description     | Service Code |
| ------------- | --------------- | ------------ |
| `Language.BN` | Bengali         | `bn-IN`      |
| `Language.EN` | English (India) | `en-IN`      |
| `Language.GU` | Gujarati        | `gu-IN`      |
| `Language.HI` | Hindi           | `hi-IN`      |
| `Language.KN` | Kannada         | `kn-IN`      |
| `Language.ML` | Malayalam       | `ml-IN`      |
| `Language.MR` | Marathi         | `mr-IN`      |
| `Language.OR` | Odia            | `od-IN`      |
| `Language.PA` | Punjabi         | `pa-IN`      |
| `Language.TA` | Tamil           | `ta-IN`      |
| `Language.TE` | Telugu          | `te-IN`      |

## TTS Models

- **bulbul:v1** - First generation model
- **bulbul:v2** - Enhanced model with improved quality (recommended)

## Usage Example

### Basic Configuration

Initialize the Sarvam TTS service with your API key and desired voice:

```python
from pipecat.services.sarvam.tts import SarvamTTSService
from pipecat.transcriptions.language import Language
import aiohttp
import os

# Configure service with HTTP session
async with aiohttp.ClientSession() as session:
    tts = SarvamTTSService(
        api_key=os.getenv("SARVAM_API_KEY"),
        voice_id="anushka",
        model="bulbul:v2",
        aiohttp_session=session,
        params=SarvamTTSService.InputParams(
            language=Language.HI,
            pitch=0.1,
            pace=1.2,
            loudness=1.0
        )
    )

    # Use in pipeline
    pipeline = Pipeline([
        transport.input(),
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant()
    ])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `SarvamTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="meera",
  )
)

```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Language Specialization**: Optimized for Indian languages with native voice quality
- **Voice Quality**: High-quality synthesis with natural prosody for Indian languages



================================================
FILE: server/services/tts/xtts.mdx
================================================
---
title: "XTTS"
description: "Text-to-speech service implementation using Coqui’s XTTS streaming server"
---

<Warning>
  Coqui, the XTTS maintainer, has shut down. XTTS may not receive future updates
  or support.
</Warning>

## Overview

XTTS (Cross-lingual Text-to-Speech) provides multilingual voice synthesis with voice cloning capabilities through a locally hosted streaming server. The service supports real-time streaming and custom voice training using Coqui's XTTS-v2 model.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.xtts.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="XTTS Repository"
    icon="book"
    href="https://github.com/coqui-ai/xtts-streaming-server"
  >
    Official XTTS streaming server repository
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07i-interruptible-xtts.py"
  >
    Working example with voice cloning
  </Card>
</CardGroup>

## Installation

XTTS requires a running streaming server. Start the server using Docker:

```bash
docker run --gpus=all -e COQUI_TOS_AGREED=1 --rm -p 8000:80 \
  ghcr.io/coqui-ai/xtts-streaming-server:latest-cuda121
```

<Note>
  GPU acceleration is recommended for optimal performance. The server requires
  CUDA support.
</Note>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data (streaming, resampled from 24kHz)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - Server connection or processing errors

## Language Support

XTTS supports multiple languages with cross-lingual capabilities:

| Language Code | Description          | Service Code |
| ------------- | -------------------- | ------------ |
| `Language.CS` | Czech                | `cs`         |
| `Language.DE` | German               | `de`         |
| `Language.EN` | English              | `en`         |
| `Language.ES` | Spanish              | `es`         |
| `Language.FR` | French               | `fr`         |
| `Language.HI` | Hindi                | `hi`         |
| `Language.HU` | Hungarian            | `hu`         |
| `Language.IT` | Italian              | `it`         |
| `Language.JA` | Japanese             | `ja`         |
| `Language.KO` | Korean               | `ko`         |
| `Language.NL` | Dutch                | `nl`         |
| `Language.PL` | Polish               | `pl`         |
| `Language.PT` | Portuguese           | `pt`         |
| `Language.RU` | Russian              | `ru`         |
| `Language.TR` | Turkish              | `tr`         |
| `Language.ZH` | Chinese (Simplified) | `zh-cn`      |

## Usage Example

### Basic Configuration

Initialize the `XTTSService` and use it in a pipeline:

```python
from pipecat.services.xtts.tts import XTTSService
from pipecat.transcriptions.language import Language
import aiohttp

async def setup_tts():
    # Create HTTP session for server communication
    session = aiohttp.ClientSession()

    tts = XTTSService(
        aiohttp_session=session,
        voice_id="Claribel Dervla",
        base_url="http://localhost:8000",
        language=Language.EN
    )

    # Use in pipeline
    pipeline = Pipeline([
        transport.input(),
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant()
    ])

    return pipeline, session
```

### Dynamic Configuration

Make settings updates by pushing an `TTSUpdateSettingsFrame` for the `XTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="your-voice-id",
  )
)
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Streaming Performance** - Buffer utilization and chunk processing

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Local Deployment**: Runs entirely on local infrastructure for privacy
- **Voice Cloning**: Supports custom voice training with audio samples
- **Cross-lingual**: Can synthesize multiple languages with same voice



================================================
FILE: server/services/video/simli.mdx
================================================
---
title: "Simli"
description: "Video service for generating AI avatar responses using Simli's WebRTC API"
---

## Overview

`SimliVideoService` creates AI avatar video responses by converting audio input into synchronized video and audio output through Simli's WebRTC platform. It handles real-time audio streaming, video generation, and automatic audio resampling.

## Installation

Install the required dependencies:

```bash
pip install "pipecat-ai[simli]"
```

## Configuration

### Required Environment Variables

```bash
SIMLI_API_KEY=your_api_key
SIMLI_FACE_ID=your_face_id
```

<Tip>
  Get your API key and Face ID by signing up at [Simli](https://www.simli.com/)
</Tip>
## Configuration

```python
SimliVideoService(
    SimliConfig(SIMLI_API_KEY, SIMLI_FACE_ID), useTurnServer=False, latencyInterval=60
)
```

### Constructor Parameters for `SimliConfig`

<ParamField path="apiKey" type="str" required>
  Your Simli API key. This key is required for authenticating API requests.
</ParamField>

<ParamField path="faceId" type="str" required>
  The face identifier for Simli. This is used to associate API interactions with
  a specific face or persona.
</ParamField>

<ParamField path="syncAudio" type="bool" default="True">
  Indicates whether to synchronize audio streams. Defaults to `True`. Set to
  `False` to disable audio synchronization.
</ParamField>

<ParamField path="handleSilence" type="bool" default="True">
  Determines if silence in audio streams should be handled automatically.
  Defaults to `True`.
</ParamField>

<ParamField path="maxSessionLength" type="int" default="600">
  The maximum length of a session in seconds. Defaults to `600` (10 minutes).
</ParamField>

<ParamField path="maxIdleTime" type="int" default="30">
  The maximum idle time (in seconds) allowed during a session before it is
  automatically terminated. Defaults to `30` seconds.
</ParamField>

### Constructor Parameters for `SimliVideoService`

<ParamField path="simli_config" type="SimliConfig" required>
  The configuration object for Simli. This must be an instance of `simli_config`
  and contains essential settings such as API key, face ID, and other
  session-related configurations.
</ParamField>

<ParamField path="use_turn_server" type="bool" default="False">
  Determines whether a TURN server should be used for establishing connections.
  Defaults to `False`. Set to `True` if your network requires TURN for WebRTC
  connections.
</ParamField>

<ParamField path="latency_interval" type="int" default="0">
  Delay (in seconds) between ping calls to calculate latency between your
  machine and simli server. Set to `0` to disable.
</ParamField>

## Input Frames

### Audio Input

<ParamField path="TTSAudioRawFrame" type="Frame">
  Raw audio data for avatar speech
</ParamField>

### Control Frames

<ParamField path="TTSStartedFrame" type="Frame">
  Signals start of speech synthesis
</ParamField>

<ParamField path="TTSStoppedFrame" type="Frame">
  Signals end of speech synthesis
</ParamField>

<ParamField path="StartInterruptionFrame" type="Frame">
  Signals conversation interruption
</ParamField>

<ParamField path="EndFrame" type="Frame">
  Signals end of conversation
</ParamField>

<ParamField path="CancelFrame" type="Frame">
  Signals conversation cancellation
</ParamField>

## Usage Example

```python
from pipecat.pipeline.pipeline import Pipeline
from pipecat.services.simli.video import SimliVideoService
from simli import SimliConfig
import os

async def create_avatar_pipeline():
    # Initialize Simli service
    simli = SimliVideoService(
        SimliConfig(
            api_key=os.getenv("SIMLI_API_KEY"),
            face_id=os.getenv("SIMLI_FACE_ID")
        )
    )

    # Create pipeline with Simli
    pipeline = Pipeline([
        transport.input(),    # Your audio input
        llm,                  # Language model service
        tts,                  # Text-to-speech service
        simli,                # Simli video generation
        transport.output(),   # Your video output handler
    ])

    return pipeline
```

## Frame Flow

```mermaid
graph TD
    A[InputAudioRawFrame] --> B[SimliVideoService]
    B --> C[Video Generation]
```

## Metrics Support

The service collects processing metrics:

- Processing duration
- Time to First Byte (TTFB)
- API response times
- Audio processing metrics

## Common Use Cases

1. **AI Video Avatars**

   - Virtual assistants
   - Customer service
   - Educational content

2. **Interactive Presentations**

   - Product demonstrations
   - Training materials
   - Marketing content

3. **Real-time Communication**
   - Video conferencing
   - Virtual meetings
   - Interactive broadcasts

## Notes

- Handles real-time audio streaming
- Supports conversation interruptions
- Manages conversation lifecycle
- Automatic audio resampling
- Thread-safe processing
- WebRTC integration through [Daily](/server/services/transport/daily)
- Includes comprehensive error handling



================================================
FILE: server/services/video/tavus.mdx
================================================
---
title: "Tavus"
description: "Video service implementation for generating AI avatar responses using Tavus"
---

## Overview

Pipecat integrates with [Tavus](https://tavus.io) to support AI-generated video avatars. The integration includes:

- `TavusVideoService` – a pipeline service that handles audio streaming and requests Tavus to generate avatar video responses.
- `TavusTransport` – a transport layer that directly streams generated Tavus video to users in real-time. All the details [here](/server/services/transport/tavus)

For full integration details, visit [Tavus + Pipecat Docs](https://docs.tavus.io/sections/integrations/pipecat#pipecat).

---

## Installation

Install the Tavus integration for Pipecat:

```bash
pip install "pipecat-ai[tavus]"
```

Set the following environment variables:

- `TAVUS_API_KEY` – Your Tavus API key
- `TAVUS_REPLICA_ID` – Tavus replica identifier
- (Optional) `TAVUS_PERSONA_ID` – Persona ID for video generation

Sign up to get your API key at [Tavus](https://platform.tavus.io/auth/sign-up?plan=free).

---

## TavusVideoService

### Purpose

Handles real-time audio and coordinates Tavus avatar video generation.

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Tavus API key
</ParamField>

<ParamField path="replica_id" type="str" required>
  Tavus replica identifier
</ParamField>

<ParamField path="persona_id" type="str" default="pipecat-stream">
  Optional persona ID (default is `pipecat-stream`)
</ParamField>

<ParamField path="session" type="aiohttp.ClientSession" required>
  HTTP client session for API communication
</ParamField>

### Input Frames

#### Audio Input

<ParamField path="TTSAudioRawFrame" type="Frame">
  Raw audio data for avatar speech
</ParamField>

#### Control Frames

<ParamField path="StartInterruptionFrame" type="Frame">
  Signals conversation interruption
</ParamField>

<ParamField path="EndFrame" type="Frame">
  Signals end of conversation
</ParamField>

<ParamField path="CancelFrame" type="Frame">
  Signals conversation cancellation
</ParamField>

## Usage Example

```python
from pipecat.services.tavus.video import TavusVideoService
import aiohttp

async with aiohttp.ClientSession() as session:
    # Configure service
    tavus = TavusVideoService(
        api_key=os.getenv("TAVUS_API_KEY"),
        replica_id=os.getenv("TAVUS_REPLICA_ID"),
        session=session,
    )

    # Use in pipeline
    pipeline = Pipeline(
        [
            transport.input(),                # Transport user input
            stt,                              # STT
            context_aggregator.user(),        # User responses
            llm,                              # LLM
            tts,                              # TTS
            tavus,                            # Tavus output layer
            transport.output(),               # Transport bot output
            context_aggregator.assistant(),   # Assistant spoken responses
        ]
    )
```

---

## Frame Flow

```mermaid
graph TD
    A[InputAudioRawFrame] --> B[TavusVideoService]
    B --> C[Video Generation]
    B --> D[Conversation Management]
```

## Metrics Support

The service collects processing metrics:

- Processing duration
- Time to First Byte (TTFB)
- API response times
- Audio processing metrics

## Common Use Cases

1. **AI Video Avatars**

   - Virtual assistants
   - Customer service
   - Educational content

2. **Interactive Presentations**

   - Product demonstrations
   - Training materials
   - Marketing content

3. **Real-time Communication**
   - Video conferencing
   - Virtual meetings
   - Interactive broadcasts

## Notes

- Handles real-time audio streaming
- Supports conversation interruptions
- Manages conversation lifecycle
- Automatic audio resampling
- Thread-safe processing
- Includes comprehensive error handling



================================================
FILE: server/services/vision/moondream.mdx
================================================
---
title: "Moondream"
description: "Vision service implementation using Moondream for local image analysis and question answering"
---

## Overview

`MoondreamService` provides local image analysis and question-answering capabilities using the Moondream model. It runs entirely on your local machine, supporting various hardware acceleration options including CUDA, Intel XPU, and Apple MPS.

## Installation

To use `MoondreamService`, install the required dependencies:

```bash
pip install "pipecat-ai[moondream]"
```

<Tip>
  You can obtain a Moondream API key by signing up at
  [Moondream](https://www.moondream.ai/).
</Tip>

## Configuration

### Constructor Parameters

<ParamField path="model" type="str" default="vikhyatk/moondream2">
  Hugging Face model identifier
</ParamField>

<ParamField path="revision" type="str" default="2024-08-26">
  Model revision/version
</ParamField>

<ParamField path="use_cpu" type="bool" default="False">
  Force CPU usage instead of available accelerators
</ParamField>

### Hardware Acceleration

The service automatically detects and uses the best available hardware:

1. Intel XPU (if intel_extension_for_pytorch is installed)
2. NVIDIA CUDA
3. Apple Metal (MPS)
4. CPU (fallback)

## Input

### VisionImageRawFrame

<ParamField path="format" type="str">
  Image format (e.g., 'RGB', 'RGBA')
</ParamField>

<ParamField path="size" type="tuple">
  Image dimensions (width, height)
</ParamField>

<ParamField path="image" type="bytes">
  Raw image data
</ParamField>

<ParamField path="text" type="str">
  Question about the image
</ParamField>

## Output Frames

### TextFrame

<ParamField path="text" type="str">
  Generated description or answer about the image
</ParamField>

### ErrorFrame

<ParamField path="error" type="str">
  Error information if processing fails
</ParamField>

## Methods

See the [Vision base class methods](/server/base-classes/media#visionservice) for additional functionality.

## Usage Example

```python
from pipecat.services.moondream.vision import MoondreamService
from pipecat.frames.frames import VisionImageRawFrame
from PIL import Image

# Configure service
service = MoondreamService(
    model="vikhyatk/moondream2",
    revision="2024-08-26"
)

# Create pipeline
pipeline = Pipeline([
    image_input,      # Produces VisionImageRawFrame
    service,          # Analyzes images
    text_handler      # Handles text responses
])

# Example frame processing
image = Image.open("example.jpg")
frame = VisionImageRawFrame(
    format=image.mode,
    size=image.size,
    image=image.tobytes(),
    text="What objects are in this image?"
)
```

## Hardware Configuration Examples

### CUDA (NVIDIA GPU)

```python
# Automatically uses CUDA if available
service = MoondreamService()
```

### Intel XPU

```python
# Requires intel_extension_for_pytorch
import intel_extension_for_pytorch
service = MoondreamService()
```

### Force CPU Usage

```python
service = MoondreamService(use_cpu=True)
```

## Frame Flow

```mermaid
graph TD
    A[VisionImageRawFrame] --> B[MoondreamService]
    B --> C[TextFrame]
    B --> D[ErrorFrame]
```

## Metrics Support

The service collects processing metrics:

- Processing duration
- Model loading time
- Inference time

## Performance Considerations

### Memory Usage

- Model size varies by version
- GPU memory requirements depend on image size
- CPU mode uses more system memory

### Processing Speed

Relative performance by hardware:

1. NVIDIA GPU (fastest)
2. Intel XPU
3. Apple MPS
4. CPU (slowest)

## Best Practices

### 1. Image Preparation

```python
# Optimize image before processing
def prepare_image(image_path):
    image = Image.open(image_path)
    # Resize if needed
    if max(image.size) > 1024:
        image.thumbnail((1024, 1024))
    return image
```

### 2. Error Handling

```python
try:
    async for frame in service.run_vision(vision_frame):
        if isinstance(frame, ErrorFrame):
            logger.error(f"Vision processing error: {frame.error}")
        elif isinstance(frame, TextFrame):
            process_result(frame.text)
except Exception as e:
    logger.error(f"Unexpected error: {e}")
```

### 3. Resource Management

```python
# Initialize once, reuse for multiple images
service = MoondreamService()
try:
    # Process multiple images
    for image in images:
        await process_image(service, image)
finally:
    # Cleanup if needed
    await service.cleanup()
```

## Notes

- Runs completely offline after model download
- First run requires model download
- Supports multiple hardware acceleration options
- Thread-safe processing
- Automatic error handling
- Manages model lifecycle
- Supports various image formats



================================================
FILE: server/utilities/dtmf-aggregator.mdx
================================================
---
title: "DTMFAggregator"
description: "Aggregates DTMF (phone keypad) input into meaningful sequences for LLM processing"
---

## Overview

`DTMFAggregator` processes incoming DTMF (Dual-Tone Multi-Frequency) frames from phone keypad input and aggregates them into complete sequences that can be understood by LLM services. It buffers individual digit presses and flushes them as transcription frames when a termination digit is pressed, a timeout occurs, or an interruption happens.

This aggregator is essential for telephony applications where users interact via phone keypad buttons, converting raw DTMF input into structured text that LLMs can process alongside voice transcriptions.

## Constructor

```python
aggregator = DTMFAggregator(
    timeout=2.0,
    termination_digit=KeypadEntry.POUND,
    prefix="DTMF: "
)
```

<ParamField path="timeout" type="float" default="2.0">
  Idle timeout in seconds before flushing the aggregated digits
</ParamField>

<ParamField
  path="termination_digit"
  type="KeypadEntry"
  default="KeypadEntry.POUND"
>
  Digit that triggers immediate flush of the aggregation
</ParamField>

<ParamField path="prefix" type="str" default="DTMF: ">
  Prefix added to DTMF sequence in the output transcription
</ParamField>

## Input Frames

<ParamField path="InputDTMFFrame" type="Frame">
  Contains a single keypad button press with a KeypadEntry value
</ParamField>

<ParamField path="StartInterruptionFrame" type="Frame">
  Flushes any pending aggregation when user interruption begins
</ParamField>

<ParamField path="EndFrame" type="Frame">
  Flushes pending aggregation and stops the aggregation task
</ParamField>

## Output Frames

<ParamField path="TranscriptionFrame" type="Frame">
  Contains the aggregated DTMF sequence as text with the configured prefix
</ParamField>

All input frames are passed through downstream, including the original `InputDTMFFrame` instances.

## Keypad Entries

The aggregator processes these standard phone keypad entries:

| KeypadEntry           | Value         | Description       |
| --------------------- | ------------- | ----------------- |
| `ZERO` through `NINE` | `"0"` - `"9"` | Numeric digits    |
| `STAR`                | `"*"`         | Star/asterisk key |
| `POUND`               | `"#"`         | Pound/hash key    |

## Aggregation Behavior

The aggregator flushes (emits a TranscriptionFrame) when:

1. **Termination digit**: The configured termination digit is pressed (default: `#`)
2. **Timeout**: No new digits received within the timeout period (default: 2 seconds)
3. **Interruption**: A `StartInterruptionFrame` is received
4. **Pipeline end**: An `EndFrame` is received

## Usage Examples

### Basic Telephony Integration

```python
from pipecat.processors.aggregators.dtmf_aggregator import DTMFAggregator
from pipecat.serializers.twilio import TwilioFrameSerializer

# Create DTMF aggregator with default settings
dtmf_aggregator = DTMFAggregator()

# Set up Twilio serializer for phone integration
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,
    account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
    auth_token=os.getenv("TWILIO_AUTH_TOKEN")
)

# Create pipeline with DTMF processing
pipeline = Pipeline([
    transport.input(),      # Websocket input from Twilio
    dtmf_aggregator,       # Process DTMF before STT
    stt,                   # Speech-to-text service
    context_aggregator.user(),
    llm,                   # LLM processes both voice and DTMF
    tts,                   # Text-to-speech
    transport.output(),
    context_aggregator.assistant(),
])
```

### Custom Configuration for Menu Systems

```python
# Configure for menu system with star termination
menu_dtmf = DTMFAggregator(
    timeout=5.0,                    # Longer timeout for menu selection
    termination_digit=KeypadEntry.STAR,  # Use * to confirm selection
    prefix="Menu selection: "       # Clear prefix for LLM
)

# Update system prompt to handle DTMF input
messages = [
    {
        "role": "system",
        "content": """You are a phone menu assistant.

When you receive input starting with "Menu selection:", this represents
button presses on the phone keypad:
- Single digits (1-9): Menu options
- 0: Often "speak to operator"
- *: Confirmation or "go back"
- #: Usually "repeat menu"

Respond appropriately to both voice and keypad input."""
    }
]
```

## Sequence Examples

| User Input         | Aggregation Trigger    | Output TranscriptionFrame |
| ------------------ | ---------------------- | ------------------------- |
| `1`, `2`, `3`, `#` | Termination digit      | `"DTMF: 123#"`            |
| `*`, `0`           | 2-second timeout       | `"DTMF: *0"`              |
| `5`, interruption  | StartInterruptionFrame | `"DTMF: 5"`               |
| `9`, `9`, EndFrame | Pipeline shutdown      | `"DTMF: 99"`              |

## Frame Flow

```mermaid
graph TD
    A[Twilio WebSocket] --> B[TwilioFrameSerializer]
    B --> C[DTMFAggregator]
    C --> D[STT Service]
    C --> E[TranscriptionFrame]
    E --> F[LLM Context Aggregator]
    D --> F
    F --> G[LLM Service]
```

## Error Handling

The aggregator gracefully handles:

- Invalid DTMF digits (logged and ignored)
- Pipeline interruptions (flushes pending sequences)
- Rapid key presses (buffers efficiently)
- Mixed voice and DTMF input (processes independently)

## Best Practices

1. **System Prompt Design**: Train your LLM to recognize and respond to DTMF prefixed input
2. **Timeout Configuration**: Use shorter timeouts (1-2s) for rapid entry, longer (3-5s) for menu selection
3. **Termination Strategy**: Use `#` for confirmation, `*` for cancel/back operations
4. **Pipeline Placement**: Always place before the user context aggregator to ensure proper frame ordering



================================================
FILE: server/utilities/interruption-strategies.mdx
================================================
---
title: "Interruption Strategies"
description: "Configure when users can interrupt the bot to prevent unwanted interruptions from brief affirmations"
---

## Overview

Interruption strategies allow you to control when users can interrupt the bot during speech. By default, any user speech immediately interrupts the bot, but this can be problematic when users engage in backchanneling—brief vocal responses like "yeah", "okay", or "mm-hmm" that indicate they're listening without intending to interrupt.

With interruption strategies, you can require users to meet specific criteria (such as speaking a minimum number of words or reaching a certain audio volume) before their speech will interrupt the bot, creating a more natural conversation flow.

<Tip>
  Want to try it out? Check out the [interruption strategies foundational
  demo](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/42-interruption-config.py)
</Tip>

## Configuration

Interruption strategies are configured via the `interruption_strategies` parameter in `PipelineParams`. When specified, the normal immediate interruption behavior is replaced with conditional interruption based on your criteria.

<ParamField
  path="interruption_strategies"
  type="List[BaseInterruptionStrategy]"
  default="[]"
>
  List of interruption strategies to apply. When multiple strategies are
  provided, the first one that evaluates to true will trigger the interruption.
  If empty, normal interruption behavior applies.
</ParamField>

## Base Strategy Interface

All interruption strategies inherit from `BaseInterruptionStrategy`, which provides a common interface for evaluating interruption conditions.

<ParamField path="append_audio(audio, sample_rate)" type="async method">
  Appends audio data to the strategy for analysis. Not all strategies handle
  audio.
</ParamField>

<ParamField path="append_text(text)" type="async method">
  Appends text to the strategy for analysis. Not all strategies handle text.
</ParamField>

<ParamField path="should_interrupt()" type="async method">
  Called when the user stops speaking to determine if interruption should occur
  based on accumulated audio and/or text.
</ParamField>

<ParamField path="reset()" type="async method">
  Resets accumulated text and/or audio data.
</ParamField>

## Available Strategies

### MinWordsInterruptionStrategy

Requires users to speak a minimum number of words before interrupting the bot.

<ParamField path="min_words" type="int" required>
  Minimum number of words the user must speak to interrupt the bot. Must be
  greater than 0.
</ParamField>

```python
from pipecat.audio.interruptions.min_words_interruption_strategy import MinWordsInterruptionStrategy

strategy = MinWordsInterruptionStrategy(min_words=3)
```

## How It Works

When interruption strategies are configured:

1. **Bot not speaking**: User speech interrupts immediately (normal behavior)
2. **Bot speaking**: User speech and audio are collected and fed to strategies
3. **User stops speaking**: Strategies are evaluated in order
4. **First match wins**: The first strategy that returns `True` triggers interruption
5. **No matches**: User speech is discarded

The system automatically handles both audio and text input:

- Audio frames (`InputAudioRawFrame`) are fed to `append_audio()`
- Transcription text is fed to `append_text()`
- Strategies can use either or both data types

## Usage Examples

### Basic Word Count Interruption

Require users to speak at least 3 words to interrupt the bot:

```python
from pipecat.audio.interruptions.min_words_interruption_strategy import MinWordsInterruptionStrategy
from pipecat.pipeline.task import PipelineParams, PipelineTask

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        interruption_strategies=[MinWordsInterruptionStrategy(min_words=3)]
    )
)
```

### Multiple Strategies with Priority

Strategies are evaluated in order, with the first match triggering interruption:

```python
# Prioritize word count, then volume (hypothetical future strategy)
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        interruption_strategies=[
            MinWordsInterruptionStrategy(min_words=2),        # Check first
            # VolumeInterruptionStrategy(min_volume=0.8),     # Your custom strategy
        ]
    )
)
```

## Behavior Comparison

| Scenario                                      | Without Strategy          | With MinWordsInterruptionStrategy(min_words=3) |
| --------------------------------------------- | ------------------------- | ---------------------------------------------- |
| User says "okay" while bot speaks             | ✅ Interrupts immediately | ❌ Ignored (only 1 word)                       |
| User says "yes that's right" while bot speaks | ✅ Interrupts immediately | ✅ Interrupts (3 words)                        |
| User speaks while bot is silent               | ✅ Processed immediately  | ✅ Processed immediately                       |

## Notes

- Interruption strategies only affect behavior when the bot is actively speaking
- When the bot is not speaking, user input is processed immediately regardless of strategy configuration
- The `allow_interruptions` parameter must be `True` for interruption strategies to work
- User speech that doesn't meet interruption criteria is discarded, not queued
- Strategies are evaluated in order - first match wins
- Both audio and text data are automatically fed to strategies based on their implementation
- Word counting uses simple whitespace splitting for word boundaries



================================================
FILE: server/utilities/opentelemetry.mdx
================================================
---
title: "OpenTelemetry Tracing"
sidebarTitle: "OpenTelemetry"
description: "Monitor and analyze your Pipecat conversational pipelines using OpenTelemetry"
---

## Overview

Pipecat includes built-in support for OpenTelemetry tracing, allowing you to gain deep visibility into your voice applications. Tracing helps you:

- Track latency and performance across your conversation pipeline
- Monitor service health and identify bottlenecks
- Visualize conversation turns and service dependencies
- Collect usage metrics and operational analytics

## Installation

To use OpenTelemetry tracing with Pipecat, install the tracing dependencies:

```bash
pip install "pipecat-ai[tracing]"
```

<Tip>
For local development and testing, we recommend using Jaeger as a trace collector.
You can run it with Docker:

```bash
docker run -d --name jaeger \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

Then access the UI at [http://localhost:16686](http://localhost:16686)

</Tip>

## Basic Setup

Enabling tracing in your Pipecat application requires two steps:

1. **Initialize the OpenTelemetry SDK** with your preferred exporter
2. **Enable tracing in your PipelineTask**

```python
import os
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from pipecat.utils.tracing.setup import setup_tracing
from pipecat.pipeline.task import PipelineTask, PipelineParams

# Step 1: Initialize OpenTelemetry with your chosen exporter
exporter = OTLPSpanExporter(
    endpoint="http://localhost:4317",  # Jaeger or other collector endpoint
    insecure=True,
)

setup_tracing(
    service_name="my-voice-app",
    exporter=exporter,
    console_export=False,  # Set to True for debug output
)

# Step 2: Enable tracing in your PipelineTask
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        enable_metrics=True,                              # Required for some service metrics
    ),
    enable_tracing=True,                                  # Enable tracing for this task
    enable_turn_tracking=True,                            # Enable turn tracking for this task
    conversation_id="customer-123",                       # Optional - will auto-generate if not provided
    additional_span_attributes={"session.id": "abc-123"} # Optional - additional attributes to attach to the otel span
)
```

<Info>
For complete working examples, see our sample implementations:

- [Jaeger Tracing Example](https://github.com/pipecat-ai/pipecat-examples/tree/main/open-telemetry/jaeger) - Uses gRPC exporter with Jaeger
- [Langfuse Tracing Example](https://github.com/pipecat-ai/pipecat-examples/tree/main/open-telemetry/langfuse) - Uses HTTP exporter with Langfuse for LLM-focused observability

</Info>

## Trace Structure

Pipecat organizes traces hierarchically, following the natural structure of conversations:

```
Conversation (conversation)
├── turn
│   ├── stt
│   ├── llm
│   └── tts
└── turn
    ├── stt
    ├── llm
    └── tts
    turn...
```

For real-time multimodal services like Gemini Live and OpenAI Realtime, the structure adapts to their specific patterns:

```
Conversation (conversation)
├── turn
│   ├── llm_setup (session configuration)
│   ├── stt (user input)
│   ├── llm_response (complete response with usage)
│   └── llm_tool_call/llm_tool_result (for function calls)
└── turn
    ├── stt (user input)
    └── llm_response (complete response)
    turn...
```

This hierarchical structure makes it easy to:

- Track the full lifecycle of a conversation
- Measure latency for individual turns
- Identify which services are contributing to delays
- Compare performance across different conversations

## Exporter Options

Pipecat supports any OpenTelemetry-compatible exporter. Common options include:

### OTLP Exporter (for Jaeger, Grafana, etc.)

```python
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    endpoint="http://localhost:4317",  # Your collector endpoint
    insecure=True,  # Use False for TLS connections
)
```

### HTTP OTLP Exporter (for Langfuse, etc.)

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    # Configure with environment variables:
    # OTEL_EXPORTER_OTLP_ENDPOINT
    # OTEL_EXPORTER_OTLP_HEADERS
)
```

See our [Langfuse example](https://github.com/pipecat-ai/pipecat-examples/tree/main/open-telemetry/langfuse) for details on configuring this exporter.

### Console Exporter (for debugging)

The console exporter can be enabled alongside any other exporter by setting `console_export=True`:

```python
setup_tracing(
    service_name="my-voice-app",
    exporter=otlp_exporter,
    console_export=True,  # Prints traces to stdout
)
```

### Cloud Provider Exporters

Many cloud providers offer OpenTelemetry-compatible observability services:

- **AWS X-Ray**
- **Google Cloud Trace**
- **Azure Monitor**
- **Datadog APM**

Check the OpenTelemetry documentation for specific exporter configurations: [OpenTelemetry Vendors](https://opentelemetry.io/ecosystem/vendors/)

## Span Attributes

Pipecat enriches spans with detailed attributes about service operations:

### TTS Service Spans

- `gen_ai.system`: Service provider (e.g., "cartesia")
- `gen_ai.request.model`: Model ID/name
- `voice_id`: Voice identifier
- `text`: The text being synthesized
- `metrics.character_count`: Number of characters in the text
- `metrics.ttfb`: Time to first byte in seconds
- `settings.*`: Service-specific configuration parameters

### STT Service Spans

- `gen_ai.system`: Service provider (e.g., "deepgram")
- `gen_ai.request.model`: Model ID/name
- `transcript`: The transcribed text
- `is_final`: Whether the transcription is final
- `language`: Detected or configured language
- `vad_enabled`: Whether voice activity detection is enabled
- `metrics.ttfb`: Time to first byte in seconds
- `settings.*`: Service-specific configuration parameters

### LLM Service Spans

- `gen_ai.system`: Service provider (e.g., "openai", "gcp.gemini")
- `gen_ai.request.model`: Model ID/name
- `gen_ai.operation.name`: Operation type (e.g., "chat")
- `stream`: Whether streaming is enabled
- `input`: JSON-serialized input messages
- `output`: Complete response text
- `tools`: JSON-serialized tools configuration
- `tools.count`: Number of tools available
- `tools.names`: Comma-separated tool names
- `system`: System message content
- `gen_ai.usage.input_tokens`: Number of prompt tokens
- `gen_ai.usage.output_tokens`: Number of completion tokens
- `metrics.ttfb`: Time to first byte in seconds
- `gen_ai.request.*`: Standard parameters (temperature, max_tokens, etc.)

### Multimodal Service Spans (Gemini Live & OpenAI Realtime)

#### Setup Spans

- `gen_ai.system`: "gcp.gemini" or "openai"
- `gen_ai.request.model`: Model identifier
- `tools.count`: Number of available tools
- `tools.definitions`: JSON-serialized tool schemas
- `system_instruction`: System prompt (truncated)
- `session.*`: Session configuration parameters

#### Request Spans (OpenAI Realtime)

- `input`: JSON-serialized context messages being sent
- `gen_ai.operation.name`: "llm_request"

#### Response Spans

- `output`: Complete assistant response text
- `output_modality`: "TEXT" or "AUDIO" (Gemini Live)
- `gen_ai.usage.input_tokens`: Prompt tokens used
- `gen_ai.usage.output_tokens`: Completion tokens generated
- `function_calls.count`: Number of function calls made
- `function_calls.names`: Comma-separated function names
- `metrics.ttfb`: Time to first response in seconds

#### Tool Call/Result Spans (Gemini Live)

- `tool.function_name`: Name of the function being called
- `tool.call_id`: Unique identifier for the call
- `tool.arguments`: Function arguments (truncated)
- `tool.result`: Function execution result (truncated)
- `tool.result_status`: "completed", "error", or "parse_error"

### Turn Spans

- `turn.number`: Sequential turn number
- `turn.type`: Type of turn (e.g., "conversation")
- `turn.duration_seconds`: Duration of the turn
- `turn.was_interrupted`: Whether the turn was interrupted
- `conversation.id`: ID of the parent conversation

### Conversation Spans

- `conversation.id`: Unique identifier for the conversation
- `conversation.type`: Type of conversation (e.g., "voice")

## Usage Metrics

Pipecat's tracing implementation automatically captures usage metrics for LLM and TTS services:

### LLM Token Usage

Token usage is captured in LLM spans as:

- `gen_ai.usage.input_tokens`
- `gen_ai.usage.output_tokens`

### TTS Character Count

Character count is captured in TTS spans as:

- `metrics.character_count`

## Performance Metrics

Pipecat traces capture key performance metrics for each service:

### Time To First Byte (TTFB)

The time it takes for a service to produce its first response:

- `metrics.ttfb` (in seconds)

### Processing Duration

The total time spent processing in each service is captured in the span duration.

## Configuration Options

### PipelineTask Parameters

<ParamField path="enable_tracing" type="bool" default="True">
  Enable or disable tracing for the pipeline
</ParamField>

<ParamField path="enable_turn_tracking" type="bool" default="False">
  Whether to enable turn tracking.
</ParamField>

<ParamField path="conversation_id" type="Optional[str]" default="None">
  Custom ID for the conversation. If not provided, a UUID will be generated
</ParamField>

<ParamField
  path="additional_span_attributes"
  type="Optional[dict]"
  default="None"
>
  Any additional attributes to add to top-level OpenTelemetry conversation span.
</ParamField>

### setup_tracing() Parameters

<ParamField path="service_name" type="str" default="pipecat">
  Name of the service for traces
</ParamField>

<ParamField path="exporter" type="Optional[SpanExporter]" default="None">
  A pre-configured OpenTelemetry span exporter instance
</ParamField>

<ParamField path="console_export" type="bool" default="False">
  Whether to also export traces to console (useful for debugging)
</ParamField>

## Example

Here's a complete example showing OpenTelemetry tracing setup with Jaeger:

```python
import os
from dotenv import load_dotenv
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.utils.tracing.setup import setup_tracing

load_dotenv()

# Initialize tracing if enabled
if os.getenv("ENABLE_TRACING"):
    # Create the exporter
    otlp_exporter = OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True,
    )

    # Set up tracing with the exporter
    setup_tracing(
        service_name="pipecat-demo",
        exporter=otlp_exporter,
        console_export=bool(os.getenv("OTEL_CONSOLE_EXPORT")),
    )

# Create your services
stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="your-voice-id"
)

# Build pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant(),
])

# Create pipeline task with tracing enabled
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
    enable_tracing=True,
    enable_turn_tracking=True,
    conversation_id="customer-123",                       # Optional - will auto-generate if not provided
    additional_span_attributes={"session.id": "abc-123"} # Optional - additional attributes to attach to the otel span
)

# Run the pipeline
runner = PipelineRunner()
await runner.run(task)
```

## Troubleshooting

If you're having issues with tracing:

- **No Traces Visible**: Ensure the OpenTelemetry packages are installed and that your collector endpoint is correct
- **Missing Service Data**: Verify that `enable_metrics=True` is set in PipelineParams
- **Debugging Tracing**: Enable console export with `console_export=True` to view traces in your logs
- **Connection Errors**: Check network connectivity to your trace collector
- **Collector Configuration**: Verify your collector is properly set up to receive traces

## References

- [OpenTelemetry Python Documentation](https://opentelemetry-python.readthedocs.io/)
- [OpenTelemetry Tracing Specification](https://opentelemetry.io/docs/reference/specification/trace/api/)
- [Jaeger Documentation](https://www.jaegertracing.io/docs/latest/)
- [Langfuse OpenTelemetry Documentation](https://langfuse.com/docs/opentelemetry/get-started)



================================================
FILE: server/utilities/transcript-processor.mdx
================================================
---
title: "TranscriptProcessor"
description: "Factory for creating and managing conversation transcript processors with shared event handling"
---

## Overview

The `TranscriptProcessor` is a factory class that creates and manages processors for handling conversation transcripts from both users and assistants. It provides unified access to transcript processors with shared event handling, making it easy to track and respond to conversation updates in real-time.

The processor normalizes messages from various sources into a consistent `TranscriptionMessage` format and emits events when new messages are added to the conversation.

## Constructor

```python
TranscriptProcessor()
```

Creates a new transcript processor factory with no parameters.

## Methods

### user()

```python
def user(**kwargs) -> UserTranscriptProcessor
```

Get or create the user transcript processor instance. This processor handles `TranscriptionFrame`s from STT services.

**Parameters:**

- `**kwargs`: Arguments passed to the `UserTranscriptProcessor` constructor

**Returns:** `UserTranscriptProcessor` instance for processing user messages.

### assistant()

```python
def assistant(**kwargs) -> AssistantTranscriptProcessor
```

Get or create the assistant transcript processor instance. This processor handles `TTSTextFrame`s from TTS services and aggregates them into complete utterances.

**Parameters:**

- `**kwargs`: Arguments passed to the `AssistantTranscriptProcessor` constructor

**Returns:** `AssistantTranscriptProcessor` instance for processing assistant messages.

### event_handler()

```python
def event_handler(event_name: str)
```

Decorator that registers event handlers for both user and assistant processors.

**Parameters:**

- `event_name`: Name of the event to handle

**Returns:** Decorator function that registers the handler with both processors.

## Event Handlers

### on_transcript_update

Triggered when new messages are added to the conversation transcript.

```python
@transcript.event_handler("on_transcript_update")
async def handle_transcript_update(processor, frame):
    # Handle transcript updates
    pass
```

**Parameters:**

- `processor`: The specific processor instance that emitted the event (UserTranscriptProcessor or AssistantTranscriptProcessor)
- `frame`: `TranscriptionUpdateFrame` containing the new messages

## Data Structures

### TranscriptionMessage

```python
@dataclass
class TranscriptionMessage:
    role: Literal["user", "assistant"]
    content: str
    timestamp: str | None = None
    user_id: str | None = None
```

**Fields:**

- `role`: The message sender type ("user" or "assistant")
- `content`: The transcribed text content
- `timestamp`: ISO 8601 timestamp when the message was created
- `user_id`: Optional user identifier (for user messages only)

### TranscriptionUpdateFrame

Frame containing new transcript messages, emitted by the `on_transcript_update` event.

**Properties:**

- `messages`: List of `TranscriptionMessage` objects containing the new transcript content

## Frames

### UserTranscriptProcessor

- **Input:** `TranscriptionFrame` from STT services
- **Output:** `TranscriptionMessage` with role "user"

### AssistantTranscriptProcessor

- **Input:** `TTSTextFrame` from TTS services
- **Output:** `TranscriptionMessage` with role "assistant"

## Integration Notes

### Pipeline Placement

Place the processors at specific positions in your pipeline for accurate transcript collection:

```python
pipeline = Pipeline([
    transport.input(),
    stt,                        # Speech-to-text service
    transcript.user(),          # Place after STT
    context_aggregator.user(),
    llm,
    tts,                        # Text-to-speech service
    transport.output(),
    transcript.assistant(),     # Place after transport.output()
    context_aggregator.assistant(),
])
```

### Event Handler Registration

Event handlers are automatically applied to both user and assistant processors:

```python
transcript = TranscriptProcessor()

# This handler will receive events from both processors
@transcript.event_handler("on_transcript_update")
async def handle_update(processor, frame):
    for message in frame.messages:
        print(f"{message.role}: {message.content}")
```



================================================
FILE: server/utilities/user-idle-processor.mdx
================================================
---
title: "UserIdleProcessor"
description: "A processor that monitors user inactivity and triggers callbacks after specified timeout periods"
---

The `UserIdleProcessor` is a specialized frame processor that monitors user activity in a conversation and executes callbacks when the user becomes idle. It's particularly useful for maintaining engagement by detecting periods of user inactivity and providing escalating responses to inactivity.

## Constructor Parameters

<ParamField path="callback" type="Union[BasicCallback, RetryCallback]" required>
An async function that will be called when user inactivity is detected. Can be
either:

- Basic callback: `async def(processor: UserIdleProcessor) -> None`

- Retry callback: `async def(processor: UserIdleProcessor, retry_count: int) ->
bool` where returning `False` stops idle monitoring

</ParamField>

<ParamField path="timeout" type="float" required>
  The number of seconds to wait before considering the user idle.
</ParamField>

## Behavior

The processor starts monitoring for inactivity only after the first conversation activity (either `UserStartedSpeakingFrame` or `BotSpeakingFrame`). It manages idle state based on the following rules:

- Resets idle timer when user starts or stops speaking
- Pauses idle monitoring while user is speaking
- Resets idle timer when bot is speaking
- Stops monitoring on conversation end or cancellation
- Manages a retry count for the retry callback
- Stops monitoring when retry callback returns `False`

## Properties

<ParamField path="retry_count" type="int">
  The current number of retry attempts made to engage the user.
</ParamField>

## Example Implementations

Here are two example showing how to use the `UserIdleProcessor`: one with the basic callback and one with the retry callback:

<Tabs>
  <Tab title="Basic Callback">
    ```python
    from pipecat.frames.frames import LLMMessagesFrame
    from pipecat.pipeline.pipeline import Pipeline
    from pipecat.processors.user_idle_processor import UserIdleProcessor

    async def handle_idle(user_idle: UserIdleProcessor) -> None:
        messages.append({
            "role": "system",
            "content": "Ask the user if they are still there and try to prompt for some input."
        })
        await user_idle.push_frame(LLMMessagesFrame(messages))

    # Create the processor
    user_idle = UserIdleProcessor(
        callback=handle_idle,
        timeout=5.0
    )

    # Add to pipeline
    pipeline = Pipeline([
        transport.input(),
        user_idle,  # Add the processor to monitor user activity
        context_aggregator.user(),
        # ... rest of pipeline
    ])
    ```

  </Tab>
  <Tab title="Retry Callback">
    ```python
    from pipecat.frames.frames import EndFrame, LLMMessagesFrame, TTSSpeakFrame
    from pipecat.pipeline.pipeline import Pipeline
    from pipecat.processors.user_idle_processor import UserIdleProcessor

    async def handle_user_idle(user_idle: UserIdleProcessor, retry_count: int) -> bool:
        if retry_count == 1:
            # First attempt: Gentle reminder
            messages.append({
                "role": "system",
                "content": "The user has been quiet. Politely and briefly ask if they're still there."
            })
            await user_idle.push_frame(LLMMessagesFrame(messages))
            return True
        elif retry_count == 2:
            # Second attempt: Direct prompt
            messages.append({
                "role": "system",
                "content": "The user is still inactive. Ask if they'd like to continue our conversation."
            })
            await user_idle.push_frame(LLMMessagesFrame(messages))
            return True
        else:
            # Third attempt: End conversation
            await user_idle.push_frame(
                TTSSpeakFrame("It seems like you're busy right now. Have a nice day!")
            )
            await task.queue_frame(EndFrame())
            return False  # Stop monitoring

    # Create the processor
    user_idle = UserIdleProcessor(
        callback=handle_user_idle,
        timeout=5.0
    )

    # Add to pipeline
    pipeline = Pipeline([
        transport.input(),
        user_idle,  # Add the processor to monitor user activity
        context_aggregator.user(),
        # ... rest of pipeline
    ])
    ```

  </Tab>
</Tabs>

## Frame Handling

The processor handles the following frame types:

- `UserStartedSpeakingFrame`: Marks user as active, resets idle timer and retry count
- `UserStoppedSpeakingFrame`: Starts idle monitoring
- `BotSpeakingFrame`: Resets idle timer
- `EndFrame` / `CancelFrame`: Stops idle monitoring

## Notes

- The idle callback won't be triggered while the user or bot is actively speaking
- The processor automatically cleans up its resources when the pipeline ends
- Basic callbacks are supported for backward compatibility



================================================
FILE: server/utilities/watchdog-timers.mdx
================================================
---
title: "Watchdog Timers"
description: "Monitor task freezes and processing times"
---

## Overview

Watchdog timers are used to detect if a Pipecat task is taking longer than expected. By default, if watchdog timers are enabled and a task takes longer than 5 seconds to reset the timer, a warning will be logged. Watchdog timers are very common in real-time applications to detect if things are frozen or taking too much time. Usually, you start the watchdog and you need to keep resetting it before the watchdog timer timeout expires.

In Pipecat, watchdog timers are available if you create a task using the `FrameProcessor.create_task()` method. If you use `asyncio.create_task()` or `loop.create_task()`, watchdog timers will not work.

## Configuration

Watchdog timers are disabled by default. You can enable and configure them using the following `PipelineTask` constructor arguments:

<ParamField
  path="watchdog_timeout_secs"
  type="float"
  default="5.0"
>
  Watchdog timer timeout.
</ParamField>

<ParamField
  path="enable_watchdog_logging"
  type="bool"
  default="False"
>
  Whether to print task processing times.
</ParamField>

<ParamField
  path="enable_watchdog_timers"
  type="bool"
  default="False"
>
  Whether to enable watchdog timers.
</ParamField>

It is possible to configure watchdog timers individually for each `FrameProcessor`, using the same argument names in the constructor, or even per task when creating them with `FrameProcessor.create_task()`.

## How It Works

Watchdog timers are always available for every created Pipecat task:

1. We enable watchdog timers using `enable_watchdog_timers`.
2. A task is created with `FrameProcessor.create_task()` and its watchdog timer is started.
3. The task needs to periodically call `self.reset_watchdog()` to prevent the watchdog timer to expire
4. If the watchdog timer is not reset a warning will be logged

## Usage Examples

```python
class MyFrameProcessor(FrameProcessor):

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        ...
        self._task = self.create_task(self._task_handler())
        ...

    async def _task_handler(self):
        while True:
            item = await self._queue.get()

            ...
            # Perform any processing
            ...
            self.reset_watchdog()
```

## Notes

- Watchdog timers are disabled by default
- Watchdog timers only work with Pipecat tasks (use `FrameProcessor.create_task()`)
- Watchdog timers can be enabled and configure globally with `PipelineTask`, per `FrameProcessor` or per task



================================================
FILE: server/utilities/audio/audio-buffer-processor.mdx
================================================
---
title: "AudioBufferProcessor"
description: "Process and buffer audio frames from conversations with flexible event handling"
---

## Overview

The `AudioBufferProcessor` captures and buffers audio frames from both input (user) and output (bot) sources during conversations. It provides synchronized audio streams with configurable sample rates, supports both mono and stereo output, and offers flexible event handlers for various audio processing workflows.

## Constructor

```python
AudioBufferProcessor(
    sample_rate=None,
    num_channels=1,
    buffer_size=0,
    enable_turn_audio=False,
    **kwargs
)
```

### Parameters

<ParamField path="sample_rate" type="Optional[int]" default="None">
  The desired output sample rate in Hz. If `None`, uses the transport's sample
  rate from the `StartFrame`.
</ParamField>

<ParamField path="num_channels" type="int" default="1">

Number of output audio channels:

- `1`: Mono output (user and bot audio are mixed together)
- `2`: Stereo output (user audio on left channel, bot audio on right channel)

</ParamField>

<ParamField path="buffer_size" type="int" default="0">

Buffer size in bytes that triggers audio data events:

- `0`: Events only trigger when recording stops
- `>0`: Events trigger whenever buffer reaches this size (useful for chunked processing)

</ParamField>

<ParamField path="enable_turn_audio" type="bool" default="False">
  Whether to enable per-turn audio event handlers (`on_user_turn_audio_data` and
  `on_bot_turn_audio_data`).
</ParamField>

## Properties

### sample_rate

```python
@property
def sample_rate(self) -> int
```

The current sample rate of the audio processor in Hz.

### num_channels

```python
@property
def num_channels(self) -> int
```

The number of channels in the audio output (1 for mono, 2 for stereo).

## Methods

### start_recording()

```python
async def start_recording()
```

Start recording audio from both user and bot sources. Initializes recording state and resets audio buffers.

### stop_recording()

```python
async def stop_recording()
```

Stop recording and trigger final audio data handlers with any remaining buffered audio.

### has_audio()

```python
def has_audio() -> bool
```

Check if both user and bot audio buffers contain data.

**Returns:** `True` if both buffers contain audio data.

## Event Handlers

The processor supports multiple event handlers for different audio processing workflows. Register handlers using the `@processor.event_handler()` decorator.

### on_audio_data

Triggered when `buffer_size` is reached or recording stops, providing merged audio.

```python
@audiobuffer.event_handler("on_audio_data")
async def on_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):
    # Handle merged audio data
    pass
```

**Parameters:**

- `buffer`: The AudioBufferProcessor instance
- `audio`: Merged audio data (format depends on `num_channels` setting)
- `sample_rate`: Sample rate in Hz
- `num_channels`: Number of channels (1 or 2)

### on_track_audio_data

Triggered alongside `on_audio_data`, providing separate user and bot audio tracks.

```python
@audiobuffer.event_handler("on_track_audio_data")
async def on_track_audio_data(buffer, user_audio: bytes, bot_audio: bytes,
                             sample_rate: int, num_channels: int):
    # Handle separate audio tracks
    pass
```

**Parameters:**

- `buffer`: The AudioBufferProcessor instance
- `user_audio`: Raw user audio bytes (always mono)
- `bot_audio`: Raw bot audio bytes (always mono)
- `sample_rate`: Sample rate in Hz
- `num_channels`: Always 1 for individual tracks

### on_user_turn_audio_data

Triggered when a user speaking turn ends. Requires `enable_turn_audio=True`.

```python
@audiobuffer.event_handler("on_user_turn_audio_data")
async def on_user_turn_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):
    # Handle user turn audio
    pass
```

**Parameters:**

- `buffer`: The AudioBufferProcessor instance
- `audio`: Audio data from the user's speaking turn
- `sample_rate`: Sample rate in Hz
- `num_channels`: Always 1 (mono)

### on_bot_turn_audio_data

Triggered when a bot speaking turn ends. Requires `enable_turn_audio=True`.

```python
@audiobuffer.event_handler("on_bot_turn_audio_data")
async def on_bot_turn_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):
    # Handle bot turn audio
    pass
```

**Parameters:**

- `buffer`: The AudioBufferProcessor instance
- `audio`: Audio data from the bot's speaking turn
- `sample_rate`: Sample rate in Hz
- `num_channels`: Always 1 (mono)

## Audio Processing Features

- **Automatic resampling**: Converts incoming audio to the specified sample rate
- **Buffer synchronization**: Aligns user and bot audio streams temporally
- **Silence insertion**: Fills gaps in non-continuous audio streams to maintain timing
- **Turn tracking**: Monitors speaking turns when `enable_turn_audio=True`

## Integration Notes

### STT Audio Passthrough

If using an STT service in your pipeline, enable audio passthrough to make audio available to the AudioBufferProcessor:

```python
stt = DeepgramSTTService(
    api_key=os.getenv("DEEPGRAM_API_KEY"),
    audio_passthrough=True,
)
```

<Note>`audio_passthrough` is enabled by default.</Note>

### Pipeline Placement

Add the AudioBufferProcessor after `transport.output()` to capture both user and bot audio:

```python
pipeline = Pipeline([
    transport.input(),
    # ... other processors ...
    transport.output(),
    audiobuffer,  # Place after audio output
    # ... remaining processors ...
])
```



================================================
FILE: server/utilities/audio/koala-filter.mdx
================================================
---
title: "KoalaFilter"
description: "Audio noise reduction filter using Koala AI technology from Picovoice"
---

## Overview

`KoalaFilter` is an audio processor that reduces background noise in real-time audio streams using Koala Noise Suppression technology from Picovoice. It inherits from `BaseAudioFilter` and processes audio frames to improve audio quality by removing unwanted noise.

To use Koala, you need a Picovoice access key. Get started at [Picovoice Console](https://console.picovoice.ai/signup).

## Installation

The Koala filter requires additional dependencies:

```bash
pip install "pipecat-ai[koala]"
```

You'll also need to set up your Koala access key as an environment variable: `KOALA_ACCESS_KEY`

## Constructor Parameters

<ParamField path="access_key" type="str" required>
  Picovoice access key for using the Koala noise suppression service
</ParamField>

## Input Frames

<ParamField path="FilterEnableFrame" type="Frame">

Specific control frame to toggle filtering on/off

```python
from pipecat.frames.frames import FilterEnableFrame

# Disable noise reduction
await task.queue_frame(FilterEnableFrame(False))

# Re-enable noise reduction
await task.queue_frame(FilterEnableFrame(True))
```

</ParamField>

## Usage Example

```python
from pipecat.audio.filters.koala_filter import KoalaFilter

transport = DailyTransport(
    room_url,
    token,
    "Respond bot",
    DailyParams(
        audio_in_filter=KoalaFilter(access_key=os.getenv("KOALA_ACCESS_KEY")), # Enable Koala noise reduction
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)
```

## Audio Flow

```mermaid
graph TD
    A[AudioRawFrame] --> B[KoalaFilter]
    B[KoalaFilter] --> C[VAD]
    C[VAD] --> D[STT]
```

## Notes

- Requires Picovoice access key
- Supports real-time audio processing
- Handles 16-bit PCM audio format
- Can be dynamically enabled/disabled
- Maintains audio quality while reducing noise
- Efficient processing for low latency
- Automatically handles audio frame buffering
- Sample rate must match Koala's required sample rate



================================================
FILE: server/utilities/audio/krisp-filter.mdx
================================================
---
title: "KrispFilter"
description: "Audio noise reduction filter using Krisp AI technology"
---

## Overview

`KrispFilter` is an audio processor that reduces background noise in real-time audio streams using Krisp AI technology. It inherits from `BaseAudioFilter` and processes audio frames to improve audio quality by removing unwanted noise.

To use Krisp, you need a Krisp SDK license. Get started at [Krisp.ai](https://krisp.ai/developers/).

<Tip>
  Looking for help getting started with Krisp and Pipecat? Checkout our [Krisp
  noise cancellation guide](/guides/features/krisp).
</Tip>

## Installation

The Krisp filter requires additional dependencies:

```bash
pip install "pipecat-ai[krisp]"
```

## Environment Variables

You need to provide the path to the Krisp model. This can either be done by setting the `KRISP_MODEL_PATH` environment variable or by setting the `model_path` in the constructor.

## Constructor Parameters

<ParamField path="sample_type" type="str" default="PCM_16">
  Audio sample type format
</ParamField>

<ParamField path="channels" type="int" default="1">
  Number of audio channels
</ParamField>

<ParamField path="model_path" type="str" default="None">
Path to the Krisp model file.

You can set the `model_path` directly. Alternatively, you can set the `KRISP_MODEL_PATH` environment variable to the model file path.

</ParamField>

## Input Frames

<ParamField path="FilterEnableFrame" type="Frame">
  Specific control frame to toggle filtering on/off

```python
from pipecat.frames.frames import FilterEnableFrame

# Disable noise reduction
await task.queue_frame(FilterEnableFrame(False))

# Re-enable noise reduction
await task.queue_frame(FilterEnableFrame(True))
```

</ParamField>

## Usage Example

```python
from pipecat.audio.filters.krisp_filter import KrispFilter

transport = DailyTransport(
    room_url,
    token,
    "Respond bot",
    DailyParams(
        audio_in_filter=KrispFilter(), # Enable Krisp noise reduction
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)
```

## Audio Flow

```mermaid
graph TD
    A[AudioRawFrame] --> B[KrispFilter]
    B[KrispFilter] --> C[VAD]
    C[VAD] --> D[STT]
```

## Notes

- Requires Krisp SDK and model file to be available
- Supports real-time audio processing
- Supports additional features like background voice removal
- Handles PCM_16 audio format
- Thread-safe for pipeline processing
- Can be dynamically enabled/disabled
- Maintains audio quality while reducing noise
- Efficient processing for low latency



================================================
FILE: server/utilities/audio/noisereduce-filter.mdx
================================================
---
title: "NoisereduceFilter"
description: "Audio noise reduction filter using the noisereduce library"
---

## Overview

`NoisereduceFilter` is an audio processor that reduces background noise in real-time audio streams using the noisereduce library. It inherits from `BaseAudioFilter` and processes audio frames to improve audio quality by removing unwanted noise.

## Installation

The noisereduce filter requires additional dependencies:

```bash
pip install "pipecat-ai[noisereduce]"
```

## Constructor Parameters

This filter has no configurable parameters in its constructor.

## Input Frames

<ParamField path="FilterEnableFrame" type="Frame">

Specific control frame to toggle filtering on/off

```python
from pipecat.frames.frames import FilterEnableFrame

# Disable noise reduction
await task.queue_frame(FilterEnableFrame(False))

# Re-enable noise reduction
await task.queue_frame(FilterEnableFrame(True))
```

</ParamField>

## Usage Example

```python
from pipecat.audio.filters.noisereduce_filter import NoisereduceFilter

transport = DailyTransport(
    room_url,
    token,
    "Respond bot",
    DailyParams(
        audio_in_filter=NoisereduceFilter(), # Enable noise reduction
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)
```

## Audio Flow

```mermaid
graph TD
    A[AudioRawFrame] --> B[NoisereduceFilter]
    B[NoisereduceFilter] --> C[VAD]
    C[VAD] --> D[STT]
```

## Notes

- Lightweight alternative to Krisp for noise reduction
- Supports real-time audio processing
- Handles PCM_16 audio format
- Thread-safe for pipeline processing
- Can be dynamically enabled/disabled
- No additional configuration required
- Uses statistical noise reduction techniques



================================================
FILE: server/utilities/audio/silero-vad-analyzer.mdx
================================================
---
title: "SileroVADAnalyzer"
description: "Voice Activity Detection analyzer using the Silero VAD ONNX model"
---

## Overview

`SileroVADAnalyzer` is a Voice Activity Detection (VAD) analyzer that uses the Silero VAD ONNX model to detect speech in audio streams. It provides high-accuracy speech detection with efficient processing using ONNX runtime.

## Installation

The Silero VAD analyzer requires additional dependencies:

```bash
pip install "pipecat-ai[silero]"
```

## Constructor Parameters

<ParamField path="sample_rate" type="int" default="None">
  Audio sample rate in Hz. Must be either 8000 or 16000.
</ParamField>

<ParamField path="params" type="VADParams" default="VADParams()">
  Voice Activity Detection parameters object
  <Expandable title="properties">
    <ParamField path="confidence" type="float" default="0.7">
      Confidence threshold for speech detection. Higher values make detection more strict. Must be between 0 and 1.
    </ParamField>

    <ParamField path="start_secs" type="float" default="0.2">
      Time in seconds that speech must be detected before transitioning to SPEAKING state.
    </ParamField>

    <ParamField path="stop_secs" type="float" default="0.8">
      Time in seconds of silence required before transitioning back to QUIET state.
    </ParamField>

    <ParamField path="min_volume" type="float" default="0.6">
      Minimum audio volume threshold for speech detection. Must be between 0 and 1.
    </ParamField>

  </Expandable>
</ParamField>

## Usage Example

```python
transport = DailyTransport(
    room_url,
    token,
    "Respond bot",
    DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.5)),
    ),
)
```

## Technical Details

### Sample Rate Requirements

The analyzer supports two sample rates:

- 8000 Hz (256 samples per frame)
- 16000 Hz (512 samples per frame)

Model Management

- Uses ONNX runtime for efficient inference
- Automatically resets model state every 5 seconds to manage memory
- Runs on CPU by default for consistent performance
- Includes built-in model file

## Notes

- High-accuracy speech detection
- Efficient ONNX-based processing
- Automatic memory management
- Thread-safe for pipeline processing
- Built-in model file included
- CPU-optimized inference
- Supports 8kHz and 16kHz audio



================================================
FILE: server/utilities/audio/soundfile-mixer.mdx
================================================
---
title: "SoundfileMixer"
description: "Audio mixer for combining real-time audio with sound files"
---

## Overview

`SoundfileMixer` is an audio mixer that combines incoming audio with audio from files. It supports multiple audio file formats through the soundfile library and can handle runtime volume adjustments and sound switching.

## Installation

The soundfile mixer requires additional dependencies:

```bash
pip install "pipecat-ai[soundfile]"
```

## Constructor Parameters

<ParamField path="sound_files" type="Mapping[str, str]" required>

Dictionary mapping sound names to file paths. Files must be mono (single channel).

</ParamField>

<ParamField path="default_sound" type="str" required>

Name of the default sound to play (must be a key in sound_files).

</ParamField>

<ParamField path="volume" type="float" default="0.4">

Initial volume for the mixed sound. Values typically range from 0.0 to 1.0, but can go higher.

</ParamField>

<ParamField path="loop" type="bool" default="true">

Whether to loop the sound file when it reaches the end.

</ParamField>

## Control Frames

<ParamField path="MixerUpdateSettingsFrame" type="Frame">

Updates mixer settings at runtime

<Expandable title="properties">
<ParamField path="sound" type="str">
Changes the current playing sound (must be a key in sound_files)
</ParamField>
<ParamField path="volume" type="float">
Updates the mixing volume
</ParamField>

<ParamField path="loop" type="bool">
Updates whether the sound should loop
</ParamField>
</Expandable>
</ParamField>

<ParamField path="MixerEnableFrame" type="Frame">
Enables or disables the mixer

<Expandable title="properties">
<ParamField path="enable" type="bool">
Whether mixing should be enabled
</ParamField> 
</Expandable>
</ParamField>

## Usage Example

```python
# Initialize mixer with sound files
mixer = SoundfileMixer(
    sound_files={"office": "office_ambience.wav"},
    default_sound="office",
    volume=2.0,
)

# Add to transport
transport = DailyTransport(
    room_url,
    token,
    "Audio Bot",
    DailyParams(
        audio_out_enabled=True,
        audio_out_mixer=mixer,
    ),
)

# Control mixer at runtime
await task.queue_frame(MixerUpdateSettingsFrame({"volume": 0.5}))
await task.queue_frame(MixerEnableFrame(False))  # Disable mixing
await task.queue_frame(MixerEnableFrame(True))   # Enable mixing
```

## Notes

- Supports any audio format that soundfile can read
- Automatically resamples audio files to match output sample rate
- Files must be mono (single channel)
- Thread-safe for pipeline processing
- Can dynamically switch between multiple sound files
- Volume can be adjusted in real-time
- Mixing can be enabled/disabled on demand



================================================
FILE: server/utilities/daily/rest-helper.mdx
================================================
---
title: "DailyRESTHelper"
description: "Classes and methods for interacting with the Daily API to manage rooms and tokens"
---

<Card
  title="Daily REST API Documentation"
  icon="link"
  href="https://docs.daily.co/reference/rest-api"
>
  For complete Daily REST API reference and additional details
</Card>

## Classes

### DailyRoomSipParams

Configuration for SIP (Session Initiation Protocol) parameters.

<ResponseField name="display_name" type="string" default="sw-sip-dialin">
  Display name for the SIP endpoint
</ResponseField>
<ResponseField name="video" type="boolean" default={false}>
  Whether video is enabled for SIP
</ResponseField>
<ResponseField name="sip_mode" type="string" default="dial-in">
  SIP connection mode
</ResponseField>
<ResponseField name="num_endpoints" type="integer" default={1}>
  Number of SIP endpoints
</ResponseField>

```python
from pipecat.transports.services.helpers.daily_rest import DailyRoomSipParams

sip_params = DailyRoomSipParams(
    display_name="conference-line",
    video=True,
    num_endpoints=2
)
```

### RecordingsBucketConfig

Configuration for storing Daily recordings in a custom S3 bucket.

<ResponseField name="bucket_name" type="string" required>
  Name of the S3 bucket for storing recordings
</ResponseField>
<ResponseField name="bucket_region" type="string" required>
  AWS region where the S3 bucket is located
</ResponseField>
<ResponseField name="assume_role_arn" type="string" required>
  ARN of the IAM role to assume for S3 access
</ResponseField>
<ResponseField name="allow_api_access" type="boolean" default={false}>
  Whether to allow API access to the recordings
</ResponseField>

```python
from pipecat.transports.services.helpers.daily_rest import RecordingsBucketConfig

bucket_config = RecordingsBucketConfig(
    bucket_name="my-recordings-bucket",
    bucket_region="us-west-2",
    assume_role_arn="arn:aws:iam::123456789012:role/DailyRecordingsRole",
    allow_api_access=True
)
```

### DailyRoomProperties

Properties that configure a Daily room's behavior and features.

<ResponseField name="exp" type="float" optional>
  Room expiration time as Unix timestamp (e.g., time.time() + 300 for 5 minutes)
</ResponseField>
<ResponseField name="enable_chat" type="boolean" default={false}>
  Whether chat is enabled in the room
</ResponseField>
<ResponseField name="enable_prejoin_ui" type="boolean" default={false}>
  Whether the prejoin lobby UI is enabled
</ResponseField>
<ResponseField name="enable_emoji_reactions" type="boolean" default={false}>
  Whether emoji reactions are enabled
</ResponseField>
<ResponseField name="eject_at_room_exp" type="boolean" default={false}>
  Whether to eject participants when room expires
</ResponseField>
<ResponseField name="enable_dialout" type="boolean" optional>
  Whether dial-out is enabled
</ResponseField>
<ResponseField name="enable_recording" type="string" optional>
  Recording settings ("cloud", "local", or "raw-tracks")
</ResponseField>
<ResponseField name="geo" type="string" optional>
  Geographic region for room
</ResponseField>
<ResponseField name="max_participants" type="number" optional>
  Maximum number of participants allowed in the room
</ResponseField>
<ResponseField name="recordings_bucket" type="RecordingsBucketConfig" optional>
  Configuration for custom S3 bucket recordings
</ResponseField>
<ResponseField name="sip" type="DailyRoomSipParams" optional>
  SIP configuration parameters
</ResponseField>
<ResponseField name="sip_uri" type="dict" optional>
  SIP URI configuration (returned by Daily)
</ResponseField>
<ResponseField name="start_video_off" type="boolean" default={false}>
  Whether the camera video is turned off by default
</ResponseField>

The class also includes a `sip_endpoint` property that returns the SIP endpoint URI if available.

```python
import time
from pipecat.transports.services.helpers.daily_rest import (
    DailyRoomProperties,
    DailyRoomSipParams,
    RecordingsBucketConfig,
)

properties = DailyRoomProperties(
    exp=time.time() + 3600,  # 1 hour from now
    enable_chat=True,
    enable_emoji_reactions=True,
    enable_recording="cloud",
    geo="us-west",
    max_participants=50,
    sip=DailyRoomSipParams(display_name="conference"),
    recordings_bucket=RecordingsBucketConfig(
        bucket_name="my-bucket",
        bucket_region="us-west-2",
        assume_role_arn="arn:aws:iam::123456789012:role/DailyRole"
    )
)

# Access SIP endpoint if available
if properties.sip_endpoint:
    print(f"SIP endpoint: {properties.sip_endpoint}")
```

### DailyRoomParams

Parameters for creating a new Daily room.

<ResponseField name="name" type="string" optional>
  Room name (if not provided, one will be generated)
</ResponseField>
<ResponseField name="privacy" type="string" default="public">
  Room privacy setting ("private" or "public")
</ResponseField>
<ResponseField name="properties" type="DailyRoomProperties">
  Room configuration properties
</ResponseField>

```python
import time
from pipecat.transports.services.helpers.daily_rest import (
    DailyRoomParams,
    DailyRoomProperties,
)

params = DailyRoomParams(
    name="team-meeting",
    privacy="private",
    properties=DailyRoomProperties(
        enable_chat=True,
        exp=time.time() + 7200  # 2 hours from now
    )
)
```

### DailyRoomObject

Response object representing a Daily room.

<ResponseField name="id" type="string">
  Unique room identifier
</ResponseField>
<ResponseField name="name" type="string">
  Room name
</ResponseField>
<ResponseField name="api_created" type="boolean">
  Whether the room was created via API
</ResponseField>
<ResponseField name="privacy" type="string">
  Room privacy setting
</ResponseField>
<ResponseField name="url" type="string">
  Complete room URL
</ResponseField>
<ResponseField name="created_at" type="string">
  Room creation timestamp in ISO 8601 format
</ResponseField>
<ResponseField name="config" type="DailyRoomProperties">
  Room configuration
</ResponseField>

```python
from pipecat.transports.services.helpers.daily_rest import (
    DailyRoomObject,
    DailyRoomProperties,
)

# Example of what a DailyRoomObject looks like when received
room = DailyRoomObject(
    id="abc123",
    name="team-meeting",
    api_created=True,
    privacy="private",
    url="https://your-domain.daily.co/team-meeting",
    created_at="2024-01-20T10:00:00.000Z",
    config=DailyRoomProperties(
        enable_chat=True,
        exp=1705743600
    )
)
```

### DailyMeetingTokenProperties

Properties for configuring a Daily meeting token.

<ResponseField name="room_name" type="string" optional>
  The room this token is valid for. If not set, token is valid for all rooms.
</ResponseField>
<ResponseField name="eject_at_token_exp" type="boolean" optional>
  Whether to eject user when token expires
</ResponseField>
<ResponseField name="eject_after_elapsed" type="integer" optional>
  Eject user after this many seconds
</ResponseField>
<ResponseField name="nbf" type="integer" optional>
  "Not before" timestamp - users cannot join before this time
</ResponseField>
<ResponseField name="exp" type="integer" optional>
  Expiration timestamp - users cannot join after this time
</ResponseField>
<ResponseField name="is_owner" type="boolean" optional>
  Whether token grants owner privileges
</ResponseField>
<ResponseField name="user_name" type="string" optional>
  User's display name in the meeting
</ResponseField>
<ResponseField name="user_id" type="string" optional>
  Unique identifier for the user (36 char limit)
</ResponseField>
<ResponseField name="enable_screenshare" type="boolean" optional>
  Whether user can share their screen
</ResponseField>
<ResponseField name="start_video_off" type="boolean" optional>
  Whether to join with video off
</ResponseField>
<ResponseField name="start_audio_off" type="boolean" optional>
  Whether to join with audio off
</ResponseField>
<ResponseField name="enable_recording" type="string" optional>
  Recording settings ("cloud", "local", or "raw-tracks")
</ResponseField>
<ResponseField name="enable_prejoin_ui" type="boolean" optional>
  Whether to show prejoin UI
</ResponseField>
<ResponseField name="start_cloud_recording" type="boolean" optional>
  Whether to start cloud recording when user joins
</ResponseField>
<ResponseField name="permissions" type="dict" optional>
  Initial default permissions for a non-meeting-owner participant
</ResponseField>

### DailyMeetingTokenParams

Parameters for creating a Daily meeting token.

<ResponseField name="properties" type="DailyMeetingTokenProperties">
  Token configuration properties
</ResponseField>

```python
from pipecat.transports.services.helpers.daily_rest import (
    DailyMeetingTokenParams,
    DailyMeetingTokenProperties,
)

token_params = DailyMeetingTokenParams(
    properties=DailyMeetingTokenProperties(
        user_name="John Doe",
        enable_screenshare=True,
        start_video_off=True,
        permissions={"canSend": ["video", "audio"]}
    )
)
```

## Initialize DailyRESTHelper

Create a new instance of the Daily REST helper.

<ParamField path="daily_api_key" type="string" required>
  Your Daily API key
</ParamField>

<ParamField
  path="daily_api_url"
  type="string"
  default="https://api.daily.co/v1"
>
  The Daily API base URL
</ParamField>

<ParamField path="aiohttp_session" type="aiohttp.ClientSession" required>
  An aiohttp client session for making HTTP requests
</ParamField>

```python
helper = DailyRESTHelper(
    daily_api_key="your-api-key",
    aiohttp_session=session
)
```

## Create Room

Creates a new Daily room with specified parameters.

<ParamField path="params" type="DailyRoomParams" required>
  Room configuration parameters including name, privacy, and properties
</ParamField>

```python
# Create a room that expires in 1 hour
params = DailyRoomParams(
    name="my-room",
    privacy="private",
    properties=DailyRoomProperties(
        exp=time.time() + 3600,
        enable_chat=True
    )
)
room = await helper.create_room(params)
print(f"Room URL: {room.url}")
```

## Get Room From URL

Retrieves room information using a Daily room URL.

<ParamField path="room_url" type="string" required>
  The complete Daily room URL
</ParamField>

```python
room = await helper.get_room_from_url("https://your-domain.daily.co/my-room")
print(f"Room name: {room.name}")
```

## Get Token

Generates a meeting token for a specific room.

<ParamField path="room_url" type="string" required>
  The complete Daily room URL
</ParamField>

<ParamField path="expiry_time" type="float" default="3600">
  Token expiration time in seconds
</ParamField>

<ParamField path="eject_at_token_exp" type="bool" default="False">
  Whether to eject user when token expires
</ParamField>

<ParamField path="owner" type="bool" default="True">
  Whether the token should have owner privileges (overrides any setting in
  params)
</ParamField>

<ParamField path="params" type="DailyMeetingTokenParams" optional>
  Additional token configuration. Note that `room_name`, `exp`,
  `eject_at_token_exp`, and `is_owner` will be set based on the other function
  parameters.
</ParamField>

```python
# Basic token generation
token = await helper.get_token(
    room_url="https://your-domain.daily.co/my-room",
    expiry_time=1800,  # 30 minutes
    owner=True,
    eject_at_token_exp=True
)

# Advanced token generation with additional properties
token_params = DailyMeetingTokenParams(
    properties=DailyMeetingTokenProperties(
        user_name="John Doe",
        start_video_off=True
    )
)
token = await helper.get_token(
    room_url="https://your-domain.daily.co/my-room",
    expiry_time=1800,
    owner=False,
    eject_at_token_exp=True,
    params=token_params
)
```

## Delete Room By URL

Deletes a room using its URL.

<ParamField path="room_url" type="string" required>
  The complete Daily room URL
</ParamField>

```python
success = await helper.delete_room_by_url("https://your-domain.daily.co/my-room")
if success:
    print("Room deleted successfully")
```

## Delete Room By Name

Deletes a room using its name.

<ParamField path="room_name" type="string" required>
  The name of the Daily room
</ParamField>

```python
success = await helper.delete_room_by_name("my-room")
if success:
    print("Room deleted successfully")
```

## Get Name From URL

Extracts the room name from a Daily room URL.

<ParamField path="room_url" type="string" required>
  The complete Daily room URL
</ParamField>

```python
room_name = helper.get_name_from_url("https://your-domain.daily.co/my-room")
print(f"Room name: {room_name}")  # Outputs: "my-room"
```



================================================
FILE: server/utilities/filters/frame-filter.mdx
================================================
---
title: "FrameFilter"
description: "Processor that selectively passes through only specified frame types"
---

## Overview

`FrameFilter` is a processor that filters frames based on their types, only passing through frames that match specified types (plus some system frames like `EndFrame` and `SystemFrame`).

## Constructor Parameters

<ParamField path="types" type="Tuple[Type[Frame], ...]" required>
  Tuple of frame types that should be passed through the filter
</ParamField>

## Functionality

When a frame passes through the filter, it is checked against the provided types. Only frames that match one of the specified types (or are system frames) will be passed downstream. All other frames are dropped.

## Output Frames

The processor always passes through:

- Frames matching any of the specified types
- `EndFrame` and `SystemFrame` instances (always allowed, so as to not block the pipeline)

## Usage Example

```python
from pipecat.frames.frames import TextFrame, AudioRawFrame, Frame
from pipecat.processors.filters import FrameFilter
from typing import Tuple, Type

# Create a filter that only passes TextFrames and AudioRawFrames
text_and_audio_filter = FrameFilter(
    types=(TextFrame, AudioRawFrame)
)

# Add to pipeline
pipeline = Pipeline([
    source,
    text_and_audio_filter,  # Filters out all other frame types
    destination
])
```

## Frame Flow

```mermaid
graph TD
    A[Input Frames] --> B[FrameFilter]
    B --> C{Frame Type Check}
    C -->|Matches Allowed Types| D[Output Frame]
    C -->|System Frame| D
    C -->|Other Frame Types| E[Dropped]
```

## Notes

- Simple but powerful way to restrict which frame types flow through parts of your pipeline
- Always allows system frames to pass through for proper pipeline operation
- Can be used to isolate specific parts of your pipeline from certain frame types
- Efficient implementation with minimal overhead



================================================
FILE: server/utilities/filters/function-filter.mdx
================================================
---
title: "FunctionFilter"
description: "Processor that filters frames using a custom filter function"
---

## Overview

`FunctionFilter` is a flexible processor that uses a custom async function to determine which frames to pass through. This allows for complex, dynamic filtering logic beyond simple type checking.

## Constructor Parameters

<ParamField path="filter" type="Callable[[Frame], Awaitable[bool]]" required>
  Async function that examines each frame and returns True to allow it or False
  to filter it out
</ParamField>

<ParamField
  path="direction"
  type="FrameDirection"
  default="FrameDirection.DOWNSTREAM"
>
  Which direction of frames to filter (DOWNSTREAM or UPSTREAM)
</ParamField>

## Functionality

When a frame passes through the processor:

1. System frames and end frames are always passed through
2. Frames moving in a different direction than specified are always passed through
3. Other frames are passed to the filter function
4. If the filter function returns True, the frame is passed through

## Output Frames

The processor conditionally passes through frames based on:

- Frame type (system frames and end frames always pass)
- Frame direction (only filters in the specified direction)
- Result of the custom filter function

## Usage Example

```python
from pipecat.frames.frames import TextFrame, Frame
from pipecat.processors.filters import FunctionFilter
from pipecat.processors.frame_processor import FrameDirection

# Create filter that only allows TextFrames with more than 10 characters
async def long_text_filter(frame: Frame) -> bool:
    if isinstance(frame, TextFrame):
        return len(frame.text) > 10
    return False

# Apply filter to downstream frames only
text_length_filter = FunctionFilter(
    filter=long_text_filter,
    direction=FrameDirection.DOWNSTREAM
)

# Add to pipeline
pipeline = Pipeline([
    source,
    text_length_filter,  # Filters out short text frames
    destination
])
```

## Frame Flow

```mermaid
graph TD
    A[Input Frames] --> B[FunctionFilter]
    B --> C{System/End Frame?}
    C -->|Yes| F[Output Frame]
    C -->|No| D{Correct Direction?}
    D -->|No| F
    D -->|Yes| E{Filter Function}
    E -->|Returns True| F
    E -->|Returns False| G[Dropped]
```

## Notes

- Provides maximum flexibility for complex filtering logic
- Can incorporate dynamic conditions that change at runtime
- Only filters frames moving in the specified direction
- Always passes through system frames for proper pipeline operation
- Can be used to create sophisticated content-based filters
- Supports async filter functions for complex processing



================================================
FILE: server/utilities/filters/identify-filter.mdx
================================================
---
title: "IdentityFilter"
description: "Processor that passes all frames through without modification"
---

## Overview

`IdentityFilter` is a simple pass-through processor that forwards all frames without any modification or filtering. It acts as a transparent layer in your pipeline, allowing all frames to flow through unchanged.

<Tip>
  Check out Observers for an option that delivers similar functionality but
  doesn't require a processor to reside in the Pipeline.
</Tip>

## Constructor Parameters

The `IdentityFilter` constructor accepts no specific parameters beyond those inherited from `FrameProcessor`.

## Functionality

When a frame passes through the processor, it is immediately forwarded in the same direction with no changes. This applies to all frame types and both directions (upstream and downstream).

## Use Cases

While functionally equivalent to having no filter at all, `IdentityFilter` can be useful in several scenarios:

- Testing `ParallelPipeline` configurations to ensure frames aren't duplicated
- Acting as a placeholder where a more complex filter might be added later
- Monitoring frame flow in pipelines by adding logging in subclasses
- Creating a base class for more complex conditional filters

## Usage Example

```python
from pipecat.processors.filters import IdentityFilter

# Create an identity filter
pass_through = IdentityFilter()

# Add to pipeline
pipeline = Pipeline([
    source,
    pass_through,  # All frames pass through unchanged
    destination
])
```

## Frame Flow

```mermaid
graph LR
    A[Input Frame] --> B[IdentityFilter] --> C[Output Frame]
```

## Notes

- Simplest possible filter implementation
- Passes all frames through without modification
- Useful in testing parallel pipelines
- Can serve as a placeholder or base class
- Zero overhead in normal operation



================================================
FILE: server/utilities/filters/null-filter.mdx
================================================
---
title: "NullFilter"
description: "Processor that blocks all frames except system frames"
---

## Overview

`NullFilter` is a filtering processor that blocks all frames from passing through, with the exception of system frames and end frames which are required for proper pipeline operation.

## Constructor Parameters

The `NullFilter` constructor accepts no specific parameters beyond those inherited from `FrameProcessor`.

## Functionality

When a frame passes through the processor:

- If the frame is a `SystemFrame` or `EndFrame`, it is passed through
- All other frame types are blocked and do not continue through the pipeline

This filter effectively acts as a barrier that allows only the essential system frames required for pipeline initialization, shutdown, and management.

## Use Cases

`NullFilter` is useful in several scenarios:

- Temporarily disabling parts of a pipeline without removing components
- Creating dead-end branches in parallel pipelines
- Testing pipeline behavior with blocked communication
- Implementing conditional pipelines where certain paths should be blocked

## Usage Example

```python
from pipecat.processors.filters import NullFilter

# Create a null filter that blocks all non-system frames
blocker = NullFilter()

# Add to pipeline
pipeline = Pipeline([
    source,
    blocker,  # Blocks all regular frames
    destination  # Will only receive system frames
])
```

## Frame Flow

```mermaid
graph TD
    A[Input Frames] --> B[NullFilter]
    B --> C{System/End Frame?}
    C -->|Yes| D[Output Frame]
    C -->|No| E[Blocked]
```

## Notes

- Blocks all regular frames in both directions
- Only allows system frames and end frames to pass through
- Useful for testing, debugging, and creating conditional pipelines
- Minimal overhead as it performs simple type checking
- Can be used to temporarily disable parts of a pipeline



================================================
FILE: server/utilities/filters/stt-mute.mdx
================================================
---
title: "STTMuteFilter"
description: "Processor for controlling STT muting and interruption handling during bot speech and function calls"
---

## Overview

`STTMuteFilter` is a general-purpose processor that combines STT muting and interruption control. When active, it prevents both transcription and interruptions during specified conditions (e.g., bot speech, function calls), providing a cleaner conversation flow.

The processor supports multiple simultaneous strategies for when to mute the STT service, making it flexible for different use cases.

<Tip>
  Want to try it out? Check out the [STTMuteFilter foundational
  demo](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/24-stt-mute-filter.py)
</Tip>

## Constructor Parameters

<ParamField path="config" type="STTMuteConfig" required>
  Configuration object that defines the muting strategies and optional custom
  logic
</ParamField>

<ParamField path="stt_service" type="Optional[STTService]" required>
  The STT service to control (deprecated, will be removed in a future version)
</ParamField>

## Configuration

The processor is configured using `STTMuteConfig`, which determines when and how the STT service should be muted:

<ParamField path="strategies" type="set[STTMuteStrategy]">
  Set of muting strategies to apply
</ParamField>

<ParamField
  path="should_mute_callback"
  type="Callable[[STTMuteFilter], Awaitable[bool]]"
  default="None"
>
  Optional callback for custom muting logic (required when strategy is `CUSTOM`)
</ParamField>

### Muting Strategies

`STTMuteConfig` accepts a set of these `STTMuteStrategy` values:

<ParamField path="FIRST_SPEECH" type="STTMuteStrategy">
  Mute only during the bot's first speech (typically during introduction)
</ParamField>

<ParamField path="MUTE_UNTIL_FIRST_BOT_COMPLETE" type="STTMuteStrategy">
  Start muted and remain muted until first bot speech completes. Useful when bot
  speaks first and you want to ensure its first response cannot be interrupted.
</ParamField>

<ParamField path="FUNCTION_CALL" type="STTMuteStrategy">
  Mute during LLM function calls (e.g., API requests, external service calls)
</ParamField>

<ParamField path="ALWAYS" type="STTMuteStrategy">
  Mute during all bot speech
</ParamField>

<ParamField path="CUSTOM" type="STTMuteStrategy">
  Use custom logic provided via callback to determine when to mute. The callback
  is invoked when the bot is speaking and can use application state to decide
  whether to mute. When the bot stops speaking, unmuting occurs automatically if
  no other strategy requires muting.
</ParamField>

<Note>
  `MUTE_UNTIL_FIRST_BOT_COMPLETE` and `FIRST_SPEECH` strategies should not be
  used together as they handle the first bot speech differently.
</Note>

## Input Frames

<ParamField path="BotStartedSpeakingFrame" type="Frame">
  Indicates bot has started speaking
</ParamField>

<ParamField path="BotStoppedSpeakingFrame" type="Frame">
  Indicates bot has stopped speaking
</ParamField>

<ParamField path="FunctionCallInProgressFrame" type="Frame">
  Indicates a function call has started
</ParamField>

<ParamField path="FunctionCallResultFrame" type="Frame">
  Indicates a function call has completed
</ParamField>

<ParamField path="InterimTranscriptionFrame" type="Frame">
  Indicates an interim transcription result (suppressed when muted)
</ParamField>

<ParamField path="StartInterruptionFrame" type="Frame">
  User interruption start event (suppressed when muted)
</ParamField>

<ParamField path="StopInterruptionFrame" type="Frame">
  User interruption stop event (suppressed when muted)
</ParamField>

<ParamField path="TranscriptionFrame" type="Frame">
  Indicates a transcription result (suppressed when muted)
</ParamField>

<ParamField path="UserStartedSpeakingFrame" type="Frame">
  Indicates user has started speaking (suppressed when muted)
</ParamField>

<ParamField path="UserStoppedSpeakingFrame" type="Frame">
  Indicates user has stopped speaking (suppressed when muted)
</ParamField>

## Output Frames

<ParamField path="STTMuteFrame" type="Frame">
  Control frame to mute/unmute the STT service
</ParamField>

All input frames are passed through except VAD-related frames (interruptions and user speaking events) when muted.

## Usage Examples

### Basic Usage (Mute During Bot's First Speech)

```python
stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
stt_mute_filter = STTMuteFilter(
    config=STTMuteConfig(strategies={
        STTMuteStrategy.FIRST_SPEECH
    })
)

pipeline = Pipeline([
    transport.input(),
    stt,
    stt_mute_filter,  # Between the STT service and context aggregator
    context_aggregator.user(),
    # ... rest of pipeline
])
```

### Mute Until First Bot Response Completes

```python
stt_mute_filter = STTMuteFilter(
    config=STTMuteConfig(strategies={STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE})
)
```

This ensures no user speech is processed until after the bot's first complete response.

### Always Mute During Bot Speech

```python
stt_mute_filter = STTMuteFilter(
    config=STTMuteConfig(strategies={STTMuteStrategy.ALWAYS})
)
```

### Custom Muting Logic

The `CUSTOM` strategy allows you to control muting based on application state when the bot is speaking. The callback will be invoked whenever the bot is speaking, and your logic decides whether to mute:

```python
# Create a state manager
class SessionState:
    def __init__(self):
        self.session_ending = False

session_state = SessionState()

# Callback function that determines whether to mute
async def session_state_mute_logic(stt_filter: STTMuteFilter) -> bool:
    # Return True to mute, False otherwise
    # This is called when the bot is speaking
    return session_state.session_ending

# Configure filter with CUSTOM strategy
stt_mute_filter = STTMuteFilter(
    config=STTMuteConfig(
        strategies={STTMuteStrategy.CUSTOM},
        should_mute_callback=session_state_mute_logic
    )
)

# Later, when you want to trigger muting (e.g., during session timeout):
async def handle_session_timeout():
    # Update state that will be checked by the callback
    session_state.session_ending = True

    # Send goodbye message
    goodbye_message = "Thank you for using our service. This session is now ending."
    await pipeline.push_frame(TTSSpeakFrame(text=goodbye_message))

    # The system will automatically mute during this message because:
    # 1. Bot starts speaking, triggering the callback
    # 2. Callback returns True (session_ending is True)
    # 3. When bot stops speaking, unmuting happens automatically
```

### Combining Multiple Strategies

```python
async def custom_mute_logic(processor: STTMuteFilter) -> bool:
    # Example: Mute during business hours only
    current_hour = datetime.now().hour
    return 9 <= current_hour < 17

stt_mute_filter = STTMuteFilter(
    config=STTMuteConfig(
        strategies={
            STTMuteStrategy.FUNCTION_CALL,       # Mute during function calls
            STTMuteStrategy.CUSTOM,              # And during business hours
            STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE  # And until first bot speech completes
        },
        should_mute_callback=custom_mute_logic
    )
)
```

### Frame Flow

```mermaid
graph TD
    A[Transport Input] --> B[STTMuteFilter]
    B --> C[STT Service]
    B -- "Suppressed when muted" --> D[VAD-related Frames]
    B -- "STTMuteFrame" --> C
```

## Notes

- Combines STT muting and interruption control into a single concept
- Muting prevents both transcription and interruptions
- Multiple strategies can be active simultaneously
- CUSTOM strategy callback is only invoked when the bot is speaking
- Unmuting happens automatically when bot speech ends (if no other strategy requires muting)
- Placed between the STT service and context aggregator in pipeline
- Maintains conversation flow during bot speech and function calls
- Efficient state tracking for minimal overhead



================================================
FILE: server/utilities/filters/wake-check-filter.mdx
================================================
---
title: "WakeCheckFilter"
description: "Processor that passes frames only after detecting wake phrases in transcriptions"
---

## Overview

`WakeCheckFilter` monitors `TranscriptionFrame`s for specified wake phrases and only allows frames to pass through after a wake phrase has been detected. It includes a keepalive timeout to maintain the awake state for a period after detection, allowing continuous conversation without requiring repeated wake phrases.

## Constructor Parameters

<ParamField path="wake_phrases" type="list[str]" required>
  List of wake phrases to detect in transcriptions
</ParamField>

<ParamField path="keepalive_timeout" type="float" default="3">
  Number of seconds to remain in the awake state after each transcription
</ParamField>

## Functionality

The filter maintains state for each participant and processes frames as follows:

1. `TranscriptionFrame` objects are checked for wake phrases
2. If a wake phrase is detected, the filter enters the "AWAKE" state
3. While in the "AWAKE" state, all transcription frames pass through
4. After no activity for the keepalive timeout period, the filter returns to "IDLE"
5. All non-transcription frames pass through normally

Wake phrases are detected using regular expressions that match whole words with flexible spacing, making detection resilient to minor transcription variations.

## States

<ParamField path="IDLE" type="WakeState">
  Default state - only non-transcription frames pass through
</ParamField>

<ParamField path="AWAKE" type="WakeState">
  Active state after wake phrase detection - all frames pass through
</ParamField>

## Output Frames

- All non-transcription frames pass through unchanged
- After wake phrase detection, transcription frames pass through
- When awake, transcription frames reset the keepalive timer

## Usage Example

```python
from pipecat.processors.filters import WakeCheckFilter

# Create filter with wake phrases
wake_filter = WakeCheckFilter(
    wake_phrases=["hey assistant", "ok computer", "listen up"],
    keepalive_timeout=5.0  # Stay awake for 5 seconds after each transcription
)

# Add to pipeline
pipeline = Pipeline([
    transport.input(),
    stt_service,
    wake_filter,  # Only passes transcriptions after wake phrases
    llm_service,
    tts_service,
    transport.output()
])
```

## Frame Flow

```mermaid
graph TD
    A[Input Frames] --> B[WakeCheckFilter]
    B --> C{Transcription Frame?}
    C -->|No| F[Output Frame]
    C -->|Yes| D{Wake State}
    D -->|AWAKE| E{Keepalive Expired?}
    E -->|No| F
    E -->|Yes| G[Return to IDLE]
    D -->|IDLE| H{Contains Wake Phrase?}
    H -->|Yes| I[Set AWAKE] --> F
    H -->|No| J[Filtered Out]
```

## Notes

- Maintains separate state for each participant ID
- Uses regex pattern matching for resilient wake phrase detection
- Accumulates transcription text to detect phrases across multiple frames
- Trims accumulated text when wake phrase is detected
- Supports multiple wake phrases
- Passes all non-transcription frames through unchanged
- Error handling produces ErrorFrames for robust operation
- Case-insensitive matching for natural language use



================================================
FILE: server/utilities/filters/wake-notifier-filter.mdx
================================================
---
title: "WakeNotifierFilter"
description: "Processor that triggers a notifier when specified frame types pass a custom filter"
---

## Overview

`WakeNotifierFilter` monitors the pipeline for specific frame types and triggers a notification when those frames pass a custom filter condition. It passes all frames through unchanged while performing this notification side-effect.

## Constructor Parameters

<ParamField path="notifier" type="BaseNotifier" required>
  The notifier object to trigger when conditions are met
</ParamField>

<ParamField path="types" type="Tuple[Type[Frame]]" required>
  Tuple of frame types to monitor
</ParamField>

<ParamField path="filter" type="Callable[[Frame], Awaitable[bool]]" required>
  Async function that examines each matching frame and returns True to trigger
  notification
</ParamField>

## Functionality

The processor operates as follows:

1. Checks if the incoming frame matches any of the specified types
2. If it's a matching type, calls the filter function with the frame
3. If the filter returns True, triggers the notifier
4. Passes all frames through unchanged, regardless of the filtering result

This allows for notification side-effects without modifying the pipeline's data flow.

## Output Frames

- All frames pass through unchanged in their original direction
- No frames are modified or filtered out

## Usage Example

```python
from pipecat.frames.frames import TranscriptionFrame, UserStartedSpeakingFrame
from pipecat.processors.filters import WakeNotifierFilter
from pipecat.sync.event_notifier import EventNotifier

# Create an event notifier
wake_event = EventNotifier()

# Create filter that notifies when certain wake phrases are detected
async def wake_phrase_filter(frame):
    if isinstance(frame, TranscriptionFrame):
        return "hey assistant" in frame.text.lower()
    return False

# Add to pipeline
wake_notifier = WakeNotifierFilter(
    notifier=wake_event,
    types=(TranscriptionFrame, UserStartedSpeakingFrame),
    filter=wake_phrase_filter
)

# In another component, wait for the notification
async def handle_wake_event():
    await wake_event.wait()
    print("Wake phrase detected!")
```

## Frame Flow

```mermaid
graph TD
    A[Input Frame] --> B[WakeNotifierFilter]
    B --> C{Matching Type?}
    C -->|Yes| D{Filter Function}
    D -->|Returns True| E[Notify]
    D -->|Returns False| F[Pass Through]
    C -->|No| F
    E --> F
```

## Notes

- Acts as a transparent pass-through for all frames
- Can trigger external events without modifying pipeline flow
- Useful for signaling between pipeline components
- Can monitor for multiple frame types simultaneously
- Uses async filter function for complex conditions
- Functions as a "listener" that doesn't affect the data stream
- Can be used for logging, analytics, or coordinating external systems



================================================
FILE: server/utilities/frame/producer-consumer.mdx
================================================
---
title: "Producer & Consumer Processors"
description: "Route frames between different parts of a pipeline, allowing selective frame sharing across parallel branches or within complex pipelines"
---

## Overview

The Producer and Consumer processors work as a pair to route frames between different parts of a pipeline, particularly useful when working with [`ParallelPipeline`](/server/pipeline/parallel-pipeline). They allow you to selectively capture frames from one pipeline branch and inject them into another.

## ProducerProcessor

`ProducerProcessor` examines frames flowing through the pipeline, applies a filter to decide which frames to share, and optionally transforms these frames before sending them to connected consumers.

### Constructor Parameters

<ParamField path="filter" type="Callable[[Frame], Awaitable[bool]]" required>
  An async function that determines which frames should be sent to consumers.
  Should return `True` for frames to be shared.
</ParamField>

<ParamField
  path="transformer"
  type="Callable[[Frame], Awaitable[Frame]]"
  default="identity_transformer"
>
  Optional async function that transforms frames before sending to consumers. By
  default, passes frames unchanged.
</ParamField>

<ParamField path="passthrough" type="bool" default="True">
  When `True`, passes all frames through the normal pipeline flow. When `False`,
  only passes through frames that don't match the filter.
</ParamField>

## ConsumerProcessor

`ConsumerProcessor` receives frames from a `ProducerProcessor` and injects them into its pipeline branch.

### Constructor Parameters

<ParamField path="producer" type="ProducerProcessor" required>
  The producer processor that will send frames to this consumer.
</ParamField>

<ParamField
  path="transformer"
  type="Callable[[Frame], Awaitable[Frame]]"
  default="identity_transformer"
>
  Optional async function that transforms frames before injecting them into the
  pipeline.
</ParamField>

<ParamField
  path="direction"
  type="FrameDirection"
  default="FrameDirection.DOWNSTREAM"
>
  The direction in which to push received frames. Usually `DOWNSTREAM` to send
  frames forward in the pipeline.
</ParamField>

## Usage Examples

### Basic Usage: Moving TTS Audio Between Branches

```python
# Create a producer that captures TTS audio frames
async def is_tts_audio(frame: Frame) -> bool:
    return isinstance(frame, TTSAudioRawFrame)

# Define an async transformer function
async def tts_to_input_audio_transformer(frame: Frame) -> Frame:
    if isinstance(frame, TTSAudioRawFrame):
        # Convert TTS audio to input audio format
        return InputAudioRawFrame(
            audio=frame.audio,
            sample_rate=frame.sample_rate,
            num_channels=frame.num_channels
        )
    return frame

producer = ProducerProcessor(
    filter=is_tts_audio,
    transformer=tts_to_input_audio_transformer
    passthrough=True  # Keep these frames in original pipeline
)

# Create a consumer to receive the frames
consumer = ConsumerProcessor(
    producer=producer,
    direction=FrameDirection.DOWNSTREAM
)

# Use in a ParallelPipeline
pipeline = Pipeline([
    transport.input(),
    ParallelPipeline(
        # Branch 1: LLM for bot responses
        [
            llm,
            tts,
            producer,  # Capture TTS audio here
        ],
        # Branch 2: Audio processing branch
        [
            consumer,  # Receive TTS audio here
            llm, # Speech-to-Speech LLM (audio in)
        ]
    ),
    transport.output(),
])
```



================================================
FILE: server/utilities/mcp/mcp.mdx
================================================
---
title: "MCPClient"
description: "Service to connect to MCP (Model Context Protocol) servers"
---

## Overview

MCP is an open standard for enabling AI agents to interact with external data and tools. `MCPClient` provides a way to access and call tools via MCP. For example, instead of writing bespoke function call implementations for an external API, you may use an MCP server that provides a bridge to the API. _Be aware there may be security implications._ See [MCP documenation](https://github.com/modelcontextprotocol) for more details.

## Installation

To use `MCPClient`, install the required dependencies:

```bash
pip install "pipecat-ai[mcp]"
```

You may also need to set environment variables as required by the specific MCP server to which you are connecting.

## Configuration

### Constructor Parameters

You can connect to your MCP server via Stdio or SSE transport. See [here](https://modelcontextprotocol.io/docs/concepts/transports#built-in-transport-types) for more documentation on MCP transports.

<ParamField path="server_params" type="str | StdioServerParameters" required>

You can provide either:

- URL: "https://your.mcp.server/sse"
- StdioServerParameters, which are defined as:

```python
  StdioServerParameters(
        command="python",  # Executable
        args=["example_server.py"],  # Optional command line arguments
        env=None,  # Optional environment variables
    )
```

</ParamField>

### Input Parameters

See more information regarding server params [here](https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#writing-mcp-clients).

## Usage Example

### MCP Stdio Transport Implementation

```python

# Import MCPClient and StdioServerParameters
...
from mcp import StdioServerParameters
from pipecat.services.mcp_service import MCPClient
...

# Initialize an LLM
llm = ...

# Initialize and configure MCPClient with server parameters
mcp = MCPClient(
        server_params=StdioServerParameters(
            command=shutil.which("npx"),
            args=["-y", "@name/mcp-server-name@latest"],
            env={"ENV_API_KEY": "<env_api_key>"},
        )
    )

# Create tools schema from the MCP server and register them with llm
tools = await mcp.register_tools(llm)

# Create context with system message and tools
# Tip: Let the LLM know it has access to tools from an MCP server by including it in the system prompt.
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant in a voice conversation. You have access to MCP tools. Keep responses concise."
        }
    ],
    tools=tools
)
```

### MCP SSE Transport Implementation

```python

# Import MCPClient
...
from pipecat.services.mcp_service import MCPClient
...

# Initialize an LLM
llm = ...

# Initialize and configure MCPClient with MCP SSE server url
mcp = MCPClient(server_params="https://your.mcp.server/sse")

# Create tools schema from the MCP server and register them with llm
tools = await mcp.register_tools(llm)

# Create context with system message and tools
# Tip: Let the LLM know it has access to tools from an MCP server by including it in the system prompt.
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant in a voice conversation. You have access to MCP tools. Keep responses concise."
        }
    ],
    tools=tools
)
```

## Methods

<ResponseField name="register_tools" type="async method">
  Converts MCP tools to Pipecat-friendly function definitions and registers the
  functions with the llm.
</ResponseField>

```python
async def register_tools(self, llm) -> ToolsSchema:
```

## Additional documentation

<Note>
  See [MCP's docs](https://github.com/modelcontextprotocol/python-sdk) for MCP
  related updates.
</Note>



================================================
FILE: server/utilities/observers/debug-observer.mdx
================================================
---
title: "Debug Log Observer"
sidebarTitle: "Debug Observer"
description: "Comprehensive frame logging with configurable filtering in Pipecat"
---

The `DebugLogObserver` provides detailed logging of frame activity in your Pipecat pipeline, with full visibility into frame content and flexible filtering options.

## Features

- Log all frame types and their content
- Filter by specific frame types
- Filter by source or destination components
- Automatic formatting of frame fields
- Special handling for complex data structures

## Usage

### Log All Frames

Log all frames passing through the pipeline:

```python
from pipecat.observers.loggers.debug_log_observer import DebugLogObserver

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[DebugLogObserver()],
    ),
)
```

### Filter by Frame Types

Log only specific frame types:

```python
from pipecat.frames.frames import TranscriptionFrame, InterimTranscriptionFrame
from pipecat.observers.loggers.debug_log_observer import DebugLogObserver

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[
            DebugLogObserver(frame_types=(
                TranscriptionFrame,
                InterimTranscriptionFrame
            ))
        ],
    ),
)
```

### Advanced Source/Destination Filtering

Filter frames based on their type and source/destination:

```python
from pipecat.frames.frames import StartInterruptionFrame, UserStartedSpeakingFrame, LLMTextFrame
from pipecat.observers.loggers.debug_log_observer import DebugLogObserver, FrameEndpoint
from pipecat.transports.base_output_transport import BaseOutputTransport
from pipecat.services.stt_service import STTService

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[
            DebugLogObserver(frame_types={
                # Only log StartInterruptionFrame when source is BaseOutputTransport
                StartInterruptionFrame: (BaseOutputTransport, FrameEndpoint.SOURCE),

                # Only log UserStartedSpeakingFrame when destination is STTService
                UserStartedSpeakingFrame: (STTService, FrameEndpoint.DESTINATION),

                # Log LLMTextFrame regardless of source or destination
                LLMTextFrame: None
            })
        ],
    ),
)
```

## Log Output Format

The observer logs each frame with its complete details:

```
[Source] → [Destination]: [FrameType] [field1: value1, field2: value2, ...] at [timestamp]s
```

For example:

```
OpenAILLMService#0 → DailyTransport#0: LLMTextFrame text: 'Hello, how can I help you today?' at 1.24s
```

## Configuration Options

| Parameter        | Type                                                                                   | Description                                                     |
| ---------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| `frame_types`    | `Tuple[Type[Frame], ...]` or `Dict[Type[Frame], Optional[Tuple[Type, FrameEndpoint]]]` | Frame types to log, with optional source/destination filtering  |
| `exclude_fields` | `Set[str]`                                                                             | Field names to exclude from logging (defaults to binary fields) |

## FrameEndpoint Enum

The `FrameEndpoint` enum is used for source/destination filtering:

- `FrameEndpoint.SOURCE`: Filter by source component
- `FrameEndpoint.DESTINATION`: Filter by destination component



================================================
FILE: server/utilities/observers/llm-observer.mdx
================================================
---
title: "LLM Log Observer"
sidebarTitle: "LLM Observer"
description: "Logging LLM activity in Pipecat"
---

The `LLMLogObserver` provides detailed logging of Large Language Model (LLM) activity within your Pipecat pipeline. It tracks the entire lifecycle of LLM interactions, from initial prompts to final responses.

## Frame Types Monitored

The observer tracks the following frame types (only from/to LLM service):

- **LLMFullResponseStartFrame**: When the LLM begins generating a response
- **LLMFullResponseEndFrame**: When the LLM completes its response
- **LLMTextFrame**: Individual text chunks generated by the LLM
- **FunctionCallInProgressFrame**: Function/tool calls made by the LLM
- **LLMMessagesFrame**: Input messages sent to the LLM
- **OpenAILLMContextFrame**: Context information for OpenAI LLM calls
- **FunctionCallResultFrame**: Results returned from function calls

## Usage

```python
from pipecat.observers.loggers.llm_log_observer import LLMLogObserver

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[LLMLogObserver()],
    ),
)
```

## Log Output Format

The observer uses emojis and consistent formatting for easy log reading:

- 🧠 [Source] → LLM START/END RESPONSE
- 🧠 [Source] → LLM GENERATING: [text]
- 🧠 [Source] → LLM FUNCTION CALL: [details]
- 🧠 → [Destination] LLM MESSAGES FRAME: [messages]
- 🧠 → [Destination] LLM CONTEXT FRAME: [context]

All log entries include timestamps for precise timing analysis.



================================================
FILE: server/utilities/observers/observer-pattern.mdx
================================================
---
title: "Observer Pattern"
description: "Understanding and implementing observers in Pipecat"
---

The Observer pattern in Pipecat allows non-intrusive monitoring of frames as they flow through the pipeline. Observers can watch frame traffic without affecting the pipeline's core functionality.

## Base Observer

All observers must inherit from `BaseObserver` and implement the `on_push_frame` method:

```python
from pipecat.observers.base_observer import BaseObserver

class CustomObserver(BaseObserver):
    async def on_push_frame(
        self,
        src: FrameProcessor,
        dst: FrameProcessor,
        frame: Frame,
        direction: FrameDirection,
        timestamp: int,
    ):
        # Your frame observation logic here
        pass
```

## Available Observers

Pipecat provides several built-in observers:

- **LLMLogObserver**: Logs LLM activity and responses
- **TranscriptionLogObserver**: Logs speech-to-text transcription events
- **RTVIObserver**: Converts internal frames to RTVI protocol messages for server to client messaging

## Using Multiple Observers

You can attach multiple observers to a pipeline task. Each observer will be notified of all frames:

```python
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[LLMLogObserver(), TranscriptionLogObserver(), CustomObserver()],
    ),
)
```

## Example: Debug Observer

Here's an example observer that logs interruptions and bot speaking events:

```python
class DebugObserver(BaseObserver):
    """Observer to log interruptions and bot speaking events to the console.

    Logs all frame instances of:
    - StartInterruptionFrame
    - BotStartedSpeakingFrame
    - BotStoppedSpeakingFrame

    This allows you to see the frame flow from processor to processor through the pipeline for these frames.
    Log format: [EVENT TYPE]: [source processor] → [destination processor] at [timestamp]s
    """

    async def on_push_frame(
        self,
        src: FrameProcessor,
        dst: FrameProcessor,
        frame: Frame,
        direction: FrameDirection,
        timestamp: int,
    ):
        time_sec = timestamp / 1_000_000_000
        arrow = "→" if direction == FrameDirection.DOWNSTREAM else "←"

        if isinstance(frame, StartInterruptionFrame):
            logger.info(f"⚡ INTERRUPTION START: {src} {arrow} {dst} at {time_sec:.2f}s")
        elif isinstance(frame, BotStartedSpeakingFrame):
            logger.info(f"🤖 BOT START SPEAKING: {src} {arrow} {dst} at {time_sec:.2f}s")
        elif isinstance(frame, BotStoppedSpeakingFrame):
            logger.info(f"🤖 BOT STOP SPEAKING: {src} {arrow} {dst} at {time_sec:.2f}s")
```

## Common Use Cases

Observers are particularly useful for:

- Debugging frame flow
- Logging specific events
- Monitoring pipeline behavior
- Collecting metrics
- Converting internal frames to external messages



================================================
FILE: server/utilities/observers/transcription-observer.mdx
================================================
---
title: "Transcription Log Observer"
sidebarTitle: "Transcription Observer"
description: "Logging speech-to-text transcription activity in Pipecat"
---

The `TranscriptionLogObserver` logs all speech-to-text transcription activity in your Pipecat pipeline, providing visibility into both final and interim transcription results.

## Frame Types Monitored

The observer tracks the following frame types (only from STT service):

- **TranscriptionFrame**: Final transcription results
- **InterimTranscriptionFrame**: In-progress transcription results

## Usage

```python
from pipecat.observers.loggers.transcription_log_observer import TranscriptionLogObserver

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        observers=[TranscriptionLogObserver()],
    ),
)
```

## Log Output Format

The observer uses consistent formatting with emoji indicators:

- 💬 [Source] → TRANSCRIPTION: [text] from [user_id]
- 💬 [Source] → INTERIM TRANSCRIPTION: [text] from [user_id]

All log entries include timestamps for precise timing analysis.



================================================
FILE: server/utilities/observers/turn-tracking-observer.mdx
================================================
---
title: "Turn Tracking Observer"
description: "Track conversation turns and events in your Pipecat pipeline"
---

The `TurnTrackingObserver` monitors and tracks conversational turns in your Pipecat pipeline, providing events when turns start and end. It intelligently identifies when a user-bot interaction cycle begins and completes.

## Turn Lifecycle

A turn represents a complete user-bot interaction cycle:

1. **Start**: When the user starts speaking (or pipeline starts for first turn)
2. **Processing**: User speaks, bot processes and responds
3. **End**: After the bot finishes speaking and either:
   - The user starts speaking again
   - A timeout period elapses with no further activity

## Events

The observer emits two main events:

- **`on_turn_started`**: When a new turn begins
  - Parameters: `turn_number` (int)
- **`on_turn_ended`**: When a turn completes
  - Parameters: `turn_number` (int), `duration` (float, in seconds), `was_interrupted` (bool)

## Usage

The observer is automatically created when you initialize a `PipelineTask` with `enable_turn_tracking=True` (which is the default):

```python
task = PipelineTask(
    pipeline,
    params=PipelineParams(allow_interruptions=True),
    # Turn tracking is enabled by default
)

# Access the observer
turn_observer = task.turn_tracking_observer

# Register event handlers
@turn_observer.event_handler("on_turn_started")
async def on_turn_started(observer, turn_number):
    logger.info(f"Turn {turn_number} started")

@turn_observer.event_handler("on_turn_ended")
async def on_turn_ended(observer, turn_number, duration, was_interrupted):
    status = "interrupted" if was_interrupted else "completed"
    logger.info(f"Turn {turn_number} {status} in {duration:.2f}s")
```

## Configuration

You can configure the observer's behavior when creating a `PipelineTask`:

```python
from pipecat.observers.turn_tracking_observer import TurnTrackingObserver

# Create a custom observer instance
custom_turn_tracker = TurnTrackingObserver(
    turn_end_timeout_secs=3.5,     # Turn end timeout (default: 2.5)
)

# Add it as a regular observer
task = PipelineTask(
    pipeline,
    observers=[custom_turn_tracker],
    # Disable the default one if adding your own
    enable_turn_tracking=False,
)
```

## Interruptions

The observer automatically detects interruptions when the user starts speaking while the bot is still speaking. In this case:

- The current turn is marked as interrupted (`was_interrupted=True`)
- A new turn begins immediately

## How It Works

The observer monitors specific frame types to track conversation flow:

- **StartFrame**: Initiates the first turn
- **UserStartedSpeakingFrame**: Starts user speech or triggers a new turn
- **BotStartedSpeakingFrame**: Marks bot speech beginning
- **BotStoppedSpeakingFrame**: Starts the turn end timeout

After a bot stops speaking, the observer waits for the configured timeout period. If no further bot speech occurs, the turn ends; otherwise, it continues as part of the same turn.

## Use Cases

- **Analytics**: Measure turn durations, interruption rates, and conversation flow
- **Logging**: Record turn-based logs for diagnostics and analysis
- **Visualization**: Show turn-based conversation timelines in UIs
- **Tracing**: Group spans and metrics by conversation turns



================================================
FILE: server/utilities/runner/guide.mdx
================================================
---
title: "Development Runner"
description: "Unified runner for building voice AI bots with Daily, WebRTC, and telephony transports"
---

## Overview

The Pipecat development runner provides a unified way to run voice AI bots across multiple transport types. It handles infrastructure setup - creating Daily rooms, managing WebRTC connections, and routing telephony calls.

## Installation

```bash
pip install pipecat-ai[runner]
```

## What is a Runner?

A runner in Pipecat refers to a "bot runner", an HTTP service that provides a gateway for spawning bots on-demand. It's the component that enables your bot to run by providing it with server infrastructure and connection details like rooms and tokens.

A bot runner typically creates transport sessions (like Daily WebRTC rooms), generates authentication tokens for both bots and users, spawns new bot processes when users request sessions, and manages bot lifecycle and cleanup. Think of it as the bridge between incoming user connections and your bot logic.

## How the Development Runner Works

The development runner operates as a FastAPI web server that automatically discovers and executes your bot code. When you start the runner, it creates the necessary web endpoints and infrastructure for your chosen transport type.

- **WebRTC connections**: It serves a built-in web interface where users can connect directly as well as an endpoint to create new WebRTC sessions
- **Daily integration**: It provides endpoints that create new rooms and tokens and redirect users to join them
- **Telephony providers**: For Twilio, it sets up webhook endpoints that handle incoming calls and establish WebSocket connections for audio streaming

The runner automatically detects which transport type you're using and configures the appropriate infrastructure. It then discovers your bot function and spawns new instances whenever users connect. This means you can focus on writing your bot logic while the runner handles all the server infrastructure, connection management, and transport-specific details.

Your bot code receives runner arguments that contain everything it needs, including Daily room URLs and tokens, WebRTC connections, or WebSocket streams for telephony. The runner abstracts away the complexity of managing these different connection types, providing a unified interface for building bots that work across multiple platforms.

## Pipecat Cloud Ready

The bot runner is designed to be cloud-ready, meaning that you can run the same bot code locally and deployed to Pipecat Cloud without any modifications. It automatically handles the differences in transport setup, providing you with the flexibility to test locally using a free transport, like SmallWebRTCTransport, but run in production using Daily or telephony transports.

## Building with the Runner

Now let's build a practical example to see how this works. The key insight is that your bot code is structured into two parts: the core bot logic that works with any transport, and the entry point that creates the appropriate transport based on the runner arguments.

Here's the basic structure:

```python
# Your imports
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.runner.types import RunnerArguments
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport

async def run_bot(transport: BaseTransport):
    """Your core bot logic here:
    - Define services (STT, TTS, LLM)
    - Initialize messages and context
    - Create the pipeline
    - Add event handlers
    - Run the pipeline
    """

    # Your bot logic goes here
    # Define STT, LLM, TTS...
    # ...
    # Run your pipeline
    await runner.run(task)

async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible for local dev and Pipecat Cloud."""
    transport = SmallWebRTCTransport(
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
        webrtc_connection=runner_args.webrtc_connection,
    )

    await run_bot(transport)

if __name__ == "__main__":
    from pipecat.runner.run import main
    main()
```

The `run_bot()` function contains your actual bot logic and is transport-agnostic. The `bot()` function is the entry point that the runner calls - it creates the appropriate transport and passes it to your bot logic. This separation allows the same bot code to work across different transports.

When you run this with `python bot.py`, the development runner starts a web server and opens a browser interface at `http://localhost:7860/client`. Each time someone connects, the runner calls your `bot()` function with WebRTC runner arguments.

## Supporting Multiple Transports

To make your bot work across different platforms, you can detect the transport type from the runner arguments and create the appropriate transport. Here's how to support both Daily and WebRTC:

```python
from pipecat.runner.types import DailyRunnerArguments, RunnerArguments, SmallWebRTCRunnerArguments

async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport = None

    if isinstance(runner_args, DailyRunnerArguments):
        from pipecat.transports.services.daily import DailyParams, DailyTransport

        transport = DailyTransport(
            runner_args.room_url,
            runner_args.token,
            "Pipecat Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

    elif isinstance(runner_args, SmallWebRTCRunnerArguments):
        from pipecat.transports.base_transport import TransportParams
        from pipecat.transports.network.small_webrtc import SmallWebRTCTransport

        transport = SmallWebRTCTransport(
            params=TransportParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
            webrtc_connection=runner_args.webrtc_connection,
        )

    else:
        logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
        return

    if transport is None:
        logger.error("Failed to create transport")
        return

    await run_bot(transport)

if __name__ == "__main__":
    from pipecat.runner.run import main
    main()
```

Now you can run your bot with different transports:

```bash
python bot.py -t webrtc  # Uses SmallWebRTCRunnerArguments
python bot.py -t daily   # Uses DailyRunnerArguments
```

### Understanding Runner Arguments

Runner arguments are how the runner communicates transport-specific information to your bot. The runner determines which transport to use based on the command-line arguments, then creates the appropriate runner arguments:

- **`DailyRunnerArguments`**: Contains `room_url` and `token` for joining Daily rooms
- **`SmallWebRTCRunnerArguments`**: Contains `webrtc_connection` for local WebRTC sessions
- **`WebSocketRunnerArguments`**: Contains `websocket` for telephony connections

The runner handles all the complex setup - creating Daily rooms, generating tokens, establishing WebSocket connections - and provides your bot with everything it needs through these runner arguments.

Notice how we use lazy imports (`from pipecat.transports.services.daily import ...`) inside the conditional blocks. This ensures that transport-specific dependencies are only required when that transport is actually used, making your bot more portable.

<Info>
  `RunnerArguments` is the base class for all runner arguments. It provides a
  common interface for the runner to pass transport-specific information to your
  bot.
</Info>

## Environment Detection

When building bots that work both locally and in production, you often need to detect the execution environment to enable different features. The development runner sets the `ENV` environment variable to help with this:

```python
import os

async def bot(runner_args: RunnerArguments):
    # Check if running in local development environment
    is_local = os.environ.get("ENV") == "local"

    # Enable production features only when deployed
    if not is_local:
        from pipecat.audio.filters.krisp_filter import KrispFilter
        krisp_filter = KrispFilter()
    else:
        krisp_filter = None

    transport = DailyTransport(
        runner_args.room_url,
        runner_args.token,
        "Pipecat Bot",
        params=DailyParams(
            audio_in_filter=krisp_filter,  # Krisp filter only in production
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )
```

### Environment Values

The development runner automatically sets environment variables based on how your bot is running:

- **Local development**: `ENV=local` (set by the development runner)
- **Production/Cloud deployment**: `ENV` is not set or has a different value

This allows you to easily customize behavior between development and production environments:

## All Supported Transports

The development runner supports five transport types, each designed for different use cases:

### WebRTC (`-t webrtc`)

Local WebRTC connections with a built-in browser interface. Perfect for development and testing.

```bash
python bot.py -t webrtc
# Opens http://localhost:7860/client

# ESP32 compatibility mode
python bot.py -t webrtc --esp32 --host 192.168.1.100
```

**Runner Arguments**: `SmallWebRTCRunnerArguments`

- `webrtc_connection`: Pre-configured WebRTC peer connection

### Daily (`-t daily`)

Integration with Daily for production video conferencing with rooms and participant management.

```bash
python bot.py -t daily
# Opens http://localhost:7860 with room creation interface

# Direct connection for testing (bypasses web server)
python bot.py -d
```

**Runner Arguments**: `DailyRunnerArguments`

- `room_url`: Daily room URL to join
- `token`: Authentication token for the room
- `body`: Additional request data

### Telephony (`-t twilio|telnyx|plivo`)

Phone call integration through telephony providers. Requires a public webhook endpoint.

```bash
python bot.py -t twilio -x yourproxy.ngrok.io
python bot.py -t telnyx -x yourproxy.ngrok.io
python bot.py -t plivo -x yourproxy.ngrok.io
```

**Runner Arguments**: `WebSocketRunnerArguments`

- `websocket`: WebSocket connection for audio streaming

The runner automatically detects the telephony provider (Twilio, Telnyx, or Plivo) from the WebSocket messages and configures the appropriate serializers and audio settings.

## Command Line Options

The development runner accepts several command-line arguments to customize its behavior:

```bash
python bot.py [OPTIONS]

Options:
  --host TEXT          Server host address (default: localhost)
  --port INTEGER       Server port (default: 7860)
  -t, --transport      Transport type: daily, webrtc, twilio, telnyx, plivo (default: webrtc)
  -x, --proxy TEXT     Public proxy hostname for telephony webhooks (required for telephony)
  --esp32              Enable SDP munging for ESP32 WebRTC compatibility
  -d, --direct         Connect directly to Daily room for testing (automatically sets transport to daily)
  -v, --verbose        Increase logging verbosity
```

### Key Arguments

**`--transport` / `-t`**: Determines which transport infrastructure to set up

- `webrtc`: Local WebRTC with browser interface
- `daily`: Daily.co integration with room management
- `twilio`, `telnyx`, `plivo`: Telephony provider integration

**`--proxy` / `-x`**: Required for telephony transports. This should be a publicly accessible hostname (like `yourbot.ngrok.io`) that can receive webhooks from telephony providers.

**`--direct` / `-d`**: Special mode for Daily that bypasses the web server and connects your bot directly to a Daily room. Useful for quick testing but not recommended for production use.

**`--esp32`**: Enables SDP (Session Description Protocol) modifications needed for ESP32 WebRTC compatibility. Must be used with a specific IP address via `--host`.

### Environment Variables

Different transports require different environment variables:

**Daily**:

- `DAILY_API_KEY`: Daily API key for creating rooms and tokens
- `DAILY_SAMPLE_ROOM_URL` (optional): Existing room URL to use

**Telephony**:

- `TWILIO_ACCOUNT_SID`, `TWILIO_AUTH_TOKEN`: Twilio credentials
- `PLIVO_AUTH_ID`, `PLIVO_AUTH_TOKEN`: Plivo credentials
- `TELNYX_API_KEY`: Telnyx API key

The runner automatically uses these environment variables when creating transport sessions and authentication tokens.

## Simplifying with the Transport Utility

While the manual approach gives you full control, the `create_transport` utility provides a much cleaner way to handle multiple transports. Instead of writing conditional logic for each transport type, you define transport configurations upfront and let the utility handle the selection:

```python
from pipecat.runner.utils import create_transport

# Define transport configurations using factory functions
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        # add_wav_header and serializer handled automatically
    ),
}

async def bot(runner_args):
    """Simplified bot entry point using the transport utility."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)
```

Now your bot supports three transport types with just two lines of code in the `bot()` function.

## Quick Reference

| Transport    | Command                                             | Access                       |
| ------------ | --------------------------------------------------- | ---------------------------- |
| WebRTC       | `python bot.py`                                     | http://localhost:7860/client |
| Daily        | `python bot.py -t daily`                            | http://localhost:7860        |
| Daily Direct | `python bot.py -d`                                  | Direct connection            |
| Twilio       | `python bot.py -t twilio -x proxy.ngrok.io`         | Phone calls                  |
| Telnyx       | `python bot.py -t telnyx -x proxy.ngrok.io`         | Phone calls                  |
| Plivo        | `python bot.py -t plivo -x proxy.ngrok.io`          | Phone calls                  |
| ESP32 WebRTC | `python bot.py -t webrtc --esp32 --host <ESP32_IP>` | ESP32 WebRTC connection      |

## Examples

For practical examples of using the development runner with different transports, check out the following:

<Card
  title="Runner Examples"
  icon="code"
  href="https://github.com/pipecat-ai/pipecat-examples/tree/main/runner-examples"
>
  Explore the examples for different ways to use the development runner with
  various transports.
</Card>



================================================
FILE: server/utilities/runner/transport-utils.mdx
================================================
---
title: "Transport Utilities"
description: "Configuration and helper utilities for Daily, LiveKit, telephony, and WebRTC transports"
---

## Overview

Pipecat provides several utility modules for configuring transports and handling transport-specific operations. While the [development runner](/server/utilities/runner/guide) handles most of these automatically, these utilities are useful for custom setups, advanced configurations, or when building your own deployment infrastructure.

## Daily Configuration

The Daily utilities handle room creation, token generation, and authentication setup for Daily.co integration.

### Basic Configuration

Use `configure()` for simple room and token setup:

```python
import aiohttp
from pipecat.runner.daily import configure

async with aiohttp.ClientSession() as session:
    room_url, token = await configure(session)

    # Use with DailyTransport
    transport = DailyTransport(room_url, token, "Bot Name", params=DailyParams())
```

This function:

- Uses `DAILY_SAMPLE_ROOM_URL` environment variable if set, otherwise creates a new room
- Generates an authentication token using `DAILY_API_KEY`
- Returns a room URL and token ready for use

### Configuration with Arguments

For command-line integration, use `configure_with_args()`:

```python
import argparse
from pipecat.runner.daily import configure_with_args

parser = argparse.ArgumentParser()
# Your other arguments...

async with aiohttp.ClientSession() as session:
    room_url, token, args = await configure_with_args(session, parser)
```

This supports additional command-line options:

- `-u, --url`: Specify a Daily room URL directly
- `-k, --apikey`: Override the Daily API key

### Environment Variables

**Required**:

- `DAILY_API_KEY`: Daily API key for creating rooms and tokens

**Optional**:

- `DAILY_SAMPLE_ROOM_URL`: Use an existing room instead of creating one
- `DAILY_API_URL`: Override Daily API endpoint (defaults to https://api.daily.co/v1)

### Token Management

Tokens are generated with a 2-hour expiration time and include necessary permissions for bot participation. The utilities handle all the Daily REST API interactions automatically.

## LiveKit Configuration

LiveKit utilities manage authentication tokens, room setup, and agent permissions for LiveKit server integration.

### Basic Configuration

Use `configure()` for standard setup:

```python
from pipecat.runner.livekit import configure

url, token, room_name = await configure()

# Use with LiveKitTransport
transport = LiveKitTransport(url=url, token=token, room_name=room_name, params=LiveKitParams())
```

### Configuration with Arguments

For command-line integration:

```python
import argparse
from pipecat.runner.livekit import configure_with_args

parser = argparse.ArgumentParser()
url, token, room_name, args = await configure_with_args(parser)
```

Supports these command-line options:

- `-r, --room`: Specify LiveKit room name
- `-u, --url`: Specify LiveKit server URL

### Token Generation

LiveKit provides two token generation functions:

**`generate_token(room_name, participant_name, api_key, api_secret)`**
Creates a standard participant token for users or testing.

**`generate_token_with_agent(room_name, participant_name, api_key, api_secret)`**
Creates an agent token with special permissions. Use this for your bots.

```python
from pipecat.runner.livekit import generate_token_with_agent

# Generate agent token for your bot
agent_token = generate_token_with_agent("my-room", "Pipecat Bot", api_key, api_secret)

# Generate user token for testing
user_token = generate_token("my-room", "Test User", api_key, api_secret)
```

### Environment Variables

**Required**:

- `LIVEKIT_API_KEY`: LiveKit API key
- `LIVEKIT_API_SECRET`: LiveKit API secret
- `LIVEKIT_URL`: LiveKit server URL
- `LIVEKIT_ROOM_NAME`: Default room name

All environment variables are required for LiveKit to function properly.

## WebSocket and Transport Utilities

The transport utilities provide helper functions for WebSocket parsing, SDP manipulation, and transport management.

### Telephony WebSocket Parsing

Use `parse_telephony_websocket()` to auto-detect telephony providers and extract call data:

```python
from pipecat.runner.utils import parse_telephony_websocket

transport_type, call_data = await parse_telephony_websocket(websocket)

if transport_type == "twilio":
    stream_id = call_data["stream_id"]
    call_id = call_data["call_id"]
elif transport_type == "telnyx":
    stream_id = call_data["stream_id"]
    call_control_id = call_data["call_control_id"]
    outbound_encoding = call_data["outbound_encoding"]
elif transport_type == "plivo":
    stream_id = call_data["stream_id"]
    call_id = call_data["call_id"]
```

The function automatically:

- Reads and parses initial WebSocket messages
- Detects provider based on message structure
- Extracts provider-specific call information
- Returns structured data for transport configuration

### Transport Helper Functions

**Client ID Detection**:

```python
from pipecat.runner.utils import get_transport_client_id

client_id = get_transport_client_id(transport, client)
# Returns pc_id for WebRTC or participant ID for Daily
```

**Video Capture** (Daily only):

```python
from pipecat.runner.utils import maybe_capture_participant_camera, maybe_capture_participant_screen

# Capture participant's camera
await maybe_capture_participant_camera(transport, client, framerate=30)

# Capture participant's screen
await maybe_capture_participant_screen(transport, client, framerate=15)
```

These functions safely handle transport detection and only execute if the transport supports the operation.

## When to Use These Utilities

### Automatic Usage (Most Common)

The development runner and `create_transport` utility handle these automatically. Most users won't need to call these functions directly.

### Manual Usage (Advanced)

Use these utilities directly when:

**Custom deployment infrastructure**: Building your own bot runner or deployment system

**Advanced transport configuration**: Need specific room settings, token permissions, or custom authentication

**Non-runner scenarios**: Integrating Pipecat transports into existing applications

**Testing and debugging**: Need to create rooms/tokens independently for testing

### Integration Example

Here's how you might use these utilities in a custom deployment:

```python
import aiohttp
from pipecat.runner.daily import configure
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.transports.services.daily import DailyTransport, DailyParams

async def create_custom_bot_session(transport_type: str):
    if transport_type == "daily":
        async with aiohttp.ClientSession() as session:
            room_url, token = await configure(session)
            return DailyTransport(room_url, token, "Custom Bot", DailyParams())

    elif transport_type == "telephony":
        # Handle custom telephony setup
        transport_type, call_data = await parse_telephony_websocket(websocket)
        # Configure based on detected provider...
```

These utilities provide the building blocks for any transport configuration scenario while maintaining the same reliability and functionality as the development runner.



================================================
FILE: server/utilities/serializers/introduction.mdx
================================================
---
title: "Frame Serializers"
description: "Overview of frame serializers for converting between Pipecat frames and external protocols"
---

## Overview

Frame serializers are components that convert between Pipecat's internal frame format and external protocols or formats. They're essential when integrating with third-party services or APIs that have their own message formats.

## Core Responsibilities

Serializers handle:

1. **Serialization**: Converting Pipecat frames to external formats or protocols
2. **Deserialization**: Converting external messages to Pipecat frames
3. **Protocol-specific behaviors**: Managing unique aspects of each integration

## Available Serializers

Pipecat includes serializers for popular voice and communications platforms:

<CardGroup cols={3}>
  <Card
    title="Twilio Serializer"
    icon="phone"
    href="/server/services/serializers/twilio"
  >
    For integrating with Twilio Media Streams WebSocket protocol
  </Card>
  <Card
    title="Telnyx Serializer"
    icon="phone"
    href="/server/services/serializers/telnyx"
  >
    For integrating with Telnyx WebSocket media streaming
  </Card>
  <Card
    title="Plivo Serializer"
    icon="phone"
    href="/server/services/serializers/plivo"
  >
    For integrating with Telnyx WebSocket media streaming
  </Card>
</CardGroup>

## Custom Serializers

You can create custom serializers by implementing the `FrameSerializer` base class:

```python
from pipecat.serializers.base_serializer import FrameSerializer, FrameSerializerType
from pipecat.frames.frames import Frame, StartFrame

class MyCustomSerializer(FrameSerializer):
    @property
    def type(self) -> FrameSerializerType:
        return FrameSerializerType.TEXT  # or BINARY

    async def setup(self, frame: StartFrame):
        # Initialize with pipeline configuration
        pass

    async def serialize(self, frame: Frame) -> str | bytes | None:
        # Convert Pipecat frame to external format
        pass

    async def deserialize(self, data: str | bytes) -> Frame | None:
        # Convert external data to Pipecat frame
        pass
```



================================================
FILE: server/utilities/serializers/plivo.mdx
================================================
---
title: "Plivo Frame Serializer"
description: "Serializer for Plivo Audio Streaming WebSocket protocol"
---

## Overview

`PlivoFrameSerializer` enables integration with Plivo's Audio Streaming WebSocket protocol, allowing your Pipecat application to handle phone calls via Plivo's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Plivo
- DTMF (touch-tone) event handling
- Automatic call termination via Plivo's REST API
- μ-law audio encoding/decoding

## Installation

The `PlivoFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_id" type="str" required>
  The Plivo Stream ID
</ParamField>

<ParamField path="call_id" type="Optional[str]" default="None">
  The associated Plivo Call ID (required for auto hang-up)
</ParamField>

<ParamField path="auth_id" type="Optional[str]" default="None">
  Plivo auth ID (required for auto hang-up)
</ParamField>

<ParamField path="auth_token" type="Optional[str]" default="None">
  Plivo auth token (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="plivo_sample_rate" type="int" default="8000">
  Sample rate used by Plivo (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.plivo import PlivoFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Plivo WebSocket connection
stream_id = start_message["start"]["streamId"]
call_id = start_message["start"]["callId"]

# Create serializer
serializer = PlivoFrameSerializer(
    stream_id=stream_id,
    call_id=call_id,
    auth_id="your_plivo_auth_id",
    auth_token="your_plivo_auth_token"
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Plivo call when an `EndFrame` or `CancelFrame` is processed, using Plivo's REST API:

```python
# Properly configured with hang-up support
serializer = PlivoFrameSerializer(
    stream_id=stream_id,
    call_id=call_id,                             # Required for auto hang-up
    auth_id=os.getenv("PLIVO_AUTH_ID"),          # Required for auto hang-up
    auth_token=os.getenv("PLIVO_AUTH_TOKEN"),    # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Plivo WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.plivo import PlivoFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read the start message from Plivo
    start_data = websocket.iter_text()
    start_message = json.loads(await start_data.__anext__())

    # Extract Plivo-specific IDs from the start event
    start_info = start_message.get("start", {})
    stream_id = start_info.get("streamId")
    call_id = start_info.get("callId")

    # Create serializer with authentication for auto hang-up
    serializer = PlivoFrameSerializer(
        stream_id=stream_id,
        call_id=call_id,
        auth_id=os.getenv("PLIVO_AUTH_ID"),
        auth_token=os.getenv("PLIVO_AUTH_TOKEN"),
    )

    # Continue with transport and pipeline setup...
```

## Plivo XML Configuration

To enable audio streaming with Plivo, you'll need to configure your Plivo application to return appropriate XML:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Stream
    keepCallAlive="true"
    bidirectional="true"
    contentType="audio/x-mulaw;rate=8000"
  >
    wss://your-websocket-url/ws
  </Stream>
</Response>
```

<Note>
  The `bidirectional="true"` attribute is required for two-way audio
  communication, and `keepCallAlive="true"` prevents the call from being
  disconnected after XML execution.
</Note>

## Key Differences from Twilio

- **Stream Identifier**: Plivo uses `streamId` instead of `streamSid`
- **Call Identifier**: Plivo uses `callId` instead of `callSid`
- **XML Structure**: Plivo uses `<Stream>` element directly instead of `<Connect><Stream>`
- **Authentication**: Plivo uses Auth ID and Auth Token instead of Account SID and Auth Token

<Note>
  See the [Plivo Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/plivo-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/utilities/serializers/telnyx.mdx
================================================
---
title: "Telnyx Frame Serializer"
description: "Serializer for Telnyx WebSocket media streaming protocol"
---

## Overview

`TelnyxFrameSerializer` enables integration with Telnyx's WebSocket media streaming protocol, allowing your Pipecat application to handle phone calls via Telnyx's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Telnyx
- DTMF (touch-tone) event handling
- Automatic call termination via Telnyx's REST API
- Support for multiple audio encodings (PCMU, PCMA)

## Installation

The `TelnyxFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_id" type="str" required>
  The Stream ID for Telnyx
</ParamField>

<ParamField path="outbound_encoding" type="str" required>
  The encoding type for outbound audio (e.g., "PCMU", "PCMA")
</ParamField>

<ParamField path="inbound_encoding" type="str" required>
  The encoding type for inbound audio (e.g., "PCMU", "PCMA")
</ParamField>

<ParamField path="call_control_id" type="Optional[str]" default="None">
  The Call Control ID for the Telnyx call (required for auto hang-up)
</ParamField>

<ParamField path="api_key" type="Optional[str]" default="None">
  Your Telnyx API key (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="telnyx_sample_rate" type="int" default="8000">
  Sample rate used by Telnyx (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="inbound_encoding" type="str" default="PCMU">
  Audio encoding for data sent to Telnyx
</ParamField>

<ParamField path="outbound_encoding" type="str" default="PCMU">
  Audio encoding for data received from Telnyx
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.telnyx import TelnyxFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Telnyx WebSocket connection
stream_id = call_data["stream_id"]
call_control_id = call_data["start"]["call_control_id"]
outbound_encoding = call_data["start"]["media_format"]["encoding"]

# Create serializer
serializer = TelnyxFrameSerializer(
    stream_id=stream_id,
    outbound_encoding=outbound_encoding,
    inbound_encoding="PCMU",
    call_control_id=call_control_id,
    api_key=os.getenv("TELNYX_API_KEY")
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Telnyx call when an `EndFrame` or `CancelFrame` is processed, using Telnyx's REST API:

```python
# Properly configured with hang-up support
serializer = TelnyxFrameSerializer(
    stream_id=stream_id,
    outbound_encoding=outbound_encoding,
    inbound_encoding="PCMU",
    call_control_id=call_control_id,    # Required for auto hang-up
    api_key=os.getenv("TELNYX_API_KEY") # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Telnyx WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.telnyx import TelnyxFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial messages from Telnyx
    start_data = websocket.iter_text()
    await start_data.__anext__()  # Skip first message

    # Parse the second message to get call details
    call_data = json.loads(await start_data.__anext__())

    # Extract Telnyx-specific IDs and encoding
    stream_id = call_data["stream_id"]
    call_control_id = call_data["start"]["call_control_id"]
    outbound_encoding = call_data["start"]["media_format"]["encoding"]

    # Create serializer with API key for auto hang-up
    serializer = TelnyxFrameSerializer(
        stream_id=stream_id,
        outbound_encoding=outbound_encoding,
        inbound_encoding="PCMU",
        call_control_id=call_control_id,
        api_key=os.getenv("TELNYX_API_KEY"),
    )

    # Continue with transport and pipeline setup...
```

<Note>
  See the [Telnyx Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/telnyx-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/utilities/serializers/twilio.mdx
================================================
---
title: "Twilio Frame Serializer"
description: "Serializer for Twilio Media Streams WebSocket protocol"
---

## Overview

`TwilioFrameSerializer` enables integration with Twilio's Media Streams WebSocket protocol, allowing your Pipecat application to handle phone calls via Twilio's voice services.

## Features

- Bidirectional audio conversion between Pipecat and Twilio
- DTMF (touch-tone) event handling
- Automatic call termination via Twilio's REST API
- μ-law audio encoding/decoding

## Installation

The `TwilioFrameSerializer` does not require any additional dependencies beyond the core Pipecat library.

## Configuration

### Constructor Parameters

<ParamField path="stream_sid" type="str" required>
  The Twilio Media Stream SID
</ParamField>

<ParamField path="call_sid" type="Optional[str]" default="None">
  The associated Twilio Call SID (required for auto hang-up)
</ParamField>

<ParamField path="account_sid" type="Optional[str]" default="None">
  Twilio account SID (required for auto hang-up)
</ParamField>

<ParamField path="auth_token" type="Optional[str]" default="None">
  Twilio auth token (required for auto hang-up)
</ParamField>

<ParamField path="params" type="InputParams" default="InputParams()">
  Configuration parameters
</ParamField>

### InputParams Configuration

<ParamField path="twilio_sample_rate" type="int" default="8000">
  Sample rate used by Twilio (typically 8kHz)
</ParamField>

<ParamField path="sample_rate" type="int | None" default="None">
  Optional override for pipeline input sample rate
</ParamField>

<ParamField path="auto_hang_up" type="bool" default="True">
  Whether to automatically terminate call on EndFrame
</ParamField>

## Basic Usage

```python
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.transports.network.fastapi_websocket import (
    FastAPIWebsocketTransport,
    FastAPIWebsocketParams
)

# Extract required values from Twilio WebSocket connection
stream_sid = call_data["start"]["streamSid"]
call_sid = call_data["start"]["callSid"]

# Create serializer
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,
    account_sid="your_twilio_account_sid",
    auth_token="your_twilio_auth_token"
)

# Use with FastAPIWebsocketTransport
transport = FastAPIWebsocketTransport(
    websocket=websocket,
    params=FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        serializer=serializer,
    )
)
```

## Hang-up Functionality

When `auto_hang_up` is enabled, the serializer will automatically hang up the Twilio call when an `EndFrame` or `CancelFrame` is processed, using Twilio's REST API:

```python
# Properly configured with hang-up support
serializer = TwilioFrameSerializer(
    stream_sid=stream_sid,
    call_sid=call_sid,              # Required for auto hang-up
    account_sid=os.getenv("TWILIO_ACCOUNT_SID"),  # Required for auto hang-up
    auth_token=os.getenv("TWILIO_AUTH_TOKEN"),    # Required for auto hang-up
)
```

## Server Code Example

Here's a complete example of handling a Twilio WebSocket connection:

```python
from fastapi import FastAPI, WebSocket
from pipecat.serializers.twilio import TwilioFrameSerializer
import json
import os

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Read initial messages from Twilio
    start_data = websocket.iter_text()
    await start_data.__anext__()  # Skip first message

    # Parse the second message to get call details
    call_data = json.loads(await start_data.__anext__())

    # Extract Twilio-specific IDs
    stream_sid = call_data["start"]["streamSid"]
    call_sid = call_data["start"]["callSid"]

    # Create serializer with authentication for auto hang-up
    serializer = TwilioFrameSerializer(
        stream_sid=stream_sid,
        call_sid=call_sid,
        account_sid=os.getenv("TWILIO_ACCOUNT_SID"),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN"),
    )

    # Continue with transport and pipeline setup...
```

<Note>
  See the [Twilio Chatbot
  example](https://github.com/pipecat-ai/pipecat-examples/tree/main/twilio-chatbot)
  for a complete implementation.
</Note>



================================================
FILE: server/utilities/smart-turn/fal-smart-turn.mdx
================================================
---
title: "Fal Smart Turn"
description: "Cloud-hosted Smart Turn detection using Fal.ai"
---

## Overview

`FalSmartTurnAnalyzer` provides an easy way to use Smart Turn detection via Fal.ai's cloud infrastructure. This implementation requires minimal setup - just an API key - and offers scalable inference without having to manage your own servers.

## Installation

```bash
pip install "pipecat-ai[remote-smart-turn]"
```

## Requirements

- A Fal.ai account and API key (get one at [Fal.ai](https://fal.ai))
- Internet connectivity for making API calls

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="Optional[str]" default="None">
  Your Fal.ai API key for authentication (required unless using a custom
  deployment)
</ParamField>

<ParamField
  path="url"
  type="str"
  default="https://fal.run/fal-ai/smart-turn/raw"
>
  URL endpoint for the Smart Turn API (defaults to the official Fal deployment)
</ParamField>

<ParamField path="aiohttp_session" type="aiohttp.ClientSession" required>
  An aiohttp client session for making HTTP requests
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection. See
  [SmartTurnParams](/server/utilities/smart-turn/smart-turn-overview#configuration)
  for details.
</ParamField>

## Example

```python
import os
import aiohttp
from pipecat.audio.turn.smart_turn.fal_smart_turn import FalSmartTurnAnalyzer
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.transports.base_transport import TransportParams

async def setup_transport():
    async with aiohttp.ClientSession() as session:
        transport = SmallWebRTCTransport(
            webrtc_connection=webrtc_connection,
            params=TransportParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=FalSmartTurnAnalyzer(
                    api_key=os.getenv("FAL_SMART_TURN_API_KEY"),
                    aiohttp_session=session
                ),
            ),
        )

        # Continue with pipeline setup...
```

## Custom Deployment

You can also deploy the Smart Turn model yourself on Fal.ai and point to your custom deployment:

```python
turn_analyzer=FalSmartTurnAnalyzer(
    url="https://fal.run/your-username/your-deployment/raw",
    api_key=os.getenv("FAL_API_KEY"),
    aiohttp_session=session
)
```

## Performance Considerations

- **Latency**: While Fal provides global infrastructure, there will be network latency compared to local inference
- **Reliability**: Depends on network connectivity and Fal.ai service availability
- **Scalability**: Handles scaling automatically based on your usage

## Notes

- Fal handles the model hosting, scaling, and infrastructure management
- The session timeout is controlled by the `stop_secs` parameter
- For high-throughput applications, consider deploying your own inference service



================================================
FILE: server/utilities/smart-turn/local-coreml-smart-turn.mdx
================================================
---
title: "Local CoreML Smart Turn"
description: "Local Smart Turn detection on Apple Silicon using CoreML"
---

## Overview

`LocalCoreMLSmartTurnAnalyzer` runs Smart Turn inference directly on your Mac using Apple's CoreML framework. This provides low-latency inference without external API dependencies, making it ideal for development and applications where network access is limited or latency is critical.

## Installation

```bash
pip install "pipecat-ai[local-smart-turn]"
```

## Requirements

- Apple Silicon Mac (M1/M2/M3 series)
- macOS 11.0 or later

## Local Model Setup

To use the `LocalCoreMLSmartTurnAnalyzer`, you need to set up the CoreML model locally:

1. Install Git LFS (Large File Storage):

   <CodeGroup>

   ```bash macOS
   brew install git-lfs
   ```

   ```bash Ubuntu/Debian
   sudo apt-get install git-lfs
   ```

   </CodeGroup>

2. Initialize Git LFS

   ```bash
   git lfs install
   ```

3. Clone the Smart Turn model repository:

   ```bash
   git clone https://huggingface.co/pipecat-ai/smart-turn
   ```

4. Set the environment variable to the cloned repository path:

   ```bash
   # Add to your .env file or environment
   export LOCAL_SMART_TURN_MODEL_PATH=/path/to/smart-turn
   ```

## Configuration

### Constructor Parameters

<ParamField path="smart_turn_model_path" type="str" required>
  Path to the directory containing the Smart Turn model files
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection. See
  [SmartTurnParams](/server/utilities/smart-turn/smart-turn-overview#configuration)
  for details.
</ParamField>

## Example

```python
import os
from pipecat.audio.turn.smart_turn.local_coreml_smart_turn import LocalCoreMLSmartTurnAnalyzer
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.transports.base_transport import TransportParams

# Get the path to the Smart Turn model
smart_turn_model_path = os.getenv("LOCAL_SMART_TURN_MODEL_PATH")

# Create transport with local Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalCoreMLSmartTurnAnalyzer(
            smart_turn_model_path=smart_turn_model_path,
            params=SmartTurnParams(
                stop_secs=2.0,  # Shorter stop time when using Smart Turn
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Performance Considerations

- **Latency**: Very low latency since inference happens locally
- **Resource Usage**: Uses local CPU/GPU resources
- **Reliability**: No dependency on external services or network connectivity

## Notes

- Optimal for development environments and latency-sensitive applications
- The CoreML model is optimized for Apple Silicon but will work on Intel Macs with reduced performance
- First inference may be slower as the model is loaded and compiled



================================================
FILE: server/utilities/smart-turn/smart-turn-overview.mdx
================================================
---
title: "Smart Turn Overview"
description: "Advanced conversational turn detection powered by the smart-turn model"
---

## Overview

Smart Turn Detection is an advanced feature in Pipecat that determines when a user has finished speaking and the bot should respond. Unlike basic Voice Activity Detection (VAD) which only detects speech vs. non-speech, Smart Turn Detection uses a machine learning model to recognize natural conversational cues like intonation patterns and linguistic signals.

<CardGroup cols={3}>
  <Card
    title="Smart Turn Model"
    icon="github"
    href="https://github.com/pipecat-ai/smart-turn"
  >
    Open source model for advanced conversational turn detection. Contribute to
    model training and development.
  </Card>
  <Card
    title="Data Collector"
    icon="microphone"
    href="https://turn-training.pipecat.ai/"
  >
    Contribute conversational data to improve the smart-turn model
  </Card>
  <Card
    title="Data Classifier"
    icon="check-circle"
    href="https://smart-turn-dataset.pipecat.ai/"
  >
    Help classify turn completion patterns in conversations
  </Card>
</CardGroup>

Pipecat provides three implementations of Smart Turn Detection:

1. **FalSmartTurnAnalyzer** - Uses a Fal's hosted smart-turn model for inference
2. **LocalCoreMLSmartTurnAnalyzer** - Runs inference locally on Apple Silicon using CoreML (not currently recommended)
3. **LocalSmartTurnAnalyzerV2** - Runs inference locally using PyTorch and Hugging Face Transformers

All implementations share the same underlying API and parameters, making it easy to switch between them based on your deployment requirements.

## Installation

The Smart Turn Detection feature requires additional dependencies depending on which implementation you choose.

For Fal's hosted service inference:

```bash
pip install "pipecat-ai[remote-smart-turn]"
```

For local inference (CoreML or PyTorch based):

```bash
pip install "pipecat-ai[local-smart-turn]"
```

## Integration with Transport

Smart Turn Detection is integrated into your application by setting one of the available turn analyzers as the `turn_analyzer` parameter in your transport configuration:

```python
from pipecat.transports.base_transport import TransportParams

transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        # Other transport parameters...
        turn_analyzer=FalSmartTurnAnalyzer(url=remote_smart_turn_url),
    ),
)
```

<Tip>
  Smart Turn Detection requires VAD to be enabled and works best when the VAD analyzer is set to a short `stop_secs` value. We recommend 0.2 seconds.

```python
audio_in_enabled=True,
vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2))
```

</Tip>

## Configuration

All implementations use the same `SmartTurnParams` class to configure behavior:

<ParamField path="stop_secs" type="float" default="3.0">
  Duration of silence in seconds required before triggering a silence-based end
  of turn
</ParamField>

<ParamField path="pre_speech_ms" type="float" default="0.0">
  Amount of audio (in milliseconds) to include before speech is detected
</ParamField>

<ParamField path="max_duration_secs" type="float" default="8.0">
  Maximum allowed segment duration in seconds. For segments longer than this
  value, a rolling window is used.
</ParamField>

## Remote Smart Turn

The `FalSmartTurnAnalyzer` class uses a remote service for turn detection inference.

### Constructor Parameters

<ParamField path="url" type="str" required>
  The URL of the remote Smart Turn service
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection
</ParamField>

### Example

```python
import os
from pipecat.audio.turn.smart_turn.fal_smart_turn import FalSmartTurnAnalyzer
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.transports.base_transport import TransportParams

# Get the URL for the remote Smart Turn service
remote_smart_turn_url = os.getenv("REMOTE_SMART_TURN_URL")

# Create the transport with Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=FalSmartTurnAnalyzer(
            url=remote_smart_turn_url,
            params=SmartTurnParams(
                stop_secs=3.0,
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Local Smart Turn (CoreML)

The `LocalCoreMLSmartTurnAnalyzer` runs inference locally using CoreML, providing lower latency and no network dependencies.

We currently recommend using the PyTorch implementation with the MPS backend on Apple Silicon, rather than CoreML, due to improved performance.

### Constructor Parameters

<ParamField path="smart_turn_model_path" type="str" required>
  Path to the directory containing the Smart Turn model
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection
</ParamField>

### Example

```python
import os
from pipecat.audio.turn.smart_turn.local_coreml_smart_turn import LocalCoreMLSmartTurnAnalyzer
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.transports.base_transport import TransportParams

# Path to the Smart Turn model directory
smart_turn_model_path = os.getenv("LOCAL_SMART_TURN_MODEL_PATH")

# Create the transport with local Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalCoreMLSmartTurnAnalyzer(
            smart_turn_model_path=smart_turn_model_path,
            params=SmartTurnParams(
                stop_secs=2.0,  # Shorter stop time when using Smart Turn
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Local Smart Turn (PyTorch)

The `LocalSmartTurnAnalyzerV2` runs inference locally using PyTorch and Hugging Face Transformers, providing a cross-platform solution.

### Constructor Parameters

<ParamField
  path="smart_turn_model_path"
  type="str"
  default="pipecat-ai/smart-turn-v2"
>
  Path to the Smart Turn model or Hugging Face model identifier. Defaults to the
  official "pipecat-ai/smart-turn-v2" model.
</ParamField>

<ParamField path="sample_rate" type="Optional[int]" default="None">
  Audio sample rate (will be set by the transport if not provided)
</ParamField>

<ParamField path="params" type="SmartTurnParams" default="SmartTurnParams()">
  Configuration parameters for turn detection
</ParamField>

### Example

```python
import os
from pipecat.audio.turn.smart_turn.local_smart_turn import LocalSmartTurnAnalyzerV2
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.transports.base_transport import TransportParams

# Optional: Path to the local Smart Turn model
# If not provided, it will download from Hugging Face
smart_turn_model_path = os.getenv("LOCAL_SMART_TURN_MODEL_PATH")

# Create the transport with PyTorch-based Smart Turn detection
transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV2(
            smart_turn_model_path=smart_turn_model_path,
            params=SmartTurnParams(
                stop_secs=2.0,
                pre_speech_ms=0.0,
                max_duration_secs=8.0
            )
        ),
    ),
)
```

## Local Model Setup

To use the `LocalCoreMLSmartTurnAnalyzer` or `LocalSmartTurnAnalyzerV2`, you need to set up the model locally:

1. Install Git LFS (Large File Storage):

   <CodeGroup>

   ```bash macOS
   brew install git-lfs
   ```

   ```bash Ubuntu/Debian
   sudo apt-get install git-lfs
   ```

   </CodeGroup>

2. Initialize Git LFS

   ```bash
   git lfs install
   ```

3. Clone the Smart Turn model repository:

   ```bash
   git clone https://huggingface.co/pipecat-ai/smart-turn-v2
   ```

4. Set the environment variable to the cloned repository path:

   ```bash
   # Add to your .env file or environment
   export LOCAL_SMART_TURN_MODEL_PATH=/path/to/smart-turn-v2
   ```

## How It Works

Smart Turn Detection continuously analyzes audio streams to identify natural turn completion points:

1. **Audio Buffering**: The system continuously buffers audio with timestamps, maintaining a small buffer of pre-speech audio.

2. **VAD Processing**: Voice Activity Detection segments the audio into speech and non-speech portions.

3. **Turn Analysis**: When VAD detects a pause in speech:
   - The ML model analyzes the speech segment for natural completion cues
   - It identifies acoustic and linguistic patterns that indicate turn completion
   - A decision is made whether the turn is complete or incomplete

The system includes a fallback mechanism: if a turn is classified as incomplete but silence continues for longer than `stop_secs`, the turn is automatically marked as complete.

## Notes

- The model supports 14 languages, see the [source repository](https://github.com/pipecat-ai/smart-turn) for more details
- You can adjust the `stop_secs` parameter based on your application's needs for responsiveness
- Smart Turn generally provides a more natural conversational experience but is computationally more intensive than simple VAD
- The PyTorch-based `LocalSmartTurnAnalyzerV2` will use CUDA or MPS if available, or will otherwise run on CPU



================================================
FILE: server/utilities/text/markdown-text-filter.mdx
================================================
---
title: "MarkdownTextFilter"
description: "Converts Markdown-formatted text to TTS-friendly plain text while preserving structure"
---

## Overview

`MarkdownTextFilter` transforms Markdown-formatted text into plain text that's suitable for text-to-speech (TTS) systems. It intelligently removes formatting elements while preserving the content structure, including proper spacing and list formatting.

This filter is especially valuable for LLM-generated content, which often includes Markdown formatting that would sound unnatural if read aloud by a TTS system.

## Constructor

```python
filter = MarkdownTextFilter(params=InputParams())
```

<ParamField path="params" type="InputParams">
  Configuration parameters for the filter
</ParamField>

### Input Parameters

Configure the filter behavior with these options:

<ParamField path="enable_text_filter" type="bool" default="True">
  Whether the filter is active (when False, text passes through unchanged)
</ParamField>

<ParamField path="filter_code" type="bool" default="False">
  Whether to remove code blocks from the output
</ParamField>

<ParamField path="filter_tables" type="bool" default="False">
  Whether to remove Markdown tables from the output
</ParamField>

## Features

The filter handles these Markdown elements:

- **Basic Formatting**: Removes `*italic*`, `**bold**`, and other formatting markers
- **Code**: Removes inline code ticks and optionally removes code blocks
- **Lists**: Preserves numbered lists while removing Markdown formatting
- **Tables**: Optionally removes Markdown tables
- **Whitespace**: Carefully preserves meaningful whitespace for natural speech
- **HTML**: Removes HTML tags and converts entities to their plain text equivalents

## Usage Examples

### Basic Usage with TTS Service

```python
from pipecat.utils.text.markdown_text_filter import MarkdownTextFilter
from pipecat.services.cartesia.tts import CartesiaTTSService

# Create the filter
md_filter = MarkdownTextFilter()

# Use with TTS service
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="voice_id_here",
    text_filter=md_filter
)
```

### Custom Configuration

```python
# Create filter that removes code blocks and tables
md_filter = MarkdownTextFilter(
    params=MarkdownTextFilter.InputParams(
        filter_code=True,
        filter_tables=True
    )
)
```

## What Gets Removed

| Markdown Feature           | Example                  | Result       |
| -------------------------- | ------------------------ | ------------ |
| Bold                       | `**important**`          | `important`  |
| Italic                     | `*emphasized*`           | `emphasized` |
| Headers                    | `## Section`             | `Section`    |
| Code (inline)              | `` `code` ``             | `code`       |
| Code blocks (when enabled) | ` ```python\ncode\n``` ` | ` `          |
| Tables (when enabled)      | `\|A\|B\|\n\|--\|--\|`   | ` `          |
| HTML tags                  | `<em>text</em>`          | `text`       |
| Repeated characters        | `!!!!!!!`                | `!`          |

## Notes

- Preserves sentence structure and readability
- Maintains whitespace that affects speech prosody
- Handles streaming text with partial Markdown elements
- Efficiently converts HTML entities to plain text characters
- Smart handling of code blocks and tables with state tracking
- Integrates directly with TTS services in the Pipecat pipeline



================================================
FILE: server/utilities/text/pattern-pair-aggregator.mdx
================================================
---
title: "PatternPairAggregator"
description: "Text aggregator that identifies and processes content between pattern pairs in streaming text"
---

## Overview

`PatternPairAggregator` is a specialized text aggregator that buffers streaming text until it can identify complete pattern pairs (like XML tags, markdown formatting, or custom delimiters). It processes the content between these patterns using registered handlers and returns text at sentence boundaries (therefore allowing normal TTS processing to occur).

This aggregator is particularly useful for applications like voice switching, structured content processing, and extracting metadata from LLM outputs, ensuring that patterns spanning multiple text chunks are correctly identified.

<Tip>
  Want to see it in action? Check out the [voice switching
  demo](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/35-pattern-pair-voice-switching.py).
</Tip>

## Constructor

```python
aggregator = PatternPairAggregator()
```

No parameters are required for initialization. The aggregator starts with an empty buffer and no registered patterns.

## Methods

### add_pattern_pair

```python
aggregator.add_pattern_pair(pattern_id, start_pattern, end_pattern, remove_match=True)
```

Registers a new pattern pair to detect in the text.

<ParamField path="pattern_id" type="str" required>
  Unique identifier for this pattern pair
</ParamField>

<ParamField path="start_pattern" type="str" required>
  Pattern that marks the beginning of content
</ParamField>

<ParamField path="end_pattern" type="str" required>
  Pattern that marks the end of content
</ParamField>

<ParamField path="remove_match" type="bool" default="True">
  Whether to remove the matched patterns from the output text
</ParamField>

<ResponseField name="Returns">Self for method chaining</ResponseField>

### on_pattern_match

```python
aggregator.on_pattern_match(pattern_id, handler)
```

Registers a handler function to be called when a specific pattern pair is matched.

<ParamField path="pattern_id" type="str" required>
  ID of the pattern pair to match
</ParamField>

<ParamField path="handler" type="Callable[[PatternMatch], None]" required>
  Function to call when the pattern is matched. The function should accept a
  PatternMatch object.
</ParamField>

<ResponseField name="Returns">Self for method chaining</ResponseField>

## Pattern Match Object

When a pattern is matched, the handler function receives a `PatternMatch` object with these attributes:

<ResponseField name="pattern_id" type="str">
  The identifier of the matched pattern pair
</ResponseField>

<ResponseField name="full_match" type="str">
  The complete text including start and end patterns
</ResponseField>

<ResponseField name="content" type="str">
  The text content between the start and end patterns
</ResponseField>

## Usage Examples

### Voice Switching in TTS

```python
# Define voice IDs
VOICE_IDS = {
    "narrator": "c45bc5ec-dc68-4feb-8829-6e6b2748095d",
    "female": "71a7ad14-091c-4e8e-a314-022ece01c121",
    "male": "7cf0e2b1-8daf-4fe4-89ad-f6039398f359",
}

# Create pattern aggregator
pattern_aggregator = PatternPairAggregator()

# Add pattern for voice tags
pattern_aggregator.add_pattern_pair(
    pattern_id="voice_tag",
    start_pattern="<voice>",
    end_pattern="</voice>",
    remove_match=True
)

# Register handler for voice switching
def on_voice_tag(match: PatternMatch):
    voice_name = match.content.strip().lower()
    if voice_name in VOICE_IDS:
        voice_id = VOICE_IDS[voice_name]
        tts.set_voice(voice_id)
        logger.info(f"Switched to {voice_name} voice")

pattern_aggregator.on_pattern_match("voice_tag", on_voice_tag)

# Set the aggregator on a TTS service
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id=VOICE_IDS["narrator"],
    text_aggregator=pattern_aggregator
)
```

### Extracting Structured Data from LLM Outputs

````python
# Create pattern aggregator
data_extractor = PatternPairAggregator()

# Add pattern for JSON data
data_extractor.add_pattern_pair(
    pattern_id="json_data",
    start_pattern="```json",
    end_pattern="```",
    remove_match=True
)

# Track extracted data
extracted_data = {}

# Register handler for JSON data
def on_json_data(match: PatternMatch):
    try:
        data = json.loads(match.content)
        extracted_data.update(data)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON: {e}")

data_extractor.on_pattern_match("json_data", on_json_data)
````

### Concept Explanation with Multiple Patterns

```python
# Create pattern aggregator
concept_aggregator = PatternPairAggregator()

# Add patterns for different parts of an explanation
concept_aggregator.add_pattern_pair(
    pattern_id="definition",
    start_pattern="<definition>",
    end_pattern="</definition>",
    remove_match=False  # Keep the tags in the output
)

concept_aggregator.add_pattern_pair(
    pattern_id="example",
    start_pattern="<example>",
    end_pattern="</example>",
    remove_match=False
)

# Register handlers
def on_definition(match: PatternMatch):
    logger.info(f"Found definition: {match.content}")
    # Could format differently, store for later, etc.

def on_example(match: PatternMatch):
    logger.info(f"Found example: {match.content}")
    # Could create a visual representation, etc.

concept_aggregator.on_pattern_match("definition", on_definition)
concept_aggregator.on_pattern_match("example", on_example)
```

## How It Works

```mermaid
graph TD
    A[New Text] --> B[Add to Buffer]
    B --> C{Complete Pattern?}
    C -->|Yes| D[Call Handler]
    D --> E[Remove Pattern if Configured]
    C -->|No| F{Incomplete Patterns?}
    E --> F
    F -->|Yes| G[Continue Buffering]
    F -->|No| H{Sentence Boundary?}
    H -->|Yes| I[Return Text Up to Boundary]
    H -->|No| G
```

## Notes

- Patterns are processed in the order they appear in the text
- Handlers are called when complete patterns are found
- Patterns can span multiple sentences of text, but be aware that encoding many "reasoning" tokens may slow down the LLM response



================================================
FILE: snippets/snippet-intro.mdx
================================================
One of the core principles of software development is DRY (Don't Repeat
Yourself). This is a principle that apply to documentation as
well. If you find yourself repeating the same content in multiple places, you
should consider creating a custom snippet to keep your content in sync.


